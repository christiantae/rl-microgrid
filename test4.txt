Training for 5000 steps ...
    1/5000: episode: 1, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -1424.614, mean reward: -1424.614 [-1424.614, -1424.614], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
    2/5000: episode: 2, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -119.796, mean reward: -119.796 [-119.796, -119.796], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
    3/5000: episode: 3, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5466.626, mean reward: -5466.626 [-5466.626, -5466.626], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
    4/5000: episode: 4, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -4097.204, mean reward: -4097.204 [-4097.204, -4097.204], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
    5/5000: episode: 5, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -569.686, mean reward: -569.686 [-569.686, -569.686], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
    6/5000: episode: 6, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -144.772, mean reward: -144.772 [-144.772, -144.772], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
    7/5000: episode: 7, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -222.575, mean reward: -222.575 [-222.575, -222.575], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
    8/5000: episode: 8, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3137.711, mean reward: -3137.711 [-3137.711, -3137.711], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
    9/5000: episode: 9, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7455.191, mean reward: -7455.191 [-7455.191, -7455.191], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   10/5000: episode: 10, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2963.078, mean reward: -2963.078 [-2963.078, -2963.078], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   11/5000: episode: 11, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8439.992, mean reward: -8439.992 [-8439.992, -8439.992], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   12/5000: episode: 12, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -2218.021, mean reward: -2218.021 [-2218.021, -2218.021], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   13/5000: episode: 13, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4891.287, mean reward: -4891.287 [-4891.287, -4891.287], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   14/5000: episode: 14, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5530.618, mean reward: -5530.618 [-5530.618, -5530.618], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   15/5000: episode: 15, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -1955.346, mean reward: -1955.346 [-1955.346, -1955.346], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   16/5000: episode: 16, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -3817.719, mean reward: -3817.719 [-3817.719, -3817.719], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   17/5000: episode: 17, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1567.269, mean reward: -1567.269 [-1567.269, -1567.269], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   18/5000: episode: 18, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -356.204, mean reward: -356.204 [-356.204, -356.204], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   19/5000: episode: 19, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3848.489, mean reward: -3848.489 [-3848.489, -3848.489], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   20/5000: episode: 20, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2180.502, mean reward: -2180.502 [-2180.502, -2180.502], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   21/5000: episode: 21, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6697.513, mean reward: -6697.513 [-6697.513, -6697.513], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   22/5000: episode: 22, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -11.576, mean reward: -11.576 [-11.576, -11.576], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   23/5000: episode: 23, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3638.561, mean reward: -3638.561 [-3638.561, -3638.561], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   24/5000: episode: 24, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3386.967, mean reward: -3386.967 [-3386.967, -3386.967], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   25/5000: episode: 25, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4965.615, mean reward: -4965.615 [-4965.615, -4965.615], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   26/5000: episode: 26, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4503.202, mean reward: -4503.202 [-4503.202, -4503.202], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   27/5000: episode: 27, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6838.905, mean reward: -6838.905 [-6838.905, -6838.905], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   28/5000: episode: 28, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4784.703, mean reward: -4784.703 [-4784.703, -4784.703], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   29/5000: episode: 29, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -8228.899, mean reward: -8228.899 [-8228.899, -8228.899], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
   30/5000: episode: 30, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -346.972, mean reward: -346.972 [-346.972, -346.972], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   31/5000: episode: 31, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -209.222, mean reward: -209.222 [-209.222, -209.222], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   32/5000: episode: 32, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -263.200, mean reward: -263.200 [-263.200, -263.200], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   33/5000: episode: 33, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -3567.514, mean reward: -3567.514 [-3567.514, -3567.514], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   34/5000: episode: 34, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1471.946, mean reward: -1471.946 [-1471.946, -1471.946], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   35/5000: episode: 35, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4487.028, mean reward: -4487.028 [-4487.028, -4487.028], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   36/5000: episode: 36, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -785.442, mean reward: -785.442 [-785.442, -785.442], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   37/5000: episode: 37, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -937.147, mean reward: -937.147 [-937.147, -937.147], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   38/5000: episode: 38, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -519.849, mean reward: -519.849 [-519.849, -519.849], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   39/5000: episode: 39, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1137.812, mean reward: -1137.812 [-1137.812, -1137.812], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   40/5000: episode: 40, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1875.006, mean reward: -1875.006 [-1875.006, -1875.006], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   41/5000: episode: 41, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5536.328, mean reward: -5536.328 [-5536.328, -5536.328], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
   42/5000: episode: 42, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -12.599, mean reward: -12.599 [-12.599, -12.599], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   43/5000: episode: 43, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -4942.053, mean reward: -4942.053 [-4942.053, -4942.053], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   44/5000: episode: 44, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -8557.149, mean reward: -8557.149 [-8557.149, -8557.149], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   45/5000: episode: 45, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -4210.364, mean reward: -4210.364 [-4210.364, -4210.364], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
   46/5000: episode: 46, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -1924.259, mean reward: -1924.259 [-1924.259, -1924.259], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   47/5000: episode: 47, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -140.693, mean reward: -140.693 [-140.693, -140.693], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   48/5000: episode: 48, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -136.250, mean reward: -136.250 [-136.250, -136.250], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   49/5000: episode: 49, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4099.020, mean reward: -4099.020 [-4099.020, -4099.020], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   50/5000: episode: 50, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4961.859, mean reward: -4961.859 [-4961.859, -4961.859], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   51/5000: episode: 51, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7422.530, mean reward: -7422.530 [-7422.530, -7422.530], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   52/5000: episode: 52, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1093.802, mean reward: -1093.802 [-1093.802, -1093.802], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   53/5000: episode: 53, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -1927.534, mean reward: -1927.534 [-1927.534, -1927.534], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   54/5000: episode: 54, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5415.038, mean reward: -5415.038 [-5415.038, -5415.038], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   55/5000: episode: 55, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -880.141, mean reward: -880.141 [-880.141, -880.141], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   56/5000: episode: 56, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3854.044, mean reward: -3854.044 [-3854.044, -3854.044], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   57/5000: episode: 57, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6805.211, mean reward: -6805.211 [-6805.211, -6805.211], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   58/5000: episode: 58, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -342.937, mean reward: -342.937 [-342.937, -342.937], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   59/5000: episode: 59, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4919.300, mean reward: -4919.300 [-4919.300, -4919.300], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   60/5000: episode: 60, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -419.978, mean reward: -419.978 [-419.978, -419.978], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   61/5000: episode: 61, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -268.081, mean reward: -268.081 [-268.081, -268.081], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   62/5000: episode: 62, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2208.494, mean reward: -2208.494 [-2208.494, -2208.494], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   63/5000: episode: 63, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1663.628, mean reward: -1663.628 [-1663.628, -1663.628], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   64/5000: episode: 64, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2047.720, mean reward: -2047.720 [-2047.720, -2047.720], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   65/5000: episode: 65, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4336.748, mean reward: -4336.748 [-4336.748, -4336.748], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   66/5000: episode: 66, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -8751.857, mean reward: -8751.857 [-8751.857, -8751.857], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
   67/5000: episode: 67, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1632.898, mean reward: -1632.898 [-1632.898, -1632.898], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   68/5000: episode: 68, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1424.337, mean reward: -1424.337 [-1424.337, -1424.337], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   69/5000: episode: 69, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -632.272, mean reward: -632.272 [-632.272, -632.272], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   70/5000: episode: 70, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5924.126, mean reward: -5924.126 [-5924.126, -5924.126], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   71/5000: episode: 71, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4733.997, mean reward: -4733.997 [-4733.997, -4733.997], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   72/5000: episode: 72, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1521.381, mean reward: -1521.381 [-1521.381, -1521.381], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   73/5000: episode: 73, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -6324.159, mean reward: -6324.159 [-6324.159, -6324.159], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   74/5000: episode: 74, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -7761.250, mean reward: -7761.250 [-7761.250, -7761.250], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   75/5000: episode: 75, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1396.389, mean reward: -1396.389 [-1396.389, -1396.389], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   76/5000: episode: 76, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3589.402, mean reward: -3589.402 [-3589.402, -3589.402], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   77/5000: episode: 77, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -7642.679, mean reward: -7642.679 [-7642.679, -7642.679], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   78/5000: episode: 78, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -564.723, mean reward: -564.723 [-564.723, -564.723], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   79/5000: episode: 79, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -674.019, mean reward: -674.019 [-674.019, -674.019], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   80/5000: episode: 80, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4202.922, mean reward: -4202.922 [-4202.922, -4202.922], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   81/5000: episode: 81, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -661.773, mean reward: -661.773 [-661.773, -661.773], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   82/5000: episode: 82, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4414.130, mean reward: -4414.130 [-4414.130, -4414.130], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   83/5000: episode: 83, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8314.827, mean reward: -8314.827 [-8314.827, -8314.827], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   84/5000: episode: 84, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1010.849, mean reward: -1010.849 [-1010.849, -1010.849], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   85/5000: episode: 85, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -13742.585, mean reward: -13742.585 [-13742.585, -13742.585], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   86/5000: episode: 86, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -8236.704, mean reward: -8236.704 [-8236.704, -8236.704], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   87/5000: episode: 87, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2142.238, mean reward: -2142.238 [-2142.238, -2142.238], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   88/5000: episode: 88, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -4729.370, mean reward: -4729.370 [-4729.370, -4729.370], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   89/5000: episode: 89, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1984.930, mean reward: -1984.930 [-1984.930, -1984.930], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   90/5000: episode: 90, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -4493.743, mean reward: -4493.743 [-4493.743, -4493.743], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   91/5000: episode: 91, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5207.263, mean reward: -5207.263 [-5207.263, -5207.263], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   92/5000: episode: 92, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2894.912, mean reward: -2894.912 [-2894.912, -2894.912], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   93/5000: episode: 93, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6365.702, mean reward: -6365.702 [-6365.702, -6365.702], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   94/5000: episode: 94, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2754.233, mean reward: -2754.233 [-2754.233, -2754.233], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   95/5000: episode: 95, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -710.366, mean reward: -710.366 [-710.366, -710.366], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   96/5000: episode: 96, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -8583.028, mean reward: -8583.028 [-8583.028, -8583.028], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   97/5000: episode: 97, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4228.447, mean reward: -4228.447 [-4228.447, -4228.447], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   98/5000: episode: 98, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6541.563, mean reward: -6541.563 [-6541.563, -6541.563], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   99/5000: episode: 99, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -4314.090, mean reward: -4314.090 [-4314.090, -4314.090], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  100/5000: episode: 100, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -8696.928, mean reward: -8696.928 [-8696.928, -8696.928], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  101/5000: episode: 101, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2986.091, mean reward: -2986.091 [-2986.091, -2986.091], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  102/5000: episode: 102, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3058.299, mean reward: -3058.299 [-3058.299, -3058.299], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  103/5000: episode: 103, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1942.311, mean reward: -1942.311 [-1942.311, -1942.311], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  104/5000: episode: 104, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -30.778, mean reward: -30.778 [-30.778, -30.778], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  105/5000: episode: 105, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -278.643, mean reward: -278.643 [-278.643, -278.643], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  106/5000: episode: 106, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -153.890, mean reward: -153.890 [-153.890, -153.890], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  107/5000: episode: 107, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4374.341, mean reward: -4374.341 [-4374.341, -4374.341], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  108/5000: episode: 108, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -8146.956, mean reward: -8146.956 [-8146.956, -8146.956], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  109/5000: episode: 109, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -10195.583, mean reward: -10195.583 [-10195.583, -10195.583], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  110/5000: episode: 110, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -6504.387, mean reward: -6504.387 [-6504.387, -6504.387], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  111/5000: episode: 111, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -449.263, mean reward: -449.263 [-449.263, -449.263], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  112/5000: episode: 112, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2711.371, mean reward: -2711.371 [-2711.371, -2711.371], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  113/5000: episode: 113, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -680.947, mean reward: -680.947 [-680.947, -680.947], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  114/5000: episode: 114, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4560.744, mean reward: -4560.744 [-4560.744, -4560.744], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  115/5000: episode: 115, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1669.452, mean reward: -1669.452 [-1669.452, -1669.452], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  116/5000: episode: 116, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -3393.585, mean reward: -3393.585 [-3393.585, -3393.585], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  117/5000: episode: 117, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5030.088, mean reward: -5030.088 [-5030.088, -5030.088], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  118/5000: episode: 118, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9516.847, mean reward: -9516.847 [-9516.847, -9516.847], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  119/5000: episode: 119, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -13046.517, mean reward: -13046.517 [-13046.517, -13046.517], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  120/5000: episode: 120, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -3995.228, mean reward: -3995.228 [-3995.228, -3995.228], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  121/5000: episode: 121, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -365.878, mean reward: -365.878 [-365.878, -365.878], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  122/5000: episode: 122, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -3898.544, mean reward: -3898.544 [-3898.544, -3898.544], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  123/5000: episode: 123, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2728.085, mean reward: -2728.085 [-2728.085, -2728.085], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  124/5000: episode: 124, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -158.486, mean reward: -158.486 [-158.486, -158.486], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  125/5000: episode: 125, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9313.892, mean reward: -9313.892 [-9313.892, -9313.892], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  126/5000: episode: 126, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6466.866, mean reward: -6466.866 [-6466.866, -6466.866], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  127/5000: episode: 127, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6323.578, mean reward: -6323.578 [-6323.578, -6323.578], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  128/5000: episode: 128, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -8877.714, mean reward: -8877.714 [-8877.714, -8877.714], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  129/5000: episode: 129, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -3084.892, mean reward: -3084.892 [-3084.892, -3084.892], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  130/5000: episode: 130, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6281.771, mean reward: -6281.771 [-6281.771, -6281.771], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  131/5000: episode: 131, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -782.856, mean reward: -782.856 [-782.856, -782.856], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  132/5000: episode: 132, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -4232.175, mean reward: -4232.175 [-4232.175, -4232.175], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  133/5000: episode: 133, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -4795.011, mean reward: -4795.011 [-4795.011, -4795.011], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  134/5000: episode: 134, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -2988.531, mean reward: -2988.531 [-2988.531, -2988.531], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  135/5000: episode: 135, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1105.831, mean reward: -1105.831 [-1105.831, -1105.831], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  136/5000: episode: 136, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2473.870, mean reward: -2473.870 [-2473.870, -2473.870], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  137/5000: episode: 137, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5754.562, mean reward: -5754.562 [-5754.562, -5754.562], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  138/5000: episode: 138, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -730.543, mean reward: -730.543 [-730.543, -730.543], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  139/5000: episode: 139, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5360.901, mean reward: -5360.901 [-5360.901, -5360.901], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  140/5000: episode: 140, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2724.769, mean reward: -2724.769 [-2724.769, -2724.769], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  141/5000: episode: 141, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5118.101, mean reward: -5118.101 [-5118.101, -5118.101], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  142/5000: episode: 142, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -137.571, mean reward: -137.571 [-137.571, -137.571], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  143/5000: episode: 143, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3197.715, mean reward: -3197.715 [-3197.715, -3197.715], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  144/5000: episode: 144, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -927.408, mean reward: -927.408 [-927.408, -927.408], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  145/5000: episode: 145, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1530.682, mean reward: -1530.682 [-1530.682, -1530.682], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  146/5000: episode: 146, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3557.800, mean reward: -3557.800 [-3557.800, -3557.800], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  147/5000: episode: 147, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1565.798, mean reward: -1565.798 [-1565.798, -1565.798], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  148/5000: episode: 148, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1187.206, mean reward: -1187.206 [-1187.206, -1187.206], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  149/5000: episode: 149, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7721.034, mean reward: -7721.034 [-7721.034, -7721.034], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  150/5000: episode: 150, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2231.597, mean reward: -2231.597 [-2231.597, -2231.597], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  151/5000: episode: 151, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -1254.679, mean reward: -1254.679 [-1254.679, -1254.679], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  152/5000: episode: 152, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -8702.028, mean reward: -8702.028 [-8702.028, -8702.028], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  153/5000: episode: 153, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1377.401, mean reward: -1377.401 [-1377.401, -1377.401], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  154/5000: episode: 154, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1004.346, mean reward: -1004.346 [-1004.346, -1004.346], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  155/5000: episode: 155, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2347.925, mean reward: -2347.925 [-2347.925, -2347.925], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  156/5000: episode: 156, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -8456.743, mean reward: -8456.743 [-8456.743, -8456.743], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  157/5000: episode: 157, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3491.820, mean reward: -3491.820 [-3491.820, -3491.820], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  158/5000: episode: 158, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2615.264, mean reward: -2615.264 [-2615.264, -2615.264], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  159/5000: episode: 159, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2246.525, mean reward: -2246.525 [-2246.525, -2246.525], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  160/5000: episode: 160, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -10911.077, mean reward: -10911.077 [-10911.077, -10911.077], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  161/5000: episode: 161, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6955.750, mean reward: -6955.750 [-6955.750, -6955.750], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  162/5000: episode: 162, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -3442.884, mean reward: -3442.884 [-3442.884, -3442.884], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  163/5000: episode: 163, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -8263.626, mean reward: -8263.626 [-8263.626, -8263.626], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  164/5000: episode: 164, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -183.483, mean reward: -183.483 [-183.483, -183.483], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  165/5000: episode: 165, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7024.830, mean reward: -7024.830 [-7024.830, -7024.830], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  166/5000: episode: 166, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -1817.680, mean reward: -1817.680 [-1817.680, -1817.680], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  167/5000: episode: 167, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -75.307, mean reward: -75.307 [-75.307, -75.307], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  168/5000: episode: 168, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -20.777, mean reward: -20.777 [-20.777, -20.777], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  169/5000: episode: 169, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -9042.018, mean reward: -9042.018 [-9042.018, -9042.018], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  170/5000: episode: 170, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4687.127, mean reward: -4687.127 [-4687.127, -4687.127], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  171/5000: episode: 171, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3330.113, mean reward: -3330.113 [-3330.113, -3330.113], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  172/5000: episode: 172, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -7582.216, mean reward: -7582.216 [-7582.216, -7582.216], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  173/5000: episode: 173, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4457.593, mean reward: -4457.593 [-4457.593, -4457.593], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  174/5000: episode: 174, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8048.461, mean reward: -8048.461 [-8048.461, -8048.461], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  175/5000: episode: 175, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1298.008, mean reward: -1298.008 [-1298.008, -1298.008], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  176/5000: episode: 176, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -24.364, mean reward: -24.364 [-24.364, -24.364], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  177/5000: episode: 177, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5674.265, mean reward: -5674.265 [-5674.265, -5674.265], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  178/5000: episode: 178, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1108.920, mean reward: -1108.920 [-1108.920, -1108.920], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  179/5000: episode: 179, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5837.571, mean reward: -5837.571 [-5837.571, -5837.571], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  180/5000: episode: 180, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -432.854, mean reward: -432.854 [-432.854, -432.854], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  181/5000: episode: 181, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -7832.553, mean reward: -7832.553 [-7832.553, -7832.553], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  182/5000: episode: 182, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3981.127, mean reward: -3981.127 [-3981.127, -3981.127], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  183/5000: episode: 183, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5060.454, mean reward: -5060.454 [-5060.454, -5060.454], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  184/5000: episode: 184, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8062.575, mean reward: -8062.575 [-8062.575, -8062.575], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  185/5000: episode: 185, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5501.112, mean reward: -5501.112 [-5501.112, -5501.112], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  186/5000: episode: 186, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3158.886, mean reward: -3158.886 [-3158.886, -3158.886], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  187/5000: episode: 187, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6452.929, mean reward: -6452.929 [-6452.929, -6452.929], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  188/5000: episode: 188, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4230.301, mean reward: -4230.301 [-4230.301, -4230.301], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  189/5000: episode: 189, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -494.567, mean reward: -494.567 [-494.567, -494.567], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  190/5000: episode: 190, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -1270.670, mean reward: -1270.670 [-1270.670, -1270.670], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  191/5000: episode: 191, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4401.791, mean reward: -4401.791 [-4401.791, -4401.791], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  192/5000: episode: 192, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3095.335, mean reward: -3095.335 [-3095.335, -3095.335], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  193/5000: episode: 193, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8012.644, mean reward: -8012.644 [-8012.644, -8012.644], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  194/5000: episode: 194, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1250.561, mean reward: -1250.561 [-1250.561, -1250.561], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  195/5000: episode: 195, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -625.924, mean reward: -625.924 [-625.924, -625.924], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  196/5000: episode: 196, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1727.410, mean reward: -1727.410 [-1727.410, -1727.410], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  197/5000: episode: 197, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1122.146, mean reward: -1122.146 [-1122.146, -1122.146], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  198/5000: episode: 198, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -1943.968, mean reward: -1943.968 [-1943.968, -1943.968], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  199/5000: episode: 199, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -3014.245, mean reward: -3014.245 [-3014.245, -3014.245], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  200/5000: episode: 200, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -7338.768, mean reward: -7338.768 [-7338.768, -7338.768], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  201/5000: episode: 201, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -1474.033, mean reward: -1474.033 [-1474.033, -1474.033], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  202/5000: episode: 202, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -2443.732, mean reward: -2443.732 [-2443.732, -2443.732], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  203/5000: episode: 203, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2197.119, mean reward: -2197.119 [-2197.119, -2197.119], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  204/5000: episode: 204, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -872.996, mean reward: -872.996 [-872.996, -872.996], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  205/5000: episode: 205, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -758.855, mean reward: -758.855 [-758.855, -758.855], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  206/5000: episode: 206, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -7264.663, mean reward: -7264.663 [-7264.663, -7264.663], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  207/5000: episode: 207, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -844.805, mean reward: -844.805 [-844.805, -844.805], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  208/5000: episode: 208, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -9688.336, mean reward: -9688.336 [-9688.336, -9688.336], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  209/5000: episode: 209, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -4024.483, mean reward: -4024.483 [-4024.483, -4024.483], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  210/5000: episode: 210, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -4946.895, mean reward: -4946.895 [-4946.895, -4946.895], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  211/5000: episode: 211, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5617.424, mean reward: -5617.424 [-5617.424, -5617.424], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  212/5000: episode: 212, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5906.029, mean reward: -5906.029 [-5906.029, -5906.029], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  213/5000: episode: 213, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7418.590, mean reward: -7418.590 [-7418.590, -7418.590], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  214/5000: episode: 214, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5401.404, mean reward: -5401.404 [-5401.404, -5401.404], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  215/5000: episode: 215, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -436.154, mean reward: -436.154 [-436.154, -436.154], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  216/5000: episode: 216, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -4693.887, mean reward: -4693.887 [-4693.887, -4693.887], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  217/5000: episode: 217, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -3871.587, mean reward: -3871.587 [-3871.587, -3871.587], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  218/5000: episode: 218, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3048.297, mean reward: -3048.297 [-3048.297, -3048.297], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  219/5000: episode: 219, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -132.269, mean reward: -132.269 [-132.269, -132.269], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  220/5000: episode: 220, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -9178.529, mean reward: -9178.529 [-9178.529, -9178.529], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  221/5000: episode: 221, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -6452.102, mean reward: -6452.102 [-6452.102, -6452.102], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  222/5000: episode: 222, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3193.851, mean reward: -3193.851 [-3193.851, -3193.851], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  223/5000: episode: 223, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -1664.438, mean reward: -1664.438 [-1664.438, -1664.438], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  224/5000: episode: 224, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -1520.352, mean reward: -1520.352 [-1520.352, -1520.352], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  225/5000: episode: 225, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2758.618, mean reward: -2758.618 [-2758.618, -2758.618], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  226/5000: episode: 226, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -630.143, mean reward: -630.143 [-630.143, -630.143], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  227/5000: episode: 227, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5237.864, mean reward: -5237.864 [-5237.864, -5237.864], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  228/5000: episode: 228, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -2269.877, mean reward: -2269.877 [-2269.877, -2269.877], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  229/5000: episode: 229, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -747.772, mean reward: -747.772 [-747.772, -747.772], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  230/5000: episode: 230, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -1837.206, mean reward: -1837.206 [-1837.206, -1837.206], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  231/5000: episode: 231, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -393.366, mean reward: -393.366 [-393.366, -393.366], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  232/5000: episode: 232, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4252.056, mean reward: -4252.056 [-4252.056, -4252.056], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  233/5000: episode: 233, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -3766.917, mean reward: -3766.917 [-3766.917, -3766.917], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  234/5000: episode: 234, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5405.307, mean reward: -5405.307 [-5405.307, -5405.307], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  235/5000: episode: 235, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -1297.107, mean reward: -1297.107 [-1297.107, -1297.107], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  236/5000: episode: 236, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3000.541, mean reward: -3000.541 [-3000.541, -3000.541], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  237/5000: episode: 237, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3443.027, mean reward: -3443.027 [-3443.027, -3443.027], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  238/5000: episode: 238, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5380.767, mean reward: -5380.767 [-5380.767, -5380.767], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  239/5000: episode: 239, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -7044.989, mean reward: -7044.989 [-7044.989, -7044.989], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  240/5000: episode: 240, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -12.603, mean reward: -12.603 [-12.603, -12.603], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  241/5000: episode: 241, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1886.161, mean reward: -1886.161 [-1886.161, -1886.161], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  242/5000: episode: 242, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -8750.636, mean reward: -8750.636 [-8750.636, -8750.636], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  243/5000: episode: 243, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2126.031, mean reward: -2126.031 [-2126.031, -2126.031], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  244/5000: episode: 244, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1466.192, mean reward: -1466.192 [-1466.192, -1466.192], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  245/5000: episode: 245, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2930.080, mean reward: -2930.080 [-2930.080, -2930.080], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  246/5000: episode: 246, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5401.178, mean reward: -5401.178 [-5401.178, -5401.178], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  247/5000: episode: 247, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2311.843, mean reward: -2311.843 [-2311.843, -2311.843], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  248/5000: episode: 248, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3407.405, mean reward: -3407.405 [-3407.405, -3407.405], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  249/5000: episode: 249, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -2534.642, mean reward: -2534.642 [-2534.642, -2534.642], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  250/5000: episode: 250, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4372.317, mean reward: -4372.317 [-4372.317, -4372.317], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  251/5000: episode: 251, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -1365.189, mean reward: -1365.189 [-1365.189, -1365.189], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  252/5000: episode: 252, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5258.358, mean reward: -5258.358 [-5258.358, -5258.358], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  253/5000: episode: 253, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -2243.037, mean reward: -2243.037 [-2243.037, -2243.037], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  254/5000: episode: 254, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1694.561, mean reward: -1694.561 [-1694.561, -1694.561], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  255/5000: episode: 255, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -4104.432, mean reward: -4104.432 [-4104.432, -4104.432], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  256/5000: episode: 256, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6156.937, mean reward: -6156.937 [-6156.937, -6156.937], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  257/5000: episode: 257, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -554.954, mean reward: -554.954 [-554.954, -554.954], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  258/5000: episode: 258, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -113.422, mean reward: -113.422 [-113.422, -113.422], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  259/5000: episode: 259, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -9206.320, mean reward: -9206.320 [-9206.320, -9206.320], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  260/5000: episode: 260, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4800.403, mean reward: -4800.403 [-4800.403, -4800.403], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  261/5000: episode: 261, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2271.899, mean reward: -2271.899 [-2271.899, -2271.899], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  262/5000: episode: 262, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1571.761, mean reward: -1571.761 [-1571.761, -1571.761], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  263/5000: episode: 263, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3303.074, mean reward: -3303.074 [-3303.074, -3303.074], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  264/5000: episode: 264, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -11775.283, mean reward: -11775.283 [-11775.283, -11775.283], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  265/5000: episode: 265, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3274.394, mean reward: -3274.394 [-3274.394, -3274.394], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  266/5000: episode: 266, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5057.445, mean reward: -5057.445 [-5057.445, -5057.445], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  267/5000: episode: 267, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -9937.667, mean reward: -9937.667 [-9937.667, -9937.667], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  268/5000: episode: 268, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1233.064, mean reward: -1233.064 [-1233.064, -1233.064], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  269/5000: episode: 269, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -9696.432, mean reward: -9696.432 [-9696.432, -9696.432], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  270/5000: episode: 270, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -1530.698, mean reward: -1530.698 [-1530.698, -1530.698], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  271/5000: episode: 271, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8634.432, mean reward: -8634.432 [-8634.432, -8634.432], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  272/5000: episode: 272, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -768.008, mean reward: -768.008 [-768.008, -768.008], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  273/5000: episode: 273, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4379.308, mean reward: -4379.308 [-4379.308, -4379.308], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  274/5000: episode: 274, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4021.995, mean reward: -4021.995 [-4021.995, -4021.995], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  275/5000: episode: 275, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4376.567, mean reward: -4376.567 [-4376.567, -4376.567], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  276/5000: episode: 276, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -1865.835, mean reward: -1865.835 [-1865.835, -1865.835], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  277/5000: episode: 277, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -9031.110, mean reward: -9031.110 [-9031.110, -9031.110], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  278/5000: episode: 278, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -9099.958, mean reward: -9099.958 [-9099.958, -9099.958], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  279/5000: episode: 279, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5555.023, mean reward: -5555.023 [-5555.023, -5555.023], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  280/5000: episode: 280, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -194.551, mean reward: -194.551 [-194.551, -194.551], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  281/5000: episode: 281, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -735.630, mean reward: -735.630 [-735.630, -735.630], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  282/5000: episode: 282, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1475.754, mean reward: -1475.754 [-1475.754, -1475.754], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  283/5000: episode: 283, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2319.952, mean reward: -2319.952 [-2319.952, -2319.952], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  284/5000: episode: 284, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -1510.668, mean reward: -1510.668 [-1510.668, -1510.668], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  285/5000: episode: 285, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -1005.118, mean reward: -1005.118 [-1005.118, -1005.118], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  286/5000: episode: 286, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7647.734, mean reward: -7647.734 [-7647.734, -7647.734], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  287/5000: episode: 287, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -163.805, mean reward: -163.805 [-163.805, -163.805], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  288/5000: episode: 288, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2270.050, mean reward: -2270.050 [-2270.050, -2270.050], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  289/5000: episode: 289, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -2239.141, mean reward: -2239.141 [-2239.141, -2239.141], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  290/5000: episode: 290, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2508.074, mean reward: -2508.074 [-2508.074, -2508.074], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  291/5000: episode: 291, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4862.760, mean reward: -4862.760 [-4862.760, -4862.760], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  292/5000: episode: 292, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -6993.764, mean reward: -6993.764 [-6993.764, -6993.764], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  293/5000: episode: 293, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -855.002, mean reward: -855.002 [-855.002, -855.002], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  294/5000: episode: 294, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3501.772, mean reward: -3501.772 [-3501.772, -3501.772], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  295/5000: episode: 295, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -8240.788, mean reward: -8240.788 [-8240.788, -8240.788], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  296/5000: episode: 296, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2024.809, mean reward: -2024.809 [-2024.809, -2024.809], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  297/5000: episode: 297, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -51.919, mean reward: -51.919 [-51.919, -51.919], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  298/5000: episode: 298, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6377.852, mean reward: -6377.852 [-6377.852, -6377.852], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  299/5000: episode: 299, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -4566.350, mean reward: -4566.350 [-4566.350, -4566.350], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  300/5000: episode: 300, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -7847.484, mean reward: -7847.484 [-7847.484, -7847.484], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  301/5000: episode: 301, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -2426.620, mean reward: -2426.620 [-2426.620, -2426.620], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  302/5000: episode: 302, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4410.468, mean reward: -4410.468 [-4410.468, -4410.468], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  303/5000: episode: 303, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6911.958, mean reward: -6911.958 [-6911.958, -6911.958], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  304/5000: episode: 304, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3345.281, mean reward: -3345.281 [-3345.281, -3345.281], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  305/5000: episode: 305, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -331.903, mean reward: -331.903 [-331.903, -331.903], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  306/5000: episode: 306, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -9168.951, mean reward: -9168.951 [-9168.951, -9168.951], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  307/5000: episode: 307, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3270.848, mean reward: -3270.848 [-3270.848, -3270.848], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  308/5000: episode: 308, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -8501.061, mean reward: -8501.061 [-8501.061, -8501.061], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  309/5000: episode: 309, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -4464.432, mean reward: -4464.432 [-4464.432, -4464.432], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  310/5000: episode: 310, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3319.876, mean reward: -3319.876 [-3319.876, -3319.876], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  311/5000: episode: 311, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -485.865, mean reward: -485.865 [-485.865, -485.865], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  312/5000: episode: 312, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3142.179, mean reward: -3142.179 [-3142.179, -3142.179], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  313/5000: episode: 313, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -3517.568, mean reward: -3517.568 [-3517.568, -3517.568], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  314/5000: episode: 314, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1857.617, mean reward: -1857.617 [-1857.617, -1857.617], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  315/5000: episode: 315, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -391.113, mean reward: -391.113 [-391.113, -391.113], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  316/5000: episode: 316, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -15303.095, mean reward: -15303.095 [-15303.095, -15303.095], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  317/5000: episode: 317, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -8699.526, mean reward: -8699.526 [-8699.526, -8699.526], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  318/5000: episode: 318, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9378.771, mean reward: -9378.771 [-9378.771, -9378.771], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  319/5000: episode: 319, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -4809.881, mean reward: -4809.881 [-4809.881, -4809.881], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  320/5000: episode: 320, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -9547.759, mean reward: -9547.759 [-9547.759, -9547.759], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  321/5000: episode: 321, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -9829.174, mean reward: -9829.174 [-9829.174, -9829.174], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  322/5000: episode: 322, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2584.907, mean reward: -2584.907 [-2584.907, -2584.907], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  323/5000: episode: 323, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5263.726, mean reward: -5263.726 [-5263.726, -5263.726], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  324/5000: episode: 324, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -22.739, mean reward: -22.739 [-22.739, -22.739], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  325/5000: episode: 325, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5001.184, mean reward: -5001.184 [-5001.184, -5001.184], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  326/5000: episode: 326, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3585.855, mean reward: -3585.855 [-3585.855, -3585.855], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  327/5000: episode: 327, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -491.487, mean reward: -491.487 [-491.487, -491.487], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  328/5000: episode: 328, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3673.488, mean reward: -3673.488 [-3673.488, -3673.488], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  329/5000: episode: 329, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -6663.300, mean reward: -6663.300 [-6663.300, -6663.300], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  330/5000: episode: 330, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7531.457, mean reward: -7531.457 [-7531.457, -7531.457], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  331/5000: episode: 331, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -198.799, mean reward: -198.799 [-198.799, -198.799], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  332/5000: episode: 332, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2762.679, mean reward: -2762.679 [-2762.679, -2762.679], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  333/5000: episode: 333, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -533.104, mean reward: -533.104 [-533.104, -533.104], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  334/5000: episode: 334, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2975.174, mean reward: -2975.174 [-2975.174, -2975.174], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  335/5000: episode: 335, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -8999.238, mean reward: -8999.238 [-8999.238, -8999.238], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  336/5000: episode: 336, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -76.221, mean reward: -76.221 [-76.221, -76.221], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  337/5000: episode: 337, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2285.040, mean reward: -2285.040 [-2285.040, -2285.040], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  338/5000: episode: 338, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6148.472, mean reward: -6148.472 [-6148.472, -6148.472], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  339/5000: episode: 339, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1807.080, mean reward: -1807.080 [-1807.080, -1807.080], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  340/5000: episode: 340, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -2923.154, mean reward: -2923.154 [-2923.154, -2923.154], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  341/5000: episode: 341, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -503.582, mean reward: -503.582 [-503.582, -503.582], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  342/5000: episode: 342, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2605.029, mean reward: -2605.029 [-2605.029, -2605.029], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  343/5000: episode: 343, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3353.240, mean reward: -3353.240 [-3353.240, -3353.240], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  344/5000: episode: 344, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3539.936, mean reward: -3539.936 [-3539.936, -3539.936], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  345/5000: episode: 345, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -729.697, mean reward: -729.697 [-729.697, -729.697], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  346/5000: episode: 346, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -38.907, mean reward: -38.907 [-38.907, -38.907], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  347/5000: episode: 347, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -2224.278, mean reward: -2224.278 [-2224.278, -2224.278], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  348/5000: episode: 348, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1090.275, mean reward: -1090.275 [-1090.275, -1090.275], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  349/5000: episode: 349, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -971.250, mean reward: -971.250 [-971.250, -971.250], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  350/5000: episode: 350, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -601.740, mean reward: -601.740 [-601.740, -601.740], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  351/5000: episode: 351, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1973.995, mean reward: -1973.995 [-1973.995, -1973.995], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  352/5000: episode: 352, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2363.966, mean reward: -2363.966 [-2363.966, -2363.966], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  353/5000: episode: 353, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -14233.688, mean reward: -14233.688 [-14233.688, -14233.688], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  354/5000: episode: 354, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6727.439, mean reward: -6727.439 [-6727.439, -6727.439], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  355/5000: episode: 355, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2261.329, mean reward: -2261.329 [-2261.329, -2261.329], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  356/5000: episode: 356, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -285.920, mean reward: -285.920 [-285.920, -285.920], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  357/5000: episode: 357, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -7510.319, mean reward: -7510.319 [-7510.319, -7510.319], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  358/5000: episode: 358, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1978.815, mean reward: -1978.815 [-1978.815, -1978.815], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  359/5000: episode: 359, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9972.915, mean reward: -9972.915 [-9972.915, -9972.915], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  360/5000: episode: 360, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8108.187, mean reward: -8108.187 [-8108.187, -8108.187], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  361/5000: episode: 361, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4744.797, mean reward: -4744.797 [-4744.797, -4744.797], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  362/5000: episode: 362, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2266.882, mean reward: -2266.882 [-2266.882, -2266.882], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  363/5000: episode: 363, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7213.584, mean reward: -7213.584 [-7213.584, -7213.584], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  364/5000: episode: 364, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -318.713, mean reward: -318.713 [-318.713, -318.713], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  365/5000: episode: 365, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -6121.422, mean reward: -6121.422 [-6121.422, -6121.422], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  366/5000: episode: 366, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -311.479, mean reward: -311.479 [-311.479, -311.479], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  367/5000: episode: 367, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3103.667, mean reward: -3103.667 [-3103.667, -3103.667], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  368/5000: episode: 368, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7192.319, mean reward: -7192.319 [-7192.319, -7192.319], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  369/5000: episode: 369, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4065.291, mean reward: -4065.291 [-4065.291, -4065.291], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  370/5000: episode: 370, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4184.630, mean reward: -4184.630 [-4184.630, -4184.630], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  371/5000: episode: 371, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -244.712, mean reward: -244.712 [-244.712, -244.712], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  372/5000: episode: 372, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2503.104, mean reward: -2503.104 [-2503.104, -2503.104], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  373/5000: episode: 373, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -7880.448, mean reward: -7880.448 [-7880.448, -7880.448], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  374/5000: episode: 374, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1309.015, mean reward: -1309.015 [-1309.015, -1309.015], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  375/5000: episode: 375, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -7453.169, mean reward: -7453.169 [-7453.169, -7453.169], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  376/5000: episode: 376, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2405.071, mean reward: -2405.071 [-2405.071, -2405.071], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  377/5000: episode: 377, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -2637.462, mean reward: -2637.462 [-2637.462, -2637.462], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  378/5000: episode: 378, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1064.469, mean reward: -1064.469 [-1064.469, -1064.469], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  379/5000: episode: 379, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4650.927, mean reward: -4650.927 [-4650.927, -4650.927], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  380/5000: episode: 380, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5534.964, mean reward: -5534.964 [-5534.964, -5534.964], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  381/5000: episode: 381, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1315.086, mean reward: -1315.086 [-1315.086, -1315.086], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  382/5000: episode: 382, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4744.423, mean reward: -4744.423 [-4744.423, -4744.423], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  383/5000: episode: 383, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -2111.491, mean reward: -2111.491 [-2111.491, -2111.491], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  384/5000: episode: 384, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -1440.066, mean reward: -1440.066 [-1440.066, -1440.066], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  385/5000: episode: 385, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7255.709, mean reward: -7255.709 [-7255.709, -7255.709], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  386/5000: episode: 386, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -7687.311, mean reward: -7687.311 [-7687.311, -7687.311], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  387/5000: episode: 387, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2072.946, mean reward: -2072.946 [-2072.946, -2072.946], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  388/5000: episode: 388, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8582.807, mean reward: -8582.807 [-8582.807, -8582.807], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  389/5000: episode: 389, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1972.389, mean reward: -1972.389 [-1972.389, -1972.389], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  390/5000: episode: 390, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -95.070, mean reward: -95.070 [-95.070, -95.070], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  391/5000: episode: 391, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -744.254, mean reward: -744.254 [-744.254, -744.254], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  392/5000: episode: 392, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -183.948, mean reward: -183.948 [-183.948, -183.948], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  393/5000: episode: 393, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -6542.864, mean reward: -6542.864 [-6542.864, -6542.864], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  394/5000: episode: 394, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -4770.815, mean reward: -4770.815 [-4770.815, -4770.815], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  395/5000: episode: 395, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7264.371, mean reward: -7264.371 [-7264.371, -7264.371], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  396/5000: episode: 396, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -394.069, mean reward: -394.069 [-394.069, -394.069], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  397/5000: episode: 397, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1759.881, mean reward: -1759.881 [-1759.881, -1759.881], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  398/5000: episode: 398, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -1278.388, mean reward: -1278.388 [-1278.388, -1278.388], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  399/5000: episode: 399, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -11012.652, mean reward: -11012.652 [-11012.652, -11012.652], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  400/5000: episode: 400, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4992.825, mean reward: -4992.825 [-4992.825, -4992.825], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  401/5000: episode: 401, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -7032.936, mean reward: -7032.936 [-7032.936, -7032.936], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  402/5000: episode: 402, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1999.264, mean reward: -1999.264 [-1999.264, -1999.264], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  403/5000: episode: 403, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -4982.852, mean reward: -4982.852 [-4982.852, -4982.852], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  404/5000: episode: 404, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3423.685, mean reward: -3423.685 [-3423.685, -3423.685], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  405/5000: episode: 405, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2452.075, mean reward: -2452.075 [-2452.075, -2452.075], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  406/5000: episode: 406, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1648.220, mean reward: -1648.220 [-1648.220, -1648.220], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  407/5000: episode: 407, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -181.655, mean reward: -181.655 [-181.655, -181.655], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  408/5000: episode: 408, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9727.840, mean reward: -9727.840 [-9727.840, -9727.840], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  409/5000: episode: 409, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -8308.754, mean reward: -8308.754 [-8308.754, -8308.754], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  410/5000: episode: 410, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5858.667, mean reward: -5858.667 [-5858.667, -5858.667], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  411/5000: episode: 411, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5397.558, mean reward: -5397.558 [-5397.558, -5397.558], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  412/5000: episode: 412, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2543.943, mean reward: -2543.943 [-2543.943, -2543.943], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  413/5000: episode: 413, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4041.890, mean reward: -4041.890 [-4041.890, -4041.890], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  414/5000: episode: 414, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1157.016, mean reward: -1157.016 [-1157.016, -1157.016], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  415/5000: episode: 415, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2222.919, mean reward: -2222.919 [-2222.919, -2222.919], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  416/5000: episode: 416, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -2115.042, mean reward: -2115.042 [-2115.042, -2115.042], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  417/5000: episode: 417, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2618.323, mean reward: -2618.323 [-2618.323, -2618.323], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  418/5000: episode: 418, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -13394.064, mean reward: -13394.064 [-13394.064, -13394.064], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  419/5000: episode: 419, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4940.309, mean reward: -4940.309 [-4940.309, -4940.309], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  420/5000: episode: 420, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -981.401, mean reward: -981.401 [-981.401, -981.401], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  421/5000: episode: 421, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -1436.652, mean reward: -1436.652 [-1436.652, -1436.652], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  422/5000: episode: 422, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1788.552, mean reward: -1788.552 [-1788.552, -1788.552], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  423/5000: episode: 423, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6973.375, mean reward: -6973.375 [-6973.375, -6973.375], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  424/5000: episode: 424, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10571.390, mean reward: -10571.390 [-10571.390, -10571.390], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  425/5000: episode: 425, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -136.822, mean reward: -136.822 [-136.822, -136.822], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  426/5000: episode: 426, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2663.002, mean reward: -2663.002 [-2663.002, -2663.002], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  427/5000: episode: 427, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3626.273, mean reward: -3626.273 [-3626.273, -3626.273], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  428/5000: episode: 428, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3822.276, mean reward: -3822.276 [-3822.276, -3822.276], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  429/5000: episode: 429, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4054.894, mean reward: -4054.894 [-4054.894, -4054.894], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  430/5000: episode: 430, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1508.065, mean reward: -1508.065 [-1508.065, -1508.065], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  431/5000: episode: 431, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1272.756, mean reward: -1272.756 [-1272.756, -1272.756], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  432/5000: episode: 432, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7385.647, mean reward: -7385.647 [-7385.647, -7385.647], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  433/5000: episode: 433, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -123.917, mean reward: -123.917 [-123.917, -123.917], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  434/5000: episode: 434, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8656.376, mean reward: -8656.376 [-8656.376, -8656.376], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  435/5000: episode: 435, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -624.932, mean reward: -624.932 [-624.932, -624.932], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  436/5000: episode: 436, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -768.928, mean reward: -768.928 [-768.928, -768.928], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  437/5000: episode: 437, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -1428.568, mean reward: -1428.568 [-1428.568, -1428.568], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  438/5000: episode: 438, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -960.566, mean reward: -960.566 [-960.566, -960.566], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  439/5000: episode: 439, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -9280.513, mean reward: -9280.513 [-9280.513, -9280.513], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  440/5000: episode: 440, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -986.174, mean reward: -986.174 [-986.174, -986.174], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  441/5000: episode: 441, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -6022.733, mean reward: -6022.733 [-6022.733, -6022.733], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  442/5000: episode: 442, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -3894.959, mean reward: -3894.959 [-3894.959, -3894.959], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  443/5000: episode: 443, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -491.336, mean reward: -491.336 [-491.336, -491.336], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  444/5000: episode: 444, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2886.016, mean reward: -2886.016 [-2886.016, -2886.016], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  445/5000: episode: 445, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -990.147, mean reward: -990.147 [-990.147, -990.147], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  446/5000: episode: 446, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2037.154, mean reward: -2037.154 [-2037.154, -2037.154], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  447/5000: episode: 447, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4358.181, mean reward: -4358.181 [-4358.181, -4358.181], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  448/5000: episode: 448, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -931.906, mean reward: -931.906 [-931.906, -931.906], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  449/5000: episode: 449, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2350.954, mean reward: -2350.954 [-2350.954, -2350.954], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  450/5000: episode: 450, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9724.544, mean reward: -9724.544 [-9724.544, -9724.544], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  451/5000: episode: 451, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1623.683, mean reward: -1623.683 [-1623.683, -1623.683], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  452/5000: episode: 452, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -49.011, mean reward: -49.011 [-49.011, -49.011], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  453/5000: episode: 453, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -6148.137, mean reward: -6148.137 [-6148.137, -6148.137], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  454/5000: episode: 454, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1181.500, mean reward: -1181.500 [-1181.500, -1181.500], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  455/5000: episode: 455, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3072.872, mean reward: -3072.872 [-3072.872, -3072.872], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  456/5000: episode: 456, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3515.824, mean reward: -3515.824 [-3515.824, -3515.824], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  457/5000: episode: 457, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3984.780, mean reward: -3984.780 [-3984.780, -3984.780], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  458/5000: episode: 458, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1750.535, mean reward: -1750.535 [-1750.535, -1750.535], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  459/5000: episode: 459, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4057.908, mean reward: -4057.908 [-4057.908, -4057.908], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  460/5000: episode: 460, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -177.419, mean reward: -177.419 [-177.419, -177.419], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  461/5000: episode: 461, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6562.646, mean reward: -6562.646 [-6562.646, -6562.646], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  462/5000: episode: 462, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3074.429, mean reward: -3074.429 [-3074.429, -3074.429], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  463/5000: episode: 463, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -7404.264, mean reward: -7404.264 [-7404.264, -7404.264], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  464/5000: episode: 464, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -537.660, mean reward: -537.660 [-537.660, -537.660], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  465/5000: episode: 465, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5369.644, mean reward: -5369.644 [-5369.644, -5369.644], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  466/5000: episode: 466, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4326.347, mean reward: -4326.347 [-4326.347, -4326.347], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  467/5000: episode: 467, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -67.264, mean reward: -67.264 [-67.264, -67.264], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  468/5000: episode: 468, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2559.878, mean reward: -2559.878 [-2559.878, -2559.878], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  469/5000: episode: 469, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5170.083, mean reward: -5170.083 [-5170.083, -5170.083], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  470/5000: episode: 470, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -379.341, mean reward: -379.341 [-379.341, -379.341], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  471/5000: episode: 471, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -503.418, mean reward: -503.418 [-503.418, -503.418], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  472/5000: episode: 472, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -1524.826, mean reward: -1524.826 [-1524.826, -1524.826], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  473/5000: episode: 473, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4029.715, mean reward: -4029.715 [-4029.715, -4029.715], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  474/5000: episode: 474, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5893.495, mean reward: -5893.495 [-5893.495, -5893.495], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  475/5000: episode: 475, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3458.487, mean reward: -3458.487 [-3458.487, -3458.487], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  476/5000: episode: 476, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1311.228, mean reward: -1311.228 [-1311.228, -1311.228], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  477/5000: episode: 477, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1919.934, mean reward: -1919.934 [-1919.934, -1919.934], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  478/5000: episode: 478, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3854.370, mean reward: -3854.370 [-3854.370, -3854.370], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  479/5000: episode: 479, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1152.795, mean reward: -1152.795 [-1152.795, -1152.795], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  480/5000: episode: 480, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1016.931, mean reward: -1016.931 [-1016.931, -1016.931], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  481/5000: episode: 481, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6393.015, mean reward: -6393.015 [-6393.015, -6393.015], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  482/5000: episode: 482, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6331.254, mean reward: -6331.254 [-6331.254, -6331.254], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  483/5000: episode: 483, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1203.187, mean reward: -1203.187 [-1203.187, -1203.187], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  484/5000: episode: 484, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2328.708, mean reward: -2328.708 [-2328.708, -2328.708], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  485/5000: episode: 485, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6232.857, mean reward: -6232.857 [-6232.857, -6232.857], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  486/5000: episode: 486, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -419.534, mean reward: -419.534 [-419.534, -419.534], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  487/5000: episode: 487, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -282.447, mean reward: -282.447 [-282.447, -282.447], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  488/5000: episode: 488, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -342.711, mean reward: -342.711 [-342.711, -342.711], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  489/5000: episode: 489, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2353.240, mean reward: -2353.240 [-2353.240, -2353.240], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  490/5000: episode: 490, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6296.996, mean reward: -6296.996 [-6296.996, -6296.996], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  491/5000: episode: 491, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4875.024, mean reward: -4875.024 [-4875.024, -4875.024], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  492/5000: episode: 492, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -2638.471, mean reward: -2638.471 [-2638.471, -2638.471], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  493/5000: episode: 493, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4461.428, mean reward: -4461.428 [-4461.428, -4461.428], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  494/5000: episode: 494, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -341.539, mean reward: -341.539 [-341.539, -341.539], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  495/5000: episode: 495, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -508.761, mean reward: -508.761 [-508.761, -508.761], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  496/5000: episode: 496, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -2544.998, mean reward: -2544.998 [-2544.998, -2544.998], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  497/5000: episode: 497, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1242.826, mean reward: -1242.826 [-1242.826, -1242.826], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  498/5000: episode: 498, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1504.436, mean reward: -1504.436 [-1504.436, -1504.436], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  499/5000: episode: 499, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -193.695, mean reward: -193.695 [-193.695, -193.695], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  500/5000: episode: 500, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -4883.219, mean reward: -4883.219 [-4883.219, -4883.219], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  501/5000: episode: 501, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -2332.651, mean reward: -2332.651 [-2332.651, -2332.651], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  502/5000: episode: 502, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10472.105, mean reward: -10472.105 [-10472.105, -10472.105], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  503/5000: episode: 503, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8773.728, mean reward: -8773.728 [-8773.728, -8773.728], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  504/5000: episode: 504, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3016.590, mean reward: -3016.590 [-3016.590, -3016.590], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  505/5000: episode: 505, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -932.380, mean reward: -932.380 [-932.380, -932.380], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  506/5000: episode: 506, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -622.665, mean reward: -622.665 [-622.665, -622.665], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  507/5000: episode: 507, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1841.034, mean reward: -1841.034 [-1841.034, -1841.034], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  508/5000: episode: 508, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3834.928, mean reward: -3834.928 [-3834.928, -3834.928], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  509/5000: episode: 509, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3819.367, mean reward: -3819.367 [-3819.367, -3819.367], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  510/5000: episode: 510, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2258.106, mean reward: -2258.106 [-2258.106, -2258.106], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  511/5000: episode: 511, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2482.370, mean reward: -2482.370 [-2482.370, -2482.370], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  512/5000: episode: 512, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -881.817, mean reward: -881.817 [-881.817, -881.817], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  513/5000: episode: 513, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1544.039, mean reward: -1544.039 [-1544.039, -1544.039], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  514/5000: episode: 514, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1873.697, mean reward: -1873.697 [-1873.697, -1873.697], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  515/5000: episode: 515, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6173.317, mean reward: -6173.317 [-6173.317, -6173.317], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  516/5000: episode: 516, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -220.716, mean reward: -220.716 [-220.716, -220.716], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  517/5000: episode: 517, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -8053.633, mean reward: -8053.633 [-8053.633, -8053.633], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  518/5000: episode: 518, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1601.752, mean reward: -1601.752 [-1601.752, -1601.752], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  519/5000: episode: 519, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9943.127, mean reward: -9943.127 [-9943.127, -9943.127], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  520/5000: episode: 520, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4186.630, mean reward: -4186.630 [-4186.630, -4186.630], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  521/5000: episode: 521, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6808.566, mean reward: -6808.566 [-6808.566, -6808.566], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  522/5000: episode: 522, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4058.390, mean reward: -4058.390 [-4058.390, -4058.390], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  523/5000: episode: 523, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -708.820, mean reward: -708.820 [-708.820, -708.820], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  524/5000: episode: 524, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1930.038, mean reward: -1930.038 [-1930.038, -1930.038], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  525/5000: episode: 525, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5028.229, mean reward: -5028.229 [-5028.229, -5028.229], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  526/5000: episode: 526, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1224.482, mean reward: -1224.482 [-1224.482, -1224.482], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  527/5000: episode: 527, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1528.063, mean reward: -1528.063 [-1528.063, -1528.063], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  528/5000: episode: 528, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3706.296, mean reward: -3706.296 [-3706.296, -3706.296], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  529/5000: episode: 529, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9702.911, mean reward: -9702.911 [-9702.911, -9702.911], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  530/5000: episode: 530, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6371.307, mean reward: -6371.307 [-6371.307, -6371.307], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  531/5000: episode: 531, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -789.349, mean reward: -789.349 [-789.349, -789.349], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  532/5000: episode: 532, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3059.460, mean reward: -3059.460 [-3059.460, -3059.460], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  533/5000: episode: 533, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5185.819, mean reward: -5185.819 [-5185.819, -5185.819], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  534/5000: episode: 534, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1468.479, mean reward: -1468.479 [-1468.479, -1468.479], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  535/5000: episode: 535, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2485.575, mean reward: -2485.575 [-2485.575, -2485.575], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  536/5000: episode: 536, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -310.890, mean reward: -310.890 [-310.890, -310.890], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  537/5000: episode: 537, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4504.159, mean reward: -4504.159 [-4504.159, -4504.159], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  538/5000: episode: 538, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1968.715, mean reward: -1968.715 [-1968.715, -1968.715], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  539/5000: episode: 539, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5719.041, mean reward: -5719.041 [-5719.041, -5719.041], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  540/5000: episode: 540, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4085.829, mean reward: -4085.829 [-4085.829, -4085.829], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  541/5000: episode: 541, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -6892.717, mean reward: -6892.717 [-6892.717, -6892.717], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  542/5000: episode: 542, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7707.499, mean reward: -7707.499 [-7707.499, -7707.499], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  543/5000: episode: 543, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7297.286, mean reward: -7297.286 [-7297.286, -7297.286], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  544/5000: episode: 544, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2339.020, mean reward: -2339.020 [-2339.020, -2339.020], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  545/5000: episode: 545, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -3941.766, mean reward: -3941.766 [-3941.766, -3941.766], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  546/5000: episode: 546, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1032.312, mean reward: -1032.312 [-1032.312, -1032.312], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  547/5000: episode: 547, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2917.895, mean reward: -2917.895 [-2917.895, -2917.895], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  548/5000: episode: 548, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -848.320, mean reward: -848.320 [-848.320, -848.320], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  549/5000: episode: 549, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2645.710, mean reward: -2645.710 [-2645.710, -2645.710], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  550/5000: episode: 550, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -6221.794, mean reward: -6221.794 [-6221.794, -6221.794], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  551/5000: episode: 551, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -8120.339, mean reward: -8120.339 [-8120.339, -8120.339], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  552/5000: episode: 552, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5761.870, mean reward: -5761.870 [-5761.870, -5761.870], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  553/5000: episode: 553, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6188.574, mean reward: -6188.574 [-6188.574, -6188.574], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  554/5000: episode: 554, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2309.005, mean reward: -2309.005 [-2309.005, -2309.005], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  555/5000: episode: 555, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3262.266, mean reward: -3262.266 [-3262.266, -3262.266], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  556/5000: episode: 556, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -411.185, mean reward: -411.185 [-411.185, -411.185], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  557/5000: episode: 557, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6666.195, mean reward: -6666.195 [-6666.195, -6666.195], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  558/5000: episode: 558, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -1929.329, mean reward: -1929.329 [-1929.329, -1929.329], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  559/5000: episode: 559, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -160.023, mean reward: -160.023 [-160.023, -160.023], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  560/5000: episode: 560, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -9242.365, mean reward: -9242.365 [-9242.365, -9242.365], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  561/5000: episode: 561, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2077.289, mean reward: -2077.289 [-2077.289, -2077.289], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  562/5000: episode: 562, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -701.384, mean reward: -701.384 [-701.384, -701.384], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  563/5000: episode: 563, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1721.908, mean reward: -1721.908 [-1721.908, -1721.908], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  564/5000: episode: 564, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -4253.595, mean reward: -4253.595 [-4253.595, -4253.595], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  565/5000: episode: 565, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4431.826, mean reward: -4431.826 [-4431.826, -4431.826], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  566/5000: episode: 566, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3400.432, mean reward: -3400.432 [-3400.432, -3400.432], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  567/5000: episode: 567, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -840.779, mean reward: -840.779 [-840.779, -840.779], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  568/5000: episode: 568, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2585.740, mean reward: -2585.740 [-2585.740, -2585.740], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  569/5000: episode: 569, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -4792.933, mean reward: -4792.933 [-4792.933, -4792.933], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  570/5000: episode: 570, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9340.586, mean reward: -9340.586 [-9340.586, -9340.586], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  571/5000: episode: 571, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -911.691, mean reward: -911.691 [-911.691, -911.691], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  572/5000: episode: 572, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3773.016, mean reward: -3773.016 [-3773.016, -3773.016], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  573/5000: episode: 573, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3543.025, mean reward: -3543.025 [-3543.025, -3543.025], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  574/5000: episode: 574, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2615.865, mean reward: -2615.865 [-2615.865, -2615.865], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  575/5000: episode: 575, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3741.479, mean reward: -3741.479 [-3741.479, -3741.479], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  576/5000: episode: 576, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11508.159, mean reward: -11508.159 [-11508.159, -11508.159], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  577/5000: episode: 577, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2043.320, mean reward: -2043.320 [-2043.320, -2043.320], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  578/5000: episode: 578, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -7794.608, mean reward: -7794.608 [-7794.608, -7794.608], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  579/5000: episode: 579, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1401.930, mean reward: -1401.930 [-1401.930, -1401.930], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  580/5000: episode: 580, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -9525.110, mean reward: -9525.110 [-9525.110, -9525.110], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  581/5000: episode: 581, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -3118.183, mean reward: -3118.183 [-3118.183, -3118.183], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  582/5000: episode: 582, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -16.511, mean reward: -16.511 [-16.511, -16.511], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  583/5000: episode: 583, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -1163.813, mean reward: -1163.813 [-1163.813, -1163.813], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  584/5000: episode: 584, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -13462.894, mean reward: -13462.894 [-13462.894, -13462.894], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  585/5000: episode: 585, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1362.042, mean reward: -1362.042 [-1362.042, -1362.042], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  586/5000: episode: 586, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3291.840, mean reward: -3291.840 [-3291.840, -3291.840], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  587/5000: episode: 587, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -369.476, mean reward: -369.476 [-369.476, -369.476], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  588/5000: episode: 588, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7668.610, mean reward: -7668.610 [-7668.610, -7668.610], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  589/5000: episode: 589, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -1236.183, mean reward: -1236.183 [-1236.183, -1236.183], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  590/5000: episode: 590, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1480.008, mean reward: -1480.008 [-1480.008, -1480.008], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  591/5000: episode: 591, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2333.052, mean reward: -2333.052 [-2333.052, -2333.052], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  592/5000: episode: 592, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3115.928, mean reward: -3115.928 [-3115.928, -3115.928], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  593/5000: episode: 593, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -70.049, mean reward: -70.049 [-70.049, -70.049], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  594/5000: episode: 594, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5021.038, mean reward: -5021.038 [-5021.038, -5021.038], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  595/5000: episode: 595, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -1679.646, mean reward: -1679.646 [-1679.646, -1679.646], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  596/5000: episode: 596, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -1820.708, mean reward: -1820.708 [-1820.708, -1820.708], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  597/5000: episode: 597, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2106.596, mean reward: -2106.596 [-2106.596, -2106.596], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  598/5000: episode: 598, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -7788.612, mean reward: -7788.612 [-7788.612, -7788.612], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  599/5000: episode: 599, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -162.193, mean reward: -162.193 [-162.193, -162.193], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  600/5000: episode: 600, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -905.494, mean reward: -905.494 [-905.494, -905.494], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  601/5000: episode: 601, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2621.196, mean reward: -2621.196 [-2621.196, -2621.196], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  602/5000: episode: 602, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -2663.428, mean reward: -2663.428 [-2663.428, -2663.428], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  603/5000: episode: 603, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2362.815, mean reward: -2362.815 [-2362.815, -2362.815], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  604/5000: episode: 604, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4141.943, mean reward: -4141.943 [-4141.943, -4141.943], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  605/5000: episode: 605, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3097.497, mean reward: -3097.497 [-3097.497, -3097.497], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  606/5000: episode: 606, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2012.525, mean reward: -2012.525 [-2012.525, -2012.525], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  607/5000: episode: 607, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1086.004, mean reward: -1086.004 [-1086.004, -1086.004], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  608/5000: episode: 608, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -10608.755, mean reward: -10608.755 [-10608.755, -10608.755], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  609/5000: episode: 609, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -3384.781, mean reward: -3384.781 [-3384.781, -3384.781], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  610/5000: episode: 610, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3147.540, mean reward: -3147.540 [-3147.540, -3147.540], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  611/5000: episode: 611, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -556.594, mean reward: -556.594 [-556.594, -556.594], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  612/5000: episode: 612, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8385.429, mean reward: -8385.429 [-8385.429, -8385.429], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  613/5000: episode: 613, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -637.590, mean reward: -637.590 [-637.590, -637.590], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  614/5000: episode: 614, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -365.844, mean reward: -365.844 [-365.844, -365.844], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  615/5000: episode: 615, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -1747.387, mean reward: -1747.387 [-1747.387, -1747.387], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  616/5000: episode: 616, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5601.215, mean reward: -5601.215 [-5601.215, -5601.215], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  617/5000: episode: 617, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1891.880, mean reward: -1891.880 [-1891.880, -1891.880], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  618/5000: episode: 618, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2699.183, mean reward: -2699.183 [-2699.183, -2699.183], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  619/5000: episode: 619, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -679.479, mean reward: -679.479 [-679.479, -679.479], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  620/5000: episode: 620, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -3092.048, mean reward: -3092.048 [-3092.048, -3092.048], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  621/5000: episode: 621, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7628.541, mean reward: -7628.541 [-7628.541, -7628.541], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  622/5000: episode: 622, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5495.172, mean reward: -5495.172 [-5495.172, -5495.172], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  623/5000: episode: 623, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4917.470, mean reward: -4917.470 [-4917.470, -4917.470], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  624/5000: episode: 624, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2525.939, mean reward: -2525.939 [-2525.939, -2525.939], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  625/5000: episode: 625, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3216.769, mean reward: -3216.769 [-3216.769, -3216.769], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  626/5000: episode: 626, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2299.541, mean reward: -2299.541 [-2299.541, -2299.541], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  627/5000: episode: 627, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -950.416, mean reward: -950.416 [-950.416, -950.416], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  628/5000: episode: 628, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -7109.860, mean reward: -7109.860 [-7109.860, -7109.860], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  629/5000: episode: 629, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2421.782, mean reward: -2421.782 [-2421.782, -2421.782], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  630/5000: episode: 630, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -7244.070, mean reward: -7244.070 [-7244.070, -7244.070], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  631/5000: episode: 631, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4902.785, mean reward: -4902.785 [-4902.785, -4902.785], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  632/5000: episode: 632, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -35.238, mean reward: -35.238 [-35.238, -35.238], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  633/5000: episode: 633, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7574.866, mean reward: -7574.866 [-7574.866, -7574.866], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  634/5000: episode: 634, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9364.565, mean reward: -9364.565 [-9364.565, -9364.565], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  635/5000: episode: 635, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -13401.170, mean reward: -13401.170 [-13401.170, -13401.170], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  636/5000: episode: 636, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2407.590, mean reward: -2407.590 [-2407.590, -2407.590], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  637/5000: episode: 637, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -745.340, mean reward: -745.340 [-745.340, -745.340], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  638/5000: episode: 638, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3015.207, mean reward: -3015.207 [-3015.207, -3015.207], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  639/5000: episode: 639, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2260.024, mean reward: -2260.024 [-2260.024, -2260.024], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  640/5000: episode: 640, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -3287.840, mean reward: -3287.840 [-3287.840, -3287.840], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  641/5000: episode: 641, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3116.418, mean reward: -3116.418 [-3116.418, -3116.418], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  642/5000: episode: 642, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1905.089, mean reward: -1905.089 [-1905.089, -1905.089], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  643/5000: episode: 643, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -36.445, mean reward: -36.445 [-36.445, -36.445], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  644/5000: episode: 644, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -1271.226, mean reward: -1271.226 [-1271.226, -1271.226], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  645/5000: episode: 645, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -14215.899, mean reward: -14215.899 [-14215.899, -14215.899], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  646/5000: episode: 646, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5717.141, mean reward: -5717.141 [-5717.141, -5717.141], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  647/5000: episode: 647, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -370.239, mean reward: -370.239 [-370.239, -370.239], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  648/5000: episode: 648, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1399.153, mean reward: -1399.153 [-1399.153, -1399.153], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  649/5000: episode: 649, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -3397.147, mean reward: -3397.147 [-3397.147, -3397.147], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  650/5000: episode: 650, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -1517.034, mean reward: -1517.034 [-1517.034, -1517.034], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  651/5000: episode: 651, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -959.159, mean reward: -959.159 [-959.159, -959.159], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  652/5000: episode: 652, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -2796.614, mean reward: -2796.614 [-2796.614, -2796.614], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  653/5000: episode: 653, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2170.478, mean reward: -2170.478 [-2170.478, -2170.478], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  654/5000: episode: 654, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2267.320, mean reward: -2267.320 [-2267.320, -2267.320], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  655/5000: episode: 655, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -7564.867, mean reward: -7564.867 [-7564.867, -7564.867], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  656/5000: episode: 656, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7053.689, mean reward: -7053.689 [-7053.689, -7053.689], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  657/5000: episode: 657, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2269.112, mean reward: -2269.112 [-2269.112, -2269.112], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  658/5000: episode: 658, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -906.115, mean reward: -906.115 [-906.115, -906.115], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  659/5000: episode: 659, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5.629, mean reward: -5.629 [-5.629, -5.629], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  660/5000: episode: 660, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3068.256, mean reward: -3068.256 [-3068.256, -3068.256], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  661/5000: episode: 661, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -7397.692, mean reward: -7397.692 [-7397.692, -7397.692], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  662/5000: episode: 662, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -202.147, mean reward: -202.147 [-202.147, -202.147], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  663/5000: episode: 663, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1950.590, mean reward: -1950.590 [-1950.590, -1950.590], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  664/5000: episode: 664, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -12964.876, mean reward: -12964.876 [-12964.876, -12964.876], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  665/5000: episode: 665, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4369.212, mean reward: -4369.212 [-4369.212, -4369.212], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  666/5000: episode: 666, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -172.242, mean reward: -172.242 [-172.242, -172.242], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  667/5000: episode: 667, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -1351.029, mean reward: -1351.029 [-1351.029, -1351.029], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  668/5000: episode: 668, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -572.230, mean reward: -572.230 [-572.230, -572.230], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  669/5000: episode: 669, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4331.541, mean reward: -4331.541 [-4331.541, -4331.541], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  670/5000: episode: 670, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3449.414, mean reward: -3449.414 [-3449.414, -3449.414], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  671/5000: episode: 671, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1912.231, mean reward: -1912.231 [-1912.231, -1912.231], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  672/5000: episode: 672, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -2160.764, mean reward: -2160.764 [-2160.764, -2160.764], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  673/5000: episode: 673, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2510.584, mean reward: -2510.584 [-2510.584, -2510.584], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  674/5000: episode: 674, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2277.887, mean reward: -2277.887 [-2277.887, -2277.887], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  675/5000: episode: 675, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1042.331, mean reward: -1042.331 [-1042.331, -1042.331], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  676/5000: episode: 676, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3231.086, mean reward: -3231.086 [-3231.086, -3231.086], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  677/5000: episode: 677, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6246.946, mean reward: -6246.946 [-6246.946, -6246.946], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  678/5000: episode: 678, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5598.510, mean reward: -5598.510 [-5598.510, -5598.510], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  679/5000: episode: 679, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3225.456, mean reward: -3225.456 [-3225.456, -3225.456], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  680/5000: episode: 680, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2742.407, mean reward: -2742.407 [-2742.407, -2742.407], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  681/5000: episode: 681, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3356.218, mean reward: -3356.218 [-3356.218, -3356.218], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  682/5000: episode: 682, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5467.929, mean reward: -5467.929 [-5467.929, -5467.929], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  683/5000: episode: 683, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -532.638, mean reward: -532.638 [-532.638, -532.638], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  684/5000: episode: 684, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7808.999, mean reward: -7808.999 [-7808.999, -7808.999], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  685/5000: episode: 685, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -769.948, mean reward: -769.948 [-769.948, -769.948], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  686/5000: episode: 686, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -4434.588, mean reward: -4434.588 [-4434.588, -4434.588], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  687/5000: episode: 687, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5822.870, mean reward: -5822.870 [-5822.870, -5822.870], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  688/5000: episode: 688, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2589.879, mean reward: -2589.879 [-2589.879, -2589.879], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  689/5000: episode: 689, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6072.679, mean reward: -6072.679 [-6072.679, -6072.679], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  690/5000: episode: 690, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2145.613, mean reward: -2145.613 [-2145.613, -2145.613], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  691/5000: episode: 691, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3661.822, mean reward: -3661.822 [-3661.822, -3661.822], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  692/5000: episode: 692, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2254.741, mean reward: -2254.741 [-2254.741, -2254.741], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  693/5000: episode: 693, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -7056.240, mean reward: -7056.240 [-7056.240, -7056.240], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  694/5000: episode: 694, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1100.381, mean reward: -1100.381 [-1100.381, -1100.381], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  695/5000: episode: 695, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3466.738, mean reward: -3466.738 [-3466.738, -3466.738], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  696/5000: episode: 696, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4305.827, mean reward: -4305.827 [-4305.827, -4305.827], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  697/5000: episode: 697, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5116.559, mean reward: -5116.559 [-5116.559, -5116.559], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  698/5000: episode: 698, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1955.166, mean reward: -1955.166 [-1955.166, -1955.166], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  699/5000: episode: 699, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2974.467, mean reward: -2974.467 [-2974.467, -2974.467], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  700/5000: episode: 700, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -2300.219, mean reward: -2300.219 [-2300.219, -2300.219], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  701/5000: episode: 701, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -957.676, mean reward: -957.676 [-957.676, -957.676], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  702/5000: episode: 702, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -12559.551, mean reward: -12559.551 [-12559.551, -12559.551], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  703/5000: episode: 703, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -89.366, mean reward: -89.366 [-89.366, -89.366], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  704/5000: episode: 704, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -165.071, mean reward: -165.071 [-165.071, -165.071], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  705/5000: episode: 705, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4785.771, mean reward: -4785.771 [-4785.771, -4785.771], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  706/5000: episode: 706, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5747.032, mean reward: -5747.032 [-5747.032, -5747.032], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  707/5000: episode: 707, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -9309.378, mean reward: -9309.378 [-9309.378, -9309.378], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  708/5000: episode: 708, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -719.464, mean reward: -719.464 [-719.464, -719.464], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  709/5000: episode: 709, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5121.015, mean reward: -5121.015 [-5121.015, -5121.015], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  710/5000: episode: 710, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -6602.970, mean reward: -6602.970 [-6602.970, -6602.970], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  711/5000: episode: 711, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5410.892, mean reward: -5410.892 [-5410.892, -5410.892], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  712/5000: episode: 712, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3180.361, mean reward: -3180.361 [-3180.361, -3180.361], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  713/5000: episode: 713, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5452.089, mean reward: -5452.089 [-5452.089, -5452.089], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  714/5000: episode: 714, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7529.676, mean reward: -7529.676 [-7529.676, -7529.676], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  715/5000: episode: 715, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3868.990, mean reward: -3868.990 [-3868.990, -3868.990], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  716/5000: episode: 716, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2071.515, mean reward: -2071.515 [-2071.515, -2071.515], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  717/5000: episode: 717, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3236.945, mean reward: -3236.945 [-3236.945, -3236.945], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  718/5000: episode: 718, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -3693.630, mean reward: -3693.630 [-3693.630, -3693.630], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  719/5000: episode: 719, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6984.707, mean reward: -6984.707 [-6984.707, -6984.707], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  720/5000: episode: 720, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -39.965, mean reward: -39.965 [-39.965, -39.965], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  721/5000: episode: 721, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3235.202, mean reward: -3235.202 [-3235.202, -3235.202], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  722/5000: episode: 722, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1816.166, mean reward: -1816.166 [-1816.166, -1816.166], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  723/5000: episode: 723, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -9038.468, mean reward: -9038.468 [-9038.468, -9038.468], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  724/5000: episode: 724, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6225.055, mean reward: -6225.055 [-6225.055, -6225.055], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  725/5000: episode: 725, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3842.677, mean reward: -3842.677 [-3842.677, -3842.677], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  726/5000: episode: 726, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1879.866, mean reward: -1879.866 [-1879.866, -1879.866], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  727/5000: episode: 727, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1403.759, mean reward: -1403.759 [-1403.759, -1403.759], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  728/5000: episode: 728, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -680.433, mean reward: -680.433 [-680.433, -680.433], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  729/5000: episode: 729, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1004.073, mean reward: -1004.073 [-1004.073, -1004.073], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  730/5000: episode: 730, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -591.651, mean reward: -591.651 [-591.651, -591.651], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  731/5000: episode: 731, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4637.084, mean reward: -4637.084 [-4637.084, -4637.084], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  732/5000: episode: 732, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -7010.872, mean reward: -7010.872 [-7010.872, -7010.872], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  733/5000: episode: 733, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -787.753, mean reward: -787.753 [-787.753, -787.753], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  734/5000: episode: 734, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -4483.829, mean reward: -4483.829 [-4483.829, -4483.829], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  735/5000: episode: 735, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -285.655, mean reward: -285.655 [-285.655, -285.655], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  736/5000: episode: 736, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2410.396, mean reward: -2410.396 [-2410.396, -2410.396], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  737/5000: episode: 737, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1894.503, mean reward: -1894.503 [-1894.503, -1894.503], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  738/5000: episode: 738, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1810.144, mean reward: -1810.144 [-1810.144, -1810.144], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  739/5000: episode: 739, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3999.792, mean reward: -3999.792 [-3999.792, -3999.792], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  740/5000: episode: 740, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -145.956, mean reward: -145.956 [-145.956, -145.956], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  741/5000: episode: 741, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5255.715, mean reward: -5255.715 [-5255.715, -5255.715], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  742/5000: episode: 742, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5538.340, mean reward: -5538.340 [-5538.340, -5538.340], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  743/5000: episode: 743, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5932.964, mean reward: -5932.964 [-5932.964, -5932.964], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  744/5000: episode: 744, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5794.740, mean reward: -5794.740 [-5794.740, -5794.740], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  745/5000: episode: 745, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4848.618, mean reward: -4848.618 [-4848.618, -4848.618], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  746/5000: episode: 746, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3051.419, mean reward: -3051.419 [-3051.419, -3051.419], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  747/5000: episode: 747, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -790.411, mean reward: -790.411 [-790.411, -790.411], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  748/5000: episode: 748, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2541.719, mean reward: -2541.719 [-2541.719, -2541.719], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  749/5000: episode: 749, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -219.721, mean reward: -219.721 [-219.721, -219.721], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  750/5000: episode: 750, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -136.609, mean reward: -136.609 [-136.609, -136.609], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  751/5000: episode: 751, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -169.185, mean reward: -169.185 [-169.185, -169.185], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  752/5000: episode: 752, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -6530.564, mean reward: -6530.564 [-6530.564, -6530.564], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  753/5000: episode: 753, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1420.959, mean reward: -1420.959 [-1420.959, -1420.959], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  754/5000: episode: 754, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5090.207, mean reward: -5090.207 [-5090.207, -5090.207], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  755/5000: episode: 755, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2410.332, mean reward: -2410.332 [-2410.332, -2410.332], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  756/5000: episode: 756, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2410.713, mean reward: -2410.713 [-2410.713, -2410.713], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  757/5000: episode: 757, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -8337.094, mean reward: -8337.094 [-8337.094, -8337.094], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  758/5000: episode: 758, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3473.688, mean reward: -3473.688 [-3473.688, -3473.688], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  759/5000: episode: 759, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4428.809, mean reward: -4428.809 [-4428.809, -4428.809], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  760/5000: episode: 760, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5070.121, mean reward: -5070.121 [-5070.121, -5070.121], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  761/5000: episode: 761, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -148.040, mean reward: -148.040 [-148.040, -148.040], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  762/5000: episode: 762, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -672.599, mean reward: -672.599 [-672.599, -672.599], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  763/5000: episode: 763, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -186.362, mean reward: -186.362 [-186.362, -186.362], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  764/5000: episode: 764, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -198.836, mean reward: -198.836 [-198.836, -198.836], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  765/5000: episode: 765, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -505.774, mean reward: -505.774 [-505.774, -505.774], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  766/5000: episode: 766, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1467.169, mean reward: -1467.169 [-1467.169, -1467.169], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  767/5000: episode: 767, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6569.434, mean reward: -6569.434 [-6569.434, -6569.434], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  768/5000: episode: 768, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10185.941, mean reward: -10185.941 [-10185.941, -10185.941], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  769/5000: episode: 769, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5104.109, mean reward: -5104.109 [-5104.109, -5104.109], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  770/5000: episode: 770, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3598.422, mean reward: -3598.422 [-3598.422, -3598.422], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  771/5000: episode: 771, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -8238.822, mean reward: -8238.822 [-8238.822, -8238.822], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  772/5000: episode: 772, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3618.149, mean reward: -3618.149 [-3618.149, -3618.149], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  773/5000: episode: 773, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1001.037, mean reward: -1001.037 [-1001.037, -1001.037], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  774/5000: episode: 774, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3843.841, mean reward: -3843.841 [-3843.841, -3843.841], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  775/5000: episode: 775, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1432.482, mean reward: -1432.482 [-1432.482, -1432.482], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  776/5000: episode: 776, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3770.045, mean reward: -3770.045 [-3770.045, -3770.045], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  777/5000: episode: 777, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -12379.891, mean reward: -12379.891 [-12379.891, -12379.891], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  778/5000: episode: 778, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3062.855, mean reward: -3062.855 [-3062.855, -3062.855], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  779/5000: episode: 779, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2036.350, mean reward: -2036.350 [-2036.350, -2036.350], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  780/5000: episode: 780, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -28.184, mean reward: -28.184 [-28.184, -28.184], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  781/5000: episode: 781, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1898.486, mean reward: -1898.486 [-1898.486, -1898.486], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  782/5000: episode: 782, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5108.651, mean reward: -5108.651 [-5108.651, -5108.651], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  783/5000: episode: 783, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1183.350, mean reward: -1183.350 [-1183.350, -1183.350], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  784/5000: episode: 784, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1526.258, mean reward: -1526.258 [-1526.258, -1526.258], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  785/5000: episode: 785, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2423.916, mean reward: -2423.916 [-2423.916, -2423.916], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  786/5000: episode: 786, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4652.141, mean reward: -4652.141 [-4652.141, -4652.141], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  787/5000: episode: 787, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5032.231, mean reward: -5032.231 [-5032.231, -5032.231], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  788/5000: episode: 788, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3075.485, mean reward: -3075.485 [-3075.485, -3075.485], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  789/5000: episode: 789, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2561.272, mean reward: -2561.272 [-2561.272, -2561.272], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  790/5000: episode: 790, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4524.214, mean reward: -4524.214 [-4524.214, -4524.214], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  791/5000: episode: 791, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6742.134, mean reward: -6742.134 [-6742.134, -6742.134], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  792/5000: episode: 792, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1055.687, mean reward: -1055.687 [-1055.687, -1055.687], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  793/5000: episode: 793, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7216.123, mean reward: -7216.123 [-7216.123, -7216.123], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  794/5000: episode: 794, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -6337.048, mean reward: -6337.048 [-6337.048, -6337.048], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  795/5000: episode: 795, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3050.401, mean reward: -3050.401 [-3050.401, -3050.401], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  796/5000: episode: 796, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1240.303, mean reward: -1240.303 [-1240.303, -1240.303], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  797/5000: episode: 797, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4948.342, mean reward: -4948.342 [-4948.342, -4948.342], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  798/5000: episode: 798, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1873.830, mean reward: -1873.830 [-1873.830, -1873.830], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  799/5000: episode: 799, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -1747.756, mean reward: -1747.756 [-1747.756, -1747.756], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  800/5000: episode: 800, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -172.389, mean reward: -172.389 [-172.389, -172.389], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  801/5000: episode: 801, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -7788.768, mean reward: -7788.768 [-7788.768, -7788.768], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  802/5000: episode: 802, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -7439.271, mean reward: -7439.271 [-7439.271, -7439.271], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  803/5000: episode: 803, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3539.771, mean reward: -3539.771 [-3539.771, -3539.771], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  804/5000: episode: 804, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -3076.482, mean reward: -3076.482 [-3076.482, -3076.482], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  805/5000: episode: 805, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -397.823, mean reward: -397.823 [-397.823, -397.823], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  806/5000: episode: 806, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -135.873, mean reward: -135.873 [-135.873, -135.873], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  807/5000: episode: 807, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4957.400, mean reward: -4957.400 [-4957.400, -4957.400], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  808/5000: episode: 808, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -8956.712, mean reward: -8956.712 [-8956.712, -8956.712], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  809/5000: episode: 809, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5142.986, mean reward: -5142.986 [-5142.986, -5142.986], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  810/5000: episode: 810, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5398.351, mean reward: -5398.351 [-5398.351, -5398.351], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  811/5000: episode: 811, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1661.114, mean reward: -1661.114 [-1661.114, -1661.114], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  812/5000: episode: 812, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -867.898, mean reward: -867.898 [-867.898, -867.898], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  813/5000: episode: 813, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2599.437, mean reward: -2599.437 [-2599.437, -2599.437], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  814/5000: episode: 814, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2593.677, mean reward: -2593.677 [-2593.677, -2593.677], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  815/5000: episode: 815, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10.621, mean reward: -10.621 [-10.621, -10.621], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  816/5000: episode: 816, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2420.311, mean reward: -2420.311 [-2420.311, -2420.311], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  817/5000: episode: 817, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2406.674, mean reward: -2406.674 [-2406.674, -2406.674], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  818/5000: episode: 818, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4539.531, mean reward: -4539.531 [-4539.531, -4539.531], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  819/5000: episode: 819, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2074.702, mean reward: -2074.702 [-2074.702, -2074.702], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  820/5000: episode: 820, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -1069.031, mean reward: -1069.031 [-1069.031, -1069.031], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  821/5000: episode: 821, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7618.618, mean reward: -7618.618 [-7618.618, -7618.618], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  822/5000: episode: 822, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4180.815, mean reward: -4180.815 [-4180.815, -4180.815], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  823/5000: episode: 823, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -412.704, mean reward: -412.704 [-412.704, -412.704], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  824/5000: episode: 824, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2359.642, mean reward: -2359.642 [-2359.642, -2359.642], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  825/5000: episode: 825, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -2888.215, mean reward: -2888.215 [-2888.215, -2888.215], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  826/5000: episode: 826, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -729.315, mean reward: -729.315 [-729.315, -729.315], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  827/5000: episode: 827, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9302.389, mean reward: -9302.389 [-9302.389, -9302.389], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  828/5000: episode: 828, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4000.357, mean reward: -4000.357 [-4000.357, -4000.357], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  829/5000: episode: 829, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4871.867, mean reward: -4871.867 [-4871.867, -4871.867], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  830/5000: episode: 830, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2183.111, mean reward: -2183.111 [-2183.111, -2183.111], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  831/5000: episode: 831, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3978.989, mean reward: -3978.989 [-3978.989, -3978.989], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  832/5000: episode: 832, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3197.021, mean reward: -3197.021 [-3197.021, -3197.021], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  833/5000: episode: 833, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -404.118, mean reward: -404.118 [-404.118, -404.118], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  834/5000: episode: 834, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -618.238, mean reward: -618.238 [-618.238, -618.238], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  835/5000: episode: 835, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2903.838, mean reward: -2903.838 [-2903.838, -2903.838], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  836/5000: episode: 836, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1736.265, mean reward: -1736.265 [-1736.265, -1736.265], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  837/5000: episode: 837, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2420.960, mean reward: -2420.960 [-2420.960, -2420.960], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  838/5000: episode: 838, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3193.108, mean reward: -3193.108 [-3193.108, -3193.108], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  839/5000: episode: 839, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5433.089, mean reward: -5433.089 [-5433.089, -5433.089], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  840/5000: episode: 840, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -569.375, mean reward: -569.375 [-569.375, -569.375], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  841/5000: episode: 841, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6798.027, mean reward: -6798.027 [-6798.027, -6798.027], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  842/5000: episode: 842, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4464.675, mean reward: -4464.675 [-4464.675, -4464.675], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  843/5000: episode: 843, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6335.127, mean reward: -6335.127 [-6335.127, -6335.127], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  844/5000: episode: 844, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2648.797, mean reward: -2648.797 [-2648.797, -2648.797], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  845/5000: episode: 845, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -10927.450, mean reward: -10927.450 [-10927.450, -10927.450], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  846/5000: episode: 846, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -35.991, mean reward: -35.991 [-35.991, -35.991], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  847/5000: episode: 847, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2256.006, mean reward: -2256.006 [-2256.006, -2256.006], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  848/5000: episode: 848, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5.632, mean reward: -5.632 [-5.632, -5.632], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  849/5000: episode: 849, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6581.451, mean reward: -6581.451 [-6581.451, -6581.451], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  850/5000: episode: 850, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -110.748, mean reward: -110.748 [-110.748, -110.748], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  851/5000: episode: 851, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5800.392, mean reward: -5800.392 [-5800.392, -5800.392], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  852/5000: episode: 852, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -1056.080, mean reward: -1056.080 [-1056.080, -1056.080], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  853/5000: episode: 853, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3025.045, mean reward: -3025.045 [-3025.045, -3025.045], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  854/5000: episode: 854, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -656.073, mean reward: -656.073 [-656.073, -656.073], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  855/5000: episode: 855, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4066.796, mean reward: -4066.796 [-4066.796, -4066.796], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  856/5000: episode: 856, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -20.988, mean reward: -20.988 [-20.988, -20.988], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  857/5000: episode: 857, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4626.190, mean reward: -4626.190 [-4626.190, -4626.190], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  858/5000: episode: 858, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8968.455, mean reward: -8968.455 [-8968.455, -8968.455], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  859/5000: episode: 859, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8024.688, mean reward: -8024.688 [-8024.688, -8024.688], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  860/5000: episode: 860, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3814.963, mean reward: -3814.963 [-3814.963, -3814.963], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  861/5000: episode: 861, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3412.618, mean reward: -3412.618 [-3412.618, -3412.618], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  862/5000: episode: 862, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4488.290, mean reward: -4488.290 [-4488.290, -4488.290], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  863/5000: episode: 863, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -620.295, mean reward: -620.295 [-620.295, -620.295], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  864/5000: episode: 864, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4566.324, mean reward: -4566.324 [-4566.324, -4566.324], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  865/5000: episode: 865, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1798.135, mean reward: -1798.135 [-1798.135, -1798.135], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  866/5000: episode: 866, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3101.093, mean reward: -3101.093 [-3101.093, -3101.093], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  867/5000: episode: 867, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -2816.675, mean reward: -2816.675 [-2816.675, -2816.675], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  868/5000: episode: 868, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -1262.394, mean reward: -1262.394 [-1262.394, -1262.394], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  869/5000: episode: 869, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2408.437, mean reward: -2408.437 [-2408.437, -2408.437], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  870/5000: episode: 870, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -367.517, mean reward: -367.517 [-367.517, -367.517], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  871/5000: episode: 871, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4632.261, mean reward: -4632.261 [-4632.261, -4632.261], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  872/5000: episode: 872, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3532.906, mean reward: -3532.906 [-3532.906, -3532.906], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  873/5000: episode: 873, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5892.236, mean reward: -5892.236 [-5892.236, -5892.236], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  874/5000: episode: 874, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8240.884, mean reward: -8240.884 [-8240.884, -8240.884], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  875/5000: episode: 875, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -462.550, mean reward: -462.550 [-462.550, -462.550], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  876/5000: episode: 876, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -4193.763, mean reward: -4193.763 [-4193.763, -4193.763], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  877/5000: episode: 877, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2306.217, mean reward: -2306.217 [-2306.217, -2306.217], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  878/5000: episode: 878, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -68.658, mean reward: -68.658 [-68.658, -68.658], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  879/5000: episode: 879, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1710.640, mean reward: -1710.640 [-1710.640, -1710.640], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  880/5000: episode: 880, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -407.482, mean reward: -407.482 [-407.482, -407.482], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  881/5000: episode: 881, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1736.162, mean reward: -1736.162 [-1736.162, -1736.162], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  882/5000: episode: 882, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6121.466, mean reward: -6121.466 [-6121.466, -6121.466], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  883/5000: episode: 883, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8883.610, mean reward: -8883.610 [-8883.610, -8883.610], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  884/5000: episode: 884, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -8259.956, mean reward: -8259.956 [-8259.956, -8259.956], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  885/5000: episode: 885, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9262.286, mean reward: -9262.286 [-9262.286, -9262.286], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  886/5000: episode: 886, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -21.855, mean reward: -21.855 [-21.855, -21.855], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  887/5000: episode: 887, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -727.189, mean reward: -727.189 [-727.189, -727.189], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  888/5000: episode: 888, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -3132.790, mean reward: -3132.790 [-3132.790, -3132.790], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  889/5000: episode: 889, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4407.757, mean reward: -4407.757 [-4407.757, -4407.757], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  890/5000: episode: 890, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -678.045, mean reward: -678.045 [-678.045, -678.045], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  891/5000: episode: 891, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2128.628, mean reward: -2128.628 [-2128.628, -2128.628], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  892/5000: episode: 892, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1502.336, mean reward: -1502.336 [-1502.336, -1502.336], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  893/5000: episode: 893, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -901.926, mean reward: -901.926 [-901.926, -901.926], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  894/5000: episode: 894, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -353.445, mean reward: -353.445 [-353.445, -353.445], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  895/5000: episode: 895, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3737.041, mean reward: -3737.041 [-3737.041, -3737.041], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  896/5000: episode: 896, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1715.363, mean reward: -1715.363 [-1715.363, -1715.363], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  897/5000: episode: 897, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7459.656, mean reward: -7459.656 [-7459.656, -7459.656], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  898/5000: episode: 898, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2994.915, mean reward: -2994.915 [-2994.915, -2994.915], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  899/5000: episode: 899, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -546.335, mean reward: -546.335 [-546.335, -546.335], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  900/5000: episode: 900, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5475.784, mean reward: -5475.784 [-5475.784, -5475.784], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  901/5000: episode: 901, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -3264.933, mean reward: -3264.933 [-3264.933, -3264.933], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  902/5000: episode: 902, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -420.850, mean reward: -420.850 [-420.850, -420.850], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  903/5000: episode: 903, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4976.208, mean reward: -4976.208 [-4976.208, -4976.208], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  904/5000: episode: 904, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -8833.883, mean reward: -8833.883 [-8833.883, -8833.883], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  905/5000: episode: 905, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5053.639, mean reward: -5053.639 [-5053.639, -5053.639], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  906/5000: episode: 906, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -7173.698, mean reward: -7173.698 [-7173.698, -7173.698], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  907/5000: episode: 907, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1312.062, mean reward: -1312.062 [-1312.062, -1312.062], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  908/5000: episode: 908, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1726.399, mean reward: -1726.399 [-1726.399, -1726.399], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  909/5000: episode: 909, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7169.111, mean reward: -7169.111 [-7169.111, -7169.111], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  910/5000: episode: 910, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8063.687, mean reward: -8063.687 [-8063.687, -8063.687], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  911/5000: episode: 911, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -261.638, mean reward: -261.638 [-261.638, -261.638], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  912/5000: episode: 912, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6520.313, mean reward: -6520.313 [-6520.313, -6520.313], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  913/5000: episode: 913, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -1970.733, mean reward: -1970.733 [-1970.733, -1970.733], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  914/5000: episode: 914, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -256.751, mean reward: -256.751 [-256.751, -256.751], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  915/5000: episode: 915, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6377.437, mean reward: -6377.437 [-6377.437, -6377.437], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  916/5000: episode: 916, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4086.863, mean reward: -4086.863 [-4086.863, -4086.863], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  917/5000: episode: 917, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1965.365, mean reward: -1965.365 [-1965.365, -1965.365], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  918/5000: episode: 918, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2037.638, mean reward: -2037.638 [-2037.638, -2037.638], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  919/5000: episode: 919, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2772.251, mean reward: -2772.251 [-2772.251, -2772.251], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  920/5000: episode: 920, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2856.521, mean reward: -2856.521 [-2856.521, -2856.521], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  921/5000: episode: 921, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3096.040, mean reward: -3096.040 [-3096.040, -3096.040], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  922/5000: episode: 922, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -2594.753, mean reward: -2594.753 [-2594.753, -2594.753], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  923/5000: episode: 923, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1842.582, mean reward: -1842.582 [-1842.582, -1842.582], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  924/5000: episode: 924, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2898.661, mean reward: -2898.661 [-2898.661, -2898.661], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  925/5000: episode: 925, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7447.960, mean reward: -7447.960 [-7447.960, -7447.960], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  926/5000: episode: 926, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -28.876, mean reward: -28.876 [-28.876, -28.876], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  927/5000: episode: 927, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6641.786, mean reward: -6641.786 [-6641.786, -6641.786], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  928/5000: episode: 928, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1496.072, mean reward: -1496.072 [-1496.072, -1496.072], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  929/5000: episode: 929, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6311.357, mean reward: -6311.357 [-6311.357, -6311.357], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  930/5000: episode: 930, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -539.322, mean reward: -539.322 [-539.322, -539.322], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  931/5000: episode: 931, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1807.799, mean reward: -1807.799 [-1807.799, -1807.799], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  932/5000: episode: 932, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5.525, mean reward: -5.525 [-5.525, -5.525], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  933/5000: episode: 933, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1539.500, mean reward: -1539.500 [-1539.500, -1539.500], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  934/5000: episode: 934, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -124.552, mean reward: -124.552 [-124.552, -124.552], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  935/5000: episode: 935, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2631.537, mean reward: -2631.537 [-2631.537, -2631.537], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  936/5000: episode: 936, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -380.519, mean reward: -380.519 [-380.519, -380.519], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  937/5000: episode: 937, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -4106.223, mean reward: -4106.223 [-4106.223, -4106.223], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  938/5000: episode: 938, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3036.750, mean reward: -3036.750 [-3036.750, -3036.750], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  939/5000: episode: 939, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -631.757, mean reward: -631.757 [-631.757, -631.757], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  940/5000: episode: 940, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -6098.242, mean reward: -6098.242 [-6098.242, -6098.242], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  941/5000: episode: 941, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -589.896, mean reward: -589.896 [-589.896, -589.896], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  942/5000: episode: 942, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3784.637, mean reward: -3784.637 [-3784.637, -3784.637], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  943/5000: episode: 943, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5563.951, mean reward: -5563.951 [-5563.951, -5563.951], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  944/5000: episode: 944, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -241.124, mean reward: -241.124 [-241.124, -241.124], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  945/5000: episode: 945, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1600.352, mean reward: -1600.352 [-1600.352, -1600.352], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  946/5000: episode: 946, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -10333.321, mean reward: -10333.321 [-10333.321, -10333.321], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  947/5000: episode: 947, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -999.346, mean reward: -999.346 [-999.346, -999.346], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  948/5000: episode: 948, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4541.087, mean reward: -4541.087 [-4541.087, -4541.087], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  949/5000: episode: 949, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -552.231, mean reward: -552.231 [-552.231, -552.231], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  950/5000: episode: 950, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4684.494, mean reward: -4684.494 [-4684.494, -4684.494], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  951/5000: episode: 951, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -165.925, mean reward: -165.925 [-165.925, -165.925], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  952/5000: episode: 952, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -6479.317, mean reward: -6479.317 [-6479.317, -6479.317], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  953/5000: episode: 953, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3403.512, mean reward: -3403.512 [-3403.512, -3403.512], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  954/5000: episode: 954, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2000.017, mean reward: -2000.017 [-2000.017, -2000.017], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  955/5000: episode: 955, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3920.014, mean reward: -3920.014 [-3920.014, -3920.014], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  956/5000: episode: 956, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4645.570, mean reward: -4645.570 [-4645.570, -4645.570], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  957/5000: episode: 957, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4876.757, mean reward: -4876.757 [-4876.757, -4876.757], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  958/5000: episode: 958, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -322.745, mean reward: -322.745 [-322.745, -322.745], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  959/5000: episode: 959, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3801.856, mean reward: -3801.856 [-3801.856, -3801.856], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  960/5000: episode: 960, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1641.935, mean reward: -1641.935 [-1641.935, -1641.935], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  961/5000: episode: 961, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3860.498, mean reward: -3860.498 [-3860.498, -3860.498], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  962/5000: episode: 962, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -84.267, mean reward: -84.267 [-84.267, -84.267], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  963/5000: episode: 963, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -10124.324, mean reward: -10124.324 [-10124.324, -10124.324], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  964/5000: episode: 964, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -6864.135, mean reward: -6864.135 [-6864.135, -6864.135], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  965/5000: episode: 965, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2688.114, mean reward: -2688.114 [-2688.114, -2688.114], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  966/5000: episode: 966, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1507.863, mean reward: -1507.863 [-1507.863, -1507.863], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  967/5000: episode: 967, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2669.571, mean reward: -2669.571 [-2669.571, -2669.571], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  968/5000: episode: 968, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1486.253, mean reward: -1486.253 [-1486.253, -1486.253], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  969/5000: episode: 969, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7689.178, mean reward: -7689.178 [-7689.178, -7689.178], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  970/5000: episode: 970, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -920.621, mean reward: -920.621 [-920.621, -920.621], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  971/5000: episode: 971, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2318.056, mean reward: -2318.056 [-2318.056, -2318.056], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  972/5000: episode: 972, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5677.713, mean reward: -5677.713 [-5677.713, -5677.713], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  973/5000: episode: 973, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9146.575, mean reward: -9146.575 [-9146.575, -9146.575], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  974/5000: episode: 974, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3772.805, mean reward: -3772.805 [-3772.805, -3772.805], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  975/5000: episode: 975, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -7.314, mean reward: -7.314 [-7.314, -7.314], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  976/5000: episode: 976, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -751.533, mean reward: -751.533 [-751.533, -751.533], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  977/5000: episode: 977, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -9857.978, mean reward: -9857.978 [-9857.978, -9857.978], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  978/5000: episode: 978, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -3488.681, mean reward: -3488.681 [-3488.681, -3488.681], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  979/5000: episode: 979, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -6218.807, mean reward: -6218.807 [-6218.807, -6218.807], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  980/5000: episode: 980, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3995.041, mean reward: -3995.041 [-3995.041, -3995.041], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  981/5000: episode: 981, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3468.802, mean reward: -3468.802 [-3468.802, -3468.802], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  982/5000: episode: 982, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -391.275, mean reward: -391.275 [-391.275, -391.275], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  983/5000: episode: 983, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1902.447, mean reward: -1902.447 [-1902.447, -1902.447], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  984/5000: episode: 984, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3080.604, mean reward: -3080.604 [-3080.604, -3080.604], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  985/5000: episode: 985, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -159.859, mean reward: -159.859 [-159.859, -159.859], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  986/5000: episode: 986, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3627.773, mean reward: -3627.773 [-3627.773, -3627.773], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  987/5000: episode: 987, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3723.709, mean reward: -3723.709 [-3723.709, -3723.709], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  988/5000: episode: 988, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4533.453, mean reward: -4533.453 [-4533.453, -4533.453], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  989/5000: episode: 989, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2084.526, mean reward: -2084.526 [-2084.526, -2084.526], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  990/5000: episode: 990, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3652.847, mean reward: -3652.847 [-3652.847, -3652.847], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  991/5000: episode: 991, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -632.550, mean reward: -632.550 [-632.550, -632.550], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  992/5000: episode: 992, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2003.575, mean reward: -2003.575 [-2003.575, -2003.575], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  993/5000: episode: 993, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1431.537, mean reward: -1431.537 [-1431.537, -1431.537], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  994/5000: episode: 994, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1391.970, mean reward: -1391.970 [-1391.970, -1391.970], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  995/5000: episode: 995, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4590.213, mean reward: -4590.213 [-4590.213, -4590.213], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  996/5000: episode: 996, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4838.715, mean reward: -4838.715 [-4838.715, -4838.715], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  997/5000: episode: 997, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1443.280, mean reward: -1443.280 [-1443.280, -1443.280], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  998/5000: episode: 998, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -868.017, mean reward: -868.017 [-868.017, -868.017], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  999/5000: episode: 999, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -53.656, mean reward: -53.656 [-53.656, -53.656], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 1000/5000: episode: 1000, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6912.481, mean reward: -6912.481 [-6912.481, -6912.481], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 1001/5000: episode: 1001, duration: 0.395s, episode steps:   1, steps per second:   3, episode reward: -2217.997, mean reward: -2217.997 [-2217.997, -2217.997], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 1002/5000: episode: 1002, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1660.029, mean reward: -1660.029 [-1660.029, -1660.029], mean action: 0.000 [0.000, 0.000],  loss: 12709624.000000, mae: 948.272339, mean_q: 1.376125
 1003/5000: episode: 1003, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -711.203, mean reward: -711.203 [-711.203, -711.203], mean action: 2.000 [2.000, 2.000],  loss: 8993490.000000, mae: 868.353455, mean_q: 1.088340
 1004/5000: episode: 1004, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1661.948, mean reward: -1661.948 [-1661.948, -1661.948], mean action: 2.000 [2.000, 2.000],  loss: 9462728.000000, mae: 822.106445, mean_q: 1.313580
 1005/5000: episode: 1005, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3277.368, mean reward: -3277.368 [-3277.368, -3277.368], mean action: 2.000 [2.000, 2.000],  loss: 6491754.500000, mae: 735.726562, mean_q: 1.542005
 1006/5000: episode: 1006, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1830.068, mean reward: -1830.068 [-1830.068, -1830.068], mean action: 2.000 [2.000, 2.000],  loss: 7413342.000000, mae: 753.656433, mean_q: 1.620569
 1007/5000: episode: 1007, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -710.615, mean reward: -710.615 [-710.615, -710.615], mean action: 2.000 [2.000, 2.000],  loss: 8353813.000000, mae: 836.211121, mean_q: 1.506415
 1008/5000: episode: 1008, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2000.501, mean reward: -2000.501 [-2000.501, -2000.501], mean action: 2.000 [2.000, 2.000],  loss: 8923193.000000, mae: 814.320557, mean_q: 1.219899
 1009/5000: episode: 1009, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5618.792, mean reward: -5618.792 [-5618.792, -5618.792], mean action: 2.000 [2.000, 2.000],  loss: 5506651.000000, mae: 599.153381, mean_q: 0.811272
 1010/5000: episode: 1010, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3118.996, mean reward: -3118.996 [-3118.996, -3118.996], mean action: 2.000 [2.000, 2.000],  loss: 10259252.000000, mae: 888.348389, mean_q: 0.139857
 1011/5000: episode: 1011, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -8629.444, mean reward: -8629.444 [-8629.444, -8629.444], mean action: 2.000 [2.000, 2.000],  loss: 9250267.000000, mae: 839.081421, mean_q: -0.856665
 1012/5000: episode: 1012, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5.614, mean reward: -5.614 [-5.614, -5.614], mean action: 2.000 [2.000, 2.000],  loss: 13776274.000000, mae: 1096.651123, mean_q: -1.991277
 1013/5000: episode: 1013, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -1176.693, mean reward: -1176.693 [-1176.693, -1176.693], mean action: 2.000 [2.000, 2.000],  loss: 10946735.000000, mae: 920.023560, mean_q: -3.457906
 1014/5000: episode: 1014, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -13276.867, mean reward: -13276.867 [-13276.867, -13276.867], mean action: 0.000 [0.000, 0.000],  loss: 9257788.000000, mae: 889.136230, mean_q: -5.124282
 1015/5000: episode: 1015, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -540.032, mean reward: -540.032 [-540.032, -540.032], mean action: 2.000 [2.000, 2.000],  loss: 8407953.000000, mae: 855.095764, mean_q: -6.868109
 1016/5000: episode: 1016, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4016.622, mean reward: -4016.622 [-4016.622, -4016.622], mean action: 2.000 [2.000, 2.000],  loss: 13561452.000000, mae: 1036.419678, mean_q: -8.787727
 1017/5000: episode: 1017, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -499.573, mean reward: -499.573 [-499.573, -499.573], mean action: 2.000 [2.000, 2.000],  loss: 8774285.000000, mae: 893.749878, mean_q: -11.225240
 1018/5000: episode: 1018, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7359.561, mean reward: -7359.561 [-7359.561, -7359.561], mean action: 2.000 [2.000, 2.000],  loss: 15559946.000000, mae: 1148.282715, mean_q: -13.757505
 1019/5000: episode: 1019, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -298.312, mean reward: -298.312 [-298.312, -298.312], mean action: 2.000 [2.000, 2.000],  loss: 14499512.000000, mae: 1071.477539, mean_q: -16.572874
 1020/5000: episode: 1020, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1518.307, mean reward: -1518.307 [-1518.307, -1518.307], mean action: 2.000 [2.000, 2.000],  loss: 8243929.500000, mae: 855.767029, mean_q: -19.662844
 1021/5000: episode: 1021, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -172.904, mean reward: -172.904 [-172.904, -172.904], mean action: 2.000 [2.000, 2.000],  loss: 10173044.000000, mae: 859.358765, mean_q: -22.962788
 1022/5000: episode: 1022, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4448.259, mean reward: -4448.259 [-4448.259, -4448.259], mean action: 2.000 [2.000, 2.000],  loss: 9574135.000000, mae: 924.036377, mean_q: -26.894852
 1023/5000: episode: 1023, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2422.403, mean reward: -2422.403 [-2422.403, -2422.403], mean action: 2.000 [2.000, 2.000],  loss: 11909826.000000, mae: 870.554077, mean_q: -30.767714
 1024/5000: episode: 1024, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3371.125, mean reward: -3371.125 [-3371.125, -3371.125], mean action: 2.000 [2.000, 2.000],  loss: 10843723.000000, mae: 1000.539368, mean_q: -34.853092
 1025/5000: episode: 1025, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3060.324, mean reward: -3060.324 [-3060.324, -3060.324], mean action: 2.000 [2.000, 2.000],  loss: 7191357.000000, mae: 782.174927, mean_q: -38.600426
 1026/5000: episode: 1026, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1353.986, mean reward: -1353.986 [-1353.986, -1353.986], mean action: 2.000 [2.000, 2.000],  loss: 11703242.000000, mae: 1002.956482, mean_q: -42.700562
 1027/5000: episode: 1027, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1377.612, mean reward: -1377.612 [-1377.612, -1377.612], mean action: 2.000 [2.000, 2.000],  loss: 9368148.000000, mae: 883.088379, mean_q: -46.735794
 1028/5000: episode: 1028, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -9936.999, mean reward: -9936.999 [-9936.999, -9936.999], mean action: 1.000 [1.000, 1.000],  loss: 7562626.500000, mae: 795.860962, mean_q: -50.878571
 1029/5000: episode: 1029, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5186.071, mean reward: -5186.071 [-5186.071, -5186.071], mean action: 2.000 [2.000, 2.000],  loss: 13211550.000000, mae: 1067.873291, mean_q: -55.504311
 1030/5000: episode: 1030, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2969.028, mean reward: -2969.028 [-2969.028, -2969.028], mean action: 2.000 [2.000, 2.000],  loss: 4686232.000000, mae: 634.753540, mean_q: -60.351494
 1031/5000: episode: 1031, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3625.693, mean reward: -3625.693 [-3625.693, -3625.693], mean action: 2.000 [2.000, 2.000],  loss: 10269132.000000, mae: 984.906128, mean_q: -66.578140
 1032/5000: episode: 1032, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1222.243, mean reward: -1222.243 [-1222.243, -1222.243], mean action: 2.000 [2.000, 2.000],  loss: 11018282.000000, mae: 966.305908, mean_q: -73.950928
 1033/5000: episode: 1033, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -6309.032, mean reward: -6309.032 [-6309.032, -6309.032], mean action: 2.000 [2.000, 2.000],  loss: 10478636.000000, mae: 817.065186, mean_q: -80.567337
 1034/5000: episode: 1034, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4768.017, mean reward: -4768.017 [-4768.017, -4768.017], mean action: 2.000 [2.000, 2.000],  loss: 12654640.000000, mae: 1056.499146, mean_q: -87.820053
 1035/5000: episode: 1035, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2980.478, mean reward: -2980.478 [-2980.478, -2980.478], mean action: 2.000 [2.000, 2.000],  loss: 17084408.000000, mae: 1200.945679, mean_q: -95.718399
 1036/5000: episode: 1036, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4881.207, mean reward: -4881.207 [-4881.207, -4881.207], mean action: 2.000 [2.000, 2.000],  loss: 7424006.000000, mae: 798.841858, mean_q: -103.151611
 1037/5000: episode: 1037, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3038.493, mean reward: -3038.493 [-3038.493, -3038.493], mean action: 2.000 [2.000, 2.000],  loss: 7202119.000000, mae: 822.005615, mean_q: -112.258026
 1038/5000: episode: 1038, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3286.716, mean reward: -3286.716 [-3286.716, -3286.716], mean action: 2.000 [2.000, 2.000],  loss: 9503716.000000, mae: 984.703491, mean_q: -122.262878
 1039/5000: episode: 1039, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -710.911, mean reward: -710.911 [-710.911, -710.911], mean action: 2.000 [2.000, 2.000],  loss: 7197220.500000, mae: 813.266113, mean_q: -132.715973
 1040/5000: episode: 1040, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2182.161, mean reward: -2182.161 [-2182.161, -2182.161], mean action: 2.000 [2.000, 2.000],  loss: 15532741.000000, mae: 1231.510010, mean_q: -142.761719
 1041/5000: episode: 1041, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -93.986, mean reward: -93.986 [-93.986, -93.986], mean action: 2.000 [2.000, 2.000],  loss: 9610617.000000, mae: 964.713379, mean_q: -153.636505
 1042/5000: episode: 1042, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3033.983, mean reward: -3033.983 [-3033.983, -3033.983], mean action: 2.000 [2.000, 2.000],  loss: 14209710.000000, mae: 1162.278809, mean_q: -163.285980
 1043/5000: episode: 1043, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -2386.708, mean reward: -2386.708 [-2386.708, -2386.708], mean action: 2.000 [2.000, 2.000],  loss: 8092840.500000, mae: 926.943237, mean_q: -174.151367
 1044/5000: episode: 1044, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -1018.386, mean reward: -1018.386 [-1018.386, -1018.386], mean action: 2.000 [2.000, 2.000],  loss: 6787152.000000, mae: 869.370422, mean_q: -185.823029
 1045/5000: episode: 1045, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -462.645, mean reward: -462.645 [-462.645, -462.645], mean action: 2.000 [2.000, 2.000],  loss: 10253321.000000, mae: 1040.685547, mean_q: -197.963760
 1046/5000: episode: 1046, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -4422.198, mean reward: -4422.198 [-4422.198, -4422.198], mean action: 2.000 [2.000, 2.000],  loss: 12641757.000000, mae: 1146.105957, mean_q: -210.762711
 1047/5000: episode: 1047, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5417.233, mean reward: -5417.233 [-5417.233, -5417.233], mean action: 2.000 [2.000, 2.000],  loss: 6557005.000000, mae: 887.389709, mean_q: -223.431137
 1048/5000: episode: 1048, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2051.771, mean reward: -2051.771 [-2051.771, -2051.771], mean action: 2.000 [2.000, 2.000],  loss: 9864612.000000, mae: 1049.915649, mean_q: -235.336487
 1049/5000: episode: 1049, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1515.782, mean reward: -1515.782 [-1515.782, -1515.782], mean action: 2.000 [2.000, 2.000],  loss: 6934805.000000, mae: 931.656799, mean_q: -249.169708
 1050/5000: episode: 1050, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2361.044, mean reward: -2361.044 [-2361.044, -2361.044], mean action: 2.000 [2.000, 2.000],  loss: 13208212.000000, mae: 1257.595459, mean_q: -262.744446
 1051/5000: episode: 1051, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1851.139, mean reward: -1851.139 [-1851.139, -1851.139], mean action: 2.000 [2.000, 2.000],  loss: 13339096.000000, mae: 1086.468750, mean_q: -274.362183
 1052/5000: episode: 1052, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -3694.333, mean reward: -3694.333 [-3694.333, -3694.333], mean action: 2.000 [2.000, 2.000],  loss: 4224424.000000, mae: 788.220764, mean_q: -289.434906
 1053/5000: episode: 1053, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4489.332, mean reward: -4489.332 [-4489.332, -4489.332], mean action: 2.000 [2.000, 2.000],  loss: 6694258.500000, mae: 899.233276, mean_q: -304.376007
 1054/5000: episode: 1054, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5559.699, mean reward: -5559.699 [-5559.699, -5559.699], mean action: 2.000 [2.000, 2.000],  loss: 11163330.000000, mae: 1151.006104, mean_q: -321.018982
 1055/5000: episode: 1055, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2913.374, mean reward: -2913.374 [-2913.374, -2913.374], mean action: 1.000 [1.000, 1.000],  loss: 13656912.000000, mae: 1314.969604, mean_q: -334.755432
 1056/5000: episode: 1056, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5946.087, mean reward: -5946.087 [-5946.087, -5946.087], mean action: 1.000 [1.000, 1.000],  loss: 6823439.500000, mae: 952.445007, mean_q: -349.329163
 1057/5000: episode: 1057, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5158.007, mean reward: -5158.007 [-5158.007, -5158.007], mean action: 1.000 [1.000, 1.000],  loss: 6059276.500000, mae: 990.506775, mean_q: -366.977051
 1058/5000: episode: 1058, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4434.717, mean reward: -4434.717 [-4434.717, -4434.717], mean action: 1.000 [1.000, 1.000],  loss: 7331350.000000, mae: 987.741882, mean_q: -380.027374
 1059/5000: episode: 1059, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9282.609, mean reward: -9282.609 [-9282.609, -9282.609], mean action: 1.000 [1.000, 1.000],  loss: 11390818.000000, mae: 1140.779297, mean_q: -396.261658
 1060/5000: episode: 1060, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4466.400, mean reward: -4466.400 [-4466.400, -4466.400], mean action: 1.000 [1.000, 1.000],  loss: 6752740.000000, mae: 983.574951, mean_q: -413.927063
 1061/5000: episode: 1061, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2797.134, mean reward: -2797.134 [-2797.134, -2797.134], mean action: 1.000 [1.000, 1.000],  loss: 4000414.500000, mae: 899.256104, mean_q: -429.523102
 1062/5000: episode: 1062, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3794.308, mean reward: -3794.308 [-3794.308, -3794.308], mean action: 1.000 [1.000, 1.000],  loss: 6673409.000000, mae: 1093.997314, mean_q: -446.693848
 1063/5000: episode: 1063, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4575.784, mean reward: -4575.784 [-4575.784, -4575.784], mean action: 1.000 [1.000, 1.000],  loss: 7697945.500000, mae: 1113.431885, mean_q: -465.057159
 1064/5000: episode: 1064, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1360.875, mean reward: -1360.875 [-1360.875, -1360.875], mean action: 1.000 [1.000, 1.000],  loss: 11232994.000000, mae: 1247.296021, mean_q: -480.548767
 1065/5000: episode: 1065, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3222.385, mean reward: -3222.385 [-3222.385, -3222.385], mean action: 1.000 [1.000, 1.000],  loss: 6660974.000000, mae: 1036.758789, mean_q: -498.406982
 1066/5000: episode: 1066, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -8716.915, mean reward: -8716.915 [-8716.915, -8716.915], mean action: 1.000 [1.000, 1.000],  loss: 11707464.000000, mae: 1225.923950, mean_q: -517.293945
 1067/5000: episode: 1067, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2389.158, mean reward: -2389.158 [-2389.158, -2389.158], mean action: 1.000 [1.000, 1.000],  loss: 7998592.000000, mae: 1112.334473, mean_q: -535.674316
 1068/5000: episode: 1068, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -9209.067, mean reward: -9209.067 [-9209.067, -9209.067], mean action: 1.000 [1.000, 1.000],  loss: 8126370.000000, mae: 1143.893799, mean_q: -552.639893
 1069/5000: episode: 1069, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7055.307, mean reward: -7055.307 [-7055.307, -7055.307], mean action: 1.000 [1.000, 1.000],  loss: 7614533.500000, mae: 1063.109375, mean_q: -568.443542
 1070/5000: episode: 1070, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3446.522, mean reward: -3446.522 [-3446.522, -3446.522], mean action: 1.000 [1.000, 1.000],  loss: 7316120.000000, mae: 1094.858032, mean_q: -586.045959
 1071/5000: episode: 1071, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5108.627, mean reward: -5108.627 [-5108.627, -5108.627], mean action: 1.000 [1.000, 1.000],  loss: 8144069.000000, mae: 1258.469971, mean_q: -605.393982
 1072/5000: episode: 1072, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -7865.911, mean reward: -7865.911 [-7865.911, -7865.911], mean action: 1.000 [1.000, 1.000],  loss: 7002795.000000, mae: 1194.166504, mean_q: -622.142212
 1073/5000: episode: 1073, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5289.035, mean reward: -5289.035 [-5289.035, -5289.035], mean action: 0.000 [0.000, 0.000],  loss: 13277289.000000, mae: 1431.011230, mean_q: -647.266296
 1074/5000: episode: 1074, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8846.448, mean reward: -8846.448 [-8846.448, -8846.448], mean action: 0.000 [0.000, 0.000],  loss: 7331178.000000, mae: 1203.147705, mean_q: -666.479248
 1075/5000: episode: 1075, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7358.675, mean reward: -7358.675 [-7358.675, -7358.675], mean action: 0.000 [0.000, 0.000],  loss: 6496583.500000, mae: 1242.724609, mean_q: -681.747559
 1076/5000: episode: 1076, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10002.732, mean reward: -10002.732 [-10002.732, -10002.732], mean action: 0.000 [0.000, 0.000],  loss: 8283377.500000, mae: 1194.044434, mean_q: -701.049561
 1077/5000: episode: 1077, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1027.499, mean reward: -1027.499 [-1027.499, -1027.499], mean action: 2.000 [2.000, 2.000],  loss: 6443175.500000, mae: 1247.436646, mean_q: -714.394653
 1078/5000: episode: 1078, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -13032.517, mean reward: -13032.517 [-13032.517, -13032.517], mean action: 0.000 [0.000, 0.000],  loss: 7856315.500000, mae: 1363.260254, mean_q: -741.678284
 1079/5000: episode: 1079, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3849.438, mean reward: -3849.438 [-3849.438, -3849.438], mean action: 0.000 [0.000, 0.000],  loss: 8723684.000000, mae: 1359.399048, mean_q: -759.311340
 1080/5000: episode: 1080, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9859.823, mean reward: -9859.823 [-9859.823, -9859.823], mean action: 0.000 [0.000, 0.000],  loss: 7715571.000000, mae: 1352.405029, mean_q: -780.671021
 1081/5000: episode: 1081, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7925.528, mean reward: -7925.528 [-7925.528, -7925.528], mean action: 1.000 [1.000, 1.000],  loss: 9114338.000000, mae: 1436.137695, mean_q: -803.824463
 1082/5000: episode: 1082, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -9528.142, mean reward: -9528.142 [-9528.142, -9528.142], mean action: 0.000 [0.000, 0.000],  loss: 4349096.500000, mae: 1233.626709, mean_q: -828.152100
 1083/5000: episode: 1083, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -12340.994, mean reward: -12340.994 [-12340.994, -12340.994], mean action: 0.000 [0.000, 0.000],  loss: 6963418.000000, mae: 1389.920654, mean_q: -850.078064
 1084/5000: episode: 1084, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8181.582, mean reward: -8181.582 [-8181.582, -8181.582], mean action: 0.000 [0.000, 0.000],  loss: 4750333.500000, mae: 1279.875488, mean_q: -868.883911
 1085/5000: episode: 1085, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4806.551, mean reward: -4806.551 [-4806.551, -4806.551], mean action: 0.000 [0.000, 0.000],  loss: 11534106.000000, mae: 1600.733032, mean_q: -887.954468
 1086/5000: episode: 1086, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9163.251, mean reward: -9163.251 [-9163.251, -9163.251], mean action: 0.000 [0.000, 0.000],  loss: 5409364.500000, mae: 1335.357178, mean_q: -920.258911
 1087/5000: episode: 1087, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4367.579, mean reward: -4367.579 [-4367.579, -4367.579], mean action: 0.000 [0.000, 0.000],  loss: 5784947.000000, mae: 1422.232056, mean_q: -943.808350
 1088/5000: episode: 1088, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9542.478, mean reward: -9542.478 [-9542.478, -9542.478], mean action: 0.000 [0.000, 0.000],  loss: 5141718.000000, mae: 1452.862793, mean_q: -978.089478
 1089/5000: episode: 1089, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3194.139, mean reward: -3194.139 [-3194.139, -3194.139], mean action: 1.000 [1.000, 1.000],  loss: 8223441.500000, mae: 1538.599609, mean_q: -995.379333
 1090/5000: episode: 1090, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -11363.765, mean reward: -11363.765 [-11363.765, -11363.765], mean action: 0.000 [0.000, 0.000],  loss: 4147999.250000, mae: 1315.310547, mean_q: -1026.688965
 1091/5000: episode: 1091, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6225.170, mean reward: -6225.170 [-6225.170, -6225.170], mean action: 0.000 [0.000, 0.000],  loss: 7146231.000000, mae: 1535.421631, mean_q: -1053.781372
 1092/5000: episode: 1092, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9673.131, mean reward: -9673.131 [-9673.131, -9673.131], mean action: 0.000 [0.000, 0.000],  loss: 8062485.000000, mae: 1636.719727, mean_q: -1085.858398
 1093/5000: episode: 1093, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -4840.156, mean reward: -4840.156 [-4840.156, -4840.156], mean action: 0.000 [0.000, 0.000],  loss: 6505903.000000, mae: 1572.749023, mean_q: -1112.090088
 1094/5000: episode: 1094, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8679.989, mean reward: -8679.989 [-8679.989, -8679.989], mean action: 0.000 [0.000, 0.000],  loss: 10665876.000000, mae: 1780.640625, mean_q: -1142.074463
 1095/5000: episode: 1095, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7287.345, mean reward: -7287.345 [-7287.345, -7287.345], mean action: 0.000 [0.000, 0.000],  loss: 2815519.000000, mae: 1423.081787, mean_q: -1172.404175
 1096/5000: episode: 1096, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4425.577, mean reward: -4425.577 [-4425.577, -4425.577], mean action: 0.000 [0.000, 0.000],  loss: 6384856.000000, mae: 1614.481445, mean_q: -1202.723633
 1097/5000: episode: 1097, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -13705.074, mean reward: -13705.074 [-13705.074, -13705.074], mean action: 0.000 [0.000, 0.000],  loss: 6060203.000000, mae: 1545.511841, mean_q: -1234.766235
 1098/5000: episode: 1098, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5423.415, mean reward: -5423.415 [-5423.415, -5423.415], mean action: 0.000 [0.000, 0.000],  loss: 7263776.000000, mae: 1744.162231, mean_q: -1258.343140
 1099/5000: episode: 1099, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -10536.085, mean reward: -10536.085 [-10536.085, -10536.085], mean action: 0.000 [0.000, 0.000],  loss: 5165290.000000, mae: 1657.339478, mean_q: -1297.301514
 1100/5000: episode: 1100, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10749.310, mean reward: -10749.310 [-10749.310, -10749.310], mean action: 0.000 [0.000, 0.000],  loss: 5609321.000000, mae: 1608.925781, mean_q: -1318.015869
 1101/5000: episode: 1101, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3554.700, mean reward: -3554.700 [-3554.700, -3554.700], mean action: 0.000 [0.000, 0.000],  loss: 8450848.000000, mae: 1782.048950, mean_q: -1342.062988
 1102/5000: episode: 1102, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7190.951, mean reward: -7190.951 [-7190.951, -7190.951], mean action: 0.000 [0.000, 0.000],  loss: 6023336.500000, mae: 1695.868652, mean_q: -1375.912964
 1103/5000: episode: 1103, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1353.557, mean reward: -1353.557 [-1353.557, -1353.557], mean action: 0.000 [0.000, 0.000],  loss: 6280494.500000, mae: 1778.294922, mean_q: -1414.980957
 1104/5000: episode: 1104, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5227.794, mean reward: -5227.794 [-5227.794, -5227.794], mean action: 0.000 [0.000, 0.000],  loss: 6052897.000000, mae: 1743.430298, mean_q: -1432.888672
 1105/5000: episode: 1105, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -11303.326, mean reward: -11303.326 [-11303.326, -11303.326], mean action: 0.000 [0.000, 0.000],  loss: 3533885.500000, mae: 1662.989990, mean_q: -1464.084473
 1106/5000: episode: 1106, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7892.942, mean reward: -7892.942 [-7892.942, -7892.942], mean action: 0.000 [0.000, 0.000],  loss: 7878627.500000, mae: 1854.496094, mean_q: -1493.280884
 1107/5000: episode: 1107, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2922.939, mean reward: -2922.939 [-2922.939, -2922.939], mean action: 0.000 [0.000, 0.000],  loss: 6783164.500000, mae: 1870.783447, mean_q: -1525.193359
 1108/5000: episode: 1108, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4704.672, mean reward: -4704.672 [-4704.672, -4704.672], mean action: 2.000 [2.000, 2.000],  loss: 7324639.000000, mae: 1913.264282, mean_q: -1562.496094
 1109/5000: episode: 1109, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2091.548, mean reward: -2091.548 [-2091.548, -2091.548], mean action: 2.000 [2.000, 2.000],  loss: 6722642.000000, mae: 1902.415527, mean_q: -1576.663574
 1110/5000: episode: 1110, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1166.574, mean reward: -1166.574 [-1166.574, -1166.574], mean action: 2.000 [2.000, 2.000],  loss: 7383702.000000, mae: 1906.811279, mean_q: -1596.783203
 1111/5000: episode: 1111, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2988.179, mean reward: -2988.179 [-2988.179, -2988.179], mean action: 2.000 [2.000, 2.000],  loss: 5039818.000000, mae: 1868.738281, mean_q: -1633.333984
 1112/5000: episode: 1112, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1019.061, mean reward: -1019.061 [-1019.061, -1019.061], mean action: 2.000 [2.000, 2.000],  loss: 6834945.000000, mae: 1988.844116, mean_q: -1657.330566
 1113/5000: episode: 1113, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3167.130, mean reward: -3167.130 [-3167.130, -3167.130], mean action: 2.000 [2.000, 2.000],  loss: 4457209.000000, mae: 1910.179565, mean_q: -1675.105469
 1114/5000: episode: 1114, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6783.528, mean reward: -6783.528 [-6783.528, -6783.528], mean action: 2.000 [2.000, 2.000],  loss: 5968781.500000, mae: 1964.050537, mean_q: -1703.432129
 1115/5000: episode: 1115, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1729.412, mean reward: -1729.412 [-1729.412, -1729.412], mean action: 2.000 [2.000, 2.000],  loss: 4432107.500000, mae: 1943.985352, mean_q: -1726.188477
 1116/5000: episode: 1116, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2364.642, mean reward: -2364.642 [-2364.642, -2364.642], mean action: 2.000 [2.000, 2.000],  loss: 3668984.500000, mae: 1890.867920, mean_q: -1761.010254
 1117/5000: episode: 1117, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -496.900, mean reward: -496.900 [-496.900, -496.900], mean action: 2.000 [2.000, 2.000],  loss: 4262720.000000, mae: 1985.481689, mean_q: -1780.610352
 1118/5000: episode: 1118, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -809.771, mean reward: -809.771 [-809.771, -809.771], mean action: 2.000 [2.000, 2.000],  loss: 5701638.500000, mae: 2051.866211, mean_q: -1797.272461
 1119/5000: episode: 1119, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2756.373, mean reward: -2756.373 [-2756.373, -2756.373], mean action: 2.000 [2.000, 2.000],  loss: 6408453.000000, mae: 2074.881348, mean_q: -1828.673340
 1120/5000: episode: 1120, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -134.406, mean reward: -134.406 [-134.406, -134.406], mean action: 2.000 [2.000, 2.000],  loss: 6047277.000000, mae: 2102.950195, mean_q: -1840.530762
 1121/5000: episode: 1121, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2579.600, mean reward: -2579.600 [-2579.600, -2579.600], mean action: 2.000 [2.000, 2.000],  loss: 6360405.000000, mae: 2200.426514, mean_q: -1868.729980
 1122/5000: episode: 1122, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5812.726, mean reward: -5812.726 [-5812.726, -5812.726], mean action: 2.000 [2.000, 2.000],  loss: 3990245.500000, mae: 2071.729980, mean_q: -1885.247559
 1123/5000: episode: 1123, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4114.191, mean reward: -4114.191 [-4114.191, -4114.191], mean action: 2.000 [2.000, 2.000],  loss: 3808206.250000, mae: 2020.931396, mean_q: -1912.180298
 1124/5000: episode: 1124, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -889.690, mean reward: -889.690 [-889.690, -889.690], mean action: 2.000 [2.000, 2.000],  loss: 9501408.000000, mae: 2338.572266, mean_q: -1944.258057
 1125/5000: episode: 1125, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5543.807, mean reward: -5543.807 [-5543.807, -5543.807], mean action: 2.000 [2.000, 2.000],  loss: 3726149.250000, mae: 2079.757812, mean_q: -1969.371338
 1126/5000: episode: 1126, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2201.607, mean reward: -2201.607 [-2201.607, -2201.607], mean action: 3.000 [3.000, 3.000],  loss: 5794066.000000, mae: 2175.706055, mean_q: -1986.617920
 1127/5000: episode: 1127, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2829.982, mean reward: -2829.982 [-2829.982, -2829.982], mean action: 2.000 [2.000, 2.000],  loss: 3156533.500000, mae: 2148.114502, mean_q: -2017.888062
 1128/5000: episode: 1128, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -951.796, mean reward: -951.796 [-951.796, -951.796], mean action: 2.000 [2.000, 2.000],  loss: 2245655.750000, mae: 2065.209229, mean_q: -2032.742920
 1129/5000: episode: 1129, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -280.451, mean reward: -280.451 [-280.451, -280.451], mean action: 2.000 [2.000, 2.000],  loss: 6465292.000000, mae: 2292.709717, mean_q: -2055.477051
 1130/5000: episode: 1130, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6507.878, mean reward: -6507.878 [-6507.878, -6507.878], mean action: 2.000 [2.000, 2.000],  loss: 5641305.000000, mae: 2328.120605, mean_q: -2091.350586
 1131/5000: episode: 1131, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1566.550, mean reward: -1566.550 [-1566.550, -1566.550], mean action: 2.000 [2.000, 2.000],  loss: 4242986.500000, mae: 2275.014648, mean_q: -2107.166992
 1132/5000: episode: 1132, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -78.022, mean reward: -78.022 [-78.022, -78.022], mean action: 2.000 [2.000, 2.000],  loss: 6028019.000000, mae: 2427.114746, mean_q: -2124.451416
 1133/5000: episode: 1133, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7337.494, mean reward: -7337.494 [-7337.494, -7337.494], mean action: 2.000 [2.000, 2.000],  loss: 3776084.500000, mae: 2302.729492, mean_q: -2148.382812
 1134/5000: episode: 1134, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1192.346, mean reward: -1192.346 [-1192.346, -1192.346], mean action: 2.000 [2.000, 2.000],  loss: 4427551.000000, mae: 2377.039551, mean_q: -2174.954102
 1135/5000: episode: 1135, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -228.724, mean reward: -228.724 [-228.724, -228.724], mean action: 2.000 [2.000, 2.000],  loss: 2849376.000000, mae: 2255.835449, mean_q: -2210.414551
 1136/5000: episode: 1136, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1424.217, mean reward: -1424.217 [-1424.217, -1424.217], mean action: 2.000 [2.000, 2.000],  loss: 6284420.000000, mae: 2454.061523, mean_q: -2224.964844
 1137/5000: episode: 1137, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -739.447, mean reward: -739.447 [-739.447, -739.447], mean action: 2.000 [2.000, 2.000],  loss: 2154513.500000, mae: 2270.572510, mean_q: -2238.700684
 1138/5000: episode: 1138, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2261.703, mean reward: -2261.703 [-2261.703, -2261.703], mean action: 2.000 [2.000, 2.000],  loss: 4648803.500000, mae: 2404.796875, mean_q: -2275.226562
 1139/5000: episode: 1139, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3651.666, mean reward: -3651.666 [-3651.666, -3651.666], mean action: 2.000 [2.000, 2.000],  loss: 6141065.000000, mae: 2537.749512, mean_q: -2285.358643
 1140/5000: episode: 1140, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3760.375, mean reward: -3760.375 [-3760.375, -3760.375], mean action: 2.000 [2.000, 2.000],  loss: 4011061.500000, mae: 2451.698242, mean_q: -2312.366943
 1141/5000: episode: 1141, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2599.403, mean reward: -2599.403 [-2599.403, -2599.403], mean action: 2.000 [2.000, 2.000],  loss: 5843819.000000, mae: 2565.479492, mean_q: -2348.592773
 1142/5000: episode: 1142, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2448.084, mean reward: -2448.084 [-2448.084, -2448.084], mean action: 2.000 [2.000, 2.000],  loss: 4281736.000000, mae: 2497.455566, mean_q: -2353.506348
 1143/5000: episode: 1143, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -14719.348, mean reward: -14719.348 [-14719.348, -14719.348], mean action: 0.000 [0.000, 0.000],  loss: 5839319.000000, mae: 2606.459961, mean_q: -2379.659912
 1144/5000: episode: 1144, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -3835.917, mean reward: -3835.917 [-3835.917, -3835.917], mean action: 2.000 [2.000, 2.000],  loss: 3688324.000000, mae: 2534.881836, mean_q: -2396.387939
 1145/5000: episode: 1145, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2485.282, mean reward: -2485.282 [-2485.282, -2485.282], mean action: 2.000 [2.000, 2.000],  loss: 4347900.000000, mae: 2504.066162, mean_q: -2411.677246
 1146/5000: episode: 1146, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2351.568, mean reward: -2351.568 [-2351.568, -2351.568], mean action: 2.000 [2.000, 2.000],  loss: 4541493.500000, mae: 2614.513184, mean_q: -2442.459717
 1147/5000: episode: 1147, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3204.250, mean reward: -3204.250 [-3204.250, -3204.250], mean action: 2.000 [2.000, 2.000],  loss: 5635052.000000, mae: 2706.433350, mean_q: -2474.540283
 1148/5000: episode: 1148, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2986.852, mean reward: -2986.852 [-2986.852, -2986.852], mean action: 2.000 [2.000, 2.000],  loss: 3149528.500000, mae: 2537.570801, mean_q: -2472.116455
 1149/5000: episode: 1149, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4609.714, mean reward: -4609.714 [-4609.714, -4609.714], mean action: 2.000 [2.000, 2.000],  loss: 2328292.750000, mae: 2523.876221, mean_q: -2497.106201
 1150/5000: episode: 1150, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3668.084, mean reward: -3668.084 [-3668.084, -3668.084], mean action: 2.000 [2.000, 2.000],  loss: 2902189.500000, mae: 2537.588867, mean_q: -2505.305908
 1151/5000: episode: 1151, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -188.802, mean reward: -188.802 [-188.802, -188.802], mean action: 2.000 [2.000, 2.000],  loss: 4210025.500000, mae: 2674.469482, mean_q: -2514.488281
 1152/5000: episode: 1152, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4063.732, mean reward: -4063.732 [-4063.732, -4063.732], mean action: 2.000 [2.000, 2.000],  loss: 2526259.500000, mae: 2575.733398, mean_q: -2525.327148
 1153/5000: episode: 1153, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3689.068, mean reward: -3689.068 [-3689.068, -3689.068], mean action: 2.000 [2.000, 2.000],  loss: 4463356.000000, mae: 2758.000000, mean_q: -2537.917725
 1154/5000: episode: 1154, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -777.109, mean reward: -777.109 [-777.109, -777.109], mean action: 2.000 [2.000, 2.000],  loss: 5192659.500000, mae: 2713.628174, mean_q: -2555.318359
 1155/5000: episode: 1155, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3263.174, mean reward: -3263.174 [-3263.174, -3263.174], mean action: 2.000 [2.000, 2.000],  loss: 2690635.500000, mae: 2634.916992, mean_q: -2558.923584
 1156/5000: episode: 1156, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -446.573, mean reward: -446.573 [-446.573, -446.573], mean action: 2.000 [2.000, 2.000],  loss: 5142809.000000, mae: 2760.242920, mean_q: -2569.608154
 1157/5000: episode: 1157, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5436.732, mean reward: -5436.732 [-5436.732, -5436.732], mean action: 2.000 [2.000, 2.000],  loss: 3188856.000000, mae: 2681.377930, mean_q: -2589.604736
 1158/5000: episode: 1158, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3328.283, mean reward: -3328.283 [-3328.283, -3328.283], mean action: 2.000 [2.000, 2.000],  loss: 3041033.500000, mae: 2728.774414, mean_q: -2617.837402
 1159/5000: episode: 1159, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1314.198, mean reward: -1314.198 [-1314.198, -1314.198], mean action: 2.000 [2.000, 2.000],  loss: 3635430.500000, mae: 2713.247803, mean_q: -2607.633789
 1160/5000: episode: 1160, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8271.356, mean reward: -8271.356 [-8271.356, -8271.356], mean action: 2.000 [2.000, 2.000],  loss: 3302630.750000, mae: 2738.054688, mean_q: -2636.946777
 1161/5000: episode: 1161, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2439.611, mean reward: -2439.611 [-2439.611, -2439.611], mean action: 2.000 [2.000, 2.000],  loss: 2023914.000000, mae: 2631.838379, mean_q: -2640.570312
 1162/5000: episode: 1162, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7287.315, mean reward: -7287.315 [-7287.315, -7287.315], mean action: 2.000 [2.000, 2.000],  loss: 3547302.750000, mae: 2797.210693, mean_q: -2662.402100
 1163/5000: episode: 1163, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1180.232, mean reward: -1180.232 [-1180.232, -1180.232], mean action: 2.000 [2.000, 2.000],  loss: 3246421.000000, mae: 2744.977539, mean_q: -2669.717285
 1164/5000: episode: 1164, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -312.502, mean reward: -312.502 [-312.502, -312.502], mean action: 2.000 [2.000, 2.000],  loss: 2143912.000000, mae: 2738.451904, mean_q: -2692.199951
 1165/5000: episode: 1165, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1639.329, mean reward: -1639.329 [-1639.329, -1639.329], mean action: 2.000 [2.000, 2.000],  loss: 4028023.500000, mae: 2817.835449, mean_q: -2676.980469
 1166/5000: episode: 1166, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7229.050, mean reward: -7229.050 [-7229.050, -7229.050], mean action: 2.000 [2.000, 2.000],  loss: 5237292.500000, mae: 2857.531006, mean_q: -2684.923828
 1167/5000: episode: 1167, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1016.856, mean reward: -1016.856 [-1016.856, -1016.856], mean action: 2.000 [2.000, 2.000],  loss: 2914363.500000, mae: 2828.170166, mean_q: -2699.021729
 1168/5000: episode: 1168, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1228.490, mean reward: -1228.490 [-1228.490, -1228.490], mean action: 2.000 [2.000, 2.000],  loss: 6793093.000000, mae: 3024.933350, mean_q: -2705.865723
 1169/5000: episode: 1169, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1013.216, mean reward: -1013.216 [-1013.216, -1013.216], mean action: 2.000 [2.000, 2.000],  loss: 5473689.000000, mae: 2948.776123, mean_q: -2703.608887
 1170/5000: episode: 1170, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4271.534, mean reward: -4271.534 [-4271.534, -4271.534], mean action: 2.000 [2.000, 2.000],  loss: 4803995.500000, mae: 2904.434570, mean_q: -2706.433594
 1171/5000: episode: 1171, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -265.367, mean reward: -265.367 [-265.367, -265.367], mean action: 2.000 [2.000, 2.000],  loss: 5150275.500000, mae: 2967.323730, mean_q: -2723.536621
 1172/5000: episode: 1172, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6536.894, mean reward: -6536.894 [-6536.894, -6536.894], mean action: 3.000 [3.000, 3.000],  loss: 2623179.000000, mae: 2840.103027, mean_q: -2722.590088
 1173/5000: episode: 1173, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1051.295, mean reward: -1051.295 [-1051.295, -1051.295], mean action: 2.000 [2.000, 2.000],  loss: 5881210.500000, mae: 3070.733643, mean_q: -2735.062500
 1174/5000: episode: 1174, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3636.746, mean reward: -3636.746 [-3636.746, -3636.746], mean action: 1.000 [1.000, 1.000],  loss: 5875936.000000, mae: 3012.189453, mean_q: -2742.342285
 1175/5000: episode: 1175, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3477.779, mean reward: -3477.779 [-3477.779, -3477.779], mean action: 2.000 [2.000, 2.000],  loss: 4377940.000000, mae: 3036.040771, mean_q: -2750.250732
 1176/5000: episode: 1176, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5223.761, mean reward: -5223.761 [-5223.761, -5223.761], mean action: 2.000 [2.000, 2.000],  loss: 3414388.500000, mae: 2930.689941, mean_q: -2761.718262
 1177/5000: episode: 1177, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -599.115, mean reward: -599.115 [-599.115, -599.115], mean action: 2.000 [2.000, 2.000],  loss: 4662361.000000, mae: 2990.211426, mean_q: -2769.938965
 1178/5000: episode: 1178, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2799.164, mean reward: -2799.164 [-2799.164, -2799.164], mean action: 2.000 [2.000, 2.000],  loss: 6856013.000000, mae: 3084.784668, mean_q: -2775.773926
 1179/5000: episode: 1179, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -2808.448, mean reward: -2808.448 [-2808.448, -2808.448], mean action: 2.000 [2.000, 2.000],  loss: 3270223.500000, mae: 2955.074463, mean_q: -2775.260254
 1180/5000: episode: 1180, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2339.604, mean reward: -2339.604 [-2339.604, -2339.604], mean action: 2.000 [2.000, 2.000],  loss: 4689603.500000, mae: 3042.982910, mean_q: -2798.534180
 1181/5000: episode: 1181, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -8016.321, mean reward: -8016.321 [-8016.321, -8016.321], mean action: 1.000 [1.000, 1.000],  loss: 4869424.000000, mae: 3046.395020, mean_q: -2792.300293
 1182/5000: episode: 1182, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -912.055, mean reward: -912.055 [-912.055, -912.055], mean action: 2.000 [2.000, 2.000],  loss: 3022722.000000, mae: 2928.541992, mean_q: -2806.911377
 1183/5000: episode: 1183, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7460.787, mean reward: -7460.787 [-7460.787, -7460.787], mean action: 2.000 [2.000, 2.000],  loss: 3368241.750000, mae: 2913.527832, mean_q: -2812.778320
 1184/5000: episode: 1184, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5578.188, mean reward: -5578.188 [-5578.188, -5578.188], mean action: 2.000 [2.000, 2.000],  loss: 4094006.500000, mae: 3115.806641, mean_q: -2810.558105
 1185/5000: episode: 1185, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -553.405, mean reward: -553.405 [-553.405, -553.405], mean action: 2.000 [2.000, 2.000],  loss: 3712956.500000, mae: 3038.892578, mean_q: -2821.262939
 1186/5000: episode: 1186, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2929.017, mean reward: -2929.017 [-2929.017, -2929.017], mean action: 2.000 [2.000, 2.000],  loss: 5447216.500000, mae: 3088.795898, mean_q: -2817.832275
 1187/5000: episode: 1187, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1075.587, mean reward: -1075.587 [-1075.587, -1075.587], mean action: 2.000 [2.000, 2.000],  loss: 3710366.250000, mae: 2995.004639, mean_q: -2826.413086
 1188/5000: episode: 1188, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -880.631, mean reward: -880.631 [-880.631, -880.631], mean action: 2.000 [2.000, 2.000],  loss: 5862088.000000, mae: 3174.125000, mean_q: -2847.043945
 1189/5000: episode: 1189, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1142.963, mean reward: -1142.963 [-1142.963, -1142.963], mean action: 2.000 [2.000, 2.000],  loss: 4610606.000000, mae: 3179.164062, mean_q: -2852.651367
 1190/5000: episode: 1190, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6741.396, mean reward: -6741.396 [-6741.396, -6741.396], mean action: 2.000 [2.000, 2.000],  loss: 4296344.000000, mae: 3131.086426, mean_q: -2870.425781
 1191/5000: episode: 1191, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1247.071, mean reward: -1247.071 [-1247.071, -1247.071], mean action: 2.000 [2.000, 2.000],  loss: 4721822.000000, mae: 3166.779053, mean_q: -2864.070068
 1192/5000: episode: 1192, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -755.346, mean reward: -755.346 [-755.346, -755.346], mean action: 2.000 [2.000, 2.000],  loss: 2929643.500000, mae: 3096.996094, mean_q: -2867.837891
 1193/5000: episode: 1193, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2481.233, mean reward: -2481.233 [-2481.233, -2481.233], mean action: 2.000 [2.000, 2.000],  loss: 2501677.000000, mae: 3070.769531, mean_q: -2856.705811
 1194/5000: episode: 1194, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2634.103, mean reward: -2634.103 [-2634.103, -2634.103], mean action: 2.000 [2.000, 2.000],  loss: 3649602.500000, mae: 3152.986816, mean_q: -2857.948242
 1195/5000: episode: 1195, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -46.652, mean reward: -46.652 [-46.652, -46.652], mean action: 2.000 [2.000, 2.000],  loss: 2623903.000000, mae: 3075.703613, mean_q: -2859.475098
 1196/5000: episode: 1196, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4410.635, mean reward: -4410.635 [-4410.635, -4410.635], mean action: 2.000 [2.000, 2.000],  loss: 5084889.000000, mae: 3198.122559, mean_q: -2844.453125
 1197/5000: episode: 1197, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4663.934, mean reward: -4663.934 [-4663.934, -4663.934], mean action: 2.000 [2.000, 2.000],  loss: 6364868.000000, mae: 3228.176270, mean_q: -2841.268799
 1198/5000: episode: 1198, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2597.774, mean reward: -2597.774 [-2597.774, -2597.774], mean action: 2.000 [2.000, 2.000],  loss: 4089833.500000, mae: 3187.247559, mean_q: -2866.586426
 1199/5000: episode: 1199, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3421.109, mean reward: -3421.109 [-3421.109, -3421.109], mean action: 2.000 [2.000, 2.000],  loss: 4019872.250000, mae: 3153.170166, mean_q: -2861.385986
 1200/5000: episode: 1200, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6157.299, mean reward: -6157.299 [-6157.299, -6157.299], mean action: 2.000 [2.000, 2.000],  loss: 2378262.750000, mae: 3089.376465, mean_q: -2876.715820
 1201/5000: episode: 1201, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4640.308, mean reward: -4640.308 [-4640.308, -4640.308], mean action: 2.000 [2.000, 2.000],  loss: 5260978.000000, mae: 3314.826172, mean_q: -2884.292969
 1202/5000: episode: 1202, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1504.615, mean reward: -1504.615 [-1504.615, -1504.615], mean action: 2.000 [2.000, 2.000],  loss: 4619671.000000, mae: 3326.109375, mean_q: -2872.791016
 1203/5000: episode: 1203, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1075.175, mean reward: -1075.175 [-1075.175, -1075.175], mean action: 2.000 [2.000, 2.000],  loss: 3263256.750000, mae: 3136.699219, mean_q: -2862.469727
 1204/5000: episode: 1204, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1047.283, mean reward: -1047.283 [-1047.283, -1047.283], mean action: 2.000 [2.000, 2.000],  loss: 4778778.000000, mae: 3296.380371, mean_q: -2891.750000
 1205/5000: episode: 1205, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3160.258, mean reward: -3160.258 [-3160.258, -3160.258], mean action: 2.000 [2.000, 2.000],  loss: 2481970.750000, mae: 3153.278809, mean_q: -2887.832031
 1206/5000: episode: 1206, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3868.416, mean reward: -3868.416 [-3868.416, -3868.416], mean action: 2.000 [2.000, 2.000],  loss: 3103195.000000, mae: 3174.845215, mean_q: -2902.738281
 1207/5000: episode: 1207, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -305.520, mean reward: -305.520 [-305.520, -305.520], mean action: 2.000 [2.000, 2.000],  loss: 2376360.500000, mae: 3099.145996, mean_q: -2893.176270
 1208/5000: episode: 1208, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5445.110, mean reward: -5445.110 [-5445.110, -5445.110], mean action: 2.000 [2.000, 2.000],  loss: 4762932.000000, mae: 3264.426270, mean_q: -2904.853271
 1209/5000: episode: 1209, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -145.229, mean reward: -145.229 [-145.229, -145.229], mean action: 2.000 [2.000, 2.000],  loss: 5442564.500000, mae: 3384.613281, mean_q: -2913.164307
 1210/5000: episode: 1210, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2032.703, mean reward: -2032.703 [-2032.703, -2032.703], mean action: 2.000 [2.000, 2.000],  loss: 3891945.250000, mae: 3255.173340, mean_q: -2923.525391
 1211/5000: episode: 1211, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -154.090, mean reward: -154.090 [-154.090, -154.090], mean action: 2.000 [2.000, 2.000],  loss: 6123034.000000, mae: 3303.821289, mean_q: -2901.675781
 1212/5000: episode: 1212, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -788.863, mean reward: -788.863 [-788.863, -788.863], mean action: 2.000 [2.000, 2.000],  loss: 4640932.500000, mae: 3289.634277, mean_q: -2891.783691
 1213/5000: episode: 1213, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -844.760, mean reward: -844.760 [-844.760, -844.760], mean action: 2.000 [2.000, 2.000],  loss: 4575772.000000, mae: 3258.954590, mean_q: -2887.925049
 1214/5000: episode: 1214, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -3245.808, mean reward: -3245.808 [-3245.808, -3245.808], mean action: 2.000 [2.000, 2.000],  loss: 5324684.000000, mae: 3358.306885, mean_q: -2893.741699
 1215/5000: episode: 1215, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -673.416, mean reward: -673.416 [-673.416, -673.416], mean action: 2.000 [2.000, 2.000],  loss: 4140527.250000, mae: 3251.210693, mean_q: -2871.526367
 1216/5000: episode: 1216, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2378.540, mean reward: -2378.540 [-2378.540, -2378.540], mean action: 2.000 [2.000, 2.000],  loss: 3722307.500000, mae: 3329.952637, mean_q: -2891.643066
 1217/5000: episode: 1217, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1736.827, mean reward: -1736.827 [-1736.827, -1736.827], mean action: 2.000 [2.000, 2.000],  loss: 3010201.000000, mae: 3200.585938, mean_q: -2867.415527
 1218/5000: episode: 1218, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2497.710, mean reward: -2497.710 [-2497.710, -2497.710], mean action: 2.000 [2.000, 2.000],  loss: 3248803.000000, mae: 3267.024414, mean_q: -2866.614258
 1219/5000: episode: 1219, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4682.807, mean reward: -4682.807 [-4682.807, -4682.807], mean action: 2.000 [2.000, 2.000],  loss: 4646670.000000, mae: 3265.359375, mean_q: -2857.649414
 1220/5000: episode: 1220, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -141.032, mean reward: -141.032 [-141.032, -141.032], mean action: 2.000 [2.000, 2.000],  loss: 2968850.250000, mae: 3219.552246, mean_q: -2845.022461
 1221/5000: episode: 1221, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2747.971, mean reward: -2747.971 [-2747.971, -2747.971], mean action: 2.000 [2.000, 2.000],  loss: 4845098.500000, mae: 3375.123535, mean_q: -2835.282227
 1222/5000: episode: 1222, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1430.734, mean reward: -1430.734 [-1430.734, -1430.734], mean action: 2.000 [2.000, 2.000],  loss: 3840772.000000, mae: 3356.715820, mean_q: -2834.806152
 1223/5000: episode: 1223, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2472.220, mean reward: -2472.220 [-2472.220, -2472.220], mean action: 2.000 [2.000, 2.000],  loss: 3186066.000000, mae: 3240.030518, mean_q: -2812.058594
 1224/5000: episode: 1224, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2574.509, mean reward: -2574.509 [-2574.509, -2574.509], mean action: 2.000 [2.000, 2.000],  loss: 7684302.000000, mae: 3509.876953, mean_q: -2811.152100
 1225/5000: episode: 1225, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2090.978, mean reward: -2090.978 [-2090.978, -2090.978], mean action: 2.000 [2.000, 2.000],  loss: 2900918.000000, mae: 3210.182129, mean_q: -2798.255615
 1226/5000: episode: 1226, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -934.521, mean reward: -934.521 [-934.521, -934.521], mean action: 2.000 [2.000, 2.000],  loss: 2242172.250000, mae: 3189.313965, mean_q: -2794.928467
 1227/5000: episode: 1227, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5102.556, mean reward: -5102.556 [-5102.556, -5102.556], mean action: 2.000 [2.000, 2.000],  loss: 3931061.500000, mae: 3311.104736, mean_q: -2785.513184
 1228/5000: episode: 1228, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -859.304, mean reward: -859.304 [-859.304, -859.304], mean action: 3.000 [3.000, 3.000],  loss: 4328027.500000, mae: 3321.531494, mean_q: -2787.302002
 1229/5000: episode: 1229, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4220.938, mean reward: -4220.938 [-4220.938, -4220.938], mean action: 2.000 [2.000, 2.000],  loss: 3762160.750000, mae: 3329.866699, mean_q: -2783.399414
 1230/5000: episode: 1230, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1758.977, mean reward: -1758.977 [-1758.977, -1758.977], mean action: 2.000 [2.000, 2.000],  loss: 7041322.500000, mae: 3445.489990, mean_q: -2784.660645
 1231/5000: episode: 1231, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1660.822, mean reward: -1660.822 [-1660.822, -1660.822], mean action: 2.000 [2.000, 2.000],  loss: 4189875.250000, mae: 3346.336914, mean_q: -2767.372559
 1232/5000: episode: 1232, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -2689.336, mean reward: -2689.336 [-2689.336, -2689.336], mean action: 2.000 [2.000, 2.000],  loss: 3376521.000000, mae: 3299.519287, mean_q: -2775.786621
 1233/5000: episode: 1233, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -353.267, mean reward: -353.267 [-353.267, -353.267], mean action: 2.000 [2.000, 2.000],  loss: 8823658.000000, mae: 3597.503906, mean_q: -2774.551270
 1234/5000: episode: 1234, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2829.535, mean reward: -2829.535 [-2829.535, -2829.535], mean action: 2.000 [2.000, 2.000],  loss: 3167012.750000, mae: 3323.745605, mean_q: -2792.457520
 1235/5000: episode: 1235, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -864.035, mean reward: -864.035 [-864.035, -864.035], mean action: 2.000 [2.000, 2.000],  loss: 8086022.500000, mae: 3535.651611, mean_q: -2779.679688
 1236/5000: episode: 1236, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -11163.635, mean reward: -11163.635 [-11163.635, -11163.635], mean action: 0.000 [0.000, 0.000],  loss: 4495620.000000, mae: 3428.638184, mean_q: -2786.954590
 1237/5000: episode: 1237, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2046.486, mean reward: -2046.486 [-2046.486, -2046.486], mean action: 2.000 [2.000, 2.000],  loss: 4115838.500000, mae: 3392.417480, mean_q: -2786.791260
 1238/5000: episode: 1238, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -395.682, mean reward: -395.682 [-395.682, -395.682], mean action: 3.000 [3.000, 3.000],  loss: 2861375.500000, mae: 3358.525879, mean_q: -2784.342041
 1239/5000: episode: 1239, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1184.769, mean reward: -1184.769 [-1184.769, -1184.769], mean action: 2.000 [2.000, 2.000],  loss: 6715426.500000, mae: 3539.075195, mean_q: -2794.602295
 1240/5000: episode: 1240, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3091.547, mean reward: -3091.547 [-3091.547, -3091.547], mean action: 2.000 [2.000, 2.000],  loss: 4008867.500000, mae: 3400.580322, mean_q: -2794.444336
 1241/5000: episode: 1241, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1344.707, mean reward: -1344.707 [-1344.707, -1344.707], mean action: 2.000 [2.000, 2.000],  loss: 1971358.125000, mae: 3247.112793, mean_q: -2780.307129
 1242/5000: episode: 1242, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -998.975, mean reward: -998.975 [-998.975, -998.975], mean action: 2.000 [2.000, 2.000],  loss: 4166939.250000, mae: 3466.039062, mean_q: -2780.897217
 1243/5000: episode: 1243, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -829.978, mean reward: -829.978 [-829.978, -829.978], mean action: 2.000 [2.000, 2.000],  loss: 3399187.750000, mae: 3392.870361, mean_q: -2762.348145
 1244/5000: episode: 1244, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4692.957, mean reward: -4692.957 [-4692.957, -4692.957], mean action: 2.000 [2.000, 2.000],  loss: 4580828.000000, mae: 3422.377441, mean_q: -2765.649902
 1245/5000: episode: 1245, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3848.666, mean reward: -3848.666 [-3848.666, -3848.666], mean action: 2.000 [2.000, 2.000],  loss: 2825143.500000, mae: 3304.842529, mean_q: -2760.572510
 1246/5000: episode: 1246, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3750.570, mean reward: -3750.570 [-3750.570, -3750.570], mean action: 2.000 [2.000, 2.000],  loss: 3911205.500000, mae: 3472.297119, mean_q: -2744.519043
 1247/5000: episode: 1247, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2380.450, mean reward: -2380.450 [-2380.450, -2380.450], mean action: 2.000 [2.000, 2.000],  loss: 3968323.000000, mae: 3444.492676, mean_q: -2731.647217
 1248/5000: episode: 1248, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5.610, mean reward: -5.610 [-5.610, -5.610], mean action: 2.000 [2.000, 2.000],  loss: 4347151.000000, mae: 3468.779541, mean_q: -2727.733398
 1249/5000: episode: 1249, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2728.148, mean reward: -2728.148 [-2728.148, -2728.148], mean action: 2.000 [2.000, 2.000],  loss: 4474811.500000, mae: 3336.477051, mean_q: -2722.318115
 1250/5000: episode: 1250, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6.821, mean reward: -6.821 [-6.821, -6.821], mean action: 2.000 [2.000, 2.000],  loss: 3537387.000000, mae: 3453.882568, mean_q: -2721.684082
 1251/5000: episode: 1251, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -921.062, mean reward: -921.062 [-921.062, -921.062], mean action: 2.000 [2.000, 2.000],  loss: 3347190.250000, mae: 3439.848877, mean_q: -2721.395020
 1252/5000: episode: 1252, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6217.398, mean reward: -6217.398 [-6217.398, -6217.398], mean action: 2.000 [2.000, 2.000],  loss: 2134316.500000, mae: 3303.114746, mean_q: -2721.415039
 1253/5000: episode: 1253, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -527.944, mean reward: -527.944 [-527.944, -527.944], mean action: 2.000 [2.000, 2.000],  loss: 3459188.000000, mae: 3435.071777, mean_q: -2711.028076
 1254/5000: episode: 1254, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1451.582, mean reward: -1451.582 [-1451.582, -1451.582], mean action: 2.000 [2.000, 2.000],  loss: 4107251.000000, mae: 3460.248047, mean_q: -2707.118164
 1255/5000: episode: 1255, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2755.381, mean reward: -2755.381 [-2755.381, -2755.381], mean action: 2.000 [2.000, 2.000],  loss: 3274545.500000, mae: 3394.698242, mean_q: -2715.678711
 1256/5000: episode: 1256, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -943.766, mean reward: -943.766 [-943.766, -943.766], mean action: 2.000 [2.000, 2.000],  loss: 6107497.000000, mae: 3526.199219, mean_q: -2722.999512
 1257/5000: episode: 1257, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -33.720, mean reward: -33.720 [-33.720, -33.720], mean action: 2.000 [2.000, 2.000],  loss: 6283829.000000, mae: 3439.528809, mean_q: -2719.404785
 1258/5000: episode: 1258, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5261.042, mean reward: -5261.042 [-5261.042, -5261.042], mean action: 2.000 [2.000, 2.000],  loss: 4129290.250000, mae: 3511.381348, mean_q: -2744.941895
 1259/5000: episode: 1259, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3795.640, mean reward: -3795.640 [-3795.640, -3795.640], mean action: 2.000 [2.000, 2.000],  loss: 1943054.125000, mae: 3313.453125, mean_q: -2734.960938
 1260/5000: episode: 1260, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2544.051, mean reward: -2544.051 [-2544.051, -2544.051], mean action: 2.000 [2.000, 2.000],  loss: 5526424.000000, mae: 3564.511475, mean_q: -2722.555664
 1261/5000: episode: 1261, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6827.709, mean reward: -6827.709 [-6827.709, -6827.709], mean action: 2.000 [2.000, 2.000],  loss: 3242014.000000, mae: 3468.811279, mean_q: -2721.429443
 1262/5000: episode: 1262, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2643.412, mean reward: -2643.412 [-2643.412, -2643.412], mean action: 2.000 [2.000, 2.000],  loss: 3267678.000000, mae: 3482.046875, mean_q: -2715.998047
 1263/5000: episode: 1263, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1239.392, mean reward: -1239.392 [-1239.392, -1239.392], mean action: 2.000 [2.000, 2.000],  loss: 3804609.000000, mae: 3463.682617, mean_q: -2709.373047
 1264/5000: episode: 1264, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8963.089, mean reward: -8963.089 [-8963.089, -8963.089], mean action: 0.000 [0.000, 0.000],  loss: 4906138.000000, mae: 3532.924316, mean_q: -2706.394775
 1265/5000: episode: 1265, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1110.423, mean reward: -1110.423 [-1110.423, -1110.423], mean action: 2.000 [2.000, 2.000],  loss: 3083655.500000, mae: 3439.518066, mean_q: -2680.209961
 1266/5000: episode: 1266, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -582.042, mean reward: -582.042 [-582.042, -582.042], mean action: 2.000 [2.000, 2.000],  loss: 5829789.000000, mae: 3628.854980, mean_q: -2682.356445
 1267/5000: episode: 1267, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7683.642, mean reward: -7683.642 [-7683.642, -7683.642], mean action: 2.000 [2.000, 2.000],  loss: 3359103.000000, mae: 3500.039551, mean_q: -2672.757812
 1268/5000: episode: 1268, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2630.410, mean reward: -2630.410 [-2630.410, -2630.410], mean action: 2.000 [2.000, 2.000],  loss: 3160098.500000, mae: 3399.595703, mean_q: -2656.810547
 1269/5000: episode: 1269, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -911.175, mean reward: -911.175 [-911.175, -911.175], mean action: 2.000 [2.000, 2.000],  loss: 4486918.000000, mae: 3588.822266, mean_q: -2658.224609
 1270/5000: episode: 1270, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1254.792, mean reward: -1254.792 [-1254.792, -1254.792], mean action: 2.000 [2.000, 2.000],  loss: 4060799.000000, mae: 3522.904541, mean_q: -2641.226074
 1271/5000: episode: 1271, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1694.095, mean reward: -1694.095 [-1694.095, -1694.095], mean action: 2.000 [2.000, 2.000],  loss: 2986912.250000, mae: 3519.519531, mean_q: -2658.083740
 1272/5000: episode: 1272, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4675.525, mean reward: -4675.525 [-4675.525, -4675.525], mean action: 2.000 [2.000, 2.000],  loss: 3095327.000000, mae: 3517.448975, mean_q: -2656.020020
 1273/5000: episode: 1273, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8377.955, mean reward: -8377.955 [-8377.955, -8377.955], mean action: 2.000 [2.000, 2.000],  loss: 5744779.000000, mae: 3625.384766, mean_q: -2665.247559
 1274/5000: episode: 1274, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -8917.156, mean reward: -8917.156 [-8917.156, -8917.156], mean action: 2.000 [2.000, 2.000],  loss: 4564883.000000, mae: 3538.426270, mean_q: -2656.358154
 1275/5000: episode: 1275, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1096.459, mean reward: -1096.459 [-1096.459, -1096.459], mean action: 2.000 [2.000, 2.000],  loss: 2291745.500000, mae: 3507.805664, mean_q: -2671.660400
 1276/5000: episode: 1276, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1246.969, mean reward: -1246.969 [-1246.969, -1246.969], mean action: 2.000 [2.000, 2.000],  loss: 4647585.000000, mae: 3618.385498, mean_q: -2689.893555
 1277/5000: episode: 1277, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1324.386, mean reward: -1324.386 [-1324.386, -1324.386], mean action: 2.000 [2.000, 2.000],  loss: 3678690.000000, mae: 3598.808594, mean_q: -2690.084473
 1278/5000: episode: 1278, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3594.234, mean reward: -3594.234 [-3594.234, -3594.234], mean action: 1.000 [1.000, 1.000],  loss: 3177958.000000, mae: 3549.249512, mean_q: -2674.177979
 1279/5000: episode: 1279, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -278.204, mean reward: -278.204 [-278.204, -278.204], mean action: 2.000 [2.000, 2.000],  loss: 3734438.250000, mae: 3609.607910, mean_q: -2688.690186
 1280/5000: episode: 1280, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2650.885, mean reward: -2650.885 [-2650.885, -2650.885], mean action: 2.000 [2.000, 2.000],  loss: 4033755.000000, mae: 3654.696289, mean_q: -2691.291992
 1281/5000: episode: 1281, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -75.188, mean reward: -75.188 [-75.188, -75.188], mean action: 2.000 [2.000, 2.000],  loss: 2401463.500000, mae: 3523.062012, mean_q: -2689.779785
 1282/5000: episode: 1282, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2811.859, mean reward: -2811.859 [-2811.859, -2811.859], mean action: 2.000 [2.000, 2.000],  loss: 7229730.000000, mae: 3883.675293, mean_q: -2699.322754
 1283/5000: episode: 1283, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2240.332, mean reward: -2240.332 [-2240.332, -2240.332], mean action: 2.000 [2.000, 2.000],  loss: 3144762.000000, mae: 3643.027588, mean_q: -2714.561523
 1284/5000: episode: 1284, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -7227.964, mean reward: -7227.964 [-7227.964, -7227.964], mean action: 2.000 [2.000, 2.000],  loss: 3802814.500000, mae: 3738.755859, mean_q: -2732.487793
 1285/5000: episode: 1285, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4783.851, mean reward: -4783.851 [-4783.851, -4783.851], mean action: 2.000 [2.000, 2.000],  loss: 4540183.000000, mae: 3745.423584, mean_q: -2731.940430
 1286/5000: episode: 1286, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2815.231, mean reward: -2815.231 [-2815.231, -2815.231], mean action: 2.000 [2.000, 2.000],  loss: 6728751.000000, mae: 3855.324219, mean_q: -2737.204346
 1287/5000: episode: 1287, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4199.332, mean reward: -4199.332 [-4199.332, -4199.332], mean action: 2.000 [2.000, 2.000],  loss: 3536077.500000, mae: 3714.713867, mean_q: -2740.833496
 1288/5000: episode: 1288, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -492.958, mean reward: -492.958 [-492.958, -492.958], mean action: 2.000 [2.000, 2.000],  loss: 1777999.500000, mae: 3545.935547, mean_q: -2757.091797
 1289/5000: episode: 1289, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2301.622, mean reward: -2301.622 [-2301.622, -2301.622], mean action: 2.000 [2.000, 2.000],  loss: 4817375.500000, mae: 3796.062256, mean_q: -2767.342773
 1290/5000: episode: 1290, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2653.645, mean reward: -2653.645 [-2653.645, -2653.645], mean action: 2.000 [2.000, 2.000],  loss: 3448292.500000, mae: 3650.556396, mean_q: -2751.760254
 1291/5000: episode: 1291, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -149.928, mean reward: -149.928 [-149.928, -149.928], mean action: 2.000 [2.000, 2.000],  loss: 3940550.250000, mae: 3686.194336, mean_q: -2752.539551
 1292/5000: episode: 1292, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2614.056, mean reward: -2614.056 [-2614.056, -2614.056], mean action: 2.000 [2.000, 2.000],  loss: 2689610.000000, mae: 3700.747070, mean_q: -2745.792480
 1293/5000: episode: 1293, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -227.781, mean reward: -227.781 [-227.781, -227.781], mean action: 2.000 [2.000, 2.000],  loss: 2585996.000000, mae: 3660.863770, mean_q: -2730.572754
 1294/5000: episode: 1294, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5147.479, mean reward: -5147.479 [-5147.479, -5147.479], mean action: 2.000 [2.000, 2.000],  loss: 3374104.750000, mae: 3698.718750, mean_q: -2721.732422
 1295/5000: episode: 1295, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1844.140, mean reward: -1844.140 [-1844.140, -1844.140], mean action: 2.000 [2.000, 2.000],  loss: 3287015.500000, mae: 3685.543945, mean_q: -2703.987061
 1296/5000: episode: 1296, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3360.460, mean reward: -3360.460 [-3360.460, -3360.460], mean action: 2.000 [2.000, 2.000],  loss: 3867541.500000, mae: 3731.517578, mean_q: -2692.673584
 1297/5000: episode: 1297, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1265.635, mean reward: -1265.635 [-1265.635, -1265.635], mean action: 2.000 [2.000, 2.000],  loss: 4006576.000000, mae: 3707.964844, mean_q: -2696.301758
 1298/5000: episode: 1298, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2530.130, mean reward: -2530.130 [-2530.130, -2530.130], mean action: 2.000 [2.000, 2.000],  loss: 3883765.500000, mae: 3733.434082, mean_q: -2679.635254
 1299/5000: episode: 1299, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2729.273, mean reward: -2729.273 [-2729.273, -2729.273], mean action: 2.000 [2.000, 2.000],  loss: 4930010.500000, mae: 3785.778809, mean_q: -2662.129639
 1300/5000: episode: 1300, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3845.912, mean reward: -3845.912 [-3845.912, -3845.912], mean action: 2.000 [2.000, 2.000],  loss: 3640096.250000, mae: 3748.773438, mean_q: -2655.686035
 1301/5000: episode: 1301, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6349.710, mean reward: -6349.710 [-6349.710, -6349.710], mean action: 2.000 [2.000, 2.000],  loss: 4123438.000000, mae: 3807.396973, mean_q: -2653.103271
 1302/5000: episode: 1302, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5001.645, mean reward: -5001.645 [-5001.645, -5001.645], mean action: 2.000 [2.000, 2.000],  loss: 3718102.000000, mae: 3741.668945, mean_q: -2651.354980
 1303/5000: episode: 1303, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -763.086, mean reward: -763.086 [-763.086, -763.086], mean action: 2.000 [2.000, 2.000],  loss: 2477384.500000, mae: 3641.407715, mean_q: -2661.874756
 1304/5000: episode: 1304, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -500.752, mean reward: -500.752 [-500.752, -500.752], mean action: 2.000 [2.000, 2.000],  loss: 3215550.500000, mae: 3710.760254, mean_q: -2660.644287
 1305/5000: episode: 1305, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6216.024, mean reward: -6216.024 [-6216.024, -6216.024], mean action: 2.000 [2.000, 2.000],  loss: 3256645.000000, mae: 3684.334229, mean_q: -2659.149902
 1306/5000: episode: 1306, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -557.568, mean reward: -557.568 [-557.568, -557.568], mean action: 2.000 [2.000, 2.000],  loss: 2436642.500000, mae: 3703.208008, mean_q: -2659.610596
 1307/5000: episode: 1307, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1669.880, mean reward: -1669.880 [-1669.880, -1669.880], mean action: 2.000 [2.000, 2.000],  loss: 4222331.000000, mae: 3793.126465, mean_q: -2681.082520
 1308/5000: episode: 1308, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7641.209, mean reward: -7641.209 [-7641.209, -7641.209], mean action: 3.000 [3.000, 3.000],  loss: 3639728.000000, mae: 3746.304199, mean_q: -2664.088867
 1309/5000: episode: 1309, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4475.110, mean reward: -4475.110 [-4475.110, -4475.110], mean action: 2.000 [2.000, 2.000],  loss: 2428574.250000, mae: 3699.385742, mean_q: -2659.489746
 1310/5000: episode: 1310, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -9119.781, mean reward: -9119.781 [-9119.781, -9119.781], mean action: 2.000 [2.000, 2.000],  loss: 3029333.500000, mae: 3705.338379, mean_q: -2665.461182
 1311/5000: episode: 1311, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -631.352, mean reward: -631.352 [-631.352, -631.352], mean action: 2.000 [2.000, 2.000],  loss: 4090324.500000, mae: 3867.929932, mean_q: -2658.773193
 1312/5000: episode: 1312, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1115.571, mean reward: -1115.571 [-1115.571, -1115.571], mean action: 2.000 [2.000, 2.000],  loss: 5991780.000000, mae: 3832.308105, mean_q: -2656.591797
 1313/5000: episode: 1313, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7523.458, mean reward: -7523.458 [-7523.458, -7523.458], mean action: 2.000 [2.000, 2.000],  loss: 5096035.500000, mae: 3910.355713, mean_q: -2648.037842
 1314/5000: episode: 1314, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3780.416, mean reward: -3780.416 [-3780.416, -3780.416], mean action: 2.000 [2.000, 2.000],  loss: 2023209.000000, mae: 3654.746338, mean_q: -2650.945312
 1315/5000: episode: 1315, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2149.817, mean reward: -2149.817 [-2149.817, -2149.817], mean action: 2.000 [2.000, 2.000],  loss: 3475871.500000, mae: 3759.857910, mean_q: -2636.669434
 1316/5000: episode: 1316, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -467.460, mean reward: -467.460 [-467.460, -467.460], mean action: 1.000 [1.000, 1.000],  loss: 4725503.000000, mae: 3782.543457, mean_q: -2644.356201
 1317/5000: episode: 1317, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4863.463, mean reward: -4863.463 [-4863.463, -4863.463], mean action: 2.000 [2.000, 2.000],  loss: 3395843.500000, mae: 3765.845459, mean_q: -2632.310059
 1318/5000: episode: 1318, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3392.728, mean reward: -3392.728 [-3392.728, -3392.728], mean action: 2.000 [2.000, 2.000],  loss: 2469979.500000, mae: 3705.477539, mean_q: -2636.316406
 1319/5000: episode: 1319, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6266.283, mean reward: -6266.283 [-6266.283, -6266.283], mean action: 2.000 [2.000, 2.000],  loss: 6107315.000000, mae: 3979.099365, mean_q: -2643.543701
 1320/5000: episode: 1320, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3243.118, mean reward: -3243.118 [-3243.118, -3243.118], mean action: 2.000 [2.000, 2.000],  loss: 4332980.000000, mae: 3826.464355, mean_q: -2646.416016
 1321/5000: episode: 1321, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -669.827, mean reward: -669.827 [-669.827, -669.827], mean action: 2.000 [2.000, 2.000],  loss: 4912429.000000, mae: 3751.131836, mean_q: -2651.680664
 1322/5000: episode: 1322, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1986.968, mean reward: -1986.968 [-1986.968, -1986.968], mean action: 2.000 [2.000, 2.000],  loss: 3403494.000000, mae: 3734.779297, mean_q: -2666.537109
 1323/5000: episode: 1323, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -585.619, mean reward: -585.619 [-585.619, -585.619], mean action: 2.000 [2.000, 2.000],  loss: 3083733.750000, mae: 3837.860352, mean_q: -2670.898926
 1324/5000: episode: 1324, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9956.276, mean reward: -9956.276 [-9956.276, -9956.276], mean action: 0.000 [0.000, 0.000],  loss: 3544377.500000, mae: 3805.105225, mean_q: -2661.341309
 1325/5000: episode: 1325, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3222.302, mean reward: -3222.302 [-3222.302, -3222.302], mean action: 2.000 [2.000, 2.000],  loss: 4629066.000000, mae: 3884.426270, mean_q: -2685.365723
 1326/5000: episode: 1326, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -272.532, mean reward: -272.532 [-272.532, -272.532], mean action: 2.000 [2.000, 2.000],  loss: 4865044.000000, mae: 3857.987793, mean_q: -2671.838867
 1327/5000: episode: 1327, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5734.751, mean reward: -5734.751 [-5734.751, -5734.751], mean action: 0.000 [0.000, 0.000],  loss: 3805062.500000, mae: 3832.449707, mean_q: -2670.349121
 1328/5000: episode: 1328, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1611.686, mean reward: -1611.686 [-1611.686, -1611.686], mean action: 2.000 [2.000, 2.000],  loss: 4679905.000000, mae: 3922.456299, mean_q: -2683.688477
 1329/5000: episode: 1329, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -9999.313, mean reward: -9999.313 [-9999.313, -9999.313], mean action: 0.000 [0.000, 0.000],  loss: 4814425.000000, mae: 3876.597656, mean_q: -2689.890625
 1330/5000: episode: 1330, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -871.519, mean reward: -871.519 [-871.519, -871.519], mean action: 2.000 [2.000, 2.000],  loss: 3800461.000000, mae: 3813.047607, mean_q: -2689.645508
 1331/5000: episode: 1331, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4564.776, mean reward: -4564.776 [-4564.776, -4564.776], mean action: 2.000 [2.000, 2.000],  loss: 3713067.500000, mae: 3896.113037, mean_q: -2687.302490
 1332/5000: episode: 1332, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4240.986, mean reward: -4240.986 [-4240.986, -4240.986], mean action: 2.000 [2.000, 2.000],  loss: 4305787.000000, mae: 3858.153320, mean_q: -2701.495605
 1333/5000: episode: 1333, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1797.814, mean reward: -1797.814 [-1797.814, -1797.814], mean action: 2.000 [2.000, 2.000],  loss: 3953199.000000, mae: 3901.652832, mean_q: -2724.009766
 1334/5000: episode: 1334, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4041.136, mean reward: -4041.136 [-4041.136, -4041.136], mean action: 2.000 [2.000, 2.000],  loss: 2413247.500000, mae: 3713.437500, mean_q: -2705.843994
 1335/5000: episode: 1335, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2369.580, mean reward: -2369.580 [-2369.580, -2369.580], mean action: 2.000 [2.000, 2.000],  loss: 4855560.000000, mae: 3961.880615, mean_q: -2711.833740
 1336/5000: episode: 1336, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3274.821, mean reward: -3274.821 [-3274.821, -3274.821], mean action: 2.000 [2.000, 2.000],  loss: 5300190.000000, mae: 3881.664062, mean_q: -2733.152344
 1337/5000: episode: 1337, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4893.306, mean reward: -4893.306 [-4893.306, -4893.306], mean action: 2.000 [2.000, 2.000],  loss: 4145631.500000, mae: 3859.284180, mean_q: -2752.419434
 1338/5000: episode: 1338, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2312.191, mean reward: -2312.191 [-2312.191, -2312.191], mean action: 2.000 [2.000, 2.000],  loss: 4174020.000000, mae: 4048.642578, mean_q: -2779.260254
 1339/5000: episode: 1339, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4678.456, mean reward: -4678.456 [-4678.456, -4678.456], mean action: 2.000 [2.000, 2.000],  loss: 3978113.500000, mae: 3920.870605, mean_q: -2777.293457
 1340/5000: episode: 1340, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6618.389, mean reward: -6618.389 [-6618.389, -6618.389], mean action: 2.000 [2.000, 2.000],  loss: 2596640.000000, mae: 3924.885986, mean_q: -2808.691895
 1341/5000: episode: 1341, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -8571.729, mean reward: -8571.729 [-8571.729, -8571.729], mean action: 2.000 [2.000, 2.000],  loss: 5876782.000000, mae: 4147.709473, mean_q: -2812.031738
 1342/5000: episode: 1342, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -43.223, mean reward: -43.223 [-43.223, -43.223], mean action: 2.000 [2.000, 2.000],  loss: 2709426.500000, mae: 3924.591309, mean_q: -2821.174805
 1343/5000: episode: 1343, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3532.025, mean reward: -3532.025 [-3532.025, -3532.025], mean action: 2.000 [2.000, 2.000],  loss: 5887524.000000, mae: 4073.398926, mean_q: -2834.892578
 1344/5000: episode: 1344, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1373.439, mean reward: -1373.439 [-1373.439, -1373.439], mean action: 2.000 [2.000, 2.000],  loss: 3985488.750000, mae: 3889.675781, mean_q: -2825.849854
 1345/5000: episode: 1345, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2343.442, mean reward: -2343.442 [-2343.442, -2343.442], mean action: 2.000 [2.000, 2.000],  loss: 5901413.000000, mae: 4106.643066, mean_q: -2849.866455
 1346/5000: episode: 1346, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3895.099, mean reward: -3895.099 [-3895.099, -3895.099], mean action: 2.000 [2.000, 2.000],  loss: 3678337.000000, mae: 3931.206299, mean_q: -2838.119385
 1347/5000: episode: 1347, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7711.537, mean reward: -7711.537 [-7711.537, -7711.537], mean action: 2.000 [2.000, 2.000],  loss: 3757361.500000, mae: 4018.442871, mean_q: -2851.020996
 1348/5000: episode: 1348, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8201.008, mean reward: -8201.008 [-8201.008, -8201.008], mean action: 2.000 [2.000, 2.000],  loss: 2805569.500000, mae: 3953.590332, mean_q: -2844.865967
 1349/5000: episode: 1349, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7528.708, mean reward: -7528.708 [-7528.708, -7528.708], mean action: 2.000 [2.000, 2.000],  loss: 2878339.500000, mae: 3907.880371, mean_q: -2830.638672
 1350/5000: episode: 1350, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2205.661, mean reward: -2205.661 [-2205.661, -2205.661], mean action: 2.000 [2.000, 2.000],  loss: 4065704.750000, mae: 4071.397461, mean_q: -2819.143066
 1351/5000: episode: 1351, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3615.881, mean reward: -3615.881 [-3615.881, -3615.881], mean action: 2.000 [2.000, 2.000],  loss: 6313131.500000, mae: 4108.523926, mean_q: -2809.163330
 1352/5000: episode: 1352, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6473.707, mean reward: -6473.707 [-6473.707, -6473.707], mean action: 2.000 [2.000, 2.000],  loss: 6076471.000000, mae: 4045.723633, mean_q: -2806.154785
 1353/5000: episode: 1353, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1094.028, mean reward: -1094.028 [-1094.028, -1094.028], mean action: 2.000 [2.000, 2.000],  loss: 2769102.750000, mae: 3963.520264, mean_q: -2802.452148
 1354/5000: episode: 1354, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1754.324, mean reward: -1754.324 [-1754.324, -1754.324], mean action: 2.000 [2.000, 2.000],  loss: 3697879.000000, mae: 3997.666992, mean_q: -2798.024414
 1355/5000: episode: 1355, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4028.514, mean reward: -4028.514 [-4028.514, -4028.514], mean action: 1.000 [1.000, 1.000],  loss: 3304510.500000, mae: 3948.986084, mean_q: -2782.019043
 1356/5000: episode: 1356, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1827.038, mean reward: -1827.038 [-1827.038, -1827.038], mean action: 2.000 [2.000, 2.000],  loss: 2720501.500000, mae: 3888.470215, mean_q: -2764.111328
 1357/5000: episode: 1357, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2977.551, mean reward: -2977.551 [-2977.551, -2977.551], mean action: 2.000 [2.000, 2.000],  loss: 1570835.500000, mae: 3846.148438, mean_q: -2794.535156
 1358/5000: episode: 1358, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -481.041, mean reward: -481.041 [-481.041, -481.041], mean action: 2.000 [2.000, 2.000],  loss: 3557281.500000, mae: 4017.049316, mean_q: -2776.207275
 1359/5000: episode: 1359, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -928.497, mean reward: -928.497 [-928.497, -928.497], mean action: 2.000 [2.000, 2.000],  loss: 2546255.250000, mae: 3829.254639, mean_q: -2759.927246
 1360/5000: episode: 1360, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3683.619, mean reward: -3683.619 [-3683.619, -3683.619], mean action: 1.000 [1.000, 1.000],  loss: 3832262.250000, mae: 4018.011719, mean_q: -2769.756348
 1361/5000: episode: 1361, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3927.028, mean reward: -3927.028 [-3927.028, -3927.028], mean action: 2.000 [2.000, 2.000],  loss: 3363483.000000, mae: 3979.609375, mean_q: -2745.291016
 1362/5000: episode: 1362, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2476.569, mean reward: -2476.569 [-2476.569, -2476.569], mean action: 2.000 [2.000, 2.000],  loss: 3446250.750000, mae: 4040.524902, mean_q: -2753.754395
 1363/5000: episode: 1363, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2176.358, mean reward: -2176.358 [-2176.358, -2176.358], mean action: 2.000 [2.000, 2.000],  loss: 5096049.500000, mae: 4033.232422, mean_q: -2756.992188
 1364/5000: episode: 1364, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1548.198, mean reward: -1548.198 [-1548.198, -1548.198], mean action: 2.000 [2.000, 2.000],  loss: 4038369.500000, mae: 3966.685059, mean_q: -2764.567871
 1365/5000: episode: 1365, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2057.014, mean reward: -2057.014 [-2057.014, -2057.014], mean action: 2.000 [2.000, 2.000],  loss: 4196034.000000, mae: 4059.668701, mean_q: -2749.025879
 1366/5000: episode: 1366, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -198.837, mean reward: -198.837 [-198.837, -198.837], mean action: 2.000 [2.000, 2.000],  loss: 2511903.500000, mae: 3943.935791, mean_q: -2744.828125
 1367/5000: episode: 1367, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1941.920, mean reward: -1941.920 [-1941.920, -1941.920], mean action: 2.000 [2.000, 2.000],  loss: 4577614.000000, mae: 4056.651855, mean_q: -2747.445312
 1368/5000: episode: 1368, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5008.323, mean reward: -5008.323 [-5008.323, -5008.323], mean action: 2.000 [2.000, 2.000],  loss: 2974262.500000, mae: 3964.078613, mean_q: -2759.136719
 1369/5000: episode: 1369, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3458.270, mean reward: -3458.270 [-3458.270, -3458.270], mean action: 2.000 [2.000, 2.000],  loss: 5912782.500000, mae: 4200.799805, mean_q: -2746.510498
 1370/5000: episode: 1370, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -9128.905, mean reward: -9128.905 [-9128.905, -9128.905], mean action: 2.000 [2.000, 2.000],  loss: 3615049.500000, mae: 4073.483398, mean_q: -2752.306396
 1371/5000: episode: 1371, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6677.123, mean reward: -6677.123 [-6677.123, -6677.123], mean action: 1.000 [1.000, 1.000],  loss: 2646791.000000, mae: 3971.002441, mean_q: -2749.041504
 1372/5000: episode: 1372, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -884.267, mean reward: -884.267 [-884.267, -884.267], mean action: 2.000 [2.000, 2.000],  loss: 4203339.500000, mae: 4070.733154, mean_q: -2768.948242
 1373/5000: episode: 1373, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -660.680, mean reward: -660.680 [-660.680, -660.680], mean action: 2.000 [2.000, 2.000],  loss: 3947692.250000, mae: 4055.049805, mean_q: -2773.835938
 1374/5000: episode: 1374, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -4922.464, mean reward: -4922.464 [-4922.464, -4922.464], mean action: 3.000 [3.000, 3.000],  loss: 2341555.000000, mae: 3982.354980, mean_q: -2778.076172
 1375/5000: episode: 1375, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2815.641, mean reward: -2815.641 [-2815.641, -2815.641], mean action: 1.000 [1.000, 1.000],  loss: 3973032.500000, mae: 4046.585449, mean_q: -2791.578613
 1376/5000: episode: 1376, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2849.310, mean reward: -2849.310 [-2849.310, -2849.310], mean action: 2.000 [2.000, 2.000],  loss: 3047324.250000, mae: 3995.371338, mean_q: -2800.654785
 1377/5000: episode: 1377, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1455.418, mean reward: -1455.418 [-1455.418, -1455.418], mean action: 2.000 [2.000, 2.000],  loss: 3198791.750000, mae: 4091.652832, mean_q: -2821.789062
 1378/5000: episode: 1378, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4161.812, mean reward: -4161.812 [-4161.812, -4161.812], mean action: 2.000 [2.000, 2.000],  loss: 3649303.500000, mae: 4062.863281, mean_q: -2824.594238
 1379/5000: episode: 1379, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5185.077, mean reward: -5185.077 [-5185.077, -5185.077], mean action: 2.000 [2.000, 2.000],  loss: 2630856.500000, mae: 4038.329834, mean_q: -2820.478516
 1380/5000: episode: 1380, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -613.046, mean reward: -613.046 [-613.046, -613.046], mean action: 2.000 [2.000, 2.000],  loss: 2400083.000000, mae: 3993.252197, mean_q: -2826.724609
 1381/5000: episode: 1381, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -961.094, mean reward: -961.094 [-961.094, -961.094], mean action: 2.000 [2.000, 2.000],  loss: 2693986.750000, mae: 3988.184570, mean_q: -2835.310547
 1382/5000: episode: 1382, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5940.445, mean reward: -5940.445 [-5940.445, -5940.445], mean action: 2.000 [2.000, 2.000],  loss: 3675747.000000, mae: 4088.930908, mean_q: -2819.032227
 1383/5000: episode: 1383, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5552.880, mean reward: -5552.880 [-5552.880, -5552.880], mean action: 2.000 [2.000, 2.000],  loss: 3954518.000000, mae: 4152.937988, mean_q: -2827.597656
 1384/5000: episode: 1384, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -556.989, mean reward: -556.989 [-556.989, -556.989], mean action: 2.000 [2.000, 2.000],  loss: 3239376.750000, mae: 4026.799072, mean_q: -2814.945557
 1385/5000: episode: 1385, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2773.589, mean reward: -2773.589 [-2773.589, -2773.589], mean action: 2.000 [2.000, 2.000],  loss: 4964371.500000, mae: 4206.834961, mean_q: -2819.303223
 1386/5000: episode: 1386, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3576.483, mean reward: -3576.483 [-3576.483, -3576.483], mean action: 2.000 [2.000, 2.000],  loss: 1649903.750000, mae: 3981.244629, mean_q: -2838.518066
 1387/5000: episode: 1387, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -267.953, mean reward: -267.953 [-267.953, -267.953], mean action: 2.000 [2.000, 2.000],  loss: 4228497.000000, mae: 4089.848389, mean_q: -2819.406738
 1388/5000: episode: 1388, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8453.518, mean reward: -8453.518 [-8453.518, -8453.518], mean action: 3.000 [3.000, 3.000],  loss: 2678774.500000, mae: 4083.863770, mean_q: -2819.995117
 1389/5000: episode: 1389, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2836.276, mean reward: -2836.276 [-2836.276, -2836.276], mean action: 2.000 [2.000, 2.000],  loss: 3393983.500000, mae: 4020.850342, mean_q: -2820.734619
 1390/5000: episode: 1390, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2176.891, mean reward: -2176.891 [-2176.891, -2176.891], mean action: 2.000 [2.000, 2.000],  loss: 6512727.000000, mae: 4169.492676, mean_q: -2822.641113
 1391/5000: episode: 1391, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -796.664, mean reward: -796.664 [-796.664, -796.664], mean action: 2.000 [2.000, 2.000],  loss: 4038599.250000, mae: 4073.605713, mean_q: -2823.820801
 1392/5000: episode: 1392, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1393.622, mean reward: -1393.622 [-1393.622, -1393.622], mean action: 2.000 [2.000, 2.000],  loss: 3108909.000000, mae: 4050.877686, mean_q: -2822.300537
 1393/5000: episode: 1393, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1448.491, mean reward: -1448.491 [-1448.491, -1448.491], mean action: 2.000 [2.000, 2.000],  loss: 3077053.500000, mae: 4012.766357, mean_q: -2821.619873
 1394/5000: episode: 1394, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1771.359, mean reward: -1771.359 [-1771.359, -1771.359], mean action: 2.000 [2.000, 2.000],  loss: 4519611.000000, mae: 4109.103516, mean_q: -2819.821533
 1395/5000: episode: 1395, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2869.885, mean reward: -2869.885 [-2869.885, -2869.885], mean action: 0.000 [0.000, 0.000],  loss: 4394994.000000, mae: 4185.597656, mean_q: -2801.629395
 1396/5000: episode: 1396, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3468.260, mean reward: -3468.260 [-3468.260, -3468.260], mean action: 2.000 [2.000, 2.000],  loss: 3061537.750000, mae: 4058.778320, mean_q: -2815.539062
 1397/5000: episode: 1397, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2728.367, mean reward: -2728.367 [-2728.367, -2728.367], mean action: 2.000 [2.000, 2.000],  loss: 2887147.000000, mae: 4025.742920, mean_q: -2794.799316
 1398/5000: episode: 1398, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -611.443, mean reward: -611.443 [-611.443, -611.443], mean action: 2.000 [2.000, 2.000],  loss: 3841637.500000, mae: 4081.491699, mean_q: -2798.252686
 1399/5000: episode: 1399, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4644.615, mean reward: -4644.615 [-4644.615, -4644.615], mean action: 2.000 [2.000, 2.000],  loss: 3503374.000000, mae: 4032.965820, mean_q: -2799.790039
 1400/5000: episode: 1400, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2360.233, mean reward: -2360.233 [-2360.233, -2360.233], mean action: 2.000 [2.000, 2.000],  loss: 3932851.750000, mae: 4154.930664, mean_q: -2803.593750
 1401/5000: episode: 1401, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2538.877, mean reward: -2538.877 [-2538.877, -2538.877], mean action: 2.000 [2.000, 2.000],  loss: 3778457.000000, mae: 4068.302979, mean_q: -2808.261475
 1402/5000: episode: 1402, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9993.786, mean reward: -9993.786 [-9993.786, -9993.786], mean action: 0.000 [0.000, 0.000],  loss: 4741282.000000, mae: 4124.721680, mean_q: -2812.855469
 1403/5000: episode: 1403, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -505.375, mean reward: -505.375 [-505.375, -505.375], mean action: 2.000 [2.000, 2.000],  loss: 3201624.500000, mae: 4099.498535, mean_q: -2801.370850
 1404/5000: episode: 1404, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2543.318, mean reward: -2543.318 [-2543.318, -2543.318], mean action: 2.000 [2.000, 2.000],  loss: 2822452.750000, mae: 4060.668457, mean_q: -2814.334229
 1405/5000: episode: 1405, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1822.667, mean reward: -1822.667 [-1822.667, -1822.667], mean action: 2.000 [2.000, 2.000],  loss: 2740702.000000, mae: 4101.913086, mean_q: -2835.983887
 1406/5000: episode: 1406, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -890.733, mean reward: -890.733 [-890.733, -890.733], mean action: 2.000 [2.000, 2.000],  loss: 4142895.000000, mae: 4195.913574, mean_q: -2823.311523
 1407/5000: episode: 1407, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5521.773, mean reward: -5521.773 [-5521.773, -5521.773], mean action: 2.000 [2.000, 2.000],  loss: 3625411.000000, mae: 4101.879395, mean_q: -2823.983398
 1408/5000: episode: 1408, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2047.203, mean reward: -2047.203 [-2047.203, -2047.203], mean action: 2.000 [2.000, 2.000],  loss: 2986311.500000, mae: 3996.210449, mean_q: -2796.724609
 1409/5000: episode: 1409, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5460.077, mean reward: -5460.077 [-5460.077, -5460.077], mean action: 2.000 [2.000, 2.000],  loss: 6071071.500000, mae: 4206.236328, mean_q: -2796.197266
 1410/5000: episode: 1410, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -508.541, mean reward: -508.541 [-508.541, -508.541], mean action: 2.000 [2.000, 2.000],  loss: 2472369.750000, mae: 4043.359863, mean_q: -2801.111816
 1411/5000: episode: 1411, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2172.611, mean reward: -2172.611 [-2172.611, -2172.611], mean action: 2.000 [2.000, 2.000],  loss: 3507354.000000, mae: 4019.668457, mean_q: -2789.548828
 1412/5000: episode: 1412, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4886.347, mean reward: -4886.347 [-4886.347, -4886.347], mean action: 2.000 [2.000, 2.000],  loss: 4969333.500000, mae: 4126.232910, mean_q: -2805.861816
 1413/5000: episode: 1413, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -253.255, mean reward: -253.255 [-253.255, -253.255], mean action: 2.000 [2.000, 2.000],  loss: 3600637.000000, mae: 4072.796875, mean_q: -2802.877930
 1414/5000: episode: 1414, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -240.841, mean reward: -240.841 [-240.841, -240.841], mean action: 2.000 [2.000, 2.000],  loss: 4226758.000000, mae: 4089.174316, mean_q: -2798.041992
 1415/5000: episode: 1415, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4757.086, mean reward: -4757.086 [-4757.086, -4757.086], mean action: 0.000 [0.000, 0.000],  loss: 2472666.250000, mae: 3993.147949, mean_q: -2801.192383
 1416/5000: episode: 1416, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1270.322, mean reward: -1270.322 [-1270.322, -1270.322], mean action: 2.000 [2.000, 2.000],  loss: 2529361.500000, mae: 4011.497070, mean_q: -2807.401611
 1417/5000: episode: 1417, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3636.443, mean reward: -3636.443 [-3636.443, -3636.443], mean action: 2.000 [2.000, 2.000],  loss: 3396928.000000, mae: 4003.875000, mean_q: -2806.056885
 1418/5000: episode: 1418, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1382.243, mean reward: -1382.243 [-1382.243, -1382.243], mean action: 2.000 [2.000, 2.000],  loss: 4093266.500000, mae: 4188.993164, mean_q: -2806.109863
 1419/5000: episode: 1419, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1449.449, mean reward: -1449.449 [-1449.449, -1449.449], mean action: 2.000 [2.000, 2.000],  loss: 4512585.500000, mae: 4127.934570, mean_q: -2809.185547
 1420/5000: episode: 1420, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2547.162, mean reward: -2547.162 [-2547.162, -2547.162], mean action: 1.000 [1.000, 1.000],  loss: 2962191.250000, mae: 4004.202637, mean_q: -2796.694824
 1421/5000: episode: 1421, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2502.596, mean reward: -2502.596 [-2502.596, -2502.596], mean action: 2.000 [2.000, 2.000],  loss: 4971969.000000, mae: 4169.406250, mean_q: -2803.780029
 1422/5000: episode: 1422, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3262.508, mean reward: -3262.508 [-3262.508, -3262.508], mean action: 2.000 [2.000, 2.000],  loss: 2264618.000000, mae: 4069.072510, mean_q: -2811.397705
 1423/5000: episode: 1423, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3635.316, mean reward: -3635.316 [-3635.316, -3635.316], mean action: 2.000 [2.000, 2.000],  loss: 2541077.000000, mae: 4017.307129, mean_q: -2807.343262
 1424/5000: episode: 1424, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1926.359, mean reward: -1926.359 [-1926.359, -1926.359], mean action: 2.000 [2.000, 2.000],  loss: 2585972.750000, mae: 4046.016602, mean_q: -2786.142090
 1425/5000: episode: 1425, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3069.371, mean reward: -3069.371 [-3069.371, -3069.371], mean action: 2.000 [2.000, 2.000],  loss: 3701359.250000, mae: 4142.943359, mean_q: -2811.312012
 1426/5000: episode: 1426, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -45.849, mean reward: -45.849 [-45.849, -45.849], mean action: 2.000 [2.000, 2.000],  loss: 2253796.000000, mae: 4042.831543, mean_q: -2826.414551
 1427/5000: episode: 1427, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -282.303, mean reward: -282.303 [-282.303, -282.303], mean action: 2.000 [2.000, 2.000],  loss: 2572830.500000, mae: 3986.941406, mean_q: -2792.841797
 1428/5000: episode: 1428, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1891.453, mean reward: -1891.453 [-1891.453, -1891.453], mean action: 2.000 [2.000, 2.000],  loss: 2367446.750000, mae: 3984.221436, mean_q: -2784.399902
 1429/5000: episode: 1429, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2677.685, mean reward: -2677.685 [-2677.685, -2677.685], mean action: 3.000 [3.000, 3.000],  loss: 3444098.000000, mae: 4227.484375, mean_q: -2787.431641
 1430/5000: episode: 1430, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5739.948, mean reward: -5739.948 [-5739.948, -5739.948], mean action: 2.000 [2.000, 2.000],  loss: 3504963.500000, mae: 4139.385742, mean_q: -2765.446777
 1431/5000: episode: 1431, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -399.897, mean reward: -399.897 [-399.897, -399.897], mean action: 2.000 [2.000, 2.000],  loss: 5052415.000000, mae: 4155.404785, mean_q: -2748.473145
 1432/5000: episode: 1432, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2575.251, mean reward: -2575.251 [-2575.251, -2575.251], mean action: 2.000 [2.000, 2.000],  loss: 3811757.750000, mae: 4158.166992, mean_q: -2739.505371
 1433/5000: episode: 1433, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -684.735, mean reward: -684.735 [-684.735, -684.735], mean action: 2.000 [2.000, 2.000],  loss: 2669999.250000, mae: 4031.339355, mean_q: -2734.596191
 1434/5000: episode: 1434, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1392.036, mean reward: -1392.036 [-1392.036, -1392.036], mean action: 2.000 [2.000, 2.000],  loss: 2772905.750000, mae: 4070.339844, mean_q: -2735.045898
 1435/5000: episode: 1435, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3095.278, mean reward: -3095.278 [-3095.278, -3095.278], mean action: 2.000 [2.000, 2.000],  loss: 2415057.000000, mae: 4024.461182, mean_q: -2740.338867
 1436/5000: episode: 1436, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -573.645, mean reward: -573.645 [-573.645, -573.645], mean action: 2.000 [2.000, 2.000],  loss: 3957909.500000, mae: 4148.666992, mean_q: -2751.558838
 1437/5000: episode: 1437, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5058.163, mean reward: -5058.163 [-5058.163, -5058.163], mean action: 2.000 [2.000, 2.000],  loss: 2757653.500000, mae: 3998.389893, mean_q: -2750.451660
 1438/5000: episode: 1438, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1739.299, mean reward: -1739.299 [-1739.299, -1739.299], mean action: 2.000 [2.000, 2.000],  loss: 2691423.000000, mae: 4084.043457, mean_q: -2757.792969
 1439/5000: episode: 1439, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -547.488, mean reward: -547.488 [-547.488, -547.488], mean action: 2.000 [2.000, 2.000],  loss: 3668248.500000, mae: 4172.884766, mean_q: -2766.810059
 1440/5000: episode: 1440, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5114.028, mean reward: -5114.028 [-5114.028, -5114.028], mean action: 2.000 [2.000, 2.000],  loss: 3519632.000000, mae: 4110.784668, mean_q: -2764.053223
 1441/5000: episode: 1441, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1731.932, mean reward: -1731.932 [-1731.932, -1731.932], mean action: 2.000 [2.000, 2.000],  loss: 3720999.500000, mae: 4125.218750, mean_q: -2767.024658
 1442/5000: episode: 1442, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3085.549, mean reward: -3085.549 [-3085.549, -3085.549], mean action: 2.000 [2.000, 2.000],  loss: 1762990.250000, mae: 3931.331543, mean_q: -2778.130371
 1443/5000: episode: 1443, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4597.414, mean reward: -4597.414 [-4597.414, -4597.414], mean action: 2.000 [2.000, 2.000],  loss: 2157681.500000, mae: 3974.397461, mean_q: -2783.694092
 1444/5000: episode: 1444, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -4822.092, mean reward: -4822.092 [-4822.092, -4822.092], mean action: 2.000 [2.000, 2.000],  loss: 2327360.250000, mae: 4004.369873, mean_q: -2787.118896
 1445/5000: episode: 1445, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1757.125, mean reward: -1757.125 [-1757.125, -1757.125], mean action: 2.000 [2.000, 2.000],  loss: 2854233.000000, mae: 4082.383545, mean_q: -2780.031006
 1446/5000: episode: 1446, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1420.913, mean reward: -1420.913 [-1420.913, -1420.913], mean action: 2.000 [2.000, 2.000],  loss: 4137949.750000, mae: 4188.317383, mean_q: -2780.752930
 1447/5000: episode: 1447, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8412.740, mean reward: -8412.740 [-8412.740, -8412.740], mean action: 2.000 [2.000, 2.000],  loss: 2440356.000000, mae: 4065.947266, mean_q: -2796.012939
 1448/5000: episode: 1448, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3802.973, mean reward: -3802.973 [-3802.973, -3802.973], mean action: 2.000 [2.000, 2.000],  loss: 2671448.000000, mae: 4103.054688, mean_q: -2807.890137
 1449/5000: episode: 1449, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -351.833, mean reward: -351.833 [-351.833, -351.833], mean action: 2.000 [2.000, 2.000],  loss: 4523032.000000, mae: 4232.710449, mean_q: -2787.721924
 1450/5000: episode: 1450, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2168.343, mean reward: -2168.343 [-2168.343, -2168.343], mean action: 2.000 [2.000, 2.000],  loss: 2439434.000000, mae: 4065.035156, mean_q: -2804.740234
 1451/5000: episode: 1451, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -122.594, mean reward: -122.594 [-122.594, -122.594], mean action: 2.000 [2.000, 2.000],  loss: 2086724.875000, mae: 4064.763672, mean_q: -2795.315430
 1452/5000: episode: 1452, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5714.470, mean reward: -5714.470 [-5714.470, -5714.470], mean action: 2.000 [2.000, 2.000],  loss: 4550056.000000, mae: 4162.329102, mean_q: -2770.639404
 1453/5000: episode: 1453, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -969.876, mean reward: -969.876 [-969.876, -969.876], mean action: 2.000 [2.000, 2.000],  loss: 3060472.500000, mae: 4077.140625, mean_q: -2774.921387
 1454/5000: episode: 1454, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5640.084, mean reward: -5640.084 [-5640.084, -5640.084], mean action: 1.000 [1.000, 1.000],  loss: 4232645.000000, mae: 4249.480957, mean_q: -2777.454346
 1455/5000: episode: 1455, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2291.655, mean reward: -2291.655 [-2291.655, -2291.655], mean action: 2.000 [2.000, 2.000],  loss: 3008938.000000, mae: 4150.791504, mean_q: -2790.139648
 1456/5000: episode: 1456, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2033.805, mean reward: -2033.805 [-2033.805, -2033.805], mean action: 2.000 [2.000, 2.000],  loss: 3415343.500000, mae: 4098.624512, mean_q: -2788.454102
 1457/5000: episode: 1457, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5834.416, mean reward: -5834.416 [-5834.416, -5834.416], mean action: 2.000 [2.000, 2.000],  loss: 3143513.500000, mae: 4068.185547, mean_q: -2797.664551
 1458/5000: episode: 1458, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3368.749, mean reward: -3368.749 [-3368.749, -3368.749], mean action: 2.000 [2.000, 2.000],  loss: 2620761.000000, mae: 4061.218262, mean_q: -2796.159180
 1459/5000: episode: 1459, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1261.272, mean reward: -1261.272 [-1261.272, -1261.272], mean action: 1.000 [1.000, 1.000],  loss: 4828477.000000, mae: 4348.456543, mean_q: -2808.144531
 1460/5000: episode: 1460, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5346.445, mean reward: -5346.445 [-5346.445, -5346.445], mean action: 2.000 [2.000, 2.000],  loss: 4317497.000000, mae: 4170.842773, mean_q: -2790.563477
 1461/5000: episode: 1461, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -261.156, mean reward: -261.156 [-261.156, -261.156], mean action: 2.000 [2.000, 2.000],  loss: 2797294.750000, mae: 4017.850586, mean_q: -2775.991699
 1462/5000: episode: 1462, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1958.376, mean reward: -1958.376 [-1958.376, -1958.376], mean action: 2.000 [2.000, 2.000],  loss: 3112142.250000, mae: 4178.397461, mean_q: -2775.774902
 1463/5000: episode: 1463, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -785.653, mean reward: -785.653 [-785.653, -785.653], mean action: 2.000 [2.000, 2.000],  loss: 4000072.000000, mae: 4289.347168, mean_q: -2777.729980
 1464/5000: episode: 1464, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -427.228, mean reward: -427.228 [-427.228, -427.228], mean action: 2.000 [2.000, 2.000],  loss: 3464065.500000, mae: 4172.294922, mean_q: -2754.063232
 1465/5000: episode: 1465, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1406.166, mean reward: -1406.166 [-1406.166, -1406.166], mean action: 2.000 [2.000, 2.000],  loss: 4738551.000000, mae: 4274.388672, mean_q: -2751.789062
 1466/5000: episode: 1466, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -838.581, mean reward: -838.581 [-838.581, -838.581], mean action: 2.000 [2.000, 2.000],  loss: 4592429.000000, mae: 4240.323242, mean_q: -2763.351562
 1467/5000: episode: 1467, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -669.940, mean reward: -669.940 [-669.940, -669.940], mean action: 2.000 [2.000, 2.000],  loss: 3272822.500000, mae: 4077.636963, mean_q: -2761.900391
 1468/5000: episode: 1468, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4283.542, mean reward: -4283.542 [-4283.542, -4283.542], mean action: 2.000 [2.000, 2.000],  loss: 5296155.000000, mae: 4347.924316, mean_q: -2753.029785
 1469/5000: episode: 1469, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -173.390, mean reward: -173.390 [-173.390, -173.390], mean action: 2.000 [2.000, 2.000],  loss: 3384151.500000, mae: 4213.139648, mean_q: -2739.879883
 1470/5000: episode: 1470, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1016.755, mean reward: -1016.755 [-1016.755, -1016.755], mean action: 2.000 [2.000, 2.000],  loss: 4060285.500000, mae: 4313.285645, mean_q: -2734.273438
 1471/5000: episode: 1471, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1728.478, mean reward: -1728.478 [-1728.478, -1728.478], mean action: 2.000 [2.000, 2.000],  loss: 2378798.500000, mae: 4130.917969, mean_q: -2717.566406
 1472/5000: episode: 1472, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2292.779, mean reward: -2292.779 [-2292.779, -2292.779], mean action: 2.000 [2.000, 2.000],  loss: 3057243.000000, mae: 4192.296387, mean_q: -2693.040527
 1473/5000: episode: 1473, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -103.447, mean reward: -103.447 [-103.447, -103.447], mean action: 2.000 [2.000, 2.000],  loss: 4323702.000000, mae: 4199.413574, mean_q: -2701.451172
 1474/5000: episode: 1474, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3726.113, mean reward: -3726.113 [-3726.113, -3726.113], mean action: 2.000 [2.000, 2.000],  loss: 2294343.500000, mae: 4110.667969, mean_q: -2684.932373
 1475/5000: episode: 1475, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -437.500, mean reward: -437.500 [-437.500, -437.500], mean action: 2.000 [2.000, 2.000],  loss: 4178921.500000, mae: 4261.416992, mean_q: -2683.439453
 1476/5000: episode: 1476, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7142.044, mean reward: -7142.044 [-7142.044, -7142.044], mean action: 2.000 [2.000, 2.000],  loss: 4930255.000000, mae: 4243.272461, mean_q: -2680.055176
 1477/5000: episode: 1477, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2202.477, mean reward: -2202.477 [-2202.477, -2202.477], mean action: 3.000 [3.000, 3.000],  loss: 3679490.000000, mae: 4164.573242, mean_q: -2677.186035
 1478/5000: episode: 1478, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4386.532, mean reward: -4386.532 [-4386.532, -4386.532], mean action: 2.000 [2.000, 2.000],  loss: 2767754.750000, mae: 4144.244629, mean_q: -2653.934570
 1479/5000: episode: 1479, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3229.081, mean reward: -3229.081 [-3229.081, -3229.081], mean action: 2.000 [2.000, 2.000],  loss: 4475625.500000, mae: 4161.766602, mean_q: -2652.148926
 1480/5000: episode: 1480, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -298.563, mean reward: -298.563 [-298.563, -298.563], mean action: 2.000 [2.000, 2.000],  loss: 3794318.750000, mae: 4138.022461, mean_q: -2637.736816
 1481/5000: episode: 1481, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9759.755, mean reward: -9759.755 [-9759.755, -9759.755], mean action: 2.000 [2.000, 2.000],  loss: 3634202.500000, mae: 4202.244141, mean_q: -2639.131836
 1482/5000: episode: 1482, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2515.782, mean reward: -2515.782 [-2515.782, -2515.782], mean action: 2.000 [2.000, 2.000],  loss: 3687822.250000, mae: 4227.250977, mean_q: -2645.969727
 1483/5000: episode: 1483, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3100.347, mean reward: -3100.347 [-3100.347, -3100.347], mean action: 2.000 [2.000, 2.000],  loss: 2125440.000000, mae: 4076.103760, mean_q: -2647.644043
 1484/5000: episode: 1484, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8844.120, mean reward: -8844.120 [-8844.120, -8844.120], mean action: 2.000 [2.000, 2.000],  loss: 1809717.000000, mae: 4063.504883, mean_q: -2636.028076
 1485/5000: episode: 1485, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4389.366, mean reward: -4389.366 [-4389.366, -4389.366], mean action: 2.000 [2.000, 2.000],  loss: 4141647.500000, mae: 4196.316895, mean_q: -2641.460938
 1486/5000: episode: 1486, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5417.124, mean reward: -5417.124 [-5417.124, -5417.124], mean action: 2.000 [2.000, 2.000],  loss: 2696919.750000, mae: 4113.381836, mean_q: -2643.808594
 1487/5000: episode: 1487, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4139.984, mean reward: -4139.984 [-4139.984, -4139.984], mean action: 3.000 [3.000, 3.000],  loss: 3635000.000000, mae: 4105.833008, mean_q: -2649.696289
 1488/5000: episode: 1488, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3949.061, mean reward: -3949.061 [-3949.061, -3949.061], mean action: 2.000 [2.000, 2.000],  loss: 5382681.000000, mae: 4180.322266, mean_q: -2650.458008
 1489/5000: episode: 1489, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1048.542, mean reward: -1048.542 [-1048.542, -1048.542], mean action: 2.000 [2.000, 2.000],  loss: 2330582.500000, mae: 4106.240234, mean_q: -2656.210938
 1490/5000: episode: 1490, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3019.286, mean reward: -3019.286 [-3019.286, -3019.286], mean action: 2.000 [2.000, 2.000],  loss: 3219478.250000, mae: 4122.663086, mean_q: -2644.879883
 1491/5000: episode: 1491, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -198.133, mean reward: -198.133 [-198.133, -198.133], mean action: 2.000 [2.000, 2.000],  loss: 2328000.000000, mae: 4087.613770, mean_q: -2651.183105
 1492/5000: episode: 1492, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8296.423, mean reward: -8296.423 [-8296.423, -8296.423], mean action: 2.000 [2.000, 2.000],  loss: 3954318.500000, mae: 4193.851074, mean_q: -2657.787842
 1493/5000: episode: 1493, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -306.701, mean reward: -306.701 [-306.701, -306.701], mean action: 2.000 [2.000, 2.000],  loss: 2421953.500000, mae: 4082.731934, mean_q: -2636.795410
 1494/5000: episode: 1494, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3614.187, mean reward: -3614.187 [-3614.187, -3614.187], mean action: 2.000 [2.000, 2.000],  loss: 5588448.500000, mae: 4265.382324, mean_q: -2639.960938
 1495/5000: episode: 1495, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5296.754, mean reward: -5296.754 [-5296.754, -5296.754], mean action: 2.000 [2.000, 2.000],  loss: 2733044.250000, mae: 3923.317871, mean_q: -2645.960449
 1496/5000: episode: 1496, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -129.217, mean reward: -129.217 [-129.217, -129.217], mean action: 2.000 [2.000, 2.000],  loss: 2926318.500000, mae: 4185.985840, mean_q: -2660.864746
 1497/5000: episode: 1497, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -859.952, mean reward: -859.952 [-859.952, -859.952], mean action: 2.000 [2.000, 2.000],  loss: 3851447.250000, mae: 4164.219727, mean_q: -2661.347168
 1498/5000: episode: 1498, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -416.516, mean reward: -416.516 [-416.516, -416.516], mean action: 2.000 [2.000, 2.000],  loss: 3909647.500000, mae: 4222.786133, mean_q: -2672.496826
 1499/5000: episode: 1499, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2496.352, mean reward: -2496.352 [-2496.352, -2496.352], mean action: 2.000 [2.000, 2.000],  loss: 4737771.500000, mae: 4301.589844, mean_q: -2683.181152
 1500/5000: episode: 1500, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -2584.930, mean reward: -2584.930 [-2584.930, -2584.930], mean action: 2.000 [2.000, 2.000],  loss: 3326773.500000, mae: 4206.939453, mean_q: -2693.941406
 1501/5000: episode: 1501, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -96.050, mean reward: -96.050 [-96.050, -96.050], mean action: 2.000 [2.000, 2.000],  loss: 4713524.000000, mae: 4304.577148, mean_q: -2699.288330
 1502/5000: episode: 1502, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1536.548, mean reward: -1536.548 [-1536.548, -1536.548], mean action: 2.000 [2.000, 2.000],  loss: 2809926.500000, mae: 4126.970703, mean_q: -2696.646240
 1503/5000: episode: 1503, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -195.165, mean reward: -195.165 [-195.165, -195.165], mean action: 2.000 [2.000, 2.000],  loss: 4811500.500000, mae: 4206.266602, mean_q: -2723.962891
 1504/5000: episode: 1504, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4040.057, mean reward: -4040.057 [-4040.057, -4040.057], mean action: 2.000 [2.000, 2.000],  loss: 2168804.000000, mae: 4095.222168, mean_q: -2730.989990
 1505/5000: episode: 1505, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -2463.757, mean reward: -2463.757 [-2463.757, -2463.757], mean action: 2.000 [2.000, 2.000],  loss: 4274017.000000, mae: 4274.498047, mean_q: -2710.128174
 1506/5000: episode: 1506, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6586.854, mean reward: -6586.854 [-6586.854, -6586.854], mean action: 2.000 [2.000, 2.000],  loss: 2688060.500000, mae: 4144.212891, mean_q: -2718.426758
 1507/5000: episode: 1507, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1412.035, mean reward: -1412.035 [-1412.035, -1412.035], mean action: 2.000 [2.000, 2.000],  loss: 3603153.500000, mae: 4197.086914, mean_q: -2717.507324
 1508/5000: episode: 1508, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1435.048, mean reward: -1435.048 [-1435.048, -1435.048], mean action: 2.000 [2.000, 2.000],  loss: 3988892.500000, mae: 4196.837891, mean_q: -2707.639893
 1509/5000: episode: 1509, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -724.799, mean reward: -724.799 [-724.799, -724.799], mean action: 2.000 [2.000, 2.000],  loss: 1757020.375000, mae: 3948.546875, mean_q: -2733.535645
 1510/5000: episode: 1510, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -82.403, mean reward: -82.403 [-82.403, -82.403], mean action: 2.000 [2.000, 2.000],  loss: 3821370.500000, mae: 4207.198242, mean_q: -2727.672852
 1511/5000: episode: 1511, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2220.125, mean reward: -2220.125 [-2220.125, -2220.125], mean action: 2.000 [2.000, 2.000],  loss: 2803809.500000, mae: 4217.520508, mean_q: -2721.908203
 1512/5000: episode: 1512, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -8585.751, mean reward: -8585.751 [-8585.751, -8585.751], mean action: 2.000 [2.000, 2.000],  loss: 3214242.000000, mae: 4137.223633, mean_q: -2730.765137
 1513/5000: episode: 1513, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3154.980, mean reward: -3154.980 [-3154.980, -3154.980], mean action: 2.000 [2.000, 2.000],  loss: 3923594.250000, mae: 4246.693848, mean_q: -2735.881836
 1514/5000: episode: 1514, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -479.054, mean reward: -479.054 [-479.054, -479.054], mean action: 2.000 [2.000, 2.000],  loss: 4724809.500000, mae: 4312.727051, mean_q: -2750.128174
 1515/5000: episode: 1515, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1618.894, mean reward: -1618.894 [-1618.894, -1618.894], mean action: 2.000 [2.000, 2.000],  loss: 2432741.250000, mae: 4130.865234, mean_q: -2771.550781
 1516/5000: episode: 1516, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -704.049, mean reward: -704.049 [-704.049, -704.049], mean action: 2.000 [2.000, 2.000],  loss: 4457615.500000, mae: 4203.017578, mean_q: -2773.891113
 1517/5000: episode: 1517, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4291.497, mean reward: -4291.497 [-4291.497, -4291.497], mean action: 0.000 [0.000, 0.000],  loss: 3483746.500000, mae: 4232.918945, mean_q: -2782.941406
 1518/5000: episode: 1518, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6055.252, mean reward: -6055.252 [-6055.252, -6055.252], mean action: 2.000 [2.000, 2.000],  loss: 4632185.000000, mae: 4215.976562, mean_q: -2793.240723
 1519/5000: episode: 1519, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -223.039, mean reward: -223.039 [-223.039, -223.039], mean action: 2.000 [2.000, 2.000],  loss: 3852204.500000, mae: 4275.300781, mean_q: -2800.515137
 1520/5000: episode: 1520, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2542.467, mean reward: -2542.467 [-2542.467, -2542.467], mean action: 2.000 [2.000, 2.000],  loss: 3643414.750000, mae: 4262.142578, mean_q: -2808.099854
 1521/5000: episode: 1521, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3698.264, mean reward: -3698.264 [-3698.264, -3698.264], mean action: 2.000 [2.000, 2.000],  loss: 4102098.250000, mae: 4246.335938, mean_q: -2819.927734
 1522/5000: episode: 1522, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4051.914, mean reward: -4051.914 [-4051.914, -4051.914], mean action: 2.000 [2.000, 2.000],  loss: 2836729.500000, mae: 4157.701172, mean_q: -2818.491211
 1523/5000: episode: 1523, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3938.040, mean reward: -3938.040 [-3938.040, -3938.040], mean action: 2.000 [2.000, 2.000],  loss: 1934302.000000, mae: 4162.640137, mean_q: -2828.623047
 1524/5000: episode: 1524, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3671.254, mean reward: -3671.254 [-3671.254, -3671.254], mean action: 2.000 [2.000, 2.000],  loss: 5098109.500000, mae: 4331.522461, mean_q: -2824.709473
 1525/5000: episode: 1525, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5.525, mean reward: -5.525 [-5.525, -5.525], mean action: 2.000 [2.000, 2.000],  loss: 2910821.000000, mae: 4201.940430, mean_q: -2821.326172
 1526/5000: episode: 1526, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1570.467, mean reward: -1570.467 [-1570.467, -1570.467], mean action: 2.000 [2.000, 2.000],  loss: 3614033.500000, mae: 4255.597168, mean_q: -2830.392822
 1527/5000: episode: 1527, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3157.423, mean reward: -3157.423 [-3157.423, -3157.423], mean action: 2.000 [2.000, 2.000],  loss: 3062319.000000, mae: 4135.067383, mean_q: -2815.680420
 1528/5000: episode: 1528, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1505.583, mean reward: -1505.583 [-1505.583, -1505.583], mean action: 2.000 [2.000, 2.000],  loss: 3245517.250000, mae: 4142.983887, mean_q: -2797.923828
 1529/5000: episode: 1529, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -466.723, mean reward: -466.723 [-466.723, -466.723], mean action: 2.000 [2.000, 2.000],  loss: 4180013.750000, mae: 4305.694824, mean_q: -2808.166992
 1530/5000: episode: 1530, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -57.129, mean reward: -57.129 [-57.129, -57.129], mean action: 2.000 [2.000, 2.000],  loss: 3402472.000000, mae: 4206.172852, mean_q: -2805.420898
 1531/5000: episode: 1531, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -747.473, mean reward: -747.473 [-747.473, -747.473], mean action: 2.000 [2.000, 2.000],  loss: 3364348.250000, mae: 4312.554688, mean_q: -2787.538574
 1532/5000: episode: 1532, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4059.552, mean reward: -4059.552 [-4059.552, -4059.552], mean action: 2.000 [2.000, 2.000],  loss: 4458536.000000, mae: 4413.589355, mean_q: -2798.178711
 1533/5000: episode: 1533, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4529.409, mean reward: -4529.409 [-4529.409, -4529.409], mean action: 2.000 [2.000, 2.000],  loss: 3749856.250000, mae: 4216.195801, mean_q: -2775.320312
 1534/5000: episode: 1534, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9187.233, mean reward: -9187.233 [-9187.233, -9187.233], mean action: 2.000 [2.000, 2.000],  loss: 3245890.750000, mae: 4292.145508, mean_q: -2767.059570
 1535/5000: episode: 1535, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3349.904, mean reward: -3349.904 [-3349.904, -3349.904], mean action: 2.000 [2.000, 2.000],  loss: 3515611.000000, mae: 4234.604980, mean_q: -2740.419922
 1536/5000: episode: 1536, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -537.457, mean reward: -537.457 [-537.457, -537.457], mean action: 2.000 [2.000, 2.000],  loss: 2007759.500000, mae: 4188.425781, mean_q: -2731.162109
 1537/5000: episode: 1537, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4838.317, mean reward: -4838.317 [-4838.317, -4838.317], mean action: 2.000 [2.000, 2.000],  loss: 3534653.500000, mae: 4307.121094, mean_q: -2706.956055
 1538/5000: episode: 1538, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5907.819, mean reward: -5907.819 [-5907.819, -5907.819], mean action: 1.000 [1.000, 1.000],  loss: 3288554.750000, mae: 4254.575684, mean_q: -2697.849609
 1539/5000: episode: 1539, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4420.510, mean reward: -4420.510 [-4420.510, -4420.510], mean action: 2.000 [2.000, 2.000],  loss: 2376083.750000, mae: 4165.919922, mean_q: -2704.094238
 1540/5000: episode: 1540, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3670.907, mean reward: -3670.907 [-3670.907, -3670.907], mean action: 2.000 [2.000, 2.000],  loss: 3923097.750000, mae: 4287.186523, mean_q: -2688.921387
 1541/5000: episode: 1541, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1813.570, mean reward: -1813.570 [-1813.570, -1813.570], mean action: 2.000 [2.000, 2.000],  loss: 3598876.000000, mae: 4082.634277, mean_q: -2692.653320
 1542/5000: episode: 1542, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2826.637, mean reward: -2826.637 [-2826.637, -2826.637], mean action: 2.000 [2.000, 2.000],  loss: 2671973.000000, mae: 4184.440918, mean_q: -2672.455566
 1543/5000: episode: 1543, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1038.218, mean reward: -1038.218 [-1038.218, -1038.218], mean action: 2.000 [2.000, 2.000],  loss: 3941657.500000, mae: 4313.225098, mean_q: -2681.373047
 1544/5000: episode: 1544, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -721.161, mean reward: -721.161 [-721.161, -721.161], mean action: 2.000 [2.000, 2.000],  loss: 3927154.500000, mae: 4235.514160, mean_q: -2689.391357
 1545/5000: episode: 1545, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -667.712, mean reward: -667.712 [-667.712, -667.712], mean action: 2.000 [2.000, 2.000],  loss: 5888701.000000, mae: 4336.912109, mean_q: -2688.709961
 1546/5000: episode: 1546, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2083.489, mean reward: -2083.489 [-2083.489, -2083.489], mean action: 2.000 [2.000, 2.000],  loss: 3682709.500000, mae: 4230.393066, mean_q: -2694.313965
 1547/5000: episode: 1547, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4485.285, mean reward: -4485.285 [-4485.285, -4485.285], mean action: 2.000 [2.000, 2.000],  loss: 2991458.000000, mae: 4199.083008, mean_q: -2691.480957
 1548/5000: episode: 1548, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -367.704, mean reward: -367.704 [-367.704, -367.704], mean action: 2.000 [2.000, 2.000],  loss: 3259063.750000, mae: 4290.742676, mean_q: -2680.649414
 1549/5000: episode: 1549, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5693.600, mean reward: -5693.600 [-5693.600, -5693.600], mean action: 1.000 [1.000, 1.000],  loss: 2505082.250000, mae: 4206.365234, mean_q: -2669.904541
 1550/5000: episode: 1550, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -472.702, mean reward: -472.702 [-472.702, -472.702], mean action: 2.000 [2.000, 2.000],  loss: 3316672.500000, mae: 4168.037109, mean_q: -2665.177246
 1551/5000: episode: 1551, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1898.447, mean reward: -1898.447 [-1898.447, -1898.447], mean action: 2.000 [2.000, 2.000],  loss: 2609745.750000, mae: 4175.400879, mean_q: -2664.167969
 1552/5000: episode: 1552, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2560.295, mean reward: -2560.295 [-2560.295, -2560.295], mean action: 2.000 [2.000, 2.000],  loss: 3597738.500000, mae: 4280.521973, mean_q: -2674.576660
 1553/5000: episode: 1553, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2141.845, mean reward: -2141.845 [-2141.845, -2141.845], mean action: 2.000 [2.000, 2.000],  loss: 4019383.000000, mae: 4284.833984, mean_q: -2679.157715
 1554/5000: episode: 1554, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -699.986, mean reward: -699.986 [-699.986, -699.986], mean action: 2.000 [2.000, 2.000],  loss: 3031322.750000, mae: 4184.903320, mean_q: -2661.415771
 1555/5000: episode: 1555, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3692.096, mean reward: -3692.096 [-3692.096, -3692.096], mean action: 2.000 [2.000, 2.000],  loss: 5047718.500000, mae: 4267.511719, mean_q: -2671.266602
 1556/5000: episode: 1556, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3081.491, mean reward: -3081.491 [-3081.491, -3081.491], mean action: 2.000 [2.000, 2.000],  loss: 2723879.000000, mae: 4111.987305, mean_q: -2667.740479
 1557/5000: episode: 1557, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4449.220, mean reward: -4449.220 [-4449.220, -4449.220], mean action: 1.000 [1.000, 1.000],  loss: 2107259.250000, mae: 4034.431885, mean_q: -2669.147705
 1558/5000: episode: 1558, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1296.197, mean reward: -1296.197 [-1296.197, -1296.197], mean action: 2.000 [2.000, 2.000],  loss: 3597387.500000, mae: 4235.777344, mean_q: -2645.135986
 1559/5000: episode: 1559, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1608.890, mean reward: -1608.890 [-1608.890, -1608.890], mean action: 2.000 [2.000, 2.000],  loss: 3694378.250000, mae: 4233.635254, mean_q: -2661.804688
 1560/5000: episode: 1560, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5924.690, mean reward: -5924.690 [-5924.690, -5924.690], mean action: 3.000 [3.000, 3.000],  loss: 2352873.250000, mae: 4219.555664, mean_q: -2668.143799
 1561/5000: episode: 1561, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -6191.082, mean reward: -6191.082 [-6191.082, -6191.082], mean action: 2.000 [2.000, 2.000],  loss: 3491035.000000, mae: 4267.306641, mean_q: -2683.162598
 1562/5000: episode: 1562, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -135.442, mean reward: -135.442 [-135.442, -135.442], mean action: 2.000 [2.000, 2.000],  loss: 4104780.250000, mae: 4334.148438, mean_q: -2674.190918
 1563/5000: episode: 1563, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2123.106, mean reward: -2123.106 [-2123.106, -2123.106], mean action: 2.000 [2.000, 2.000],  loss: 3092927.750000, mae: 4123.223633, mean_q: -2668.146240
 1564/5000: episode: 1564, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -682.429, mean reward: -682.429 [-682.429, -682.429], mean action: 2.000 [2.000, 2.000],  loss: 3397494.000000, mae: 4137.272949, mean_q: -2661.449707
 1565/5000: episode: 1565, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3696.677, mean reward: -3696.677 [-3696.677, -3696.677], mean action: 2.000 [2.000, 2.000],  loss: 3865750.000000, mae: 4191.953125, mean_q: -2667.969238
 1566/5000: episode: 1566, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -1443.396, mean reward: -1443.396 [-1443.396, -1443.396], mean action: 2.000 [2.000, 2.000],  loss: 2598265.000000, mae: 4180.780273, mean_q: -2674.635986
 1567/5000: episode: 1567, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1056.050, mean reward: -1056.050 [-1056.050, -1056.050], mean action: 2.000 [2.000, 2.000],  loss: 3249219.000000, mae: 4203.387207, mean_q: -2689.517090
 1568/5000: episode: 1568, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1597.891, mean reward: -1597.891 [-1597.891, -1597.891], mean action: 2.000 [2.000, 2.000],  loss: 2833929.500000, mae: 4214.742188, mean_q: -2696.123047
 1569/5000: episode: 1569, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8576.556, mean reward: -8576.556 [-8576.556, -8576.556], mean action: 0.000 [0.000, 0.000],  loss: 3331731.000000, mae: 4236.366211, mean_q: -2699.779541
 1570/5000: episode: 1570, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -934.963, mean reward: -934.963 [-934.963, -934.963], mean action: 2.000 [2.000, 2.000],  loss: 3838293.000000, mae: 4291.816406, mean_q: -2701.036133
 1571/5000: episode: 1571, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1752.132, mean reward: -1752.132 [-1752.132, -1752.132], mean action: 2.000 [2.000, 2.000],  loss: 3333367.750000, mae: 4310.135742, mean_q: -2700.172363
 1572/5000: episode: 1572, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -891.733, mean reward: -891.733 [-891.733, -891.733], mean action: 2.000 [2.000, 2.000],  loss: 2923695.500000, mae: 4244.460938, mean_q: -2705.558350
 1573/5000: episode: 1573, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5529.588, mean reward: -5529.588 [-5529.588, -5529.588], mean action: 2.000 [2.000, 2.000],  loss: 4680870.500000, mae: 4345.586426, mean_q: -2695.851074
 1574/5000: episode: 1574, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -10.730, mean reward: -10.730 [-10.730, -10.730], mean action: 3.000 [3.000, 3.000],  loss: 3888997.500000, mae: 4274.301758, mean_q: -2724.308105
 1575/5000: episode: 1575, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -571.687, mean reward: -571.687 [-571.687, -571.687], mean action: 2.000 [2.000, 2.000],  loss: 2428529.000000, mae: 4118.557617, mean_q: -2727.446777
 1576/5000: episode: 1576, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4393.855, mean reward: -4393.855 [-4393.855, -4393.855], mean action: 2.000 [2.000, 2.000],  loss: 3379621.500000, mae: 4310.297852, mean_q: -2742.956787
 1577/5000: episode: 1577, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1395.705, mean reward: -1395.705 [-1395.705, -1395.705], mean action: 2.000 [2.000, 2.000],  loss: 2299302.000000, mae: 4120.542969, mean_q: -2764.981445
 1578/5000: episode: 1578, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1395.415, mean reward: -1395.415 [-1395.415, -1395.415], mean action: 2.000 [2.000, 2.000],  loss: 3604354.500000, mae: 4188.144531, mean_q: -2763.129883
 1579/5000: episode: 1579, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -332.772, mean reward: -332.772 [-332.772, -332.772], mean action: 2.000 [2.000, 2.000],  loss: 2810511.750000, mae: 4286.846680, mean_q: -2779.063965
 1580/5000: episode: 1580, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3340.303, mean reward: -3340.303 [-3340.303, -3340.303], mean action: 2.000 [2.000, 2.000],  loss: 3171184.500000, mae: 4171.562988, mean_q: -2789.323730
 1581/5000: episode: 1581, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7615.608, mean reward: -7615.608 [-7615.608, -7615.608], mean action: 2.000 [2.000, 2.000],  loss: 3687645.750000, mae: 4322.636719, mean_q: -2809.648682
 1582/5000: episode: 1582, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1539.951, mean reward: -1539.951 [-1539.951, -1539.951], mean action: 2.000 [2.000, 2.000],  loss: 2383396.250000, mae: 4221.816406, mean_q: -2820.529785
 1583/5000: episode: 1583, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5821.783, mean reward: -5821.783 [-5821.783, -5821.783], mean action: 2.000 [2.000, 2.000],  loss: 3396345.000000, mae: 4228.340820, mean_q: -2819.956299
 1584/5000: episode: 1584, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -48.743, mean reward: -48.743 [-48.743, -48.743], mean action: 2.000 [2.000, 2.000],  loss: 3771075.500000, mae: 4332.131348, mean_q: -2836.793213
 1585/5000: episode: 1585, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4704.333, mean reward: -4704.333 [-4704.333, -4704.333], mean action: 2.000 [2.000, 2.000],  loss: 3370547.500000, mae: 4291.791016, mean_q: -2837.554199
 1586/5000: episode: 1586, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -2915.964, mean reward: -2915.964 [-2915.964, -2915.964], mean action: 2.000 [2.000, 2.000],  loss: 3516008.500000, mae: 4276.388184, mean_q: -2825.173828
 1587/5000: episode: 1587, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4682.427, mean reward: -4682.427 [-4682.427, -4682.427], mean action: 2.000 [2.000, 2.000],  loss: 2626647.500000, mae: 4187.083984, mean_q: -2835.937012
 1588/5000: episode: 1588, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5611.061, mean reward: -5611.061 [-5611.061, -5611.061], mean action: 2.000 [2.000, 2.000],  loss: 2629614.500000, mae: 4228.210449, mean_q: -2846.641113
 1589/5000: episode: 1589, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -123.910, mean reward: -123.910 [-123.910, -123.910], mean action: 2.000 [2.000, 2.000],  loss: 2775863.000000, mae: 4243.490234, mean_q: -2858.274414
 1590/5000: episode: 1590, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -179.189, mean reward: -179.189 [-179.189, -179.189], mean action: 2.000 [2.000, 2.000],  loss: 3580564.500000, mae: 4279.957031, mean_q: -2840.971191
 1591/5000: episode: 1591, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1092.456, mean reward: -1092.456 [-1092.456, -1092.456], mean action: 2.000 [2.000, 2.000],  loss: 3130670.250000, mae: 4248.552246, mean_q: -2825.408203
 1592/5000: episode: 1592, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1335.132, mean reward: -1335.132 [-1335.132, -1335.132], mean action: 2.000 [2.000, 2.000],  loss: 1712054.625000, mae: 4156.441406, mean_q: -2813.130859
 1593/5000: episode: 1593, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3532.186, mean reward: -3532.186 [-3532.186, -3532.186], mean action: 2.000 [2.000, 2.000],  loss: 2224184.000000, mae: 4234.711426, mean_q: -2806.028809
 1594/5000: episode: 1594, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3416.525, mean reward: -3416.525 [-3416.525, -3416.525], mean action: 2.000 [2.000, 2.000],  loss: 7013157.500000, mae: 4459.650391, mean_q: -2792.935059
 1595/5000: episode: 1595, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1288.747, mean reward: -1288.747 [-1288.747, -1288.747], mean action: 2.000 [2.000, 2.000],  loss: 2594759.500000, mae: 4165.942383, mean_q: -2791.670166
 1596/5000: episode: 1596, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5535.975, mean reward: -5535.975 [-5535.975, -5535.975], mean action: 2.000 [2.000, 2.000],  loss: 3926604.250000, mae: 4395.869629, mean_q: -2787.431152
 1597/5000: episode: 1597, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -905.900, mean reward: -905.900 [-905.900, -905.900], mean action: 2.000 [2.000, 2.000],  loss: 2613997.750000, mae: 4143.396973, mean_q: -2777.962402
 1598/5000: episode: 1598, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5133.783, mean reward: -5133.783 [-5133.783, -5133.783], mean action: 2.000 [2.000, 2.000],  loss: 4779094.000000, mae: 4319.873047, mean_q: -2768.201660
 1599/5000: episode: 1599, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3041.681, mean reward: -3041.681 [-3041.681, -3041.681], mean action: 3.000 [3.000, 3.000],  loss: 2962941.250000, mae: 4287.527832, mean_q: -2750.637451
 1600/5000: episode: 1600, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1058.054, mean reward: -1058.054 [-1058.054, -1058.054], mean action: 2.000 [2.000, 2.000],  loss: 2834150.500000, mae: 4157.618164, mean_q: -2721.043457
 1601/5000: episode: 1601, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4718.621, mean reward: -4718.621 [-4718.621, -4718.621], mean action: 1.000 [1.000, 1.000],  loss: 1979094.500000, mae: 4249.061035, mean_q: -2724.638184
 1602/5000: episode: 1602, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2738.647, mean reward: -2738.647 [-2738.647, -2738.647], mean action: 2.000 [2.000, 2.000],  loss: 3090874.500000, mae: 4298.041504, mean_q: -2703.818848
 1603/5000: episode: 1603, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7161.808, mean reward: -7161.808 [-7161.808, -7161.808], mean action: 2.000 [2.000, 2.000],  loss: 3197140.500000, mae: 4220.267090, mean_q: -2708.664307
 1604/5000: episode: 1604, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1676.925, mean reward: -1676.925 [-1676.925, -1676.925], mean action: 2.000 [2.000, 2.000],  loss: 4651800.500000, mae: 4356.820312, mean_q: -2698.043457
 1605/5000: episode: 1605, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3904.831, mean reward: -3904.831 [-3904.831, -3904.831], mean action: 2.000 [2.000, 2.000],  loss: 3396729.500000, mae: 4242.480469, mean_q: -2695.153564
 1606/5000: episode: 1606, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -175.518, mean reward: -175.518 [-175.518, -175.518], mean action: 2.000 [2.000, 2.000],  loss: 3426738.750000, mae: 4166.488281, mean_q: -2690.112305
 1607/5000: episode: 1607, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1964.542, mean reward: -1964.542 [-1964.542, -1964.542], mean action: 2.000 [2.000, 2.000],  loss: 2489071.250000, mae: 4249.871094, mean_q: -2698.876221
 1608/5000: episode: 1608, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -515.749, mean reward: -515.749 [-515.749, -515.749], mean action: 2.000 [2.000, 2.000],  loss: 3924343.000000, mae: 4269.734863, mean_q: -2683.101562
 1609/5000: episode: 1609, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1609.558, mean reward: -1609.558 [-1609.558, -1609.558], mean action: 2.000 [2.000, 2.000],  loss: 2993119.000000, mae: 4208.956055, mean_q: -2679.015137
 1610/5000: episode: 1610, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1980.016, mean reward: -1980.016 [-1980.016, -1980.016], mean action: 2.000 [2.000, 2.000],  loss: 2926094.500000, mae: 4167.868164, mean_q: -2673.536621
 1611/5000: episode: 1611, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2257.480, mean reward: -2257.480 [-2257.480, -2257.480], mean action: 2.000 [2.000, 2.000],  loss: 3410243.500000, mae: 4199.603516, mean_q: -2687.920654
 1612/5000: episode: 1612, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -12996.378, mean reward: -12996.378 [-12996.378, -12996.378], mean action: 0.000 [0.000, 0.000],  loss: 4107064.500000, mae: 4278.835938, mean_q: -2693.927734
 1613/5000: episode: 1613, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4732.341, mean reward: -4732.341 [-4732.341, -4732.341], mean action: 2.000 [2.000, 2.000],  loss: 2748476.250000, mae: 4248.500977, mean_q: -2712.676514
 1614/5000: episode: 1614, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2959.385, mean reward: -2959.385 [-2959.385, -2959.385], mean action: 2.000 [2.000, 2.000],  loss: 2005065.750000, mae: 4134.833008, mean_q: -2708.611328
 1615/5000: episode: 1615, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -739.668, mean reward: -739.668 [-739.668, -739.668], mean action: 2.000 [2.000, 2.000],  loss: 5029360.000000, mae: 4243.535156, mean_q: -2716.061035
 1616/5000: episode: 1616, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -332.479, mean reward: -332.479 [-332.479, -332.479], mean action: 2.000 [2.000, 2.000],  loss: 4207931.000000, mae: 4309.315430, mean_q: -2724.835938
 1617/5000: episode: 1617, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3069.883, mean reward: -3069.883 [-3069.883, -3069.883], mean action: 2.000 [2.000, 2.000],  loss: 2264319.750000, mae: 4178.097656, mean_q: -2756.248291
 1618/5000: episode: 1618, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4013.223, mean reward: -4013.223 [-4013.223, -4013.223], mean action: 2.000 [2.000, 2.000],  loss: 2415092.000000, mae: 4252.491699, mean_q: -2734.130859
 1619/5000: episode: 1619, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -516.647, mean reward: -516.647 [-516.647, -516.647], mean action: 2.000 [2.000, 2.000],  loss: 3256061.000000, mae: 4230.549316, mean_q: -2739.144287
 1620/5000: episode: 1620, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3174.372, mean reward: -3174.372 [-3174.372, -3174.372], mean action: 2.000 [2.000, 2.000],  loss: 3456016.250000, mae: 4227.227539, mean_q: -2732.892578
 1621/5000: episode: 1621, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6489.840, mean reward: -6489.840 [-6489.840, -6489.840], mean action: 2.000 [2.000, 2.000],  loss: 4173624.500000, mae: 4309.330566, mean_q: -2741.514404
 1622/5000: episode: 1622, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -245.334, mean reward: -245.334 [-245.334, -245.334], mean action: 2.000 [2.000, 2.000],  loss: 2285253.000000, mae: 4053.123779, mean_q: -2727.746826
 1623/5000: episode: 1623, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1278.377, mean reward: -1278.377 [-1278.377, -1278.377], mean action: 2.000 [2.000, 2.000],  loss: 2951188.750000, mae: 4261.167969, mean_q: -2730.822021
 1624/5000: episode: 1624, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -66.615, mean reward: -66.615 [-66.615, -66.615], mean action: 2.000 [2.000, 2.000],  loss: 3247696.000000, mae: 4238.245605, mean_q: -2727.062012
 1625/5000: episode: 1625, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3596.770, mean reward: -3596.770 [-3596.770, -3596.770], mean action: 2.000 [2.000, 2.000],  loss: 4728017.000000, mae: 4350.645996, mean_q: -2726.759766
 1626/5000: episode: 1626, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -459.915, mean reward: -459.915 [-459.915, -459.915], mean action: 2.000 [2.000, 2.000],  loss: 3004236.500000, mae: 4264.764160, mean_q: -2726.597656
 1627/5000: episode: 1627, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2492.979, mean reward: -2492.979 [-2492.979, -2492.979], mean action: 2.000 [2.000, 2.000],  loss: 3832965.750000, mae: 4293.792969, mean_q: -2739.358398
 1628/5000: episode: 1628, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8054.655, mean reward: -8054.655 [-8054.655, -8054.655], mean action: 2.000 [2.000, 2.000],  loss: 3631442.250000, mae: 4127.149414, mean_q: -2721.595215
 1629/5000: episode: 1629, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4052.845, mean reward: -4052.845 [-4052.845, -4052.845], mean action: 2.000 [2.000, 2.000],  loss: 4257236.000000, mae: 4267.280273, mean_q: -2738.967773
 1630/5000: episode: 1630, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2122.436, mean reward: -2122.436 [-2122.436, -2122.436], mean action: 2.000 [2.000, 2.000],  loss: 3119960.000000, mae: 4192.623047, mean_q: -2751.602051
 1631/5000: episode: 1631, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2091.826, mean reward: -2091.826 [-2091.826, -2091.826], mean action: 2.000 [2.000, 2.000],  loss: 3001044.000000, mae: 4247.546387, mean_q: -2772.515137
 1632/5000: episode: 1632, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2506.223, mean reward: -2506.223 [-2506.223, -2506.223], mean action: 2.000 [2.000, 2.000],  loss: 3539631.500000, mae: 4283.486328, mean_q: -2764.623291
 1633/5000: episode: 1633, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4747.325, mean reward: -4747.325 [-4747.325, -4747.325], mean action: 2.000 [2.000, 2.000],  loss: 2635531.000000, mae: 4150.171875, mean_q: -2778.156738
 1634/5000: episode: 1634, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1405.553, mean reward: -1405.553 [-1405.553, -1405.553], mean action: 2.000 [2.000, 2.000],  loss: 2253514.500000, mae: 4170.846680, mean_q: -2762.170410
 1635/5000: episode: 1635, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1538.447, mean reward: -1538.447 [-1538.447, -1538.447], mean action: 2.000 [2.000, 2.000],  loss: 2767427.750000, mae: 4233.766113, mean_q: -2760.332031
 1636/5000: episode: 1636, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3532.792, mean reward: -3532.792 [-3532.792, -3532.792], mean action: 2.000 [2.000, 2.000],  loss: 2529827.500000, mae: 4135.563965, mean_q: -2769.798828
 1637/5000: episode: 1637, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1047.321, mean reward: -1047.321 [-1047.321, -1047.321], mean action: 2.000 [2.000, 2.000],  loss: 2422328.750000, mae: 4119.981445, mean_q: -2772.381348
 1638/5000: episode: 1638, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2208.416, mean reward: -2208.416 [-2208.416, -2208.416], mean action: 2.000 [2.000, 2.000],  loss: 5538855.000000, mae: 4440.802734, mean_q: -2777.474609
 1639/5000: episode: 1639, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3408.773, mean reward: -3408.773 [-3408.773, -3408.773], mean action: 2.000 [2.000, 2.000],  loss: 2907009.000000, mae: 4186.336914, mean_q: -2779.589844
 1640/5000: episode: 1640, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1124.964, mean reward: -1124.964 [-1124.964, -1124.964], mean action: 2.000 [2.000, 2.000],  loss: 4021550.250000, mae: 4267.385254, mean_q: -2782.373047
 1641/5000: episode: 1641, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -816.432, mean reward: -816.432 [-816.432, -816.432], mean action: 2.000 [2.000, 2.000],  loss: 2929616.250000, mae: 4220.100586, mean_q: -2788.265381
 1642/5000: episode: 1642, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1971.715, mean reward: -1971.715 [-1971.715, -1971.715], mean action: 2.000 [2.000, 2.000],  loss: 4667298.500000, mae: 4282.222656, mean_q: -2782.242432
 1643/5000: episode: 1643, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -393.295, mean reward: -393.295 [-393.295, -393.295], mean action: 2.000 [2.000, 2.000],  loss: 1940675.000000, mae: 4171.681641, mean_q: -2792.948242
 1644/5000: episode: 1644, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3945.145, mean reward: -3945.145 [-3945.145, -3945.145], mean action: 0.000 [0.000, 0.000],  loss: 3930341.000000, mae: 4395.423828, mean_q: -2810.275635
 1645/5000: episode: 1645, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1925.527, mean reward: -1925.527 [-1925.527, -1925.527], mean action: 2.000 [2.000, 2.000],  loss: 3639136.250000, mae: 4243.737305, mean_q: -2791.373535
 1646/5000: episode: 1646, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2033.619, mean reward: -2033.619 [-2033.619, -2033.619], mean action: 2.000 [2.000, 2.000],  loss: 3641489.500000, mae: 4360.187500, mean_q: -2802.809570
 1647/5000: episode: 1647, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1022.130, mean reward: -1022.130 [-1022.130, -1022.130], mean action: 2.000 [2.000, 2.000],  loss: 4382178.000000, mae: 4335.094727, mean_q: -2807.817139
 1648/5000: episode: 1648, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -470.186, mean reward: -470.186 [-470.186, -470.186], mean action: 2.000 [2.000, 2.000],  loss: 3089644.750000, mae: 4229.184082, mean_q: -2801.410645
 1649/5000: episode: 1649, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3457.924, mean reward: -3457.924 [-3457.924, -3457.924], mean action: 2.000 [2.000, 2.000],  loss: 3207403.500000, mae: 4356.022461, mean_q: -2810.625488
 1650/5000: episode: 1650, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4225.596, mean reward: -4225.596 [-4225.596, -4225.596], mean action: 2.000 [2.000, 2.000],  loss: 5102786.000000, mae: 4391.735840, mean_q: -2802.222656
 1651/5000: episode: 1651, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1072.243, mean reward: -1072.243 [-1072.243, -1072.243], mean action: 2.000 [2.000, 2.000],  loss: 2468934.000000, mae: 4178.175781, mean_q: -2798.531250
 1652/5000: episode: 1652, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2229.204, mean reward: -2229.204 [-2229.204, -2229.204], mean action: 2.000 [2.000, 2.000],  loss: 4821403.000000, mae: 4412.432617, mean_q: -2782.696045
 1653/5000: episode: 1653, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2237.990, mean reward: -2237.990 [-2237.990, -2237.990], mean action: 2.000 [2.000, 2.000],  loss: 4401614.000000, mae: 4411.445801, mean_q: -2800.266113
 1654/5000: episode: 1654, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1583.214, mean reward: -1583.214 [-1583.214, -1583.214], mean action: 2.000 [2.000, 2.000],  loss: 4353527.000000, mae: 4391.259766, mean_q: -2817.452148
 1655/5000: episode: 1655, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4026.808, mean reward: -4026.808 [-4026.808, -4026.808], mean action: 2.000 [2.000, 2.000],  loss: 4697928.000000, mae: 4408.187988, mean_q: -2813.936523
 1656/5000: episode: 1656, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4146.207, mean reward: -4146.207 [-4146.207, -4146.207], mean action: 1.000 [1.000, 1.000],  loss: 3146652.500000, mae: 4321.565430, mean_q: -2782.742432
 1657/5000: episode: 1657, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -2670.670, mean reward: -2670.670 [-2670.670, -2670.670], mean action: 2.000 [2.000, 2.000],  loss: 2678092.500000, mae: 4338.305176, mean_q: -2781.205078
 1658/5000: episode: 1658, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -738.269, mean reward: -738.269 [-738.269, -738.269], mean action: 2.000 [2.000, 2.000],  loss: 3261718.750000, mae: 4342.983398, mean_q: -2802.266602
 1659/5000: episode: 1659, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -9057.991, mean reward: -9057.991 [-9057.991, -9057.991], mean action: 2.000 [2.000, 2.000],  loss: 3424972.500000, mae: 4416.033203, mean_q: -2799.900146
 1660/5000: episode: 1660, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4143.372, mean reward: -4143.372 [-4143.372, -4143.372], mean action: 2.000 [2.000, 2.000],  loss: 2787766.000000, mae: 4332.276367, mean_q: -2794.343994
 1661/5000: episode: 1661, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1116.340, mean reward: -1116.340 [-1116.340, -1116.340], mean action: 2.000 [2.000, 2.000],  loss: 4831652.500000, mae: 4443.620117, mean_q: -2792.064209
 1662/5000: episode: 1662, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5107.764, mean reward: -5107.764 [-5107.764, -5107.764], mean action: 2.000 [2.000, 2.000],  loss: 3061144.250000, mae: 4293.729980, mean_q: -2785.633301
 1663/5000: episode: 1663, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -42.868, mean reward: -42.868 [-42.868, -42.868], mean action: 2.000 [2.000, 2.000],  loss: 3122619.000000, mae: 4273.481934, mean_q: -2783.645996
 1664/5000: episode: 1664, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -22.397, mean reward: -22.397 [-22.397, -22.397], mean action: 2.000 [2.000, 2.000],  loss: 2338455.250000, mae: 4331.285645, mean_q: -2783.089111
 1665/5000: episode: 1665, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2825.788, mean reward: -2825.788 [-2825.788, -2825.788], mean action: 2.000 [2.000, 2.000],  loss: 4112128.000000, mae: 4301.554688, mean_q: -2775.635254
 1666/5000: episode: 1666, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -1772.051, mean reward: -1772.051 [-1772.051, -1772.051], mean action: 2.000 [2.000, 2.000],  loss: 3291711.750000, mae: 4295.014160, mean_q: -2767.686768
 1667/5000: episode: 1667, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1734.571, mean reward: -1734.571 [-1734.571, -1734.571], mean action: 1.000 [1.000, 1.000],  loss: 4182918.000000, mae: 4380.632812, mean_q: -2772.010254
 1668/5000: episode: 1668, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5863.011, mean reward: -5863.011 [-5863.011, -5863.011], mean action: 2.000 [2.000, 2.000],  loss: 3974561.750000, mae: 4403.078613, mean_q: -2767.207764
 1669/5000: episode: 1669, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -194.986, mean reward: -194.986 [-194.986, -194.986], mean action: 2.000 [2.000, 2.000],  loss: 4243809.000000, mae: 4387.790039, mean_q: -2764.327637
 1670/5000: episode: 1670, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1099.632, mean reward: -1099.632 [-1099.632, -1099.632], mean action: 2.000 [2.000, 2.000],  loss: 2342071.000000, mae: 4239.404785, mean_q: -2752.260742
 1671/5000: episode: 1671, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -2861.853, mean reward: -2861.853 [-2861.853, -2861.853], mean action: 2.000 [2.000, 2.000],  loss: 2292215.000000, mae: 4301.312500, mean_q: -2765.277832
 1672/5000: episode: 1672, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -538.788, mean reward: -538.788 [-538.788, -538.788], mean action: 2.000 [2.000, 2.000],  loss: 2410747.000000, mae: 4284.042969, mean_q: -2760.566406
 1673/5000: episode: 1673, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4037.415, mean reward: -4037.415 [-4037.415, -4037.415], mean action: 2.000 [2.000, 2.000],  loss: 3074278.500000, mae: 4415.731934, mean_q: -2750.464355
 1674/5000: episode: 1674, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -924.382, mean reward: -924.382 [-924.382, -924.382], mean action: 2.000 [2.000, 2.000],  loss: 4420138.500000, mae: 4360.157227, mean_q: -2732.651611
 1675/5000: episode: 1675, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2741.264, mean reward: -2741.264 [-2741.264, -2741.264], mean action: 2.000 [2.000, 2.000],  loss: 6489201.000000, mae: 4498.891602, mean_q: -2726.259277
 1676/5000: episode: 1676, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -7426.469, mean reward: -7426.469 [-7426.469, -7426.469], mean action: 2.000 [2.000, 2.000],  loss: 3968786.750000, mae: 4290.073242, mean_q: -2707.156250
 1677/5000: episode: 1677, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3049.204, mean reward: -3049.204 [-3049.204, -3049.204], mean action: 0.000 [0.000, 0.000],  loss: 3142052.500000, mae: 4300.458984, mean_q: -2702.281494
 1678/5000: episode: 1678, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -119.858, mean reward: -119.858 [-119.858, -119.858], mean action: 2.000 [2.000, 2.000],  loss: 3645697.750000, mae: 4289.988770, mean_q: -2708.854492
 1679/5000: episode: 1679, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3763.141, mean reward: -3763.141 [-3763.141, -3763.141], mean action: 2.000 [2.000, 2.000],  loss: 2905523.500000, mae: 4268.140625, mean_q: -2691.693848
 1680/5000: episode: 1680, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3609.364, mean reward: -3609.364 [-3609.364, -3609.364], mean action: 2.000 [2.000, 2.000],  loss: 3293224.250000, mae: 4270.217285, mean_q: -2694.351562
 1681/5000: episode: 1681, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -564.328, mean reward: -564.328 [-564.328, -564.328], mean action: 2.000 [2.000, 2.000],  loss: 3598274.500000, mae: 4336.299805, mean_q: -2693.097656
 1682/5000: episode: 1682, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1021.859, mean reward: -1021.859 [-1021.859, -1021.859], mean action: 2.000 [2.000, 2.000],  loss: 3083040.000000, mae: 4314.001953, mean_q: -2682.896973
 1683/5000: episode: 1683, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1196.467, mean reward: -1196.467 [-1196.467, -1196.467], mean action: 2.000 [2.000, 2.000],  loss: 3836973.250000, mae: 4369.147461, mean_q: -2677.586426
 1684/5000: episode: 1684, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1696.673, mean reward: -1696.673 [-1696.673, -1696.673], mean action: 2.000 [2.000, 2.000],  loss: 3084103.250000, mae: 4357.868164, mean_q: -2672.351562
 1685/5000: episode: 1685, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -663.319, mean reward: -663.319 [-663.319, -663.319], mean action: 2.000 [2.000, 2.000],  loss: 3386426.000000, mae: 4188.144531, mean_q: -2669.365723
 1686/5000: episode: 1686, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1598.273, mean reward: -1598.273 [-1598.273, -1598.273], mean action: 2.000 [2.000, 2.000],  loss: 3240030.000000, mae: 4265.302734, mean_q: -2653.676758
 1687/5000: episode: 1687, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1207.705, mean reward: -1207.705 [-1207.705, -1207.705], mean action: 2.000 [2.000, 2.000],  loss: 4003689.000000, mae: 4275.583008, mean_q: -2641.377197
 1688/5000: episode: 1688, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -3440.678, mean reward: -3440.678 [-3440.678, -3440.678], mean action: 2.000 [2.000, 2.000],  loss: 3238645.250000, mae: 4237.559570, mean_q: -2637.126221
 1689/5000: episode: 1689, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2705.428, mean reward: -2705.428 [-2705.428, -2705.428], mean action: 2.000 [2.000, 2.000],  loss: 2472249.500000, mae: 4077.197266, mean_q: -2621.378662
 1690/5000: episode: 1690, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -78.089, mean reward: -78.089 [-78.089, -78.089], mean action: 2.000 [2.000, 2.000],  loss: 2965703.750000, mae: 4205.971680, mean_q: -2635.070801
 1691/5000: episode: 1691, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -822.921, mean reward: -822.921 [-822.921, -822.921], mean action: 2.000 [2.000, 2.000],  loss: 4831957.500000, mae: 4303.602539, mean_q: -2623.341797
 1692/5000: episode: 1692, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5183.943, mean reward: -5183.943 [-5183.943, -5183.943], mean action: 2.000 [2.000, 2.000],  loss: 3438229.000000, mae: 4265.416992, mean_q: -2601.900879
 1693/5000: episode: 1693, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2051.594, mean reward: -2051.594 [-2051.594, -2051.594], mean action: 2.000 [2.000, 2.000],  loss: 2952712.500000, mae: 4251.698242, mean_q: -2618.327148
 1694/5000: episode: 1694, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1614.521, mean reward: -1614.521 [-1614.521, -1614.521], mean action: 2.000 [2.000, 2.000],  loss: 4011609.500000, mae: 4195.189453, mean_q: -2605.411621
 1695/5000: episode: 1695, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3810.387, mean reward: -3810.387 [-3810.387, -3810.387], mean action: 2.000 [2.000, 2.000],  loss: 2280988.500000, mae: 4100.258301, mean_q: -2602.275879
 1696/5000: episode: 1696, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6681.175, mean reward: -6681.175 [-6681.175, -6681.175], mean action: 2.000 [2.000, 2.000],  loss: 3339423.500000, mae: 4100.275391, mean_q: -2608.285889
 1697/5000: episode: 1697, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2230.574, mean reward: -2230.574 [-2230.574, -2230.574], mean action: 2.000 [2.000, 2.000],  loss: 3904798.500000, mae: 4331.701660, mean_q: -2624.357910
 1698/5000: episode: 1698, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4581.057, mean reward: -4581.057 [-4581.057, -4581.057], mean action: 2.000 [2.000, 2.000],  loss: 3285922.750000, mae: 4209.455078, mean_q: -2623.297363
 1699/5000: episode: 1699, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1146.272, mean reward: -1146.272 [-1146.272, -1146.272], mean action: 2.000 [2.000, 2.000],  loss: 3970553.500000, mae: 4186.798828, mean_q: -2654.883545
 1700/5000: episode: 1700, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3544.924, mean reward: -3544.924 [-3544.924, -3544.924], mean action: 2.000 [2.000, 2.000],  loss: 2571938.000000, mae: 4187.117188, mean_q: -2673.416992
 1701/5000: episode: 1701, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4090.046, mean reward: -4090.046 [-4090.046, -4090.046], mean action: 3.000 [3.000, 3.000],  loss: 3748334.000000, mae: 4265.612305, mean_q: -2678.818848
 1702/5000: episode: 1702, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -444.856, mean reward: -444.856 [-444.856, -444.856], mean action: 2.000 [2.000, 2.000],  loss: 2599129.500000, mae: 4179.236328, mean_q: -2712.051270
 1703/5000: episode: 1703, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3274.825, mean reward: -3274.825 [-3274.825, -3274.825], mean action: 2.000 [2.000, 2.000],  loss: 2403291.500000, mae: 4156.583008, mean_q: -2718.205322
 1704/5000: episode: 1704, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5.585, mean reward: -5.585 [-5.585, -5.585], mean action: 2.000 [2.000, 2.000],  loss: 3181440.000000, mae: 4221.755859, mean_q: -2727.913574
 1705/5000: episode: 1705, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1369.333, mean reward: -1369.333 [-1369.333, -1369.333], mean action: 2.000 [2.000, 2.000],  loss: 3172365.500000, mae: 4179.444336, mean_q: -2744.730469
 1706/5000: episode: 1706, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -1155.028, mean reward: -1155.028 [-1155.028, -1155.028], mean action: 2.000 [2.000, 2.000],  loss: 5809241.500000, mae: 4349.025879, mean_q: -2768.946045
 1707/5000: episode: 1707, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -6742.111, mean reward: -6742.111 [-6742.111, -6742.111], mean action: 1.000 [1.000, 1.000],  loss: 3668873.500000, mae: 4336.459473, mean_q: -2780.024658
 1708/5000: episode: 1708, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -649.223, mean reward: -649.223 [-649.223, -649.223], mean action: 2.000 [2.000, 2.000],  loss: 2166027.500000, mae: 4233.388184, mean_q: -2785.805176
 1709/5000: episode: 1709, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1856.607, mean reward: -1856.607 [-1856.607, -1856.607], mean action: 2.000 [2.000, 2.000],  loss: 3328410.000000, mae: 4297.940430, mean_q: -2794.874756
 1710/5000: episode: 1710, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -7283.199, mean reward: -7283.199 [-7283.199, -7283.199], mean action: 2.000 [2.000, 2.000],  loss: 3158303.500000, mae: 4279.505371, mean_q: -2803.074219
 1711/5000: episode: 1711, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6595.646, mean reward: -6595.646 [-6595.646, -6595.646], mean action: 2.000 [2.000, 2.000],  loss: 2241073.500000, mae: 4102.841797, mean_q: -2820.162598
 1712/5000: episode: 1712, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2365.670, mean reward: -2365.670 [-2365.670, -2365.670], mean action: 2.000 [2.000, 2.000],  loss: 3084682.250000, mae: 4167.633789, mean_q: -2800.592773
 1713/5000: episode: 1713, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8683.507, mean reward: -8683.507 [-8683.507, -8683.507], mean action: 1.000 [1.000, 1.000],  loss: 3441489.250000, mae: 4293.937500, mean_q: -2809.744141
 1714/5000: episode: 1714, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1807.182, mean reward: -1807.182 [-1807.182, -1807.182], mean action: 2.000 [2.000, 2.000],  loss: 3091602.500000, mae: 4234.455078, mean_q: -2794.205566
 1715/5000: episode: 1715, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1412.411, mean reward: -1412.411 [-1412.411, -1412.411], mean action: 2.000 [2.000, 2.000],  loss: 3061202.000000, mae: 4182.178711, mean_q: -2783.698242
 1716/5000: episode: 1716, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1218.347, mean reward: -1218.347 [-1218.347, -1218.347], mean action: 2.000 [2.000, 2.000],  loss: 2352335.500000, mae: 4242.108887, mean_q: -2767.172852
 1717/5000: episode: 1717, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -661.310, mean reward: -661.310 [-661.310, -661.310], mean action: 2.000 [2.000, 2.000],  loss: 2576675.750000, mae: 4305.797363, mean_q: -2773.326416
 1718/5000: episode: 1718, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -4326.717, mean reward: -4326.717 [-4326.717, -4326.717], mean action: 1.000 [1.000, 1.000],  loss: 2807291.500000, mae: 4229.708984, mean_q: -2765.271240
 1719/5000: episode: 1719, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2171.746, mean reward: -2171.746 [-2171.746, -2171.746], mean action: 2.000 [2.000, 2.000],  loss: 3637912.500000, mae: 4340.040527, mean_q: -2764.232422
 1720/5000: episode: 1720, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -56.548, mean reward: -56.548 [-56.548, -56.548], mean action: 2.000 [2.000, 2.000],  loss: 4378611.500000, mae: 4350.923828, mean_q: -2745.331055
 1721/5000: episode: 1721, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1540.135, mean reward: -1540.135 [-1540.135, -1540.135], mean action: 2.000 [2.000, 2.000],  loss: 3915238.750000, mae: 4276.679688, mean_q: -2754.450928
 1722/5000: episode: 1722, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5584.772, mean reward: -5584.772 [-5584.772, -5584.772], mean action: 2.000 [2.000, 2.000],  loss: 3340813.000000, mae: 4268.713379, mean_q: -2747.135498
 1723/5000: episode: 1723, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -1658.435, mean reward: -1658.435 [-1658.435, -1658.435], mean action: 2.000 [2.000, 2.000],  loss: 3471812.250000, mae: 4312.742676, mean_q: -2752.890137
 1724/5000: episode: 1724, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3424.522, mean reward: -3424.522 [-3424.522, -3424.522], mean action: 2.000 [2.000, 2.000],  loss: 2794891.000000, mae: 4262.407227, mean_q: -2740.420166
 1725/5000: episode: 1725, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1066.741, mean reward: -1066.741 [-1066.741, -1066.741], mean action: 2.000 [2.000, 2.000],  loss: 3581843.500000, mae: 4265.247559, mean_q: -2761.798340
 1726/5000: episode: 1726, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1226.550, mean reward: -1226.550 [-1226.550, -1226.550], mean action: 2.000 [2.000, 2.000],  loss: 3312184.500000, mae: 4162.824707, mean_q: -2751.285645
 1727/5000: episode: 1727, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2781.228, mean reward: -2781.228 [-2781.228, -2781.228], mean action: 2.000 [2.000, 2.000],  loss: 1865532.750000, mae: 4214.447266, mean_q: -2736.965820
 1728/5000: episode: 1728, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -387.489, mean reward: -387.489 [-387.489, -387.489], mean action: 2.000 [2.000, 2.000],  loss: 3125147.500000, mae: 4333.801758, mean_q: -2743.164551
 1729/5000: episode: 1729, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4524.674, mean reward: -4524.674 [-4524.674, -4524.674], mean action: 2.000 [2.000, 2.000],  loss: 3621583.250000, mae: 4335.690430, mean_q: -2740.578125
 1730/5000: episode: 1730, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2421.124, mean reward: -2421.124 [-2421.124, -2421.124], mean action: 2.000 [2.000, 2.000],  loss: 4110099.500000, mae: 4259.612793, mean_q: -2749.634277
 1731/5000: episode: 1731, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4793.126, mean reward: -4793.126 [-4793.126, -4793.126], mean action: 2.000 [2.000, 2.000],  loss: 2762560.500000, mae: 4196.937500, mean_q: -2741.807373
 1732/5000: episode: 1732, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4846.600, mean reward: -4846.600 [-4846.600, -4846.600], mean action: 0.000 [0.000, 0.000],  loss: 3872411.000000, mae: 4322.875000, mean_q: -2759.308838
 1733/5000: episode: 1733, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3179.307, mean reward: -3179.307 [-3179.307, -3179.307], mean action: 2.000 [2.000, 2.000],  loss: 3297081.000000, mae: 4306.354980, mean_q: -2752.064209
 1734/5000: episode: 1734, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -791.021, mean reward: -791.021 [-791.021, -791.021], mean action: 2.000 [2.000, 2.000],  loss: 2582248.500000, mae: 4253.375488, mean_q: -2756.332764
 1735/5000: episode: 1735, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3339.624, mean reward: -3339.624 [-3339.624, -3339.624], mean action: 2.000 [2.000, 2.000],  loss: 3863168.000000, mae: 4384.130859, mean_q: -2739.871582
 1736/5000: episode: 1736, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3965.834, mean reward: -3965.834 [-3965.834, -3965.834], mean action: 2.000 [2.000, 2.000],  loss: 6080529.000000, mae: 4429.872070, mean_q: -2743.548340
 1737/5000: episode: 1737, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8621.941, mean reward: -8621.941 [-8621.941, -8621.941], mean action: 2.000 [2.000, 2.000],  loss: 3286264.000000, mae: 4360.621094, mean_q: -2762.014404
 1738/5000: episode: 1738, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6005.004, mean reward: -6005.004 [-6005.004, -6005.004], mean action: 2.000 [2.000, 2.000],  loss: 4341054.000000, mae: 4380.778320, mean_q: -2779.034912
 1739/5000: episode: 1739, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2118.196, mean reward: -2118.196 [-2118.196, -2118.196], mean action: 2.000 [2.000, 2.000],  loss: 2425385.750000, mae: 4206.516602, mean_q: -2767.446289
 1740/5000: episode: 1740, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -497.846, mean reward: -497.846 [-497.846, -497.846], mean action: 2.000 [2.000, 2.000],  loss: 3192361.250000, mae: 4415.486816, mean_q: -2771.358154
 1741/5000: episode: 1741, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2408.315, mean reward: -2408.315 [-2408.315, -2408.315], mean action: 2.000 [2.000, 2.000],  loss: 3507798.000000, mae: 4326.608398, mean_q: -2762.836670
 1742/5000: episode: 1742, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4588.212, mean reward: -4588.212 [-4588.212, -4588.212], mean action: 2.000 [2.000, 2.000],  loss: 2927085.500000, mae: 4342.979492, mean_q: -2753.151855
 1743/5000: episode: 1743, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3985.997, mean reward: -3985.997 [-3985.997, -3985.997], mean action: 2.000 [2.000, 2.000],  loss: 4499577.000000, mae: 4356.736816, mean_q: -2743.642578
 1744/5000: episode: 1744, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -683.740, mean reward: -683.740 [-683.740, -683.740], mean action: 2.000 [2.000, 2.000],  loss: 2848222.000000, mae: 4309.810547, mean_q: -2744.500977
 1745/5000: episode: 1745, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -705.169, mean reward: -705.169 [-705.169, -705.169], mean action: 2.000 [2.000, 2.000],  loss: 2781489.000000, mae: 4339.122070, mean_q: -2757.920898
 1746/5000: episode: 1746, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -727.979, mean reward: -727.979 [-727.979, -727.979], mean action: 2.000 [2.000, 2.000],  loss: 3783119.500000, mae: 4210.364258, mean_q: -2768.828369
 1747/5000: episode: 1747, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2994.351, mean reward: -2994.351 [-2994.351, -2994.351], mean action: 2.000 [2.000, 2.000],  loss: 2488763.000000, mae: 4288.575684, mean_q: -2788.078125
 1748/5000: episode: 1748, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -550.079, mean reward: -550.079 [-550.079, -550.079], mean action: 2.000 [2.000, 2.000],  loss: 3703571.750000, mae: 4332.265625, mean_q: -2787.887695
 1749/5000: episode: 1749, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1149.048, mean reward: -1149.048 [-1149.048, -1149.048], mean action: 2.000 [2.000, 2.000],  loss: 2486516.750000, mae: 4184.222168, mean_q: -2784.694336
 1750/5000: episode: 1750, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3057.603, mean reward: -3057.603 [-3057.603, -3057.603], mean action: 2.000 [2.000, 2.000],  loss: 3114819.750000, mae: 4283.273438, mean_q: -2790.161621
 1751/5000: episode: 1751, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4389.498, mean reward: -4389.498 [-4389.498, -4389.498], mean action: 2.000 [2.000, 2.000],  loss: 2599729.500000, mae: 4168.518066, mean_q: -2788.785156
 1752/5000: episode: 1752, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1304.359, mean reward: -1304.359 [-1304.359, -1304.359], mean action: 2.000 [2.000, 2.000],  loss: 3077298.000000, mae: 4295.246094, mean_q: -2774.532471
 1753/5000: episode: 1753, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2116.707, mean reward: -2116.707 [-2116.707, -2116.707], mean action: 2.000 [2.000, 2.000],  loss: 2909367.000000, mae: 4252.175781, mean_q: -2775.348145
 1754/5000: episode: 1754, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1116.665, mean reward: -1116.665 [-1116.665, -1116.665], mean action: 2.000 [2.000, 2.000],  loss: 3119484.750000, mae: 4192.662109, mean_q: -2768.371094
 1755/5000: episode: 1755, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2896.568, mean reward: -2896.568 [-2896.568, -2896.568], mean action: 2.000 [2.000, 2.000],  loss: 3045776.750000, mae: 4316.205078, mean_q: -2783.131836
 1756/5000: episode: 1756, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3345.972, mean reward: -3345.972 [-3345.972, -3345.972], mean action: 2.000 [2.000, 2.000],  loss: 3464463.750000, mae: 4261.661133, mean_q: -2784.014648
 1757/5000: episode: 1757, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1364.740, mean reward: -1364.740 [-1364.740, -1364.740], mean action: 2.000 [2.000, 2.000],  loss: 2849818.750000, mae: 4158.036621, mean_q: -2752.875977
 1758/5000: episode: 1758, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1827.851, mean reward: -1827.851 [-1827.851, -1827.851], mean action: 2.000 [2.000, 2.000],  loss: 2021047.750000, mae: 4216.600586, mean_q: -2756.583740
 1759/5000: episode: 1759, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4690.764, mean reward: -4690.764 [-4690.764, -4690.764], mean action: 2.000 [2.000, 2.000],  loss: 2496503.500000, mae: 4159.286133, mean_q: -2736.796387
 1760/5000: episode: 1760, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -924.228, mean reward: -924.228 [-924.228, -924.228], mean action: 2.000 [2.000, 2.000],  loss: 2948685.250000, mae: 4206.086914, mean_q: -2732.798096
 1761/5000: episode: 1761, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1613.045, mean reward: -1613.045 [-1613.045, -1613.045], mean action: 2.000 [2.000, 2.000],  loss: 2678031.500000, mae: 4221.686523, mean_q: -2713.529053
 1762/5000: episode: 1762, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -892.779, mean reward: -892.779 [-892.779, -892.779], mean action: 2.000 [2.000, 2.000],  loss: 3469243.500000, mae: 4199.750977, mean_q: -2703.737549
 1763/5000: episode: 1763, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3397.902, mean reward: -3397.902 [-3397.902, -3397.902], mean action: 2.000 [2.000, 2.000],  loss: 3318409.500000, mae: 4205.798340, mean_q: -2701.513428
 1764/5000: episode: 1764, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6947.053, mean reward: -6947.053 [-6947.053, -6947.053], mean action: 2.000 [2.000, 2.000],  loss: 2563069.000000, mae: 4202.940918, mean_q: -2707.265625
 1765/5000: episode: 1765, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1654.102, mean reward: -1654.102 [-1654.102, -1654.102], mean action: 2.000 [2.000, 2.000],  loss: 5782432.000000, mae: 4483.722656, mean_q: -2721.927246
 1766/5000: episode: 1766, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -660.441, mean reward: -660.441 [-660.441, -660.441], mean action: 2.000 [2.000, 2.000],  loss: 2332257.750000, mae: 4193.540039, mean_q: -2711.415039
 1767/5000: episode: 1767, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -39.283, mean reward: -39.283 [-39.283, -39.283], mean action: 2.000 [2.000, 2.000],  loss: 2733238.000000, mae: 4194.747559, mean_q: -2716.962891
 1768/5000: episode: 1768, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2473.478, mean reward: -2473.478 [-2473.478, -2473.478], mean action: 2.000 [2.000, 2.000],  loss: 2639104.000000, mae: 4267.863770, mean_q: -2735.031738
 1769/5000: episode: 1769, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -372.331, mean reward: -372.331 [-372.331, -372.331], mean action: 2.000 [2.000, 2.000],  loss: 3155101.250000, mae: 4145.078613, mean_q: -2712.280273
 1770/5000: episode: 1770, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -237.455, mean reward: -237.455 [-237.455, -237.455], mean action: 2.000 [2.000, 2.000],  loss: 2802724.500000, mae: 4082.238037, mean_q: -2722.384766
 1771/5000: episode: 1771, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7303.188, mean reward: -7303.188 [-7303.188, -7303.188], mean action: 2.000 [2.000, 2.000],  loss: 3050204.000000, mae: 4299.338867, mean_q: -2724.136230
 1772/5000: episode: 1772, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -780.338, mean reward: -780.338 [-780.338, -780.338], mean action: 2.000 [2.000, 2.000],  loss: 4237218.000000, mae: 4381.021484, mean_q: -2720.279541
 1773/5000: episode: 1773, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -2089.045, mean reward: -2089.045 [-2089.045, -2089.045], mean action: 2.000 [2.000, 2.000],  loss: 3607190.750000, mae: 4387.831055, mean_q: -2702.238770
 1774/5000: episode: 1774, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1027.670, mean reward: -1027.670 [-1027.670, -1027.670], mean action: 2.000 [2.000, 2.000],  loss: 4566803.500000, mae: 4393.318848, mean_q: -2672.840820
 1775/5000: episode: 1775, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2774.091, mean reward: -2774.091 [-2774.091, -2774.091], mean action: 2.000 [2.000, 2.000],  loss: 3810820.500000, mae: 4361.334961, mean_q: -2684.316895
 1776/5000: episode: 1776, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -802.378, mean reward: -802.378 [-802.378, -802.378], mean action: 2.000 [2.000, 2.000],  loss: 3748288.000000, mae: 4228.446289, mean_q: -2641.940674
 1777/5000: episode: 1777, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1055.603, mean reward: -1055.603 [-1055.603, -1055.603], mean action: 2.000 [2.000, 2.000],  loss: 2437295.500000, mae: 4261.603516, mean_q: -2615.884521
 1778/5000: episode: 1778, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -824.773, mean reward: -824.773 [-824.773, -824.773], mean action: 2.000 [2.000, 2.000],  loss: 2272439.500000, mae: 4250.605957, mean_q: -2593.902588
 1779/5000: episode: 1779, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2611.968, mean reward: -2611.968 [-2611.968, -2611.968], mean action: 2.000 [2.000, 2.000],  loss: 2874822.000000, mae: 4300.446777, mean_q: -2564.835938
 1780/5000: episode: 1780, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5570.765, mean reward: -5570.765 [-5570.765, -5570.765], mean action: 2.000 [2.000, 2.000],  loss: 3656867.000000, mae: 4249.410645, mean_q: -2567.692871
 1781/5000: episode: 1781, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2215.428, mean reward: -2215.428 [-2215.428, -2215.428], mean action: 2.000 [2.000, 2.000],  loss: 4843155.500000, mae: 4277.953613, mean_q: -2546.963379
 1782/5000: episode: 1782, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2750.633, mean reward: -2750.633 [-2750.633, -2750.633], mean action: 2.000 [2.000, 2.000],  loss: 3165664.500000, mae: 4252.665039, mean_q: -2555.544189
 1783/5000: episode: 1783, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2361.350, mean reward: -2361.350 [-2361.350, -2361.350], mean action: 2.000 [2.000, 2.000],  loss: 2704191.500000, mae: 4213.695801, mean_q: -2544.782959
 1784/5000: episode: 1784, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1848.663, mean reward: -1848.663 [-1848.663, -1848.663], mean action: 2.000 [2.000, 2.000],  loss: 4101632.500000, mae: 4303.243652, mean_q: -2535.621338
 1785/5000: episode: 1785, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4403.554, mean reward: -4403.554 [-4403.554, -4403.554], mean action: 2.000 [2.000, 2.000],  loss: 2740304.750000, mae: 4142.497070, mean_q: -2539.137695
 1786/5000: episode: 1786, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3851.292, mean reward: -3851.292 [-3851.292, -3851.292], mean action: 2.000 [2.000, 2.000],  loss: 2901122.500000, mae: 4245.697266, mean_q: -2534.528320
 1787/5000: episode: 1787, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6.165, mean reward: -6.165 [-6.165, -6.165], mean action: 2.000 [2.000, 2.000],  loss: 2758491.000000, mae: 4326.874023, mean_q: -2538.240234
 1788/5000: episode: 1788, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -725.709, mean reward: -725.709 [-725.709, -725.709], mean action: 2.000 [2.000, 2.000],  loss: 2881646.000000, mae: 4299.182617, mean_q: -2533.610596
 1789/5000: episode: 1789, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -919.566, mean reward: -919.566 [-919.566, -919.566], mean action: 2.000 [2.000, 2.000],  loss: 4108255.500000, mae: 4333.560059, mean_q: -2531.927490
 1790/5000: episode: 1790, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -714.211, mean reward: -714.211 [-714.211, -714.211], mean action: 2.000 [2.000, 2.000],  loss: 3156683.500000, mae: 4257.951660, mean_q: -2531.783203
 1791/5000: episode: 1791, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4037.686, mean reward: -4037.686 [-4037.686, -4037.686], mean action: 0.000 [0.000, 0.000],  loss: 2306012.000000, mae: 4206.222656, mean_q: -2540.037598
 1792/5000: episode: 1792, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -755.648, mean reward: -755.648 [-755.648, -755.648], mean action: 2.000 [2.000, 2.000],  loss: 4021485.000000, mae: 4358.786621, mean_q: -2524.410156
 1793/5000: episode: 1793, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1316.756, mean reward: -1316.756 [-1316.756, -1316.756], mean action: 2.000 [2.000, 2.000],  loss: 2837418.000000, mae: 4222.322754, mean_q: -2526.831299
 1794/5000: episode: 1794, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6861.368, mean reward: -6861.368 [-6861.368, -6861.368], mean action: 0.000 [0.000, 0.000],  loss: 3931810.750000, mae: 4265.105957, mean_q: -2521.072754
 1795/5000: episode: 1795, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3919.187, mean reward: -3919.187 [-3919.187, -3919.187], mean action: 2.000 [2.000, 2.000],  loss: 2735837.000000, mae: 4226.541992, mean_q: -2522.334473
 1796/5000: episode: 1796, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -324.836, mean reward: -324.836 [-324.836, -324.836], mean action: 2.000 [2.000, 2.000],  loss: 4124195.750000, mae: 4393.420898, mean_q: -2535.900879
 1797/5000: episode: 1797, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3794.847, mean reward: -3794.847 [-3794.847, -3794.847], mean action: 2.000 [2.000, 2.000],  loss: 4644811.000000, mae: 4351.690918, mean_q: -2537.644043
 1798/5000: episode: 1798, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4504.845, mean reward: -4504.845 [-4504.845, -4504.845], mean action: 3.000 [3.000, 3.000],  loss: 3110416.250000, mae: 4332.399414, mean_q: -2550.744141
 1799/5000: episode: 1799, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3863.669, mean reward: -3863.669 [-3863.669, -3863.669], mean action: 3.000 [3.000, 3.000],  loss: 2630249.750000, mae: 4265.866699, mean_q: -2554.056396
 1800/5000: episode: 1800, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -34.297, mean reward: -34.297 [-34.297, -34.297], mean action: 2.000 [2.000, 2.000],  loss: 3236282.000000, mae: 4417.340332, mean_q: -2559.321289
 1801/5000: episode: 1801, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2035.966, mean reward: -2035.966 [-2035.966, -2035.966], mean action: 2.000 [2.000, 2.000],  loss: 3614633.250000, mae: 4405.847656, mean_q: -2570.137695
 1802/5000: episode: 1802, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1854.409, mean reward: -1854.409 [-1854.409, -1854.409], mean action: 2.000 [2.000, 2.000],  loss: 2533735.250000, mae: 4231.287109, mean_q: -2585.733643
 1803/5000: episode: 1803, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3007.900, mean reward: -3007.900 [-3007.900, -3007.900], mean action: 1.000 [1.000, 1.000],  loss: 2491143.250000, mae: 4299.615234, mean_q: -2596.949463
 1804/5000: episode: 1804, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -8095.884, mean reward: -8095.884 [-8095.884, -8095.884], mean action: 2.000 [2.000, 2.000],  loss: 3175295.750000, mae: 4324.424805, mean_q: -2601.972168
 1805/5000: episode: 1805, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -965.713, mean reward: -965.713 [-965.713, -965.713], mean action: 2.000 [2.000, 2.000],  loss: 2701422.500000, mae: 4388.314453, mean_q: -2601.580078
 1806/5000: episode: 1806, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1773.684, mean reward: -1773.684 [-1773.684, -1773.684], mean action: 2.000 [2.000, 2.000],  loss: 2875500.500000, mae: 4350.636719, mean_q: -2600.086426
 1807/5000: episode: 1807, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8651.628, mean reward: -8651.628 [-8651.628, -8651.628], mean action: 2.000 [2.000, 2.000],  loss: 3293183.750000, mae: 4358.722168, mean_q: -2600.908203
 1808/5000: episode: 1808, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5332.686, mean reward: -5332.686 [-5332.686, -5332.686], mean action: 2.000 [2.000, 2.000],  loss: 3337191.500000, mae: 4274.901367, mean_q: -2585.582031
 1809/5000: episode: 1809, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1593.003, mean reward: -1593.003 [-1593.003, -1593.003], mean action: 2.000 [2.000, 2.000],  loss: 3336771.750000, mae: 4424.056641, mean_q: -2604.000488
 1810/5000: episode: 1810, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -196.470, mean reward: -196.470 [-196.470, -196.470], mean action: 2.000 [2.000, 2.000],  loss: 2900085.500000, mae: 4368.793945, mean_q: -2600.494629
 1811/5000: episode: 1811, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4173.414, mean reward: -4173.414 [-4173.414, -4173.414], mean action: 2.000 [2.000, 2.000],  loss: 2807052.500000, mae: 4359.286133, mean_q: -2607.811523
 1812/5000: episode: 1812, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2211.433, mean reward: -2211.433 [-2211.433, -2211.433], mean action: 2.000 [2.000, 2.000],  loss: 3077909.500000, mae: 4417.833008, mean_q: -2607.796387
 1813/5000: episode: 1813, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -697.566, mean reward: -697.566 [-697.566, -697.566], mean action: 2.000 [2.000, 2.000],  loss: 4235908.500000, mae: 4402.411133, mean_q: -2612.290039
 1814/5000: episode: 1814, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2605.889, mean reward: -2605.889 [-2605.889, -2605.889], mean action: 2.000 [2.000, 2.000],  loss: 3820905.750000, mae: 4410.098633, mean_q: -2614.538330
 1815/5000: episode: 1815, duration: 0.047s, episode steps:   1, steps per second:  22, episode reward: -1134.872, mean reward: -1134.872 [-1134.872, -1134.872], mean action: 2.000 [2.000, 2.000],  loss: 3704506.000000, mae: 4435.439941, mean_q: -2620.782227
 1816/5000: episode: 1816, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2046.794, mean reward: -2046.794 [-2046.794, -2046.794], mean action: 2.000 [2.000, 2.000],  loss: 4300296.000000, mae: 4408.412109, mean_q: -2620.037109
 1817/5000: episode: 1817, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3459.181, mean reward: -3459.181 [-3459.181, -3459.181], mean action: 2.000 [2.000, 2.000],  loss: 3690414.500000, mae: 4431.723633, mean_q: -2616.372314
 1818/5000: episode: 1818, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2602.986, mean reward: -2602.986 [-2602.986, -2602.986], mean action: 2.000 [2.000, 2.000],  loss: 3552674.250000, mae: 4386.126465, mean_q: -2618.114990
 1819/5000: episode: 1819, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2123.344, mean reward: -2123.344 [-2123.344, -2123.344], mean action: 2.000 [2.000, 2.000],  loss: 4223674.000000, mae: 4394.475586, mean_q: -2618.440918
 1820/5000: episode: 1820, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2440.485, mean reward: -2440.485 [-2440.485, -2440.485], mean action: 2.000 [2.000, 2.000],  loss: 2996766.500000, mae: 4351.903809, mean_q: -2618.786621
 1821/5000: episode: 1821, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3723.541, mean reward: -3723.541 [-3723.541, -3723.541], mean action: 3.000 [3.000, 3.000],  loss: 3663328.500000, mae: 4400.902344, mean_q: -2614.976074
 1822/5000: episode: 1822, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -457.196, mean reward: -457.196 [-457.196, -457.196], mean action: 2.000 [2.000, 2.000],  loss: 3488256.250000, mae: 4344.779297, mean_q: -2603.312500
 1823/5000: episode: 1823, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4451.054, mean reward: -4451.054 [-4451.054, -4451.054], mean action: 2.000 [2.000, 2.000],  loss: 3496972.500000, mae: 4501.470703, mean_q: -2615.594971
 1824/5000: episode: 1824, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3768.401, mean reward: -3768.401 [-3768.401, -3768.401], mean action: 2.000 [2.000, 2.000],  loss: 3635560.500000, mae: 4342.228027, mean_q: -2634.739990
 1825/5000: episode: 1825, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2220.713, mean reward: -2220.713 [-2220.713, -2220.713], mean action: 2.000 [2.000, 2.000],  loss: 4990296.000000, mae: 4464.173828, mean_q: -2626.200684
 1826/5000: episode: 1826, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1656.748, mean reward: -1656.748 [-1656.748, -1656.748], mean action: 2.000 [2.000, 2.000],  loss: 4304721.000000, mae: 4532.362793, mean_q: -2642.130859
 1827/5000: episode: 1827, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -722.030, mean reward: -722.030 [-722.030, -722.030], mean action: 2.000 [2.000, 2.000],  loss: 3466546.750000, mae: 4351.114746, mean_q: -2638.786621
 1828/5000: episode: 1828, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2217.134, mean reward: -2217.134 [-2217.134, -2217.134], mean action: 2.000 [2.000, 2.000],  loss: 3776184.500000, mae: 4347.823242, mean_q: -2630.932129
 1829/5000: episode: 1829, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -3437.587, mean reward: -3437.587 [-3437.587, -3437.587], mean action: 3.000 [3.000, 3.000],  loss: 3045448.000000, mae: 4263.904785, mean_q: -2650.499268
 1830/5000: episode: 1830, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5202.608, mean reward: -5202.608 [-5202.608, -5202.608], mean action: 2.000 [2.000, 2.000],  loss: 3284835.000000, mae: 4377.918945, mean_q: -2643.440674
 1831/5000: episode: 1831, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7217.257, mean reward: -7217.257 [-7217.257, -7217.257], mean action: 2.000 [2.000, 2.000],  loss: 4033822.000000, mae: 4358.904785, mean_q: -2651.502930
 1832/5000: episode: 1832, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -18.144, mean reward: -18.144 [-18.144, -18.144], mean action: 2.000 [2.000, 2.000],  loss: 3566420.000000, mae: 4434.635742, mean_q: -2657.046875
 1833/5000: episode: 1833, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3645.252, mean reward: -3645.252 [-3645.252, -3645.252], mean action: 1.000 [1.000, 1.000],  loss: 3695773.000000, mae: 4293.752441, mean_q: -2649.001465
 1834/5000: episode: 1834, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1375.766, mean reward: -1375.766 [-1375.766, -1375.766], mean action: 2.000 [2.000, 2.000],  loss: 3549940.000000, mae: 4375.111328, mean_q: -2654.751709
 1835/5000: episode: 1835, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -13.649, mean reward: -13.649 [-13.649, -13.649], mean action: 2.000 [2.000, 2.000],  loss: 6280900.000000, mae: 4365.703613, mean_q: -2658.520264
 1836/5000: episode: 1836, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -7982.552, mean reward: -7982.552 [-7982.552, -7982.552], mean action: 2.000 [2.000, 2.000],  loss: 5761221.000000, mae: 4557.277832, mean_q: -2663.657227
 1837/5000: episode: 1837, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -595.146, mean reward: -595.146 [-595.146, -595.146], mean action: 2.000 [2.000, 2.000],  loss: 2626817.000000, mae: 4245.365234, mean_q: -2654.526123
 1838/5000: episode: 1838, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1089.298, mean reward: -1089.298 [-1089.298, -1089.298], mean action: 2.000 [2.000, 2.000],  loss: 2606273.000000, mae: 4313.014648, mean_q: -2660.577393
 1839/5000: episode: 1839, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1794.070, mean reward: -1794.070 [-1794.070, -1794.070], mean action: 2.000 [2.000, 2.000],  loss: 2360129.000000, mae: 4315.262695, mean_q: -2648.627197
 1840/5000: episode: 1840, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -108.333, mean reward: -108.333 [-108.333, -108.333], mean action: 2.000 [2.000, 2.000],  loss: 5774974.000000, mae: 4479.381836, mean_q: -2660.498535
 1841/5000: episode: 1841, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -538.324, mean reward: -538.324 [-538.324, -538.324], mean action: 2.000 [2.000, 2.000],  loss: 4232612.000000, mae: 4335.526855, mean_q: -2650.075684
 1842/5000: episode: 1842, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10815.248, mean reward: -10815.248 [-10815.248, -10815.248], mean action: 0.000 [0.000, 0.000],  loss: 3350288.500000, mae: 4367.219238, mean_q: -2660.922852
 1843/5000: episode: 1843, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1042.652, mean reward: -1042.652 [-1042.652, -1042.652], mean action: 2.000 [2.000, 2.000],  loss: 2023304.500000, mae: 4327.384277, mean_q: -2652.675781
 1844/5000: episode: 1844, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6119.842, mean reward: -6119.842 [-6119.842, -6119.842], mean action: 1.000 [1.000, 1.000],  loss: 3178615.000000, mae: 4237.569824, mean_q: -2647.801758
 1845/5000: episode: 1845, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1222.632, mean reward: -1222.632 [-1222.632, -1222.632], mean action: 2.000 [2.000, 2.000],  loss: 4344023.000000, mae: 4318.247070, mean_q: -2634.651123
 1846/5000: episode: 1846, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4666.580, mean reward: -4666.580 [-4666.580, -4666.580], mean action: 2.000 [2.000, 2.000],  loss: 3286415.250000, mae: 4214.999023, mean_q: -2648.428711
 1847/5000: episode: 1847, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3879.221, mean reward: -3879.221 [-3879.221, -3879.221], mean action: 2.000 [2.000, 2.000],  loss: 2571029.500000, mae: 4343.207031, mean_q: -2646.389160
 1848/5000: episode: 1848, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -8047.926, mean reward: -8047.926 [-8047.926, -8047.926], mean action: 2.000 [2.000, 2.000],  loss: 2396166.000000, mae: 4255.774414, mean_q: -2649.682617
 1849/5000: episode: 1849, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3941.604, mean reward: -3941.604 [-3941.604, -3941.604], mean action: 2.000 [2.000, 2.000],  loss: 2361727.250000, mae: 4330.905762, mean_q: -2639.899902
 1850/5000: episode: 1850, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1087.118, mean reward: -1087.118 [-1087.118, -1087.118], mean action: 2.000 [2.000, 2.000],  loss: 3241906.500000, mae: 4224.350586, mean_q: -2617.524902
 1851/5000: episode: 1851, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -109.839, mean reward: -109.839 [-109.839, -109.839], mean action: 2.000 [2.000, 2.000],  loss: 3910107.000000, mae: 4353.233398, mean_q: -2611.162598
 1852/5000: episode: 1852, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5580.706, mean reward: -5580.706 [-5580.706, -5580.706], mean action: 2.000 [2.000, 2.000],  loss: 3495130.750000, mae: 4305.141602, mean_q: -2606.625488
 1853/5000: episode: 1853, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -27.892, mean reward: -27.892 [-27.892, -27.892], mean action: 2.000 [2.000, 2.000],  loss: 3388454.250000, mae: 4251.741699, mean_q: -2595.415283
 1854/5000: episode: 1854, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -69.541, mean reward: -69.541 [-69.541, -69.541], mean action: 2.000 [2.000, 2.000],  loss: 4246795.000000, mae: 4382.839844, mean_q: -2589.212891
 1855/5000: episode: 1855, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -522.781, mean reward: -522.781 [-522.781, -522.781], mean action: 2.000 [2.000, 2.000],  loss: 2056778.500000, mae: 4236.831055, mean_q: -2574.385742
 1856/5000: episode: 1856, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -1066.536, mean reward: -1066.536 [-1066.536, -1066.536], mean action: 2.000 [2.000, 2.000],  loss: 2643422.500000, mae: 4317.056152, mean_q: -2584.685547
 1857/5000: episode: 1857, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1576.923, mean reward: -1576.923 [-1576.923, -1576.923], mean action: 2.000 [2.000, 2.000],  loss: 2951359.000000, mae: 4128.907227, mean_q: -2558.214355
 1858/5000: episode: 1858, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1273.168, mean reward: -1273.168 [-1273.168, -1273.168], mean action: 2.000 [2.000, 2.000],  loss: 3447496.750000, mae: 4188.994141, mean_q: -2558.981445
 1859/5000: episode: 1859, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7959.277, mean reward: -7959.277 [-7959.277, -7959.277], mean action: 2.000 [2.000, 2.000],  loss: 2908493.750000, mae: 4286.324707, mean_q: -2543.814453
 1860/5000: episode: 1860, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -217.629, mean reward: -217.629 [-217.629, -217.629], mean action: 2.000 [2.000, 2.000],  loss: 3931691.750000, mae: 4244.504395, mean_q: -2533.719727
 1861/5000: episode: 1861, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1550.052, mean reward: -1550.052 [-1550.052, -1550.052], mean action: 2.000 [2.000, 2.000],  loss: 2185456.500000, mae: 4167.592773, mean_q: -2515.990479
 1862/5000: episode: 1862, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1052.468, mean reward: -1052.468 [-1052.468, -1052.468], mean action: 2.000 [2.000, 2.000],  loss: 3710576.000000, mae: 4321.973633, mean_q: -2504.456787
 1863/5000: episode: 1863, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2329.174, mean reward: -2329.174 [-2329.174, -2329.174], mean action: 2.000 [2.000, 2.000],  loss: 3317842.000000, mae: 4268.611816, mean_q: -2501.185791
 1864/5000: episode: 1864, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3207.318, mean reward: -3207.318 [-3207.318, -3207.318], mean action: 2.000 [2.000, 2.000],  loss: 4332431.000000, mae: 4263.832031, mean_q: -2501.062012
 1865/5000: episode: 1865, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2085.039, mean reward: -2085.039 [-2085.039, -2085.039], mean action: 2.000 [2.000, 2.000],  loss: 3279058.500000, mae: 4143.275879, mean_q: -2484.904541
 1866/5000: episode: 1866, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -3960.005, mean reward: -3960.005 [-3960.005, -3960.005], mean action: 2.000 [2.000, 2.000],  loss: 2996105.000000, mae: 4190.145996, mean_q: -2488.218506
 1867/5000: episode: 1867, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2217.226, mean reward: -2217.226 [-2217.226, -2217.226], mean action: 2.000 [2.000, 2.000],  loss: 1448012.500000, mae: 4024.379395, mean_q: -2480.858398
 1868/5000: episode: 1868, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1984.717, mean reward: -1984.717 [-1984.717, -1984.717], mean action: 2.000 [2.000, 2.000],  loss: 3546100.500000, mae: 4213.301270, mean_q: -2469.801758
 1869/5000: episode: 1869, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1140.009, mean reward: -1140.009 [-1140.009, -1140.009], mean action: 2.000 [2.000, 2.000],  loss: 3796137.500000, mae: 4189.945312, mean_q: -2464.739746
 1870/5000: episode: 1870, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3448.704, mean reward: -3448.704 [-3448.704, -3448.704], mean action: 2.000 [2.000, 2.000],  loss: 4300393.000000, mae: 4315.973633, mean_q: -2452.672363
 1871/5000: episode: 1871, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -3340.238, mean reward: -3340.238 [-3340.238, -3340.238], mean action: 2.000 [2.000, 2.000],  loss: 4226903.000000, mae: 4252.375977, mean_q: -2469.366699
 1872/5000: episode: 1872, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -87.828, mean reward: -87.828 [-87.828, -87.828], mean action: 2.000 [2.000, 2.000],  loss: 4701923.000000, mae: 4289.925781, mean_q: -2466.527588
 1873/5000: episode: 1873, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4808.749, mean reward: -4808.749 [-4808.749, -4808.749], mean action: 2.000 [2.000, 2.000],  loss: 2303307.250000, mae: 4110.951172, mean_q: -2467.061035
 1874/5000: episode: 1874, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -570.834, mean reward: -570.834 [-570.834, -570.834], mean action: 2.000 [2.000, 2.000],  loss: 2708847.250000, mae: 4167.253418, mean_q: -2459.526611
 1875/5000: episode: 1875, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -63.434, mean reward: -63.434 [-63.434, -63.434], mean action: 2.000 [2.000, 2.000],  loss: 3391717.000000, mae: 4272.239258, mean_q: -2480.179199
 1876/5000: episode: 1876, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -158.236, mean reward: -158.236 [-158.236, -158.236], mean action: 2.000 [2.000, 2.000],  loss: 1962275.000000, mae: 4148.958496, mean_q: -2473.347168
 1877/5000: episode: 1877, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2944.380, mean reward: -2944.380 [-2944.380, -2944.380], mean action: 2.000 [2.000, 2.000],  loss: 3683418.000000, mae: 4207.561035, mean_q: -2462.637207
 1878/5000: episode: 1878, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -201.581, mean reward: -201.581 [-201.581, -201.581], mean action: 2.000 [2.000, 2.000],  loss: 2464924.750000, mae: 4163.002930, mean_q: -2459.662109
 1879/5000: episode: 1879, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1569.332, mean reward: -1569.332 [-1569.332, -1569.332], mean action: 2.000 [2.000, 2.000],  loss: 2585589.000000, mae: 4022.405273, mean_q: -2460.435547
 1880/5000: episode: 1880, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1321.939, mean reward: -1321.939 [-1321.939, -1321.939], mean action: 2.000 [2.000, 2.000],  loss: 3575268.750000, mae: 4200.779297, mean_q: -2456.159668
 1881/5000: episode: 1881, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2584.370, mean reward: -2584.370 [-2584.370, -2584.370], mean action: 2.000 [2.000, 2.000],  loss: 2256456.000000, mae: 4137.151367, mean_q: -2450.917969
 1882/5000: episode: 1882, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2296.513, mean reward: -2296.513 [-2296.513, -2296.513], mean action: 2.000 [2.000, 2.000],  loss: 3805615.500000, mae: 4289.616211, mean_q: -2457.954834
 1883/5000: episode: 1883, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4201.134, mean reward: -4201.134 [-4201.134, -4201.134], mean action: 2.000 [2.000, 2.000],  loss: 2469746.500000, mae: 4133.185059, mean_q: -2463.437988
 1884/5000: episode: 1884, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1815.404, mean reward: -1815.404 [-1815.404, -1815.404], mean action: 2.000 [2.000, 2.000],  loss: 3063485.500000, mae: 4154.041016, mean_q: -2458.522461
 1885/5000: episode: 1885, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5480.583, mean reward: -5480.583 [-5480.583, -5480.583], mean action: 2.000 [2.000, 2.000],  loss: 2869985.500000, mae: 4183.541016, mean_q: -2455.966309
 1886/5000: episode: 1886, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2441.835, mean reward: -2441.835 [-2441.835, -2441.835], mean action: 2.000 [2.000, 2.000],  loss: 3462107.000000, mae: 4118.891602, mean_q: -2440.097900
 1887/5000: episode: 1887, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -925.697, mean reward: -925.697 [-925.697, -925.697], mean action: 2.000 [2.000, 2.000],  loss: 3632222.500000, mae: 4148.956055, mean_q: -2425.468750
 1888/5000: episode: 1888, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1316.691, mean reward: -1316.691 [-1316.691, -1316.691], mean action: 2.000 [2.000, 2.000],  loss: 3081046.250000, mae: 4201.447266, mean_q: -2455.004639
 1889/5000: episode: 1889, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2389.693, mean reward: -2389.693 [-2389.693, -2389.693], mean action: 2.000 [2.000, 2.000],  loss: 6968682.000000, mae: 4412.755859, mean_q: -2436.638672
 1890/5000: episode: 1890, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5969.269, mean reward: -5969.269 [-5969.269, -5969.269], mean action: 2.000 [2.000, 2.000],  loss: 2800399.500000, mae: 4131.644531, mean_q: -2438.959961
 1891/5000: episode: 1891, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -250.917, mean reward: -250.917 [-250.917, -250.917], mean action: 2.000 [2.000, 2.000],  loss: 4907217.000000, mae: 4314.770996, mean_q: -2452.678711
 1892/5000: episode: 1892, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2466.952, mean reward: -2466.952 [-2466.952, -2466.952], mean action: 2.000 [2.000, 2.000],  loss: 2500994.750000, mae: 4207.217285, mean_q: -2484.119873
 1893/5000: episode: 1893, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1155.703, mean reward: -1155.703 [-1155.703, -1155.703], mean action: 2.000 [2.000, 2.000],  loss: 2426754.500000, mae: 4117.477539, mean_q: -2487.742188
 1894/5000: episode: 1894, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2614.197, mean reward: -2614.197 [-2614.197, -2614.197], mean action: 2.000 [2.000, 2.000],  loss: 2054068.500000, mae: 4183.338379, mean_q: -2478.062988
 1895/5000: episode: 1895, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7770.032, mean reward: -7770.032 [-7770.032, -7770.032], mean action: 1.000 [1.000, 1.000],  loss: 2329792.750000, mae: 4031.933105, mean_q: -2468.530273
 1896/5000: episode: 1896, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -692.438, mean reward: -692.438 [-692.438, -692.438], mean action: 2.000 [2.000, 2.000],  loss: 2549914.500000, mae: 4164.218750, mean_q: -2468.083984
 1897/5000: episode: 1897, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1159.426, mean reward: -1159.426 [-1159.426, -1159.426], mean action: 2.000 [2.000, 2.000],  loss: 4122653.500000, mae: 4275.043457, mean_q: -2476.520020
 1898/5000: episode: 1898, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -150.220, mean reward: -150.220 [-150.220, -150.220], mean action: 2.000 [2.000, 2.000],  loss: 6816733.000000, mae: 4338.535156, mean_q: -2478.113525
 1899/5000: episode: 1899, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2101.646, mean reward: -2101.646 [-2101.646, -2101.646], mean action: 2.000 [2.000, 2.000],  loss: 4481060.000000, mae: 4335.145508, mean_q: -2504.666748
 1900/5000: episode: 1900, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5518.632, mean reward: -5518.632 [-5518.632, -5518.632], mean action: 2.000 [2.000, 2.000],  loss: 4163044.750000, mae: 4368.776367, mean_q: -2506.867676
 1901/5000: episode: 1901, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3230.804, mean reward: -3230.804 [-3230.804, -3230.804], mean action: 2.000 [2.000, 2.000],  loss: 3195647.000000, mae: 4201.923340, mean_q: -2529.861816
 1902/5000: episode: 1902, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2497.538, mean reward: -2497.538 [-2497.538, -2497.538], mean action: 2.000 [2.000, 2.000],  loss: 2931016.250000, mae: 4151.688477, mean_q: -2535.627197
 1903/5000: episode: 1903, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6940.258, mean reward: -6940.258 [-6940.258, -6940.258], mean action: 2.000 [2.000, 2.000],  loss: 4043955.500000, mae: 4231.265137, mean_q: -2561.204102
 1904/5000: episode: 1904, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5211.154, mean reward: -5211.154 [-5211.154, -5211.154], mean action: 2.000 [2.000, 2.000],  loss: 2439091.750000, mae: 4181.749023, mean_q: -2585.560547
 1905/5000: episode: 1905, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4778.105, mean reward: -4778.105 [-4778.105, -4778.105], mean action: 2.000 [2.000, 2.000],  loss: 3846470.000000, mae: 4271.449707, mean_q: -2598.833496
 1906/5000: episode: 1906, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5759.293, mean reward: -5759.293 [-5759.293, -5759.293], mean action: 2.000 [2.000, 2.000],  loss: 2296780.750000, mae: 4313.279297, mean_q: -2634.227051
 1907/5000: episode: 1907, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1555.244, mean reward: -1555.244 [-1555.244, -1555.244], mean action: 2.000 [2.000, 2.000],  loss: 2601500.000000, mae: 4256.297363, mean_q: -2651.692871
 1908/5000: episode: 1908, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2028.225, mean reward: -2028.225 [-2028.225, -2028.225], mean action: 2.000 [2.000, 2.000],  loss: 2590499.500000, mae: 4204.445801, mean_q: -2654.913330
 1909/5000: episode: 1909, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -37.332, mean reward: -37.332 [-37.332, -37.332], mean action: 2.000 [2.000, 2.000],  loss: 2457473.000000, mae: 4203.243164, mean_q: -2662.919922
 1910/5000: episode: 1910, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3191.831, mean reward: -3191.831 [-3191.831, -3191.831], mean action: 2.000 [2.000, 2.000],  loss: 4795438.000000, mae: 4411.280273, mean_q: -2668.441406
 1911/5000: episode: 1911, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1289.731, mean reward: -1289.731 [-1289.731, -1289.731], mean action: 2.000 [2.000, 2.000],  loss: 3334092.000000, mae: 4267.114258, mean_q: -2671.565918
 1912/5000: episode: 1912, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -873.085, mean reward: -873.085 [-873.085, -873.085], mean action: 2.000 [2.000, 2.000],  loss: 3937197.500000, mae: 4190.554688, mean_q: -2670.068848
 1913/5000: episode: 1913, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5690.027, mean reward: -5690.027 [-5690.027, -5690.027], mean action: 2.000 [2.000, 2.000],  loss: 2706987.500000, mae: 4224.263672, mean_q: -2680.593750
 1914/5000: episode: 1914, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1420.629, mean reward: -1420.629 [-1420.629, -1420.629], mean action: 2.000 [2.000, 2.000],  loss: 2962511.000000, mae: 4265.391602, mean_q: -2681.637695
 1915/5000: episode: 1915, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -46.219, mean reward: -46.219 [-46.219, -46.219], mean action: 2.000 [2.000, 2.000],  loss: 2988086.500000, mae: 4217.180176, mean_q: -2666.622314
 1916/5000: episode: 1916, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1471.706, mean reward: -1471.706 [-1471.706, -1471.706], mean action: 2.000 [2.000, 2.000],  loss: 3374774.500000, mae: 4299.817871, mean_q: -2660.894043
 1917/5000: episode: 1917, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -502.406, mean reward: -502.406 [-502.406, -502.406], mean action: 2.000 [2.000, 2.000],  loss: 2995664.750000, mae: 4089.778320, mean_q: -2620.887939
 1918/5000: episode: 1918, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8677.810, mean reward: -8677.810 [-8677.810, -8677.810], mean action: 0.000 [0.000, 0.000],  loss: 4337408.500000, mae: 4312.930664, mean_q: -2625.740723
 1919/5000: episode: 1919, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1663.646, mean reward: -1663.646 [-1663.646, -1663.646], mean action: 2.000 [2.000, 2.000],  loss: 3554555.750000, mae: 4266.701660, mean_q: -2622.676025
 1920/5000: episode: 1920, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -10948.878, mean reward: -10948.878 [-10948.878, -10948.878], mean action: 2.000 [2.000, 2.000],  loss: 3125157.000000, mae: 4063.772217, mean_q: -2598.409668
 1921/5000: episode: 1921, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -892.291, mean reward: -892.291 [-892.291, -892.291], mean action: 2.000 [2.000, 2.000],  loss: 2209492.750000, mae: 4084.005371, mean_q: -2574.533447
 1922/5000: episode: 1922, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -392.331, mean reward: -392.331 [-392.331, -392.331], mean action: 2.000 [2.000, 2.000],  loss: 2013135.750000, mae: 4116.728027, mean_q: -2574.841064
 1923/5000: episode: 1923, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8241.536, mean reward: -8241.536 [-8241.536, -8241.536], mean action: 0.000 [0.000, 0.000],  loss: 3138557.500000, mae: 4165.717773, mean_q: -2559.825439
 1924/5000: episode: 1924, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -482.322, mean reward: -482.322 [-482.322, -482.322], mean action: 2.000 [2.000, 2.000],  loss: 2476416.000000, mae: 4079.909180, mean_q: -2561.072266
 1925/5000: episode: 1925, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -137.230, mean reward: -137.230 [-137.230, -137.230], mean action: 2.000 [2.000, 2.000],  loss: 2544255.500000, mae: 4142.375977, mean_q: -2551.423340
 1926/5000: episode: 1926, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4862.125, mean reward: -4862.125 [-4862.125, -4862.125], mean action: 0.000 [0.000, 0.000],  loss: 3007218.250000, mae: 4151.048828, mean_q: -2541.337402
 1927/5000: episode: 1927, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4507.454, mean reward: -4507.454 [-4507.454, -4507.454], mean action: 2.000 [2.000, 2.000],  loss: 3242638.250000, mae: 4224.780273, mean_q: -2542.397705
 1928/5000: episode: 1928, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3422.550, mean reward: -3422.550 [-3422.550, -3422.550], mean action: 2.000 [2.000, 2.000],  loss: 2217476.500000, mae: 4054.895020, mean_q: -2543.516357
 1929/5000: episode: 1929, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2578.476, mean reward: -2578.476 [-2578.476, -2578.476], mean action: 2.000 [2.000, 2.000],  loss: 2714545.000000, mae: 4248.013184, mean_q: -2535.389648
 1930/5000: episode: 1930, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8166.898, mean reward: -8166.898 [-8166.898, -8166.898], mean action: 1.000 [1.000, 1.000],  loss: 1651487.750000, mae: 4088.619141, mean_q: -2529.651855
 1931/5000: episode: 1931, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -774.619, mean reward: -774.619 [-774.619, -774.619], mean action: 2.000 [2.000, 2.000],  loss: 1850316.000000, mae: 4116.838867, mean_q: -2518.636963
 1932/5000: episode: 1932, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -184.739, mean reward: -184.739 [-184.739, -184.739], mean action: 2.000 [2.000, 2.000],  loss: 1429214.750000, mae: 4092.966309, mean_q: -2526.016357
 1933/5000: episode: 1933, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2423.859, mean reward: -2423.859 [-2423.859, -2423.859], mean action: 2.000 [2.000, 2.000],  loss: 3125018.000000, mae: 4181.703613, mean_q: -2501.444824
 1934/5000: episode: 1934, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2563.678, mean reward: -2563.678 [-2563.678, -2563.678], mean action: 2.000 [2.000, 2.000],  loss: 6701344.000000, mae: 4245.150391, mean_q: -2482.075928
 1935/5000: episode: 1935, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3969.371, mean reward: -3969.371 [-3969.371, -3969.371], mean action: 2.000 [2.000, 2.000],  loss: 3580822.500000, mae: 4173.263184, mean_q: -2473.395508
 1936/5000: episode: 1936, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4331.834, mean reward: -4331.834 [-4331.834, -4331.834], mean action: 2.000 [2.000, 2.000],  loss: 2493264.500000, mae: 4088.292236, mean_q: -2469.875977
 1937/5000: episode: 1937, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1918.937, mean reward: -1918.937 [-1918.937, -1918.937], mean action: 2.000 [2.000, 2.000],  loss: 2613163.000000, mae: 4122.369629, mean_q: -2470.723877
 1938/5000: episode: 1938, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -797.697, mean reward: -797.697 [-797.697, -797.697], mean action: 2.000 [2.000, 2.000],  loss: 2235100.000000, mae: 4068.131836, mean_q: -2468.666504
 1939/5000: episode: 1939, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -434.078, mean reward: -434.078 [-434.078, -434.078], mean action: 2.000 [2.000, 2.000],  loss: 4956884.000000, mae: 4181.018555, mean_q: -2467.604980
 1940/5000: episode: 1940, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -854.505, mean reward: -854.505 [-854.505, -854.505], mean action: 2.000 [2.000, 2.000],  loss: 2563799.500000, mae: 4117.547852, mean_q: -2460.413818
 1941/5000: episode: 1941, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4637.802, mean reward: -4637.802 [-4637.802, -4637.802], mean action: 2.000 [2.000, 2.000],  loss: 3003435.250000, mae: 4227.053711, mean_q: -2458.231445
 1942/5000: episode: 1942, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5579.156, mean reward: -5579.156 [-5579.156, -5579.156], mean action: 2.000 [2.000, 2.000],  loss: 4322001.000000, mae: 4191.906250, mean_q: -2457.147461
 1943/5000: episode: 1943, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1922.432, mean reward: -1922.432 [-1922.432, -1922.432], mean action: 2.000 [2.000, 2.000],  loss: 2985763.250000, mae: 4147.210938, mean_q: -2473.701416
 1944/5000: episode: 1944, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2102.507, mean reward: -2102.507 [-2102.507, -2102.507], mean action: 2.000 [2.000, 2.000],  loss: 2074815.125000, mae: 4154.763184, mean_q: -2465.655762
 1945/5000: episode: 1945, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -157.399, mean reward: -157.399 [-157.399, -157.399], mean action: 2.000 [2.000, 2.000],  loss: 2302495.000000, mae: 4084.822510, mean_q: -2451.606201
 1946/5000: episode: 1946, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3288.662, mean reward: -3288.662 [-3288.662, -3288.662], mean action: 2.000 [2.000, 2.000],  loss: 1927524.000000, mae: 4050.359863, mean_q: -2432.427734
 1947/5000: episode: 1947, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2935.944, mean reward: -2935.944 [-2935.944, -2935.944], mean action: 2.000 [2.000, 2.000],  loss: 3600709.500000, mae: 4030.005371, mean_q: -2426.176270
 1948/5000: episode: 1948, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5346.191, mean reward: -5346.191 [-5346.191, -5346.191], mean action: 0.000 [0.000, 0.000],  loss: 4013670.750000, mae: 4185.844238, mean_q: -2426.941650
 1949/5000: episode: 1949, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5508.055, mean reward: -5508.055 [-5508.055, -5508.055], mean action: 2.000 [2.000, 2.000],  loss: 4596748.500000, mae: 4101.141602, mean_q: -2435.878906
 1950/5000: episode: 1950, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4110.416, mean reward: -4110.416 [-4110.416, -4110.416], mean action: 2.000 [2.000, 2.000],  loss: 3410556.500000, mae: 4146.828125, mean_q: -2446.088379
 1951/5000: episode: 1951, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1606.518, mean reward: -1606.518 [-1606.518, -1606.518], mean action: 1.000 [1.000, 1.000],  loss: 3177936.000000, mae: 4156.753906, mean_q: -2459.187988
 1952/5000: episode: 1952, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7474.668, mean reward: -7474.668 [-7474.668, -7474.668], mean action: 3.000 [3.000, 3.000],  loss: 3018571.500000, mae: 4121.583984, mean_q: -2448.932129
 1953/5000: episode: 1953, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -249.843, mean reward: -249.843 [-249.843, -249.843], mean action: 2.000 [2.000, 2.000],  loss: 2779453.000000, mae: 4143.909180, mean_q: -2476.029053
 1954/5000: episode: 1954, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3495.064, mean reward: -3495.064 [-3495.064, -3495.064], mean action: 2.000 [2.000, 2.000],  loss: 3299507.500000, mae: 4076.769775, mean_q: -2473.169922
 1955/5000: episode: 1955, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9024.469, mean reward: -9024.469 [-9024.469, -9024.469], mean action: 0.000 [0.000, 0.000],  loss: 2831072.000000, mae: 4138.993652, mean_q: -2500.604980
 1956/5000: episode: 1956, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3302.535, mean reward: -3302.535 [-3302.535, -3302.535], mean action: 2.000 [2.000, 2.000],  loss: 2289066.750000, mae: 4079.278564, mean_q: -2506.544434
 1957/5000: episode: 1957, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -503.003, mean reward: -503.003 [-503.003, -503.003], mean action: 2.000 [2.000, 2.000],  loss: 3084898.500000, mae: 4167.458008, mean_q: -2519.718018
 1958/5000: episode: 1958, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2218.866, mean reward: -2218.866 [-2218.866, -2218.866], mean action: 2.000 [2.000, 2.000],  loss: 2648832.000000, mae: 4116.359863, mean_q: -2532.375244
 1959/5000: episode: 1959, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2626.833, mean reward: -2626.833 [-2626.833, -2626.833], mean action: 2.000 [2.000, 2.000],  loss: 1235707.125000, mae: 3982.793457, mean_q: -2533.512695
 1960/5000: episode: 1960, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -629.819, mean reward: -629.819 [-629.819, -629.819], mean action: 2.000 [2.000, 2.000],  loss: 3936398.500000, mae: 4275.089844, mean_q: -2546.451172
 1961/5000: episode: 1961, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8480.762, mean reward: -8480.762 [-8480.762, -8480.762], mean action: 2.000 [2.000, 2.000],  loss: 3636948.500000, mae: 4197.762695, mean_q: -2540.899658
 1962/5000: episode: 1962, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1525.196, mean reward: -1525.196 [-1525.196, -1525.196], mean action: 2.000 [2.000, 2.000],  loss: 2630415.500000, mae: 4131.688477, mean_q: -2542.556152
 1963/5000: episode: 1963, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1311.022, mean reward: -1311.022 [-1311.022, -1311.022], mean action: 2.000 [2.000, 2.000],  loss: 3964153.000000, mae: 4262.517578, mean_q: -2548.333496
 1964/5000: episode: 1964, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6177.349, mean reward: -6177.349 [-6177.349, -6177.349], mean action: 2.000 [2.000, 2.000],  loss: 3084955.500000, mae: 4217.779297, mean_q: -2554.545898
 1965/5000: episode: 1965, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -162.865, mean reward: -162.865 [-162.865, -162.865], mean action: 2.000 [2.000, 2.000],  loss: 2608200.500000, mae: 4125.140137, mean_q: -2550.161133
 1966/5000: episode: 1966, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -992.146, mean reward: -992.146 [-992.146, -992.146], mean action: 2.000 [2.000, 2.000],  loss: 2136479.250000, mae: 4035.274902, mean_q: -2552.769775
 1967/5000: episode: 1967, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5.601, mean reward: -5.601 [-5.601, -5.601], mean action: 2.000 [2.000, 2.000],  loss: 2415974.500000, mae: 4217.379883, mean_q: -2555.028320
 1968/5000: episode: 1968, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2841.495, mean reward: -2841.495 [-2841.495, -2841.495], mean action: 2.000 [2.000, 2.000],  loss: 2756772.500000, mae: 4238.467773, mean_q: -2537.655273
 1969/5000: episode: 1969, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1101.033, mean reward: -1101.033 [-1101.033, -1101.033], mean action: 2.000 [2.000, 2.000],  loss: 3923840.750000, mae: 4245.461914, mean_q: -2542.231934
 1970/5000: episode: 1970, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1103.359, mean reward: -1103.359 [-1103.359, -1103.359], mean action: 2.000 [2.000, 2.000],  loss: 3131143.000000, mae: 4197.929688, mean_q: -2540.482666
 1971/5000: episode: 1971, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2238.282, mean reward: -2238.282 [-2238.282, -2238.282], mean action: 2.000 [2.000, 2.000],  loss: 1980426.875000, mae: 3983.787109, mean_q: -2546.470215
 1972/5000: episode: 1972, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10046.838, mean reward: -10046.838 [-10046.838, -10046.838], mean action: 2.000 [2.000, 2.000],  loss: 2888723.500000, mae: 4142.232422, mean_q: -2556.906738
 1973/5000: episode: 1973, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6380.176, mean reward: -6380.176 [-6380.176, -6380.176], mean action: 2.000 [2.000, 2.000],  loss: 4044557.750000, mae: 4283.850586, mean_q: -2556.076172
 1974/5000: episode: 1974, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -379.171, mean reward: -379.171 [-379.171, -379.171], mean action: 3.000 [3.000, 3.000],  loss: 2871823.250000, mae: 4134.947754, mean_q: -2553.722656
 1975/5000: episode: 1975, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -853.588, mean reward: -853.588 [-853.588, -853.588], mean action: 2.000 [2.000, 2.000],  loss: 3362940.750000, mae: 4158.150391, mean_q: -2562.618164
 1976/5000: episode: 1976, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8557.029, mean reward: -8557.029 [-8557.029, -8557.029], mean action: 0.000 [0.000, 0.000],  loss: 2449088.750000, mae: 4103.247070, mean_q: -2571.148682
 1977/5000: episode: 1977, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -263.602, mean reward: -263.602 [-263.602, -263.602], mean action: 2.000 [2.000, 2.000],  loss: 3713352.750000, mae: 4176.860840, mean_q: -2561.457275
 1978/5000: episode: 1978, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5610.313, mean reward: -5610.313 [-5610.313, -5610.313], mean action: 2.000 [2.000, 2.000],  loss: 4955415.000000, mae: 4236.092773, mean_q: -2555.077881
 1979/5000: episode: 1979, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3301.566, mean reward: -3301.566 [-3301.566, -3301.566], mean action: 2.000 [2.000, 2.000],  loss: 3101427.000000, mae: 4157.686523, mean_q: -2558.025391
 1980/5000: episode: 1980, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1824.236, mean reward: -1824.236 [-1824.236, -1824.236], mean action: 2.000 [2.000, 2.000],  loss: 4135223.250000, mae: 4137.276367, mean_q: -2543.112793
 1981/5000: episode: 1981, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3596.502, mean reward: -3596.502 [-3596.502, -3596.502], mean action: 2.000 [2.000, 2.000],  loss: 2595272.500000, mae: 4090.684570, mean_q: -2551.130371
 1982/5000: episode: 1982, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -2901.017, mean reward: -2901.017 [-2901.017, -2901.017], mean action: 2.000 [2.000, 2.000],  loss: 3119517.250000, mae: 4099.700195, mean_q: -2561.467773
 1983/5000: episode: 1983, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4543.272, mean reward: -4543.272 [-4543.272, -4543.272], mean action: 2.000 [2.000, 2.000],  loss: 2220392.500000, mae: 4133.258301, mean_q: -2556.972168
 1984/5000: episode: 1984, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4789.231, mean reward: -4789.231 [-4789.231, -4789.231], mean action: 2.000 [2.000, 2.000],  loss: 2861916.000000, mae: 4124.454102, mean_q: -2545.367676
 1985/5000: episode: 1985, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4858.114, mean reward: -4858.114 [-4858.114, -4858.114], mean action: 2.000 [2.000, 2.000],  loss: 2448030.750000, mae: 4167.524414, mean_q: -2569.035156
 1986/5000: episode: 1986, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2975.646, mean reward: -2975.646 [-2975.646, -2975.646], mean action: 2.000 [2.000, 2.000],  loss: 3222647.250000, mae: 4163.109375, mean_q: -2580.126709
 1987/5000: episode: 1987, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -539.618, mean reward: -539.618 [-539.618, -539.618], mean action: 2.000 [2.000, 2.000],  loss: 2969439.750000, mae: 4111.489746, mean_q: -2570.083252
 1988/5000: episode: 1988, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4097.851, mean reward: -4097.851 [-4097.851, -4097.851], mean action: 2.000 [2.000, 2.000],  loss: 4960022.500000, mae: 4252.345703, mean_q: -2575.060791
 1989/5000: episode: 1989, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5574.596, mean reward: -5574.596 [-5574.596, -5574.596], mean action: 3.000 [3.000, 3.000],  loss: 2133686.500000, mae: 4072.375977, mean_q: -2582.890137
 1990/5000: episode: 1990, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3592.771, mean reward: -3592.771 [-3592.771, -3592.771], mean action: 2.000 [2.000, 2.000],  loss: 2806678.500000, mae: 4190.494141, mean_q: -2583.771729
 1991/5000: episode: 1991, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1379.867, mean reward: -1379.867 [-1379.867, -1379.867], mean action: 2.000 [2.000, 2.000],  loss: 3148969.500000, mae: 4205.501953, mean_q: -2584.781738
 1992/5000: episode: 1992, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -3838.428, mean reward: -3838.428 [-3838.428, -3838.428], mean action: 2.000 [2.000, 2.000],  loss: 1682072.000000, mae: 4085.043945, mean_q: -2586.187012
 1993/5000: episode: 1993, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1200.854, mean reward: -1200.854 [-1200.854, -1200.854], mean action: 2.000 [2.000, 2.000],  loss: 2286870.250000, mae: 4162.366699, mean_q: -2590.379639
 1994/5000: episode: 1994, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8591.717, mean reward: -8591.717 [-8591.717, -8591.717], mean action: 2.000 [2.000, 2.000],  loss: 2062041.000000, mae: 4045.730957, mean_q: -2590.791992
 1995/5000: episode: 1995, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2555.907, mean reward: -2555.907 [-2555.907, -2555.907], mean action: 2.000 [2.000, 2.000],  loss: 3464964.250000, mae: 4153.251953, mean_q: -2587.300537
 1996/5000: episode: 1996, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3616.243, mean reward: -3616.243 [-3616.243, -3616.243], mean action: 3.000 [3.000, 3.000],  loss: 3297867.000000, mae: 4181.954102, mean_q: -2587.994629
 1997/5000: episode: 1997, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8784.437, mean reward: -8784.437 [-8784.437, -8784.437], mean action: 2.000 [2.000, 2.000],  loss: 2487367.000000, mae: 4112.419922, mean_q: -2605.228516
 1998/5000: episode: 1998, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3107.219, mean reward: -3107.219 [-3107.219, -3107.219], mean action: 2.000 [2.000, 2.000],  loss: 4471982.500000, mae: 4236.075684, mean_q: -2588.804443
 1999/5000: episode: 1999, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4481.353, mean reward: -4481.353 [-4481.353, -4481.353], mean action: 2.000 [2.000, 2.000],  loss: 2590407.000000, mae: 4095.257324, mean_q: -2577.855957
 2000/5000: episode: 2000, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3408.543, mean reward: -3408.543 [-3408.543, -3408.543], mean action: 2.000 [2.000, 2.000],  loss: 2132567.750000, mae: 4022.929199, mean_q: -2579.852051
 2001/5000: episode: 2001, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5273.221, mean reward: -5273.221 [-5273.221, -5273.221], mean action: 2.000 [2.000, 2.000],  loss: 1671889.250000, mae: 4088.728516, mean_q: -2575.883057
 2002/5000: episode: 2002, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2649.263, mean reward: -2649.263 [-2649.263, -2649.263], mean action: 2.000 [2.000, 2.000],  loss: 4093684.750000, mae: 4255.182129, mean_q: -2565.831543
 2003/5000: episode: 2003, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1515.180, mean reward: -1515.180 [-1515.180, -1515.180], mean action: 2.000 [2.000, 2.000],  loss: 2991504.750000, mae: 4095.719727, mean_q: -2563.280273
 2004/5000: episode: 2004, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1548.818, mean reward: -1548.818 [-1548.818, -1548.818], mean action: 2.000 [2.000, 2.000],  loss: 1560658.750000, mae: 4061.944824, mean_q: -2580.198486
 2005/5000: episode: 2005, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3874.714, mean reward: -3874.714 [-3874.714, -3874.714], mean action: 2.000 [2.000, 2.000],  loss: 2914896.000000, mae: 4187.849121, mean_q: -2597.424805
 2006/5000: episode: 2006, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3540.663, mean reward: -3540.663 [-3540.663, -3540.663], mean action: 2.000 [2.000, 2.000],  loss: 2027813.875000, mae: 4121.909180, mean_q: -2607.684570
 2007/5000: episode: 2007, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -797.141, mean reward: -797.141 [-797.141, -797.141], mean action: 2.000 [2.000, 2.000],  loss: 3574491.000000, mae: 4150.072754, mean_q: -2632.965820
 2008/5000: episode: 2008, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2075.418, mean reward: -2075.418 [-2075.418, -2075.418], mean action: 2.000 [2.000, 2.000],  loss: 1667520.250000, mae: 4061.197998, mean_q: -2632.249756
 2009/5000: episode: 2009, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12272.270, mean reward: -12272.270 [-12272.270, -12272.270], mean action: 0.000 [0.000, 0.000],  loss: 2609210.000000, mae: 4076.872314, mean_q: -2623.493164
 2010/5000: episode: 2010, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -8327.874, mean reward: -8327.874 [-8327.874, -8327.874], mean action: 2.000 [2.000, 2.000],  loss: 2817588.000000, mae: 4205.693359, mean_q: -2641.854980
 2011/5000: episode: 2011, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4340.429, mean reward: -4340.429 [-4340.429, -4340.429], mean action: 0.000 [0.000, 0.000],  loss: 5093974.500000, mae: 4280.208496, mean_q: -2650.013916
 2012/5000: episode: 2012, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5503.711, mean reward: -5503.711 [-5503.711, -5503.711], mean action: 2.000 [2.000, 2.000],  loss: 3094249.000000, mae: 4150.235840, mean_q: -2647.428467
 2013/5000: episode: 2013, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2823.468, mean reward: -2823.468 [-2823.468, -2823.468], mean action: 2.000 [2.000, 2.000],  loss: 2630445.750000, mae: 4099.041992, mean_q: -2653.927002
 2014/5000: episode: 2014, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7722.535, mean reward: -7722.535 [-7722.535, -7722.535], mean action: 3.000 [3.000, 3.000],  loss: 4909003.000000, mae: 4171.672363, mean_q: -2657.340820
 2015/5000: episode: 2015, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -787.727, mean reward: -787.727 [-787.727, -787.727], mean action: 2.000 [2.000, 2.000],  loss: 5444964.500000, mae: 4223.063477, mean_q: -2666.443115
 2016/5000: episode: 2016, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4263.493, mean reward: -4263.493 [-4263.493, -4263.493], mean action: 2.000 [2.000, 2.000],  loss: 2148816.750000, mae: 4247.294434, mean_q: -2688.736328
 2017/5000: episode: 2017, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4637.105, mean reward: -4637.105 [-4637.105, -4637.105], mean action: 2.000 [2.000, 2.000],  loss: 3683573.750000, mae: 4268.250977, mean_q: -2668.528320
 2018/5000: episode: 2018, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -279.622, mean reward: -279.622 [-279.622, -279.622], mean action: 2.000 [2.000, 2.000],  loss: 2175394.500000, mae: 4068.766113, mean_q: -2651.688232
 2019/5000: episode: 2019, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -163.853, mean reward: -163.853 [-163.853, -163.853], mean action: 2.000 [2.000, 2.000],  loss: 2536985.500000, mae: 4066.769775, mean_q: -2640.555664
 2020/5000: episode: 2020, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2813.248, mean reward: -2813.248 [-2813.248, -2813.248], mean action: 2.000 [2.000, 2.000],  loss: 4165697.000000, mae: 4371.899414, mean_q: -2659.093262
 2021/5000: episode: 2021, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6516.102, mean reward: -6516.102 [-6516.102, -6516.102], mean action: 2.000 [2.000, 2.000],  loss: 3397566.000000, mae: 4302.119141, mean_q: -2648.213867
 2022/5000: episode: 2022, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6793.940, mean reward: -6793.940 [-6793.940, -6793.940], mean action: 2.000 [2.000, 2.000],  loss: 3135172.500000, mae: 4206.336914, mean_q: -2632.882812
 2023/5000: episode: 2023, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2928.237, mean reward: -2928.237 [-2928.237, -2928.237], mean action: 2.000 [2.000, 2.000],  loss: 2706403.250000, mae: 4164.655273, mean_q: -2637.207031
 2024/5000: episode: 2024, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2790.209, mean reward: -2790.209 [-2790.209, -2790.209], mean action: 2.000 [2.000, 2.000],  loss: 3870700.000000, mae: 4395.508789, mean_q: -2650.282715
 2025/5000: episode: 2025, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3611.884, mean reward: -3611.884 [-3611.884, -3611.884], mean action: 2.000 [2.000, 2.000],  loss: 3299103.500000, mae: 4073.918213, mean_q: -2637.040527
 2026/5000: episode: 2026, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2204.090, mean reward: -2204.090 [-2204.090, -2204.090], mean action: 2.000 [2.000, 2.000],  loss: 3075023.500000, mae: 4283.822266, mean_q: -2650.630371
 2027/5000: episode: 2027, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2487.570, mean reward: -2487.570 [-2487.570, -2487.570], mean action: 2.000 [2.000, 2.000],  loss: 1953039.000000, mae: 4180.795898, mean_q: -2673.034912
 2028/5000: episode: 2028, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1521.006, mean reward: -1521.006 [-1521.006, -1521.006], mean action: 2.000 [2.000, 2.000],  loss: 2698704.500000, mae: 4220.412109, mean_q: -2677.386963
 2029/5000: episode: 2029, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1866.587, mean reward: -1866.587 [-1866.587, -1866.587], mean action: 2.000 [2.000, 2.000],  loss: 2766485.000000, mae: 4286.880859, mean_q: -2684.604248
 2030/5000: episode: 2030, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3195.434, mean reward: -3195.434 [-3195.434, -3195.434], mean action: 2.000 [2.000, 2.000],  loss: 2783412.500000, mae: 4235.466797, mean_q: -2677.609863
 2031/5000: episode: 2031, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -7014.550, mean reward: -7014.550 [-7014.550, -7014.550], mean action: 2.000 [2.000, 2.000],  loss: 2526829.500000, mae: 4286.381348, mean_q: -2697.513184
 2032/5000: episode: 2032, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1308.197, mean reward: -1308.197 [-1308.197, -1308.197], mean action: 2.000 [2.000, 2.000],  loss: 3335255.750000, mae: 4328.734375, mean_q: -2691.219238
 2033/5000: episode: 2033, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -4510.580, mean reward: -4510.580 [-4510.580, -4510.580], mean action: 2.000 [2.000, 2.000],  loss: 3333141.750000, mae: 4189.021484, mean_q: -2699.628418
 2034/5000: episode: 2034, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1928.246, mean reward: -1928.246 [-1928.246, -1928.246], mean action: 2.000 [2.000, 2.000],  loss: 3307066.750000, mae: 4311.060059, mean_q: -2706.087646
 2035/5000: episode: 2035, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3315.719, mean reward: -3315.719 [-3315.719, -3315.719], mean action: 2.000 [2.000, 2.000],  loss: 3321168.750000, mae: 4357.845703, mean_q: -2716.856934
 2036/5000: episode: 2036, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2757.245, mean reward: -2757.245 [-2757.245, -2757.245], mean action: 2.000 [2.000, 2.000],  loss: 2510014.000000, mae: 4231.266113, mean_q: -2722.424805
 2037/5000: episode: 2037, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1575.619, mean reward: -1575.619 [-1575.619, -1575.619], mean action: 2.000 [2.000, 2.000],  loss: 3278508.500000, mae: 4178.870117, mean_q: -2725.599121
 2038/5000: episode: 2038, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5656.435, mean reward: -5656.435 [-5656.435, -5656.435], mean action: 2.000 [2.000, 2.000],  loss: 2427903.000000, mae: 4234.298340, mean_q: -2738.831543
 2039/5000: episode: 2039, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -3178.241, mean reward: -3178.241 [-3178.241, -3178.241], mean action: 2.000 [2.000, 2.000],  loss: 3111345.500000, mae: 4226.476562, mean_q: -2738.479004
 2040/5000: episode: 2040, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -3283.094, mean reward: -3283.094 [-3283.094, -3283.094], mean action: 2.000 [2.000, 2.000],  loss: 2473809.000000, mae: 4253.758301, mean_q: -2731.145508
 2041/5000: episode: 2041, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2251.252, mean reward: -2251.252 [-2251.252, -2251.252], mean action: 2.000 [2.000, 2.000],  loss: 2889561.500000, mae: 4235.127441, mean_q: -2727.104980
 2042/5000: episode: 2042, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2324.898, mean reward: -2324.898 [-2324.898, -2324.898], mean action: 2.000 [2.000, 2.000],  loss: 2657788.500000, mae: 4303.751953, mean_q: -2732.704590
 2043/5000: episode: 2043, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4980.738, mean reward: -4980.738 [-4980.738, -4980.738], mean action: 2.000 [2.000, 2.000],  loss: 1893826.250000, mae: 4105.181641, mean_q: -2702.617676
 2044/5000: episode: 2044, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1905.525, mean reward: -1905.525 [-1905.525, -1905.525], mean action: 2.000 [2.000, 2.000],  loss: 4029068.500000, mae: 4367.003906, mean_q: -2698.196777
 2045/5000: episode: 2045, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4258.775, mean reward: -4258.775 [-4258.775, -4258.775], mean action: 2.000 [2.000, 2.000],  loss: 3293856.750000, mae: 4347.261230, mean_q: -2677.701660
 2046/5000: episode: 2046, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2227.865, mean reward: -2227.865 [-2227.865, -2227.865], mean action: 2.000 [2.000, 2.000],  loss: 6674230.500000, mae: 4382.061523, mean_q: -2669.595215
 2047/5000: episode: 2047, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1594.508, mean reward: -1594.508 [-1594.508, -1594.508], mean action: 2.000 [2.000, 2.000],  loss: 1663044.000000, mae: 4090.235596, mean_q: -2644.488037
 2048/5000: episode: 2048, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -262.001, mean reward: -262.001 [-262.001, -262.001], mean action: 2.000 [2.000, 2.000],  loss: 3330381.500000, mae: 4287.743164, mean_q: -2650.040527
 2049/5000: episode: 2049, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2382.757, mean reward: -2382.757 [-2382.757, -2382.757], mean action: 2.000 [2.000, 2.000],  loss: 3157966.500000, mae: 4246.272461, mean_q: -2637.707520
 2050/5000: episode: 2050, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2121.726, mean reward: -2121.726 [-2121.726, -2121.726], mean action: 2.000 [2.000, 2.000],  loss: 2335219.500000, mae: 4083.273926, mean_q: -2642.306152
 2051/5000: episode: 2051, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6047.728, mean reward: -6047.728 [-6047.728, -6047.728], mean action: 2.000 [2.000, 2.000],  loss: 2966840.000000, mae: 4210.617188, mean_q: -2626.791992
 2052/5000: episode: 2052, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1402.230, mean reward: -1402.230 [-1402.230, -1402.230], mean action: 2.000 [2.000, 2.000],  loss: 2776474.000000, mae: 4150.023438, mean_q: -2621.501221
 2053/5000: episode: 2053, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -60.201, mean reward: -60.201 [-60.201, -60.201], mean action: 2.000 [2.000, 2.000],  loss: 4135258.000000, mae: 4172.245117, mean_q: -2619.831543
 2054/5000: episode: 2054, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5793.756, mean reward: -5793.756 [-5793.756, -5793.756], mean action: 2.000 [2.000, 2.000],  loss: 3194520.000000, mae: 4210.633301, mean_q: -2620.030762
 2055/5000: episode: 2055, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -102.739, mean reward: -102.739 [-102.739, -102.739], mean action: 2.000 [2.000, 2.000],  loss: 3161539.500000, mae: 4270.292969, mean_q: -2616.459961
 2056/5000: episode: 2056, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -3890.046, mean reward: -3890.046 [-3890.046, -3890.046], mean action: 2.000 [2.000, 2.000],  loss: 3831684.000000, mae: 4296.762207, mean_q: -2620.152344
 2057/5000: episode: 2057, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1101.798, mean reward: -1101.798 [-1101.798, -1101.798], mean action: 2.000 [2.000, 2.000],  loss: 3917857.500000, mae: 4215.584961, mean_q: -2625.640137
 2058/5000: episode: 2058, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -4927.321, mean reward: -4927.321 [-4927.321, -4927.321], mean action: 2.000 [2.000, 2.000],  loss: 3852778.500000, mae: 4287.927246, mean_q: -2605.215820
 2059/5000: episode: 2059, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -794.376, mean reward: -794.376 [-794.376, -794.376], mean action: 2.000 [2.000, 2.000],  loss: 2663699.500000, mae: 4209.947266, mean_q: -2603.735840
 2060/5000: episode: 2060, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2147.789, mean reward: -2147.789 [-2147.789, -2147.789], mean action: 2.000 [2.000, 2.000],  loss: 1985904.500000, mae: 4080.925781, mean_q: -2591.971680
 2061/5000: episode: 2061, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1632.757, mean reward: -1632.757 [-1632.757, -1632.757], mean action: 2.000 [2.000, 2.000],  loss: 2382146.750000, mae: 4048.895020, mean_q: -2578.524170
 2062/5000: episode: 2062, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2848.143, mean reward: -2848.143 [-2848.143, -2848.143], mean action: 2.000 [2.000, 2.000],  loss: 4191562.000000, mae: 4251.099609, mean_q: -2568.179199
 2063/5000: episode: 2063, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3293.587, mean reward: -3293.587 [-3293.587, -3293.587], mean action: 2.000 [2.000, 2.000],  loss: 3035310.500000, mae: 4134.007812, mean_q: -2568.298096
 2064/5000: episode: 2064, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3786.379, mean reward: -3786.379 [-3786.379, -3786.379], mean action: 2.000 [2.000, 2.000],  loss: 2692626.500000, mae: 4153.737305, mean_q: -2582.640137
 2065/5000: episode: 2065, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -509.036, mean reward: -509.036 [-509.036, -509.036], mean action: 2.000 [2.000, 2.000],  loss: 4765258.500000, mae: 4189.533203, mean_q: -2579.076660
 2066/5000: episode: 2066, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -960.538, mean reward: -960.538 [-960.538, -960.538], mean action: 2.000 [2.000, 2.000],  loss: 3695338.750000, mae: 4148.941406, mean_q: -2596.510010
 2067/5000: episode: 2067, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -807.574, mean reward: -807.574 [-807.574, -807.574], mean action: 2.000 [2.000, 2.000],  loss: 4129105.000000, mae: 4303.653809, mean_q: -2607.396484
 2068/5000: episode: 2068, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2140.220, mean reward: -2140.220 [-2140.220, -2140.220], mean action: 2.000 [2.000, 2.000],  loss: 2988396.000000, mae: 4283.872070, mean_q: -2618.303711
 2069/5000: episode: 2069, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2290.220, mean reward: -2290.220 [-2290.220, -2290.220], mean action: 2.000 [2.000, 2.000],  loss: 2794874.750000, mae: 4218.399414, mean_q: -2614.814453
 2070/5000: episode: 2070, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -256.444, mean reward: -256.444 [-256.444, -256.444], mean action: 2.000 [2.000, 2.000],  loss: 2518722.000000, mae: 4219.102539, mean_q: -2631.878174
 2071/5000: episode: 2071, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -933.024, mean reward: -933.024 [-933.024, -933.024], mean action: 2.000 [2.000, 2.000],  loss: 2500818.000000, mae: 4232.884766, mean_q: -2618.558594
 2072/5000: episode: 2072, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5272.374, mean reward: -5272.374 [-5272.374, -5272.374], mean action: 3.000 [3.000, 3.000],  loss: 4276703.500000, mae: 4139.824707, mean_q: -2597.721191
 2073/5000: episode: 2073, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4296.430, mean reward: -4296.430 [-4296.430, -4296.430], mean action: 2.000 [2.000, 2.000],  loss: 3038150.250000, mae: 4279.981445, mean_q: -2603.735840
 2074/5000: episode: 2074, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -441.170, mean reward: -441.170 [-441.170, -441.170], mean action: 2.000 [2.000, 2.000],  loss: 3359966.000000, mae: 4140.169434, mean_q: -2610.709717
 2075/5000: episode: 2075, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1008.757, mean reward: -1008.757 [-1008.757, -1008.757], mean action: 2.000 [2.000, 2.000],  loss: 2591912.500000, mae: 4222.298828, mean_q: -2597.967773
 2076/5000: episode: 2076, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -374.595, mean reward: -374.595 [-374.595, -374.595], mean action: 2.000 [2.000, 2.000],  loss: 3227590.500000, mae: 4178.782227, mean_q: -2593.339355
 2077/5000: episode: 2077, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4509.152, mean reward: -4509.152 [-4509.152, -4509.152], mean action: 2.000 [2.000, 2.000],  loss: 5804599.000000, mae: 4348.796875, mean_q: -2595.272217
 2078/5000: episode: 2078, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2057.373, mean reward: -2057.373 [-2057.373, -2057.373], mean action: 2.000 [2.000, 2.000],  loss: 2444741.750000, mae: 4105.385742, mean_q: -2582.588623
 2079/5000: episode: 2079, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1879.514, mean reward: -1879.514 [-1879.514, -1879.514], mean action: 2.000 [2.000, 2.000],  loss: 3410815.500000, mae: 4242.666016, mean_q: -2559.216309
 2080/5000: episode: 2080, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -510.764, mean reward: -510.764 [-510.764, -510.764], mean action: 2.000 [2.000, 2.000],  loss: 2302547.000000, mae: 4221.340820, mean_q: -2556.056641
 2081/5000: episode: 2081, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3255.066, mean reward: -3255.066 [-3255.066, -3255.066], mean action: 2.000 [2.000, 2.000],  loss: 4600390.500000, mae: 4358.027344, mean_q: -2533.068115
 2082/5000: episode: 2082, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2644.709, mean reward: -2644.709 [-2644.709, -2644.709], mean action: 2.000 [2.000, 2.000],  loss: 4596070.000000, mae: 4343.000977, mean_q: -2539.845703
 2083/5000: episode: 2083, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -1860.254, mean reward: -1860.254 [-1860.254, -1860.254], mean action: 2.000 [2.000, 2.000],  loss: 3726965.250000, mae: 4265.162109, mean_q: -2539.216797
 2084/5000: episode: 2084, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3974.951, mean reward: -3974.951 [-3974.951, -3974.951], mean action: 2.000 [2.000, 2.000],  loss: 4922656.000000, mae: 4374.517578, mean_q: -2555.176270
 2085/5000: episode: 2085, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1570.491, mean reward: -1570.491 [-1570.491, -1570.491], mean action: 2.000 [2.000, 2.000],  loss: 2674882.000000, mae: 4214.069824, mean_q: -2550.423828
 2086/5000: episode: 2086, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3401.194, mean reward: -3401.194 [-3401.194, -3401.194], mean action: 2.000 [2.000, 2.000],  loss: 2916151.000000, mae: 4232.240234, mean_q: -2557.274414
 2087/5000: episode: 2087, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3255.293, mean reward: -3255.293 [-3255.293, -3255.293], mean action: 2.000 [2.000, 2.000],  loss: 3487650.250000, mae: 4269.070312, mean_q: -2567.600830
 2088/5000: episode: 2088, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3048.957, mean reward: -3048.957 [-3048.957, -3048.957], mean action: 2.000 [2.000, 2.000],  loss: 1695218.750000, mae: 4139.332520, mean_q: -2568.849365
 2089/5000: episode: 2089, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4679.124, mean reward: -4679.124 [-4679.124, -4679.124], mean action: 3.000 [3.000, 3.000],  loss: 2606666.500000, mae: 4179.277344, mean_q: -2570.982666
 2090/5000: episode: 2090, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1929.334, mean reward: -1929.334 [-1929.334, -1929.334], mean action: 1.000 [1.000, 1.000],  loss: 2150612.750000, mae: 4195.169922, mean_q: -2584.428955
 2091/5000: episode: 2091, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -8932.996, mean reward: -8932.996 [-8932.996, -8932.996], mean action: 2.000 [2.000, 2.000],  loss: 2875571.000000, mae: 4288.934570, mean_q: -2590.121582
 2092/5000: episode: 2092, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -53.517, mean reward: -53.517 [-53.517, -53.517], mean action: 2.000 [2.000, 2.000],  loss: 2549728.750000, mae: 4225.620605, mean_q: -2564.456543
 2093/5000: episode: 2093, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1583.245, mean reward: -1583.245 [-1583.245, -1583.245], mean action: 2.000 [2.000, 2.000],  loss: 4638189.000000, mae: 4316.808105, mean_q: -2569.286621
 2094/5000: episode: 2094, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3238.824, mean reward: -3238.824 [-3238.824, -3238.824], mean action: 2.000 [2.000, 2.000],  loss: 1322371.250000, mae: 4050.524902, mean_q: -2561.106689
 2095/5000: episode: 2095, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2522.306, mean reward: -2522.306 [-2522.306, -2522.306], mean action: 2.000 [2.000, 2.000],  loss: 3688765.500000, mae: 4296.608887, mean_q: -2549.182617
 2096/5000: episode: 2096, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -360.142, mean reward: -360.142 [-360.142, -360.142], mean action: 2.000 [2.000, 2.000],  loss: 3103259.750000, mae: 4141.914062, mean_q: -2567.256348
 2097/5000: episode: 2097, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5041.021, mean reward: -5041.021 [-5041.021, -5041.021], mean action: 2.000 [2.000, 2.000],  loss: 3705005.250000, mae: 4252.774414, mean_q: -2578.756592
 2098/5000: episode: 2098, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1734.482, mean reward: -1734.482 [-1734.482, -1734.482], mean action: 2.000 [2.000, 2.000],  loss: 3736741.000000, mae: 4304.599609, mean_q: -2601.750977
 2099/5000: episode: 2099, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2114.052, mean reward: -2114.052 [-2114.052, -2114.052], mean action: 2.000 [2.000, 2.000],  loss: 2742333.500000, mae: 4184.992188, mean_q: -2605.538574
 2100/5000: episode: 2100, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1472.814, mean reward: -1472.814 [-1472.814, -1472.814], mean action: 2.000 [2.000, 2.000],  loss: 2280626.750000, mae: 4155.398438, mean_q: -2607.266602
 2101/5000: episode: 2101, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10053.091, mean reward: -10053.091 [-10053.091, -10053.091], mean action: 2.000 [2.000, 2.000],  loss: 2416607.750000, mae: 4179.852051, mean_q: -2605.030762
 2102/5000: episode: 2102, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -33.021, mean reward: -33.021 [-33.021, -33.021], mean action: 2.000 [2.000, 2.000],  loss: 3993085.250000, mae: 4350.387695, mean_q: -2617.398682
 2103/5000: episode: 2103, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -847.849, mean reward: -847.849 [-847.849, -847.849], mean action: 2.000 [2.000, 2.000],  loss: 2531759.000000, mae: 4265.290039, mean_q: -2632.789551
 2104/5000: episode: 2104, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5574.570, mean reward: -5574.570 [-5574.570, -5574.570], mean action: 2.000 [2.000, 2.000],  loss: 2925044.750000, mae: 4337.865234, mean_q: -2623.511719
 2105/5000: episode: 2105, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6671.826, mean reward: -6671.826 [-6671.826, -6671.826], mean action: 2.000 [2.000, 2.000],  loss: 2975012.000000, mae: 4237.050781, mean_q: -2609.642822
 2106/5000: episode: 2106, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -572.116, mean reward: -572.116 [-572.116, -572.116], mean action: 2.000 [2.000, 2.000],  loss: 2140076.500000, mae: 4252.727051, mean_q: -2623.048828
 2107/5000: episode: 2107, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -846.514, mean reward: -846.514 [-846.514, -846.514], mean action: 2.000 [2.000, 2.000],  loss: 3682165.750000, mae: 4399.006836, mean_q: -2598.673828
 2108/5000: episode: 2108, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2726.445, mean reward: -2726.445 [-2726.445, -2726.445], mean action: 2.000 [2.000, 2.000],  loss: 2352208.000000, mae: 4281.002930, mean_q: -2605.382812
 2109/5000: episode: 2109, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -822.176, mean reward: -822.176 [-822.176, -822.176], mean action: 2.000 [2.000, 2.000],  loss: 2439742.250000, mae: 4224.242188, mean_q: -2593.487061
 2110/5000: episode: 2110, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2479.799, mean reward: -2479.799 [-2479.799, -2479.799], mean action: 2.000 [2.000, 2.000],  loss: 2563068.000000, mae: 4269.177246, mean_q: -2580.931152
 2111/5000: episode: 2111, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1009.703, mean reward: -1009.703 [-1009.703, -1009.703], mean action: 2.000 [2.000, 2.000],  loss: 2684613.250000, mae: 4298.822266, mean_q: -2577.068115
 2112/5000: episode: 2112, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2332.666, mean reward: -2332.666 [-2332.666, -2332.666], mean action: 2.000 [2.000, 2.000],  loss: 1786336.250000, mae: 4212.965820, mean_q: -2572.335938
 2113/5000: episode: 2113, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3701.062, mean reward: -3701.062 [-3701.062, -3701.062], mean action: 2.000 [2.000, 2.000],  loss: 3038957.250000, mae: 4127.463867, mean_q: -2557.739746
 2114/5000: episode: 2114, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2587.604, mean reward: -2587.604 [-2587.604, -2587.604], mean action: 2.000 [2.000, 2.000],  loss: 4667581.000000, mae: 4260.388672, mean_q: -2556.690430
 2115/5000: episode: 2115, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5944.011, mean reward: -5944.011 [-5944.011, -5944.011], mean action: 2.000 [2.000, 2.000],  loss: 3303469.500000, mae: 4294.100586, mean_q: -2563.744141
 2116/5000: episode: 2116, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -13.672, mean reward: -13.672 [-13.672, -13.672], mean action: 2.000 [2.000, 2.000],  loss: 3694692.000000, mae: 4353.053711, mean_q: -2559.119629
 2117/5000: episode: 2117, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4470.738, mean reward: -4470.738 [-4470.738, -4470.738], mean action: 2.000 [2.000, 2.000],  loss: 2642685.500000, mae: 4205.049805, mean_q: -2554.016846
 2118/5000: episode: 2118, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -859.545, mean reward: -859.545 [-859.545, -859.545], mean action: 2.000 [2.000, 2.000],  loss: 1931841.500000, mae: 4184.063477, mean_q: -2550.916016
 2119/5000: episode: 2119, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4572.913, mean reward: -4572.913 [-4572.913, -4572.913], mean action: 2.000 [2.000, 2.000],  loss: 2726882.250000, mae: 4262.995605, mean_q: -2549.816650
 2120/5000: episode: 2120, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2249.803, mean reward: -2249.803 [-2249.803, -2249.803], mean action: 2.000 [2.000, 2.000],  loss: 3930974.500000, mae: 4256.677734, mean_q: -2566.857178
 2121/5000: episode: 2121, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1624.728, mean reward: -1624.728 [-1624.728, -1624.728], mean action: 2.000 [2.000, 2.000],  loss: 2350101.000000, mae: 4153.604492, mean_q: -2566.996826
 2122/5000: episode: 2122, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1148.464, mean reward: -1148.464 [-1148.464, -1148.464], mean action: 2.000 [2.000, 2.000],  loss: 2446499.000000, mae: 4124.587891, mean_q: -2579.854492
 2123/5000: episode: 2123, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4189.662, mean reward: -4189.662 [-4189.662, -4189.662], mean action: 2.000 [2.000, 2.000],  loss: 4580944.500000, mae: 4414.669922, mean_q: -2592.046875
 2124/5000: episode: 2124, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1425.899, mean reward: -1425.899 [-1425.899, -1425.899], mean action: 2.000 [2.000, 2.000],  loss: 4917500.000000, mae: 4381.710938, mean_q: -2616.646240
 2125/5000: episode: 2125, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -549.077, mean reward: -549.077 [-549.077, -549.077], mean action: 2.000 [2.000, 2.000],  loss: 2493494.500000, mae: 4303.043945, mean_q: -2619.456543
 2126/5000: episode: 2126, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6079.858, mean reward: -6079.858 [-6079.858, -6079.858], mean action: 3.000 [3.000, 3.000],  loss: 2342864.500000, mae: 4198.217285, mean_q: -2627.805664
 2127/5000: episode: 2127, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5.556, mean reward: -5.556 [-5.556, -5.556], mean action: 2.000 [2.000, 2.000],  loss: 2170852.000000, mae: 4324.883789, mean_q: -2638.019531
 2128/5000: episode: 2128, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2021.891, mean reward: -2021.891 [-2021.891, -2021.891], mean action: 2.000 [2.000, 2.000],  loss: 5673483.000000, mae: 4437.648438, mean_q: -2635.684570
 2129/5000: episode: 2129, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -120.815, mean reward: -120.815 [-120.815, -120.815], mean action: 2.000 [2.000, 2.000],  loss: 2929901.250000, mae: 4286.372070, mean_q: -2638.420166
 2130/5000: episode: 2130, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -9287.823, mean reward: -9287.823 [-9287.823, -9287.823], mean action: 2.000 [2.000, 2.000],  loss: 2718200.750000, mae: 4221.953613, mean_q: -2654.443848
 2131/5000: episode: 2131, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1872.812, mean reward: -1872.812 [-1872.812, -1872.812], mean action: 2.000 [2.000, 2.000],  loss: 2891483.500000, mae: 4313.658203, mean_q: -2673.550293
 2132/5000: episode: 2132, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -698.104, mean reward: -698.104 [-698.104, -698.104], mean action: 2.000 [2.000, 2.000],  loss: 3115057.250000, mae: 4277.158691, mean_q: -2684.230469
 2133/5000: episode: 2133, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -2666.393, mean reward: -2666.393 [-2666.393, -2666.393], mean action: 2.000 [2.000, 2.000],  loss: 4035457.500000, mae: 4352.101562, mean_q: -2685.697021
 2134/5000: episode: 2134, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1352.164, mean reward: -1352.164 [-1352.164, -1352.164], mean action: 2.000 [2.000, 2.000],  loss: 2146527.750000, mae: 4231.486328, mean_q: -2699.139160
 2135/5000: episode: 2135, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -3445.138, mean reward: -3445.138 [-3445.138, -3445.138], mean action: 1.000 [1.000, 1.000],  loss: 2065003.750000, mae: 4264.649414, mean_q: -2689.765381
 2136/5000: episode: 2136, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4533.361, mean reward: -4533.361 [-4533.361, -4533.361], mean action: 2.000 [2.000, 2.000],  loss: 2556005.500000, mae: 4298.203125, mean_q: -2706.500977
 2137/5000: episode: 2137, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1869.141, mean reward: -1869.141 [-1869.141, -1869.141], mean action: 2.000 [2.000, 2.000],  loss: 3358082.500000, mae: 4359.839844, mean_q: -2694.233398
 2138/5000: episode: 2138, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3834.884, mean reward: -3834.884 [-3834.884, -3834.884], mean action: 2.000 [2.000, 2.000],  loss: 2623372.500000, mae: 4353.657227, mean_q: -2696.260498
 2139/5000: episode: 2139, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2109.751, mean reward: -2109.751 [-2109.751, -2109.751], mean action: 2.000 [2.000, 2.000],  loss: 3530746.500000, mae: 4325.980957, mean_q: -2708.596680
 2140/5000: episode: 2140, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2729.630, mean reward: -2729.630 [-2729.630, -2729.630], mean action: 2.000 [2.000, 2.000],  loss: 2969800.000000, mae: 4201.571289, mean_q: -2696.040527
 2141/5000: episode: 2141, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8611.178, mean reward: -8611.178 [-8611.178, -8611.178], mean action: 2.000 [2.000, 2.000],  loss: 4533535.000000, mae: 4443.309082, mean_q: -2694.098877
 2142/5000: episode: 2142, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3896.868, mean reward: -3896.868 [-3896.868, -3896.868], mean action: 2.000 [2.000, 2.000],  loss: 3872530.250000, mae: 4413.990723, mean_q: -2691.842285
 2143/5000: episode: 2143, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2497.537, mean reward: -2497.537 [-2497.537, -2497.537], mean action: 2.000 [2.000, 2.000],  loss: 2604715.750000, mae: 4320.454102, mean_q: -2681.208740
 2144/5000: episode: 2144, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1359.504, mean reward: -1359.504 [-1359.504, -1359.504], mean action: 2.000 [2.000, 2.000],  loss: 3494499.000000, mae: 4403.633789, mean_q: -2679.612549
 2145/5000: episode: 2145, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -134.293, mean reward: -134.293 [-134.293, -134.293], mean action: 2.000 [2.000, 2.000],  loss: 3198647.500000, mae: 4371.601562, mean_q: -2680.688477
 2146/5000: episode: 2146, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1130.623, mean reward: -1130.623 [-1130.623, -1130.623], mean action: 2.000 [2.000, 2.000],  loss: 3883330.000000, mae: 4254.924316, mean_q: -2679.068359
 2147/5000: episode: 2147, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2917.322, mean reward: -2917.322 [-2917.322, -2917.322], mean action: 2.000 [2.000, 2.000],  loss: 1658088.500000, mae: 4241.496094, mean_q: -2668.293945
 2148/5000: episode: 2148, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1754.177, mean reward: -1754.177 [-1754.177, -1754.177], mean action: 2.000 [2.000, 2.000],  loss: 4879348.500000, mae: 4460.811523, mean_q: -2676.994141
 2149/5000: episode: 2149, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -650.900, mean reward: -650.900 [-650.900, -650.900], mean action: 2.000 [2.000, 2.000],  loss: 2216402.250000, mae: 4374.883301, mean_q: -2674.412842
 2150/5000: episode: 2150, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5813.101, mean reward: -5813.101 [-5813.101, -5813.101], mean action: 1.000 [1.000, 1.000],  loss: 4215960.000000, mae: 4401.643555, mean_q: -2653.955322
 2151/5000: episode: 2151, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -361.159, mean reward: -361.159 [-361.159, -361.159], mean action: 2.000 [2.000, 2.000],  loss: 2225967.250000, mae: 4313.072266, mean_q: -2648.282227
 2152/5000: episode: 2152, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -944.609, mean reward: -944.609 [-944.609, -944.609], mean action: 2.000 [2.000, 2.000],  loss: 3926620.000000, mae: 4363.109375, mean_q: -2649.831543
 2153/5000: episode: 2153, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5.580, mean reward: -5.580 [-5.580, -5.580], mean action: 2.000 [2.000, 2.000],  loss: 1276276.000000, mae: 4156.549316, mean_q: -2626.235352
 2154/5000: episode: 2154, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1441.204, mean reward: -1441.204 [-1441.204, -1441.204], mean action: 2.000 [2.000, 2.000],  loss: 2173088.500000, mae: 4355.706543, mean_q: -2613.961914
 2155/5000: episode: 2155, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -224.034, mean reward: -224.034 [-224.034, -224.034], mean action: 2.000 [2.000, 2.000],  loss: 3260065.750000, mae: 4357.005859, mean_q: -2604.388916
 2156/5000: episode: 2156, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4222.428, mean reward: -4222.428 [-4222.428, -4222.428], mean action: 2.000 [2.000, 2.000],  loss: 5226729.000000, mae: 4396.029785, mean_q: -2586.203857
 2157/5000: episode: 2157, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -1427.082, mean reward: -1427.082 [-1427.082, -1427.082], mean action: 2.000 [2.000, 2.000],  loss: 2454130.750000, mae: 4270.755859, mean_q: -2582.531494
 2158/5000: episode: 2158, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5824.030, mean reward: -5824.030 [-5824.030, -5824.030], mean action: 2.000 [2.000, 2.000],  loss: 2025782.250000, mae: 4182.026367, mean_q: -2591.049805
 2159/5000: episode: 2159, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -703.405, mean reward: -703.405 [-703.405, -703.405], mean action: 2.000 [2.000, 2.000],  loss: 2516727.500000, mae: 4288.469727, mean_q: -2583.352295
 2160/5000: episode: 2160, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -380.640, mean reward: -380.640 [-380.640, -380.640], mean action: 2.000 [2.000, 2.000],  loss: 2908825.000000, mae: 4366.681641, mean_q: -2578.065430
 2161/5000: episode: 2161, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2733.689, mean reward: -2733.689 [-2733.689, -2733.689], mean action: 2.000 [2.000, 2.000],  loss: 2602897.500000, mae: 4236.094238, mean_q: -2584.582520
 2162/5000: episode: 2162, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -687.185, mean reward: -687.185 [-687.185, -687.185], mean action: 2.000 [2.000, 2.000],  loss: 2877588.750000, mae: 4253.386719, mean_q: -2568.585693
 2163/5000: episode: 2163, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6384.662, mean reward: -6384.662 [-6384.662, -6384.662], mean action: 0.000 [0.000, 0.000],  loss: 3036852.500000, mae: 4315.915039, mean_q: -2576.184326
 2164/5000: episode: 2164, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -2718.291, mean reward: -2718.291 [-2718.291, -2718.291], mean action: 2.000 [2.000, 2.000],  loss: 4701365.000000, mae: 4434.985352, mean_q: -2577.238281
 2165/5000: episode: 2165, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2857.608, mean reward: -2857.608 [-2857.608, -2857.608], mean action: 2.000 [2.000, 2.000],  loss: 3244973.000000, mae: 4368.608398, mean_q: -2588.941650
 2166/5000: episode: 2166, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -357.035, mean reward: -357.035 [-357.035, -357.035], mean action: 2.000 [2.000, 2.000],  loss: 3692906.750000, mae: 4384.465332, mean_q: -2592.059570
 2167/5000: episode: 2167, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2160.899, mean reward: -2160.899 [-2160.899, -2160.899], mean action: 2.000 [2.000, 2.000],  loss: 2901437.500000, mae: 4334.233398, mean_q: -2608.756348
 2168/5000: episode: 2168, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -1811.458, mean reward: -1811.458 [-1811.458, -1811.458], mean action: 2.000 [2.000, 2.000],  loss: 4072411.250000, mae: 4456.874023, mean_q: -2623.859863
 2169/5000: episode: 2169, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -292.378, mean reward: -292.378 [-292.378, -292.378], mean action: 2.000 [2.000, 2.000],  loss: 3430923.500000, mae: 4344.518555, mean_q: -2645.214355
 2170/5000: episode: 2170, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3115.135, mean reward: -3115.135 [-3115.135, -3115.135], mean action: 2.000 [2.000, 2.000],  loss: 2321524.000000, mae: 4372.941406, mean_q: -2656.591797
 2171/5000: episode: 2171, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4016.419, mean reward: -4016.419 [-4016.419, -4016.419], mean action: 0.000 [0.000, 0.000],  loss: 2817953.000000, mae: 4330.282227, mean_q: -2679.911133
 2172/5000: episode: 2172, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1879.719, mean reward: -1879.719 [-1879.719, -1879.719], mean action: 2.000 [2.000, 2.000],  loss: 3134908.000000, mae: 4279.277344, mean_q: -2678.056641
 2173/5000: episode: 2173, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -156.233, mean reward: -156.233 [-156.233, -156.233], mean action: 2.000 [2.000, 2.000],  loss: 4313189.000000, mae: 4349.907715, mean_q: -2668.051758
 2174/5000: episode: 2174, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1001.269, mean reward: -1001.269 [-1001.269, -1001.269], mean action: 2.000 [2.000, 2.000],  loss: 2787807.500000, mae: 4172.733398, mean_q: -2678.094238
 2175/5000: episode: 2175, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -940.311, mean reward: -940.311 [-940.311, -940.311], mean action: 2.000 [2.000, 2.000],  loss: 2831159.500000, mae: 4297.368164, mean_q: -2659.920410
 2176/5000: episode: 2176, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6224.449, mean reward: -6224.449 [-6224.449, -6224.449], mean action: 2.000 [2.000, 2.000],  loss: 2730384.000000, mae: 4309.041992, mean_q: -2645.667480
 2177/5000: episode: 2177, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -126.811, mean reward: -126.811 [-126.811, -126.811], mean action: 2.000 [2.000, 2.000],  loss: 2424052.000000, mae: 4281.817383, mean_q: -2646.869629
 2178/5000: episode: 2178, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -64.546, mean reward: -64.546 [-64.546, -64.546], mean action: 2.000 [2.000, 2.000],  loss: 2587799.750000, mae: 4218.411133, mean_q: -2629.068604
 2179/5000: episode: 2179, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2630.604, mean reward: -2630.604 [-2630.604, -2630.604], mean action: 2.000 [2.000, 2.000],  loss: 2098592.500000, mae: 4157.666992, mean_q: -2622.212891
 2180/5000: episode: 2180, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1022.219, mean reward: -1022.219 [-1022.219, -1022.219], mean action: 2.000 [2.000, 2.000],  loss: 2201058.500000, mae: 4196.455078, mean_q: -2622.787842
 2181/5000: episode: 2181, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1141.686, mean reward: -1141.686 [-1141.686, -1141.686], mean action: 2.000 [2.000, 2.000],  loss: 3437206.000000, mae: 4358.426758, mean_q: -2601.388428
 2182/5000: episode: 2182, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7685.807, mean reward: -7685.807 [-7685.807, -7685.807], mean action: 0.000 [0.000, 0.000],  loss: 3705168.250000, mae: 4284.059570, mean_q: -2595.177979
 2183/5000: episode: 2183, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -371.959, mean reward: -371.959 [-371.959, -371.959], mean action: 2.000 [2.000, 2.000],  loss: 2449781.000000, mae: 4195.625000, mean_q: -2589.485840
 2184/5000: episode: 2184, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3385.505, mean reward: -3385.505 [-3385.505, -3385.505], mean action: 2.000 [2.000, 2.000],  loss: 5087316.000000, mae: 4375.590820, mean_q: -2598.483398
 2185/5000: episode: 2185, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1498.952, mean reward: -1498.952 [-1498.952, -1498.952], mean action: 2.000 [2.000, 2.000],  loss: 2265338.750000, mae: 4156.346191, mean_q: -2598.913086
 2186/5000: episode: 2186, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -4486.786, mean reward: -4486.786 [-4486.786, -4486.786], mean action: 2.000 [2.000, 2.000],  loss: 3324795.500000, mae: 4323.121094, mean_q: -2584.901367
 2187/5000: episode: 2187, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2009.052, mean reward: -2009.052 [-2009.052, -2009.052], mean action: 2.000 [2.000, 2.000],  loss: 3723210.000000, mae: 4224.274414, mean_q: -2571.678223
 2188/5000: episode: 2188, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6345.242, mean reward: -6345.242 [-6345.242, -6345.242], mean action: 0.000 [0.000, 0.000],  loss: 3269971.500000, mae: 4245.848633, mean_q: -2566.857910
 2189/5000: episode: 2189, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5001.885, mean reward: -5001.885 [-5001.885, -5001.885], mean action: 2.000 [2.000, 2.000],  loss: 3806865.250000, mae: 4279.085938, mean_q: -2555.456787
 2190/5000: episode: 2190, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4499.585, mean reward: -4499.585 [-4499.585, -4499.585], mean action: 2.000 [2.000, 2.000],  loss: 2872480.000000, mae: 4218.508789, mean_q: -2565.648682
 2191/5000: episode: 2191, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -513.256, mean reward: -513.256 [-513.256, -513.256], mean action: 2.000 [2.000, 2.000],  loss: 2259240.750000, mae: 4110.538086, mean_q: -2546.999023
 2192/5000: episode: 2192, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3310.771, mean reward: -3310.771 [-3310.771, -3310.771], mean action: 2.000 [2.000, 2.000],  loss: 4786123.000000, mae: 4266.556641, mean_q: -2544.602539
 2193/5000: episode: 2193, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -348.693, mean reward: -348.693 [-348.693, -348.693], mean action: 2.000 [2.000, 2.000],  loss: 2318712.250000, mae: 4228.996094, mean_q: -2545.986816
 2194/5000: episode: 2194, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -8789.540, mean reward: -8789.540 [-8789.540, -8789.540], mean action: 2.000 [2.000, 2.000],  loss: 3025364.500000, mae: 4244.525391, mean_q: -2533.622803
 2195/5000: episode: 2195, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2482.652, mean reward: -2482.652 [-2482.652, -2482.652], mean action: 2.000 [2.000, 2.000],  loss: 2119061.000000, mae: 4105.127441, mean_q: -2531.250977
 2196/5000: episode: 2196, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -977.891, mean reward: -977.891 [-977.891, -977.891], mean action: 2.000 [2.000, 2.000],  loss: 3557336.750000, mae: 4168.599121, mean_q: -2531.970703
 2197/5000: episode: 2197, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1881.310, mean reward: -1881.310 [-1881.310, -1881.310], mean action: 2.000 [2.000, 2.000],  loss: 2689945.500000, mae: 4174.220703, mean_q: -2527.582275
 2198/5000: episode: 2198, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -587.395, mean reward: -587.395 [-587.395, -587.395], mean action: 2.000 [2.000, 2.000],  loss: 4039798.000000, mae: 4300.668945, mean_q: -2524.776611
 2199/5000: episode: 2199, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2655.625, mean reward: -2655.625 [-2655.625, -2655.625], mean action: 2.000 [2.000, 2.000],  loss: 3479599.750000, mae: 4253.439453, mean_q: -2528.672607
 2200/5000: episode: 2200, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2133.319, mean reward: -2133.319 [-2133.319, -2133.319], mean action: 2.000 [2.000, 2.000],  loss: 1830529.000000, mae: 4151.120117, mean_q: -2536.458496
 2201/5000: episode: 2201, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1339.062, mean reward: -1339.062 [-1339.062, -1339.062], mean action: 2.000 [2.000, 2.000],  loss: 2487261.250000, mae: 4185.788086, mean_q: -2554.348633
 2202/5000: episode: 2202, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5310.250, mean reward: -5310.250 [-5310.250, -5310.250], mean action: 3.000 [3.000, 3.000],  loss: 2260583.500000, mae: 4250.708496, mean_q: -2560.660156
 2203/5000: episode: 2203, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -976.987, mean reward: -976.987 [-976.987, -976.987], mean action: 2.000 [2.000, 2.000],  loss: 3235274.250000, mae: 4338.656250, mean_q: -2572.193604
 2204/5000: episode: 2204, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5369.756, mean reward: -5369.756 [-5369.756, -5369.756], mean action: 2.000 [2.000, 2.000],  loss: 3393585.000000, mae: 4298.038086, mean_q: -2566.398926
 2205/5000: episode: 2205, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1012.735, mean reward: -1012.735 [-1012.735, -1012.735], mean action: 2.000 [2.000, 2.000],  loss: 1900764.500000, mae: 4232.143555, mean_q: -2575.238037
 2206/5000: episode: 2206, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -842.427, mean reward: -842.427 [-842.427, -842.427], mean action: 3.000 [3.000, 3.000],  loss: 3657849.500000, mae: 4270.758301, mean_q: -2583.172363
 2207/5000: episode: 2207, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1583.558, mean reward: -1583.558 [-1583.558, -1583.558], mean action: 2.000 [2.000, 2.000],  loss: 2974885.250000, mae: 4248.849609, mean_q: -2588.602051
 2208/5000: episode: 2208, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8205.258, mean reward: -8205.258 [-8205.258, -8205.258], mean action: 2.000 [2.000, 2.000],  loss: 2747777.000000, mae: 4237.374512, mean_q: -2592.887207
 2209/5000: episode: 2209, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3390.656, mean reward: -3390.656 [-3390.656, -3390.656], mean action: 2.000 [2.000, 2.000],  loss: 4020647.250000, mae: 4307.724609, mean_q: -2591.071045
 2210/5000: episode: 2210, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2462.149, mean reward: -2462.149 [-2462.149, -2462.149], mean action: 2.000 [2.000, 2.000],  loss: 2085313.000000, mae: 4192.498047, mean_q: -2598.639893
 2211/5000: episode: 2211, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -700.814, mean reward: -700.814 [-700.814, -700.814], mean action: 2.000 [2.000, 2.000],  loss: 2575862.500000, mae: 4280.751953, mean_q: -2590.633301
 2212/5000: episode: 2212, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7846.013, mean reward: -7846.013 [-7846.013, -7846.013], mean action: 1.000 [1.000, 1.000],  loss: 4566852.000000, mae: 4316.543945, mean_q: -2584.206543
 2213/5000: episode: 2213, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -4037.518, mean reward: -4037.518 [-4037.518, -4037.518], mean action: 2.000 [2.000, 2.000],  loss: 3111349.750000, mae: 4281.460449, mean_q: -2582.922363
 2214/5000: episode: 2214, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3587.281, mean reward: -3587.281 [-3587.281, -3587.281], mean action: 2.000 [2.000, 2.000],  loss: 3092490.750000, mae: 4239.306641, mean_q: -2591.591797
 2215/5000: episode: 2215, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1020.756, mean reward: -1020.756 [-1020.756, -1020.756], mean action: 2.000 [2.000, 2.000],  loss: 6803737.000000, mae: 4390.354492, mean_q: -2598.826172
 2216/5000: episode: 2216, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3221.329, mean reward: -3221.329 [-3221.329, -3221.329], mean action: 2.000 [2.000, 2.000],  loss: 3494787.500000, mae: 4295.673340, mean_q: -2609.860107
 2217/5000: episode: 2217, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -12360.549, mean reward: -12360.549 [-12360.549, -12360.549], mean action: 0.000 [0.000, 0.000],  loss: 1916531.750000, mae: 4294.683105, mean_q: -2614.947266
 2218/5000: episode: 2218, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -9854.726, mean reward: -9854.726 [-9854.726, -9854.726], mean action: 2.000 [2.000, 2.000],  loss: 4487370.500000, mae: 4437.390137, mean_q: -2616.988770
 2219/5000: episode: 2219, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4819.416, mean reward: -4819.416 [-4819.416, -4819.416], mean action: 2.000 [2.000, 2.000],  loss: 4857770.000000, mae: 4456.490234, mean_q: -2631.200195
 2220/5000: episode: 2220, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -695.815, mean reward: -695.815 [-695.815, -695.815], mean action: 2.000 [2.000, 2.000],  loss: 3452746.000000, mae: 4295.226562, mean_q: -2640.554932
 2221/5000: episode: 2221, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3212.357, mean reward: -3212.357 [-3212.357, -3212.357], mean action: 2.000 [2.000, 2.000],  loss: 5299706.000000, mae: 4480.027832, mean_q: -2665.622070
 2222/5000: episode: 2222, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1038.536, mean reward: -1038.536 [-1038.536, -1038.536], mean action: 2.000 [2.000, 2.000],  loss: 2373678.500000, mae: 4241.130371, mean_q: -2678.536621
 2223/5000: episode: 2223, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4127.592, mean reward: -4127.592 [-4127.592, -4127.592], mean action: 2.000 [2.000, 2.000],  loss: 4304887.000000, mae: 4309.954102, mean_q: -2684.853027
 2224/5000: episode: 2224, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3387.866, mean reward: -3387.866 [-3387.866, -3387.866], mean action: 2.000 [2.000, 2.000],  loss: 2196816.000000, mae: 4237.570312, mean_q: -2696.379395
 2225/5000: episode: 2225, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2327.744, mean reward: -2327.744 [-2327.744, -2327.744], mean action: 2.000 [2.000, 2.000],  loss: 4409856.500000, mae: 4387.291992, mean_q: -2716.356934
 2226/5000: episode: 2226, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1997.418, mean reward: -1997.418 [-1997.418, -1997.418], mean action: 2.000 [2.000, 2.000],  loss: 2071000.750000, mae: 4278.647949, mean_q: -2713.681641
 2227/5000: episode: 2227, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2766.129, mean reward: -2766.129 [-2766.129, -2766.129], mean action: 2.000 [2.000, 2.000],  loss: 3589206.000000, mae: 4264.340820, mean_q: -2721.895020
 2228/5000: episode: 2228, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -675.919, mean reward: -675.919 [-675.919, -675.919], mean action: 2.000 [2.000, 2.000],  loss: 4382761.000000, mae: 4408.038086, mean_q: -2732.691406
 2229/5000: episode: 2229, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1668.178, mean reward: -1668.178 [-1668.178, -1668.178], mean action: 2.000 [2.000, 2.000],  loss: 2038435.250000, mae: 4288.087891, mean_q: -2721.746094
 2230/5000: episode: 2230, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2573.263, mean reward: -2573.263 [-2573.263, -2573.263], mean action: 2.000 [2.000, 2.000],  loss: 2270812.250000, mae: 4225.387207, mean_q: -2722.369873
 2231/5000: episode: 2231, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -399.357, mean reward: -399.357 [-399.357, -399.357], mean action: 2.000 [2.000, 2.000],  loss: 3797655.250000, mae: 4406.402344, mean_q: -2727.831787
 2232/5000: episode: 2232, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1236.288, mean reward: -1236.288 [-1236.288, -1236.288], mean action: 2.000 [2.000, 2.000],  loss: 2739312.500000, mae: 4335.135254, mean_q: -2713.787842
 2233/5000: episode: 2233, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1125.216, mean reward: -1125.216 [-1125.216, -1125.216], mean action: 2.000 [2.000, 2.000],  loss: 3207036.000000, mae: 4329.980957, mean_q: -2706.817627
 2234/5000: episode: 2234, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4076.935, mean reward: -4076.935 [-4076.935, -4076.935], mean action: 2.000 [2.000, 2.000],  loss: 4934487.500000, mae: 4473.352539, mean_q: -2714.647461
 2235/5000: episode: 2235, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3015.179, mean reward: -3015.179 [-3015.179, -3015.179], mean action: 2.000 [2.000, 2.000],  loss: 3695294.000000, mae: 4363.264160, mean_q: -2709.018555
 2236/5000: episode: 2236, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -843.801, mean reward: -843.801 [-843.801, -843.801], mean action: 2.000 [2.000, 2.000],  loss: 2620913.000000, mae: 4352.213379, mean_q: -2722.395508
 2237/5000: episode: 2237, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1294.828, mean reward: -1294.828 [-1294.828, -1294.828], mean action: 2.000 [2.000, 2.000],  loss: 2584731.000000, mae: 4325.146484, mean_q: -2718.600098
 2238/5000: episode: 2238, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -2694.375, mean reward: -2694.375 [-2694.375, -2694.375], mean action: 2.000 [2.000, 2.000],  loss: 4409136.500000, mae: 4408.925293, mean_q: -2712.236816
 2239/5000: episode: 2239, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2763.058, mean reward: -2763.058 [-2763.058, -2763.058], mean action: 2.000 [2.000, 2.000],  loss: 2022331.625000, mae: 4202.251953, mean_q: -2697.209473
 2240/5000: episode: 2240, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1820.219, mean reward: -1820.219 [-1820.219, -1820.219], mean action: 2.000 [2.000, 2.000],  loss: 2480250.500000, mae: 4269.305176, mean_q: -2687.449707
 2241/5000: episode: 2241, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -717.941, mean reward: -717.941 [-717.941, -717.941], mean action: 2.000 [2.000, 2.000],  loss: 1704717.875000, mae: 4258.735352, mean_q: -2674.301270
 2242/5000: episode: 2242, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3263.452, mean reward: -3263.452 [-3263.452, -3263.452], mean action: 2.000 [2.000, 2.000],  loss: 4710297.000000, mae: 4418.045898, mean_q: -2661.065186
 2243/5000: episode: 2243, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1883.381, mean reward: -1883.381 [-1883.381, -1883.381], mean action: 2.000 [2.000, 2.000],  loss: 3706726.500000, mae: 4339.536621, mean_q: -2655.437988
 2244/5000: episode: 2244, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2475.326, mean reward: -2475.326 [-2475.326, -2475.326], mean action: 2.000 [2.000, 2.000],  loss: 3409261.750000, mae: 4241.602539, mean_q: -2632.657959
 2245/5000: episode: 2245, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1619.971, mean reward: -1619.971 [-1619.971, -1619.971], mean action: 2.000 [2.000, 2.000],  loss: 2751939.000000, mae: 4252.813477, mean_q: -2622.461914
 2246/5000: episode: 2246, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -7439.680, mean reward: -7439.680 [-7439.680, -7439.680], mean action: 2.000 [2.000, 2.000],  loss: 2175144.500000, mae: 4152.531738, mean_q: -2613.768311
 2247/5000: episode: 2247, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4422.076, mean reward: -4422.076 [-4422.076, -4422.076], mean action: 2.000 [2.000, 2.000],  loss: 2102727.750000, mae: 4204.285645, mean_q: -2598.183594
 2248/5000: episode: 2248, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3496.440, mean reward: -3496.440 [-3496.440, -3496.440], mean action: 2.000 [2.000, 2.000],  loss: 4655564.000000, mae: 4357.404297, mean_q: -2584.410156
 2249/5000: episode: 2249, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3555.427, mean reward: -3555.427 [-3555.427, -3555.427], mean action: 2.000 [2.000, 2.000],  loss: 2009177.500000, mae: 4205.654297, mean_q: -2593.626709
 2250/5000: episode: 2250, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -283.296, mean reward: -283.296 [-283.296, -283.296], mean action: 2.000 [2.000, 2.000],  loss: 2459243.500000, mae: 4259.578125, mean_q: -2580.652588
 2251/5000: episode: 2251, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5518.191, mean reward: -5518.191 [-5518.191, -5518.191], mean action: 2.000 [2.000, 2.000],  loss: 3390626.250000, mae: 4319.301758, mean_q: -2588.240723
 2252/5000: episode: 2252, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5059.924, mean reward: -5059.924 [-5059.924, -5059.924], mean action: 2.000 [2.000, 2.000],  loss: 3980283.000000, mae: 4228.152832, mean_q: -2577.915771
 2253/5000: episode: 2253, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6038.937, mean reward: -6038.937 [-6038.937, -6038.937], mean action: 2.000 [2.000, 2.000],  loss: 2902958.000000, mae: 4138.860352, mean_q: -2572.985352
 2254/5000: episode: 2254, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4364.640, mean reward: -4364.640 [-4364.640, -4364.640], mean action: 2.000 [2.000, 2.000],  loss: 2110354.500000, mae: 4129.731445, mean_q: -2587.117920
 2255/5000: episode: 2255, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1122.610, mean reward: -1122.610 [-1122.610, -1122.610], mean action: 2.000 [2.000, 2.000],  loss: 4727632.000000, mae: 4390.033203, mean_q: -2582.375488
 2256/5000: episode: 2256, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5368.507, mean reward: -5368.507 [-5368.507, -5368.507], mean action: 2.000 [2.000, 2.000],  loss: 1992161.750000, mae: 4195.190430, mean_q: -2588.395996
 2257/5000: episode: 2257, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -360.464, mean reward: -360.464 [-360.464, -360.464], mean action: 2.000 [2.000, 2.000],  loss: 3967986.000000, mae: 4360.726562, mean_q: -2603.255859
 2258/5000: episode: 2258, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1716.562, mean reward: -1716.562 [-1716.562, -1716.562], mean action: 1.000 [1.000, 1.000],  loss: 2695140.000000, mae: 4312.360352, mean_q: -2623.935059
 2259/5000: episode: 2259, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -9034.875, mean reward: -9034.875 [-9034.875, -9034.875], mean action: 2.000 [2.000, 2.000],  loss: 1648431.750000, mae: 4218.712891, mean_q: -2628.190674
 2260/5000: episode: 2260, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -812.650, mean reward: -812.650 [-812.650, -812.650], mean action: 3.000 [3.000, 3.000],  loss: 3249330.750000, mae: 4352.179688, mean_q: -2628.777832
 2261/5000: episode: 2261, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -198.311, mean reward: -198.311 [-198.311, -198.311], mean action: 2.000 [2.000, 2.000],  loss: 2270473.250000, mae: 4272.925293, mean_q: -2629.182129
 2262/5000: episode: 2262, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5722.525, mean reward: -5722.525 [-5722.525, -5722.525], mean action: 2.000 [2.000, 2.000],  loss: 2360449.250000, mae: 4263.098633, mean_q: -2628.711182
 2263/5000: episode: 2263, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -358.678, mean reward: -358.678 [-358.678, -358.678], mean action: 2.000 [2.000, 2.000],  loss: 3519761.500000, mae: 4243.556152, mean_q: -2635.328613
 2264/5000: episode: 2264, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2162.165, mean reward: -2162.165 [-2162.165, -2162.165], mean action: 2.000 [2.000, 2.000],  loss: 2410812.500000, mae: 4276.450195, mean_q: -2627.427246
 2265/5000: episode: 2265, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -448.312, mean reward: -448.312 [-448.312, -448.312], mean action: 2.000 [2.000, 2.000],  loss: 3052550.500000, mae: 4275.810059, mean_q: -2618.829102
 2266/5000: episode: 2266, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2621.464, mean reward: -2621.464 [-2621.464, -2621.464], mean action: 2.000 [2.000, 2.000],  loss: 3106689.000000, mae: 4178.053223, mean_q: -2621.110352
 2267/5000: episode: 2267, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -7419.132, mean reward: -7419.132 [-7419.132, -7419.132], mean action: 2.000 [2.000, 2.000],  loss: 4357741.500000, mae: 4305.205078, mean_q: -2616.341064
 2268/5000: episode: 2268, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1261.895, mean reward: -1261.895 [-1261.895, -1261.895], mean action: 2.000 [2.000, 2.000],  loss: 2800404.250000, mae: 4234.038086, mean_q: -2621.895020
 2269/5000: episode: 2269, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1161.277, mean reward: -1161.277 [-1161.277, -1161.277], mean action: 2.000 [2.000, 2.000],  loss: 3050445.500000, mae: 4176.336914, mean_q: -2614.420410
 2270/5000: episode: 2270, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -936.639, mean reward: -936.639 [-936.639, -936.639], mean action: 2.000 [2.000, 2.000],  loss: 4781670.000000, mae: 4461.036133, mean_q: -2626.255371
 2271/5000: episode: 2271, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3467.127, mean reward: -3467.127 [-3467.127, -3467.127], mean action: 2.000 [2.000, 2.000],  loss: 2937884.000000, mae: 4277.169434, mean_q: -2639.847168
 2272/5000: episode: 2272, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2462.769, mean reward: -2462.769 [-2462.769, -2462.769], mean action: 1.000 [1.000, 1.000],  loss: 1829676.875000, mae: 4253.850586, mean_q: -2652.949219
 2273/5000: episode: 2273, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3566.193, mean reward: -3566.193 [-3566.193, -3566.193], mean action: 2.000 [2.000, 2.000],  loss: 3625912.500000, mae: 4463.140625, mean_q: -2657.672363
 2274/5000: episode: 2274, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3270.025, mean reward: -3270.025 [-3270.025, -3270.025], mean action: 2.000 [2.000, 2.000],  loss: 1431912.500000, mae: 4224.420898, mean_q: -2673.802002
 2275/5000: episode: 2275, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1982.007, mean reward: -1982.007 [-1982.007, -1982.007], mean action: 2.000 [2.000, 2.000],  loss: 2934364.250000, mae: 4401.730469, mean_q: -2688.182129
 2276/5000: episode: 2276, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3268.428, mean reward: -3268.428 [-3268.428, -3268.428], mean action: 2.000 [2.000, 2.000],  loss: 4763658.000000, mae: 4486.334961, mean_q: -2681.655518
 2277/5000: episode: 2277, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -285.314, mean reward: -285.314 [-285.314, -285.314], mean action: 2.000 [2.000, 2.000],  loss: 4838311.500000, mae: 4449.955078, mean_q: -2701.877930
 2278/5000: episode: 2278, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1522.335, mean reward: -1522.335 [-1522.335, -1522.335], mean action: 2.000 [2.000, 2.000],  loss: 3237420.750000, mae: 4463.162109, mean_q: -2733.693115
 2279/5000: episode: 2279, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2230.594, mean reward: -2230.594 [-2230.594, -2230.594], mean action: 2.000 [2.000, 2.000],  loss: 4038096.000000, mae: 4454.559570, mean_q: -2739.141357
 2280/5000: episode: 2280, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -973.710, mean reward: -973.710 [-973.710, -973.710], mean action: 2.000 [2.000, 2.000],  loss: 2990985.750000, mae: 4385.470215, mean_q: -2758.033691
 2281/5000: episode: 2281, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4008.148, mean reward: -4008.148 [-4008.148, -4008.148], mean action: 2.000 [2.000, 2.000],  loss: 2209853.750000, mae: 4295.621582, mean_q: -2763.058105
 2282/5000: episode: 2282, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1178.212, mean reward: -1178.212 [-1178.212, -1178.212], mean action: 2.000 [2.000, 2.000],  loss: 3110646.750000, mae: 4473.049805, mean_q: -2768.892822
 2283/5000: episode: 2283, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -970.047, mean reward: -970.047 [-970.047, -970.047], mean action: 2.000 [2.000, 2.000],  loss: 2772689.500000, mae: 4375.131836, mean_q: -2778.516602
 2284/5000: episode: 2284, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2547.127, mean reward: -2547.127 [-2547.127, -2547.127], mean action: 2.000 [2.000, 2.000],  loss: 3233305.000000, mae: 4396.208008, mean_q: -2776.702393
 2285/5000: episode: 2285, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7366.554, mean reward: -7366.554 [-7366.554, -7366.554], mean action: 2.000 [2.000, 2.000],  loss: 3007273.250000, mae: 4391.376953, mean_q: -2790.643555
 2286/5000: episode: 2286, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -56.444, mean reward: -56.444 [-56.444, -56.444], mean action: 2.000 [2.000, 2.000],  loss: 3018103.250000, mae: 4416.928711, mean_q: -2812.604980
 2287/5000: episode: 2287, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2229.493, mean reward: -2229.493 [-2229.493, -2229.493], mean action: 1.000 [1.000, 1.000],  loss: 2878104.000000, mae: 4513.208984, mean_q: -2828.768799
 2288/5000: episode: 2288, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2216.631, mean reward: -2216.631 [-2216.631, -2216.631], mean action: 2.000 [2.000, 2.000],  loss: 2874508.500000, mae: 4399.743164, mean_q: -2825.655762
 2289/5000: episode: 2289, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -868.332, mean reward: -868.332 [-868.332, -868.332], mean action: 2.000 [2.000, 2.000],  loss: 4674011.500000, mae: 4545.087891, mean_q: -2827.172363
 2290/5000: episode: 2290, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1014.595, mean reward: -1014.595 [-1014.595, -1014.595], mean action: 2.000 [2.000, 2.000],  loss: 2405065.500000, mae: 4377.568359, mean_q: -2807.141846
 2291/5000: episode: 2291, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -414.939, mean reward: -414.939 [-414.939, -414.939], mean action: 2.000 [2.000, 2.000],  loss: 3008509.750000, mae: 4460.065430, mean_q: -2830.242676
 2292/5000: episode: 2292, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -609.445, mean reward: -609.445 [-609.445, -609.445], mean action: 2.000 [2.000, 2.000],  loss: 2879065.750000, mae: 4425.449219, mean_q: -2811.670898
 2293/5000: episode: 2293, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3068.143, mean reward: -3068.143 [-3068.143, -3068.143], mean action: 2.000 [2.000, 2.000],  loss: 2525180.250000, mae: 4324.986328, mean_q: -2805.786133
 2294/5000: episode: 2294, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5888.442, mean reward: -5888.442 [-5888.442, -5888.442], mean action: 2.000 [2.000, 2.000],  loss: 3037314.500000, mae: 4351.336914, mean_q: -2782.961426
 2295/5000: episode: 2295, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1834.487, mean reward: -1834.487 [-1834.487, -1834.487], mean action: 2.000 [2.000, 2.000],  loss: 5285152.500000, mae: 4365.215820, mean_q: -2778.144287
 2296/5000: episode: 2296, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -955.542, mean reward: -955.542 [-955.542, -955.542], mean action: 2.000 [2.000, 2.000],  loss: 2934887.500000, mae: 4325.564453, mean_q: -2750.359375
 2297/5000: episode: 2297, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7512.901, mean reward: -7512.901 [-7512.901, -7512.901], mean action: 2.000 [2.000, 2.000],  loss: 3997408.500000, mae: 4412.439941, mean_q: -2755.594482
 2298/5000: episode: 2298, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1863.577, mean reward: -1863.577 [-1863.577, -1863.577], mean action: 2.000 [2.000, 2.000],  loss: 3307407.750000, mae: 4332.789062, mean_q: -2734.178711
 2299/5000: episode: 2299, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7695.897, mean reward: -7695.897 [-7695.897, -7695.897], mean action: 2.000 [2.000, 2.000],  loss: 3238273.000000, mae: 4409.282227, mean_q: -2732.123535
 2300/5000: episode: 2300, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5383.222, mean reward: -5383.222 [-5383.222, -5383.222], mean action: 2.000 [2.000, 2.000],  loss: 2865484.500000, mae: 4367.494629, mean_q: -2723.139648
 2301/5000: episode: 2301, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7273.882, mean reward: -7273.882 [-7273.882, -7273.882], mean action: 2.000 [2.000, 2.000],  loss: 4091330.000000, mae: 4421.622559, mean_q: -2697.613281
 2302/5000: episode: 2302, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4144.120, mean reward: -4144.120 [-4144.120, -4144.120], mean action: 2.000 [2.000, 2.000],  loss: 2070997.750000, mae: 4250.251465, mean_q: -2698.364258
 2303/5000: episode: 2303, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1429.178, mean reward: -1429.178 [-1429.178, -1429.178], mean action: 2.000 [2.000, 2.000],  loss: 2131943.500000, mae: 4235.434570, mean_q: -2694.620850
 2304/5000: episode: 2304, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -432.980, mean reward: -432.980 [-432.980, -432.980], mean action: 2.000 [2.000, 2.000],  loss: 2267758.500000, mae: 4352.044922, mean_q: -2685.825684
 2305/5000: episode: 2305, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3663.135, mean reward: -3663.135 [-3663.135, -3663.135], mean action: 2.000 [2.000, 2.000],  loss: 2980346.000000, mae: 4332.981445, mean_q: -2670.697754
 2306/5000: episode: 2306, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1004.588, mean reward: -1004.588 [-1004.588, -1004.588], mean action: 2.000 [2.000, 2.000],  loss: 2636697.250000, mae: 4267.080566, mean_q: -2652.979248
 2307/5000: episode: 2307, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2121.368, mean reward: -2121.368 [-2121.368, -2121.368], mean action: 1.000 [1.000, 1.000],  loss: 2318993.000000, mae: 4262.417969, mean_q: -2632.431641
 2308/5000: episode: 2308, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1147.409, mean reward: -1147.409 [-1147.409, -1147.409], mean action: 2.000 [2.000, 2.000],  loss: 3308660.000000, mae: 4281.058105, mean_q: -2641.446289
 2309/5000: episode: 2309, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4364.491, mean reward: -4364.491 [-4364.491, -4364.491], mean action: 0.000 [0.000, 0.000],  loss: 2962732.250000, mae: 4301.909668, mean_q: -2612.319824
 2310/5000: episode: 2310, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -256.052, mean reward: -256.052 [-256.052, -256.052], mean action: 2.000 [2.000, 2.000],  loss: 4036993.500000, mae: 4458.232910, mean_q: -2608.115723
 2311/5000: episode: 2311, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1324.681, mean reward: -1324.681 [-1324.681, -1324.681], mean action: 2.000 [2.000, 2.000],  loss: 3348482.250000, mae: 4196.204102, mean_q: -2589.020264
 2312/5000: episode: 2312, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -72.110, mean reward: -72.110 [-72.110, -72.110], mean action: 2.000 [2.000, 2.000],  loss: 3204707.250000, mae: 4300.865723, mean_q: -2576.088135
 2313/5000: episode: 2313, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2823.502, mean reward: -2823.502 [-2823.502, -2823.502], mean action: 2.000 [2.000, 2.000],  loss: 3470133.000000, mae: 4362.240234, mean_q: -2548.017578
 2314/5000: episode: 2314, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -769.956, mean reward: -769.956 [-769.956, -769.956], mean action: 2.000 [2.000, 2.000],  loss: 2087977.250000, mae: 4178.729004, mean_q: -2539.794678
 2315/5000: episode: 2315, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4170.297, mean reward: -4170.297 [-4170.297, -4170.297], mean action: 2.000 [2.000, 2.000],  loss: 2026466.125000, mae: 3980.620117, mean_q: -2546.883789
 2316/5000: episode: 2316, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -3120.500, mean reward: -3120.500 [-3120.500, -3120.500], mean action: 2.000 [2.000, 2.000],  loss: 2157739.000000, mae: 4130.542969, mean_q: -2537.050293
 2317/5000: episode: 2317, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1119.779, mean reward: -1119.779 [-1119.779, -1119.779], mean action: 2.000 [2.000, 2.000],  loss: 3265744.500000, mae: 4169.656738, mean_q: -2513.473389
 2318/5000: episode: 2318, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4250.069, mean reward: -4250.069 [-4250.069, -4250.069], mean action: 1.000 [1.000, 1.000],  loss: 4077641.750000, mae: 4286.271484, mean_q: -2532.569336
 2319/5000: episode: 2319, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7316.359, mean reward: -7316.359 [-7316.359, -7316.359], mean action: 1.000 [1.000, 1.000],  loss: 2400290.500000, mae: 4252.859863, mean_q: -2534.150879
 2320/5000: episode: 2320, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -157.584, mean reward: -157.584 [-157.584, -157.584], mean action: 2.000 [2.000, 2.000],  loss: 2586801.500000, mae: 4188.117188, mean_q: -2551.258301
 2321/5000: episode: 2321, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8074.767, mean reward: -8074.767 [-8074.767, -8074.767], mean action: 2.000 [2.000, 2.000],  loss: 2526084.250000, mae: 4196.280762, mean_q: -2565.268555
 2322/5000: episode: 2322, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6444.124, mean reward: -6444.124 [-6444.124, -6444.124], mean action: 2.000 [2.000, 2.000],  loss: 3157991.500000, mae: 4272.295898, mean_q: -2561.149414
 2323/5000: episode: 2323, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5418.701, mean reward: -5418.701 [-5418.701, -5418.701], mean action: 2.000 [2.000, 2.000],  loss: 2749616.750000, mae: 4231.470703, mean_q: -2581.293945
 2324/5000: episode: 2324, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1685.359, mean reward: -1685.359 [-1685.359, -1685.359], mean action: 3.000 [3.000, 3.000],  loss: 4810211.000000, mae: 4366.760742, mean_q: -2587.124023
 2325/5000: episode: 2325, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -979.572, mean reward: -979.572 [-979.572, -979.572], mean action: 2.000 [2.000, 2.000],  loss: 2912684.000000, mae: 4292.566406, mean_q: -2597.298340
 2326/5000: episode: 2326, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6197.366, mean reward: -6197.366 [-6197.366, -6197.366], mean action: 2.000 [2.000, 2.000],  loss: 2921774.000000, mae: 4270.277344, mean_q: -2616.541504
 2327/5000: episode: 2327, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -1347.575, mean reward: -1347.575 [-1347.575, -1347.575], mean action: 2.000 [2.000, 2.000],  loss: 2929710.500000, mae: 4288.706543, mean_q: -2618.864014
 2328/5000: episode: 2328, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -798.528, mean reward: -798.528 [-798.528, -798.528], mean action: 2.000 [2.000, 2.000],  loss: 3083103.750000, mae: 4274.259766, mean_q: -2616.238770
 2329/5000: episode: 2329, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5039.035, mean reward: -5039.035 [-5039.035, -5039.035], mean action: 2.000 [2.000, 2.000],  loss: 1964565.500000, mae: 4173.706543, mean_q: -2621.249268
 2330/5000: episode: 2330, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2612.489, mean reward: -2612.489 [-2612.489, -2612.489], mean action: 2.000 [2.000, 2.000],  loss: 3134468.000000, mae: 4288.441406, mean_q: -2618.684326
 2331/5000: episode: 2331, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -90.997, mean reward: -90.997 [-90.997, -90.997], mean action: 2.000 [2.000, 2.000],  loss: 1643051.500000, mae: 4202.095703, mean_q: -2639.502441
 2332/5000: episode: 2332, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1212.628, mean reward: -1212.628 [-1212.628, -1212.628], mean action: 2.000 [2.000, 2.000],  loss: 2718865.500000, mae: 4211.684570, mean_q: -2634.383789
 2333/5000: episode: 2333, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -331.960, mean reward: -331.960 [-331.960, -331.960], mean action: 2.000 [2.000, 2.000],  loss: 3275905.000000, mae: 4364.312500, mean_q: -2631.913086
 2334/5000: episode: 2334, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -387.038, mean reward: -387.038 [-387.038, -387.038], mean action: 2.000 [2.000, 2.000],  loss: 4001668.000000, mae: 4272.541016, mean_q: -2614.010254
 2335/5000: episode: 2335, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1118.877, mean reward: -1118.877 [-1118.877, -1118.877], mean action: 2.000 [2.000, 2.000],  loss: 3127350.750000, mae: 4298.143555, mean_q: -2607.905273
 2336/5000: episode: 2336, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1137.975, mean reward: -1137.975 [-1137.975, -1137.975], mean action: 2.000 [2.000, 2.000],  loss: 3377646.000000, mae: 4252.194336, mean_q: -2615.260254
 2337/5000: episode: 2337, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1788.004, mean reward: -1788.004 [-1788.004, -1788.004], mean action: 2.000 [2.000, 2.000],  loss: 3939473.750000, mae: 4232.109375, mean_q: -2627.098633
 2338/5000: episode: 2338, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -857.860, mean reward: -857.860 [-857.860, -857.860], mean action: 2.000 [2.000, 2.000],  loss: 3642248.750000, mae: 4323.422852, mean_q: -2632.794922
 2339/5000: episode: 2339, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1434.561, mean reward: -1434.561 [-1434.561, -1434.561], mean action: 2.000 [2.000, 2.000],  loss: 2939277.500000, mae: 4295.938477, mean_q: -2641.378906
 2340/5000: episode: 2340, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4500.506, mean reward: -4500.506 [-4500.506, -4500.506], mean action: 2.000 [2.000, 2.000],  loss: 1898126.125000, mae: 4140.426270, mean_q: -2643.729736
 2341/5000: episode: 2341, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -13618.923, mean reward: -13618.923 [-13618.923, -13618.923], mean action: 0.000 [0.000, 0.000],  loss: 4779081.000000, mae: 4330.411133, mean_q: -2651.140137
 2342/5000: episode: 2342, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -669.106, mean reward: -669.106 [-669.106, -669.106], mean action: 2.000 [2.000, 2.000],  loss: 3461516.000000, mae: 4229.506836, mean_q: -2646.862793
 2343/5000: episode: 2343, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3393.237, mean reward: -3393.237 [-3393.237, -3393.237], mean action: 2.000 [2.000, 2.000],  loss: 4595331.000000, mae: 4277.559570, mean_q: -2653.157715
 2344/5000: episode: 2344, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -34.041, mean reward: -34.041 [-34.041, -34.041], mean action: 2.000 [2.000, 2.000],  loss: 2903271.500000, mae: 4312.931152, mean_q: -2668.225586
 2345/5000: episode: 2345, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -990.938, mean reward: -990.938 [-990.938, -990.938], mean action: 2.000 [2.000, 2.000],  loss: 1362093.250000, mae: 4114.450684, mean_q: -2661.773682
 2346/5000: episode: 2346, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -83.710, mean reward: -83.710 [-83.710, -83.710], mean action: 2.000 [2.000, 2.000],  loss: 2252852.000000, mae: 4134.336914, mean_q: -2643.573730
 2347/5000: episode: 2347, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -435.777, mean reward: -435.777 [-435.777, -435.777], mean action: 2.000 [2.000, 2.000],  loss: 2533728.000000, mae: 4271.541504, mean_q: -2652.923340
 2348/5000: episode: 2348, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4423.021, mean reward: -4423.021 [-4423.021, -4423.021], mean action: 2.000 [2.000, 2.000],  loss: 3463068.000000, mae: 4401.053223, mean_q: -2644.985840
 2349/5000: episode: 2349, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8631.593, mean reward: -8631.593 [-8631.593, -8631.593], mean action: 0.000 [0.000, 0.000],  loss: 3643393.000000, mae: 4199.265137, mean_q: -2622.247314
 2350/5000: episode: 2350, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2816.036, mean reward: -2816.036 [-2816.036, -2816.036], mean action: 2.000 [2.000, 2.000],  loss: 4347796.000000, mae: 4376.181641, mean_q: -2625.452881
 2351/5000: episode: 2351, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2446.429, mean reward: -2446.429 [-2446.429, -2446.429], mean action: 2.000 [2.000, 2.000],  loss: 3520943.000000, mae: 4347.634766, mean_q: -2615.223877
 2352/5000: episode: 2352, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9717.346, mean reward: -9717.346 [-9717.346, -9717.346], mean action: 0.000 [0.000, 0.000],  loss: 2944044.000000, mae: 4291.627930, mean_q: -2608.335938
 2353/5000: episode: 2353, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2155.796, mean reward: -2155.796 [-2155.796, -2155.796], mean action: 2.000 [2.000, 2.000],  loss: 2251414.250000, mae: 4282.987305, mean_q: -2610.988281
 2354/5000: episode: 2354, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5259.031, mean reward: -5259.031 [-5259.031, -5259.031], mean action: 2.000 [2.000, 2.000],  loss: 4008379.000000, mae: 4341.711914, mean_q: -2587.410156
 2355/5000: episode: 2355, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -311.796, mean reward: -311.796 [-311.796, -311.796], mean action: 2.000 [2.000, 2.000],  loss: 2811904.750000, mae: 4236.994141, mean_q: -2582.670898
 2356/5000: episode: 2356, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2153.289, mean reward: -2153.289 [-2153.289, -2153.289], mean action: 2.000 [2.000, 2.000],  loss: 3990745.500000, mae: 4320.391113, mean_q: -2589.492188
 2357/5000: episode: 2357, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5951.553, mean reward: -5951.553 [-5951.553, -5951.553], mean action: 2.000 [2.000, 2.000],  loss: 2099502.000000, mae: 4188.381836, mean_q: -2568.996338
 2358/5000: episode: 2358, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -2394.592, mean reward: -2394.592 [-2394.592, -2394.592], mean action: 2.000 [2.000, 2.000],  loss: 1744198.500000, mae: 4201.887695, mean_q: -2566.661621
 2359/5000: episode: 2359, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1382.799, mean reward: -1382.799 [-1382.799, -1382.799], mean action: 2.000 [2.000, 2.000],  loss: 3315521.000000, mae: 4336.286621, mean_q: -2557.222656
 2360/5000: episode: 2360, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1465.774, mean reward: -1465.774 [-1465.774, -1465.774], mean action: 2.000 [2.000, 2.000],  loss: 2618888.000000, mae: 4193.918945, mean_q: -2547.513672
 2361/5000: episode: 2361, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -20.324, mean reward: -20.324 [-20.324, -20.324], mean action: 2.000 [2.000, 2.000],  loss: 1575154.000000, mae: 4175.314453, mean_q: -2542.849854
 2362/5000: episode: 2362, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2650.478, mean reward: -2650.478 [-2650.478, -2650.478], mean action: 2.000 [2.000, 2.000],  loss: 4053610.000000, mae: 4243.138672, mean_q: -2529.099609
 2363/5000: episode: 2363, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3516.642, mean reward: -3516.642 [-3516.642, -3516.642], mean action: 2.000 [2.000, 2.000],  loss: 3388232.250000, mae: 4202.667969, mean_q: -2532.249512
 2364/5000: episode: 2364, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1597.810, mean reward: -1597.810 [-1597.810, -1597.810], mean action: 2.000 [2.000, 2.000],  loss: 3207419.500000, mae: 4247.930664, mean_q: -2536.616699
 2365/5000: episode: 2365, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12.027, mean reward: -12.027 [-12.027, -12.027], mean action: 2.000 [2.000, 2.000],  loss: 2466567.500000, mae: 4116.717773, mean_q: -2524.770264
 2366/5000: episode: 2366, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1072.885, mean reward: -1072.885 [-1072.885, -1072.885], mean action: 2.000 [2.000, 2.000],  loss: 3054338.000000, mae: 4194.993164, mean_q: -2525.099365
 2367/5000: episode: 2367, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5048.392, mean reward: -5048.392 [-5048.392, -5048.392], mean action: 2.000 [2.000, 2.000],  loss: 2811071.000000, mae: 4243.605469, mean_q: -2514.788330
 2368/5000: episode: 2368, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -791.218, mean reward: -791.218 [-791.218, -791.218], mean action: 2.000 [2.000, 2.000],  loss: 3029463.000000, mae: 4170.094727, mean_q: -2514.058105
 2369/5000: episode: 2369, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5060.991, mean reward: -5060.991 [-5060.991, -5060.991], mean action: 2.000 [2.000, 2.000],  loss: 4270335.000000, mae: 4321.796875, mean_q: -2516.697754
 2370/5000: episode: 2370, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1100.903, mean reward: -1100.903 [-1100.903, -1100.903], mean action: 2.000 [2.000, 2.000],  loss: 3120685.750000, mae: 4245.898438, mean_q: -2523.521240
 2371/5000: episode: 2371, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -68.499, mean reward: -68.499 [-68.499, -68.499], mean action: 2.000 [2.000, 2.000],  loss: 5589485.000000, mae: 4404.247070, mean_q: -2527.517578
 2372/5000: episode: 2372, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -764.820, mean reward: -764.820 [-764.820, -764.820], mean action: 2.000 [2.000, 2.000],  loss: 2458860.000000, mae: 4216.902344, mean_q: -2529.161133
 2373/5000: episode: 2373, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -438.912, mean reward: -438.912 [-438.912, -438.912], mean action: 2.000 [2.000, 2.000],  loss: 1605814.750000, mae: 4072.904297, mean_q: -2525.953125
 2374/5000: episode: 2374, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -977.466, mean reward: -977.466 [-977.466, -977.466], mean action: 2.000 [2.000, 2.000],  loss: 3661380.250000, mae: 4250.999023, mean_q: -2521.328369
 2375/5000: episode: 2375, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4747.625, mean reward: -4747.625 [-4747.625, -4747.625], mean action: 2.000 [2.000, 2.000],  loss: 3880381.000000, mae: 4240.402344, mean_q: -2516.513672
 2376/5000: episode: 2376, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -750.731, mean reward: -750.731 [-750.731, -750.731], mean action: 2.000 [2.000, 2.000],  loss: 3041510.500000, mae: 4160.238770, mean_q: -2513.173340
 2377/5000: episode: 2377, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -1428.639, mean reward: -1428.639 [-1428.639, -1428.639], mean action: 2.000 [2.000, 2.000],  loss: 3076236.000000, mae: 4085.102783, mean_q: -2510.310059
 2378/5000: episode: 2378, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1320.214, mean reward: -1320.214 [-1320.214, -1320.214], mean action: 2.000 [2.000, 2.000],  loss: 3575009.000000, mae: 4205.662109, mean_q: -2523.362793
 2379/5000: episode: 2379, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -8218.833, mean reward: -8218.833 [-8218.833, -8218.833], mean action: 2.000 [2.000, 2.000],  loss: 2954141.250000, mae: 4151.831055, mean_q: -2537.504639
 2380/5000: episode: 2380, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8333.700, mean reward: -8333.700 [-8333.700, -8333.700], mean action: 2.000 [2.000, 2.000],  loss: 1661750.875000, mae: 4089.521973, mean_q: -2541.652832
 2381/5000: episode: 2381, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1578.112, mean reward: -1578.112 [-1578.112, -1578.112], mean action: 2.000 [2.000, 2.000],  loss: 3452242.000000, mae: 4145.950684, mean_q: -2549.626465
 2382/5000: episode: 2382, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3818.760, mean reward: -3818.760 [-3818.760, -3818.760], mean action: 2.000 [2.000, 2.000],  loss: 4008724.750000, mae: 4191.641113, mean_q: -2567.477295
 2383/5000: episode: 2383, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5507.026, mean reward: -5507.026 [-5507.026, -5507.026], mean action: 2.000 [2.000, 2.000],  loss: 3869702.500000, mae: 4128.762207, mean_q: -2572.997559
 2384/5000: episode: 2384, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4223.161, mean reward: -4223.161 [-4223.161, -4223.161], mean action: 2.000 [2.000, 2.000],  loss: 2747578.000000, mae: 4193.252930, mean_q: -2578.827881
 2385/5000: episode: 2385, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6275.675, mean reward: -6275.675 [-6275.675, -6275.675], mean action: 2.000 [2.000, 2.000],  loss: 2895345.500000, mae: 4239.758789, mean_q: -2582.400879
 2386/5000: episode: 2386, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1832.754, mean reward: -1832.754 [-1832.754, -1832.754], mean action: 2.000 [2.000, 2.000],  loss: 2168461.000000, mae: 4039.407471, mean_q: -2581.548828
 2387/5000: episode: 2387, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -932.218, mean reward: -932.218 [-932.218, -932.218], mean action: 3.000 [3.000, 3.000],  loss: 2274091.500000, mae: 4091.360107, mean_q: -2586.637695
 2388/5000: episode: 2388, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6345.672, mean reward: -6345.672 [-6345.672, -6345.672], mean action: 3.000 [3.000, 3.000],  loss: 2527867.000000, mae: 4113.481445, mean_q: -2583.871582
 2389/5000: episode: 2389, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -84.327, mean reward: -84.327 [-84.327, -84.327], mean action: 2.000 [2.000, 2.000],  loss: 2419544.500000, mae: 4201.890625, mean_q: -2595.976807
 2390/5000: episode: 2390, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1209.843, mean reward: -1209.843 [-1209.843, -1209.843], mean action: 2.000 [2.000, 2.000],  loss: 3036394.000000, mae: 4126.951172, mean_q: -2574.968506
 2391/5000: episode: 2391, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4768.055, mean reward: -4768.055 [-4768.055, -4768.055], mean action: 2.000 [2.000, 2.000],  loss: 2525761.750000, mae: 4145.743652, mean_q: -2579.452393
 2392/5000: episode: 2392, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2300.673, mean reward: -2300.673 [-2300.673, -2300.673], mean action: 2.000 [2.000, 2.000],  loss: 1970549.750000, mae: 3990.229980, mean_q: -2571.498047
 2393/5000: episode: 2393, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1018.304, mean reward: -1018.304 [-1018.304, -1018.304], mean action: 2.000 [2.000, 2.000],  loss: 2285349.500000, mae: 4091.779541, mean_q: -2573.666504
 2394/5000: episode: 2394, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -930.825, mean reward: -930.825 [-930.825, -930.825], mean action: 2.000 [2.000, 2.000],  loss: 3714076.500000, mae: 4267.986816, mean_q: -2569.850586
 2395/5000: episode: 2395, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1862.564, mean reward: -1862.564 [-1862.564, -1862.564], mean action: 2.000 [2.000, 2.000],  loss: 2932233.500000, mae: 4149.193359, mean_q: -2553.877197
 2396/5000: episode: 2396, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1497.273, mean reward: -1497.273 [-1497.273, -1497.273], mean action: 2.000 [2.000, 2.000],  loss: 1957343.875000, mae: 4067.966309, mean_q: -2552.620117
 2397/5000: episode: 2397, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3743.391, mean reward: -3743.391 [-3743.391, -3743.391], mean action: 2.000 [2.000, 2.000],  loss: 2673467.750000, mae: 4059.547852, mean_q: -2566.346191
 2398/5000: episode: 2398, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -971.400, mean reward: -971.400 [-971.400, -971.400], mean action: 2.000 [2.000, 2.000],  loss: 4045353.000000, mae: 4107.670898, mean_q: -2554.294922
 2399/5000: episode: 2399, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -943.172, mean reward: -943.172 [-943.172, -943.172], mean action: 2.000 [2.000, 2.000],  loss: 3364190.500000, mae: 4226.314453, mean_q: -2561.857178
 2400/5000: episode: 2400, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2872.201, mean reward: -2872.201 [-2872.201, -2872.201], mean action: 2.000 [2.000, 2.000],  loss: 3169407.250000, mae: 4074.781250, mean_q: -2555.623779
 2401/5000: episode: 2401, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1182.942, mean reward: -1182.942 [-1182.942, -1182.942], mean action: 2.000 [2.000, 2.000],  loss: 3802192.750000, mae: 4201.053223, mean_q: -2556.435059
 2402/5000: episode: 2402, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1885.390, mean reward: -1885.390 [-1885.390, -1885.390], mean action: 2.000 [2.000, 2.000],  loss: 2035518.250000, mae: 4117.222656, mean_q: -2553.644043
 2403/5000: episode: 2403, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4587.622, mean reward: -4587.622 [-4587.622, -4587.622], mean action: 2.000 [2.000, 2.000],  loss: 4188006.500000, mae: 4230.055176, mean_q: -2564.622070
 2404/5000: episode: 2404, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4100.765, mean reward: -4100.765 [-4100.765, -4100.765], mean action: 2.000 [2.000, 2.000],  loss: 3187762.500000, mae: 4181.039551, mean_q: -2582.085938
 2405/5000: episode: 2405, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1418.631, mean reward: -1418.631 [-1418.631, -1418.631], mean action: 2.000 [2.000, 2.000],  loss: 3091205.000000, mae: 4196.100098, mean_q: -2570.736084
 2406/5000: episode: 2406, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3985.561, mean reward: -3985.561 [-3985.561, -3985.561], mean action: 2.000 [2.000, 2.000],  loss: 2614301.250000, mae: 4212.866211, mean_q: -2578.185059
 2407/5000: episode: 2407, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2172.965, mean reward: -2172.965 [-2172.965, -2172.965], mean action: 2.000 [2.000, 2.000],  loss: 2886624.500000, mae: 4331.209473, mean_q: -2574.226807
 2408/5000: episode: 2408, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1760.440, mean reward: -1760.440 [-1760.440, -1760.440], mean action: 2.000 [2.000, 2.000],  loss: 3889134.000000, mae: 4340.951660, mean_q: -2581.111816
 2409/5000: episode: 2409, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1679.378, mean reward: -1679.378 [-1679.378, -1679.378], mean action: 2.000 [2.000, 2.000],  loss: 2329433.500000, mae: 4247.507812, mean_q: -2590.197266
 2410/5000: episode: 2410, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6414.048, mean reward: -6414.048 [-6414.048, -6414.048], mean action: 2.000 [2.000, 2.000],  loss: 4032330.500000, mae: 4342.414062, mean_q: -2594.373047
 2411/5000: episode: 2411, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -218.587, mean reward: -218.587 [-218.587, -218.587], mean action: 2.000 [2.000, 2.000],  loss: 4023072.500000, mae: 4347.268555, mean_q: -2596.918945
 2412/5000: episode: 2412, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5140.112, mean reward: -5140.112 [-5140.112, -5140.112], mean action: 2.000 [2.000, 2.000],  loss: 2975074.250000, mae: 4287.395996, mean_q: -2604.375488
 2413/5000: episode: 2413, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3446.822, mean reward: -3446.822 [-3446.822, -3446.822], mean action: 2.000 [2.000, 2.000],  loss: 3010816.000000, mae: 4369.934570, mean_q: -2606.665283
 2414/5000: episode: 2414, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -533.327, mean reward: -533.327 [-533.327, -533.327], mean action: 2.000 [2.000, 2.000],  loss: 5247593.000000, mae: 4391.955566, mean_q: -2620.902588
 2415/5000: episode: 2415, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2112.050, mean reward: -2112.050 [-2112.050, -2112.050], mean action: 2.000 [2.000, 2.000],  loss: 3904750.000000, mae: 4231.486328, mean_q: -2617.995605
 2416/5000: episode: 2416, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6973.031, mean reward: -6973.031 [-6973.031, -6973.031], mean action: 2.000 [2.000, 2.000],  loss: 3745279.750000, mae: 4383.004883, mean_q: -2620.733398
 2417/5000: episode: 2417, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6691.563, mean reward: -6691.563 [-6691.563, -6691.563], mean action: 2.000 [2.000, 2.000],  loss: 3870837.000000, mae: 4359.519531, mean_q: -2637.434326
 2418/5000: episode: 2418, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3418.888, mean reward: -3418.888 [-3418.888, -3418.888], mean action: 0.000 [0.000, 0.000],  loss: 3832725.250000, mae: 4361.902344, mean_q: -2636.571289
 2419/5000: episode: 2419, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5945.197, mean reward: -5945.197 [-5945.197, -5945.197], mean action: 2.000 [2.000, 2.000],  loss: 4165385.500000, mae: 4402.204102, mean_q: -2643.694824
 2420/5000: episode: 2420, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -389.801, mean reward: -389.801 [-389.801, -389.801], mean action: 2.000 [2.000, 2.000],  loss: 2011415.750000, mae: 4273.171875, mean_q: -2658.837891
 2421/5000: episode: 2421, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3283.388, mean reward: -3283.388 [-3283.388, -3283.388], mean action: 2.000 [2.000, 2.000],  loss: 2679595.750000, mae: 4313.180176, mean_q: -2674.170410
 2422/5000: episode: 2422, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -376.679, mean reward: -376.679 [-376.679, -376.679], mean action: 2.000 [2.000, 2.000],  loss: 3695770.000000, mae: 4320.094727, mean_q: -2676.268311
 2423/5000: episode: 2423, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2016.036, mean reward: -2016.036 [-2016.036, -2016.036], mean action: 2.000 [2.000, 2.000],  loss: 3095161.750000, mae: 4309.352539, mean_q: -2676.058350
 2424/5000: episode: 2424, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2756.238, mean reward: -2756.238 [-2756.238, -2756.238], mean action: 2.000 [2.000, 2.000],  loss: 2751693.000000, mae: 4359.884766, mean_q: -2676.051758
 2425/5000: episode: 2425, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -568.418, mean reward: -568.418 [-568.418, -568.418], mean action: 2.000 [2.000, 2.000],  loss: 4076290.000000, mae: 4454.352539, mean_q: -2680.010742
 2426/5000: episode: 2426, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8727.482, mean reward: -8727.482 [-8727.482, -8727.482], mean action: 2.000 [2.000, 2.000],  loss: 2224535.500000, mae: 4274.362305, mean_q: -2662.508057
 2427/5000: episode: 2427, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1676.838, mean reward: -1676.838 [-1676.838, -1676.838], mean action: 2.000 [2.000, 2.000],  loss: 2896921.000000, mae: 4403.658691, mean_q: -2668.404541
 2428/5000: episode: 2428, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -269.240, mean reward: -269.240 [-269.240, -269.240], mean action: 2.000 [2.000, 2.000],  loss: 4250796.000000, mae: 4384.950684, mean_q: -2679.299805
 2429/5000: episode: 2429, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -396.595, mean reward: -396.595 [-396.595, -396.595], mean action: 2.000 [2.000, 2.000],  loss: 3110804.500000, mae: 4390.227539, mean_q: -2674.572998
 2430/5000: episode: 2430, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4833.718, mean reward: -4833.718 [-4833.718, -4833.718], mean action: 2.000 [2.000, 2.000],  loss: 3025547.750000, mae: 4332.711426, mean_q: -2685.093262
 2431/5000: episode: 2431, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3775.132, mean reward: -3775.132 [-3775.132, -3775.132], mean action: 2.000 [2.000, 2.000],  loss: 3495893.750000, mae: 4374.525391, mean_q: -2686.708984
 2432/5000: episode: 2432, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3241.446, mean reward: -3241.446 [-3241.446, -3241.446], mean action: 2.000 [2.000, 2.000],  loss: 4111238.500000, mae: 4501.201660, mean_q: -2697.975586
 2433/5000: episode: 2433, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -2552.474, mean reward: -2552.474 [-2552.474, -2552.474], mean action: 2.000 [2.000, 2.000],  loss: 2457135.000000, mae: 4377.012695, mean_q: -2719.919434
 2434/5000: episode: 2434, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3113.945, mean reward: -3113.945 [-3113.945, -3113.945], mean action: 2.000 [2.000, 2.000],  loss: 3183255.000000, mae: 4302.016113, mean_q: -2708.579346
 2435/5000: episode: 2435, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2237.766, mean reward: -2237.766 [-2237.766, -2237.766], mean action: 2.000 [2.000, 2.000],  loss: 3502702.000000, mae: 4373.213867, mean_q: -2711.506104
 2436/5000: episode: 2436, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4590.919, mean reward: -4590.919 [-4590.919, -4590.919], mean action: 2.000 [2.000, 2.000],  loss: 2883916.000000, mae: 4425.965820, mean_q: -2721.751465
 2437/5000: episode: 2437, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4068.157, mean reward: -4068.157 [-4068.157, -4068.157], mean action: 2.000 [2.000, 2.000],  loss: 2677341.500000, mae: 4406.247070, mean_q: -2718.805664
 2438/5000: episode: 2438, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2105.846, mean reward: -2105.846 [-2105.846, -2105.846], mean action: 2.000 [2.000, 2.000],  loss: 4615319.000000, mae: 4426.179199, mean_q: -2727.073486
 2439/5000: episode: 2439, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3992.891, mean reward: -3992.891 [-3992.891, -3992.891], mean action: 2.000 [2.000, 2.000],  loss: 2220959.000000, mae: 4290.452148, mean_q: -2711.682129
 2440/5000: episode: 2440, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5345.782, mean reward: -5345.782 [-5345.782, -5345.782], mean action: 2.000 [2.000, 2.000],  loss: 2587153.500000, mae: 4378.448242, mean_q: -2716.291016
 2441/5000: episode: 2441, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3181.776, mean reward: -3181.776 [-3181.776, -3181.776], mean action: 2.000 [2.000, 2.000],  loss: 2569717.000000, mae: 4346.500977, mean_q: -2712.255859
 2442/5000: episode: 2442, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -164.336, mean reward: -164.336 [-164.336, -164.336], mean action: 2.000 [2.000, 2.000],  loss: 2760792.750000, mae: 4445.646484, mean_q: -2706.796875
 2443/5000: episode: 2443, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9737.441, mean reward: -9737.441 [-9737.441, -9737.441], mean action: 2.000 [2.000, 2.000],  loss: 1833282.375000, mae: 4255.451172, mean_q: -2694.164795
 2444/5000: episode: 2444, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9368.704, mean reward: -9368.704 [-9368.704, -9368.704], mean action: 2.000 [2.000, 2.000],  loss: 2612171.500000, mae: 4407.067383, mean_q: -2696.998535
 2445/5000: episode: 2445, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5841.831, mean reward: -5841.831 [-5841.831, -5841.831], mean action: 2.000 [2.000, 2.000],  loss: 1540883.375000, mae: 4288.403809, mean_q: -2706.808838
 2446/5000: episode: 2446, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -398.862, mean reward: -398.862 [-398.862, -398.862], mean action: 2.000 [2.000, 2.000],  loss: 2046408.500000, mae: 4329.234863, mean_q: -2699.942383
 2447/5000: episode: 2447, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2592.183, mean reward: -2592.183 [-2592.183, -2592.183], mean action: 2.000 [2.000, 2.000],  loss: 3758855.750000, mae: 4455.973633, mean_q: -2698.660156
 2448/5000: episode: 2448, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -452.122, mean reward: -452.122 [-452.122, -452.122], mean action: 2.000 [2.000, 2.000],  loss: 2418296.750000, mae: 4391.715820, mean_q: -2692.377930
 2449/5000: episode: 2449, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5.581, mean reward: -5.581 [-5.581, -5.581], mean action: 2.000 [2.000, 2.000],  loss: 6345934.500000, mae: 4544.282227, mean_q: -2680.259033
 2450/5000: episode: 2450, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2679.885, mean reward: -2679.885 [-2679.885, -2679.885], mean action: 2.000 [2.000, 2.000],  loss: 2463986.500000, mae: 4291.193359, mean_q: -2682.958008
 2451/5000: episode: 2451, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4709.434, mean reward: -4709.434 [-4709.434, -4709.434], mean action: 2.000 [2.000, 2.000],  loss: 4192742.000000, mae: 4496.466797, mean_q: -2674.791504
 2452/5000: episode: 2452, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2309.462, mean reward: -2309.462 [-2309.462, -2309.462], mean action: 2.000 [2.000, 2.000],  loss: 2574764.500000, mae: 4448.570801, mean_q: -2682.908691
 2453/5000: episode: 2453, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5658.969, mean reward: -5658.969 [-5658.969, -5658.969], mean action: 2.000 [2.000, 2.000],  loss: 1624884.625000, mae: 4339.842285, mean_q: -2700.593750
 2454/5000: episode: 2454, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2917.903, mean reward: -2917.903 [-2917.903, -2917.903], mean action: 2.000 [2.000, 2.000],  loss: 2928996.750000, mae: 4251.406250, mean_q: -2704.587402
 2455/5000: episode: 2455, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -224.685, mean reward: -224.685 [-224.685, -224.685], mean action: 2.000 [2.000, 2.000],  loss: 2987075.000000, mae: 4441.124023, mean_q: -2690.889893
 2456/5000: episode: 2456, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -54.988, mean reward: -54.988 [-54.988, -54.988], mean action: 2.000 [2.000, 2.000],  loss: 2515111.500000, mae: 4323.644043, mean_q: -2681.158447
 2457/5000: episode: 2457, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2590.445, mean reward: -2590.445 [-2590.445, -2590.445], mean action: 2.000 [2.000, 2.000],  loss: 1638363.250000, mae: 4281.377930, mean_q: -2660.755371
 2458/5000: episode: 2458, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3495.903, mean reward: -3495.903 [-3495.903, -3495.903], mean action: 2.000 [2.000, 2.000],  loss: 5923174.000000, mae: 4566.711914, mean_q: -2657.703613
 2459/5000: episode: 2459, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -321.891, mean reward: -321.891 [-321.891, -321.891], mean action: 2.000 [2.000, 2.000],  loss: 2899981.000000, mae: 4393.106445, mean_q: -2637.846924
 2460/5000: episode: 2460, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -404.267, mean reward: -404.267 [-404.267, -404.267], mean action: 2.000 [2.000, 2.000],  loss: 2641610.000000, mae: 4339.518555, mean_q: -2628.423828
 2461/5000: episode: 2461, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2176.585, mean reward: -2176.585 [-2176.585, -2176.585], mean action: 2.000 [2.000, 2.000],  loss: 2217874.500000, mae: 4337.688477, mean_q: -2623.457031
 2462/5000: episode: 2462, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2525.044, mean reward: -2525.044 [-2525.044, -2525.044], mean action: 2.000 [2.000, 2.000],  loss: 3844338.500000, mae: 4384.298828, mean_q: -2626.486328
 2463/5000: episode: 2463, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2735.792, mean reward: -2735.792 [-2735.792, -2735.792], mean action: 2.000 [2.000, 2.000],  loss: 2232200.000000, mae: 4324.344727, mean_q: -2612.638916
 2464/5000: episode: 2464, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -443.714, mean reward: -443.714 [-443.714, -443.714], mean action: 2.000 [2.000, 2.000],  loss: 1976767.875000, mae: 4316.892090, mean_q: -2618.792725
 2465/5000: episode: 2465, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3446.524, mean reward: -3446.524 [-3446.524, -3446.524], mean action: 2.000 [2.000, 2.000],  loss: 2017855.875000, mae: 4279.608398, mean_q: -2614.641602
 2466/5000: episode: 2466, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2972.585, mean reward: -2972.585 [-2972.585, -2972.585], mean action: 2.000 [2.000, 2.000],  loss: 2609889.500000, mae: 4370.270996, mean_q: -2608.236084
 2467/5000: episode: 2467, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -10790.800, mean reward: -10790.800 [-10790.800, -10790.800], mean action: 0.000 [0.000, 0.000],  loss: 2513204.500000, mae: 4324.781738, mean_q: -2612.707031
 2468/5000: episode: 2468, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1764.887, mean reward: -1764.887 [-1764.887, -1764.887], mean action: 2.000 [2.000, 2.000],  loss: 3209725.000000, mae: 4377.175781, mean_q: -2617.434570
 2469/5000: episode: 2469, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5575.130, mean reward: -5575.130 [-5575.130, -5575.130], mean action: 3.000 [3.000, 3.000],  loss: 2969658.500000, mae: 4390.352539, mean_q: -2628.730469
 2470/5000: episode: 2470, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4766.445, mean reward: -4766.445 [-4766.445, -4766.445], mean action: 2.000 [2.000, 2.000],  loss: 3230599.500000, mae: 4343.952148, mean_q: -2604.586426
 2471/5000: episode: 2471, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4419.065, mean reward: -4419.065 [-4419.065, -4419.065], mean action: 2.000 [2.000, 2.000],  loss: 2280898.000000, mae: 4364.706543, mean_q: -2611.785156
 2472/5000: episode: 2472, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2776.250, mean reward: -2776.250 [-2776.250, -2776.250], mean action: 2.000 [2.000, 2.000],  loss: 4193528.000000, mae: 4454.687988, mean_q: -2612.723633
 2473/5000: episode: 2473, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -616.403, mean reward: -616.403 [-616.403, -616.403], mean action: 2.000 [2.000, 2.000],  loss: 3185027.500000, mae: 4283.295898, mean_q: -2606.759766
 2474/5000: episode: 2474, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -225.473, mean reward: -225.473 [-225.473, -225.473], mean action: 2.000 [2.000, 2.000],  loss: 3763764.500000, mae: 4427.395996, mean_q: -2593.395996
 2475/5000: episode: 2475, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8817.398, mean reward: -8817.398 [-8817.398, -8817.398], mean action: 0.000 [0.000, 0.000],  loss: 3707686.750000, mae: 4492.797852, mean_q: -2584.199707
 2476/5000: episode: 2476, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1755.854, mean reward: -1755.854 [-1755.854, -1755.854], mean action: 2.000 [2.000, 2.000],  loss: 2286700.750000, mae: 4272.674805, mean_q: -2574.377441
 2477/5000: episode: 2477, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -481.258, mean reward: -481.258 [-481.258, -481.258], mean action: 2.000 [2.000, 2.000],  loss: 2206480.000000, mae: 4269.608398, mean_q: -2579.050293
 2478/5000: episode: 2478, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -2964.432, mean reward: -2964.432 [-2964.432, -2964.432], mean action: 2.000 [2.000, 2.000],  loss: 3815113.000000, mae: 4381.672852, mean_q: -2569.835449
 2479/5000: episode: 2479, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2090.584, mean reward: -2090.584 [-2090.584, -2090.584], mean action: 2.000 [2.000, 2.000],  loss: 4166128.500000, mae: 4442.432617, mean_q: -2572.131836
 2480/5000: episode: 2480, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1292.195, mean reward: -1292.195 [-1292.195, -1292.195], mean action: 2.000 [2.000, 2.000],  loss: 3278297.500000, mae: 4374.912109, mean_q: -2563.857666
 2481/5000: episode: 2481, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -443.442, mean reward: -443.442 [-443.442, -443.442], mean action: 2.000 [2.000, 2.000],  loss: 3237124.500000, mae: 4341.590332, mean_q: -2593.637451
 2482/5000: episode: 2482, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5970.722, mean reward: -5970.722 [-5970.722, -5970.722], mean action: 2.000 [2.000, 2.000],  loss: 3490464.500000, mae: 4332.965820, mean_q: -2586.271973
 2483/5000: episode: 2483, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2673.211, mean reward: -2673.211 [-2673.211, -2673.211], mean action: 2.000 [2.000, 2.000],  loss: 2169901.000000, mae: 4208.155273, mean_q: -2588.543701
 2484/5000: episode: 2484, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1498.740, mean reward: -1498.740 [-1498.740, -1498.740], mean action: 2.000 [2.000, 2.000],  loss: 4002576.750000, mae: 4313.290039, mean_q: -2598.158203
 2485/5000: episode: 2485, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6007.911, mean reward: -6007.911 [-6007.911, -6007.911], mean action: 2.000 [2.000, 2.000],  loss: 3153212.000000, mae: 4291.671875, mean_q: -2585.631348
 2486/5000: episode: 2486, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2318.422, mean reward: -2318.422 [-2318.422, -2318.422], mean action: 2.000 [2.000, 2.000],  loss: 3603062.750000, mae: 4361.414551, mean_q: -2598.610840
 2487/5000: episode: 2487, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1720.222, mean reward: -1720.222 [-1720.222, -1720.222], mean action: 2.000 [2.000, 2.000],  loss: 3015476.000000, mae: 4335.119629, mean_q: -2600.165283
 2488/5000: episode: 2488, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -550.033, mean reward: -550.033 [-550.033, -550.033], mean action: 2.000 [2.000, 2.000],  loss: 3953825.500000, mae: 4305.706055, mean_q: -2613.631592
 2489/5000: episode: 2489, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1865.852, mean reward: -1865.852 [-1865.852, -1865.852], mean action: 2.000 [2.000, 2.000],  loss: 3797310.000000, mae: 4345.805664, mean_q: -2621.995361
 2490/5000: episode: 2490, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -7007.374, mean reward: -7007.374 [-7007.374, -7007.374], mean action: 2.000 [2.000, 2.000],  loss: 2862837.500000, mae: 4247.237305, mean_q: -2612.552490
 2491/5000: episode: 2491, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3741.637, mean reward: -3741.637 [-3741.637, -3741.637], mean action: 2.000 [2.000, 2.000],  loss: 2429873.500000, mae: 4203.450195, mean_q: -2615.733887
 2492/5000: episode: 2492, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3733.933, mean reward: -3733.933 [-3733.933, -3733.933], mean action: 2.000 [2.000, 2.000],  loss: 2881553.250000, mae: 4363.314941, mean_q: -2609.589355
 2493/5000: episode: 2493, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2100.509, mean reward: -2100.509 [-2100.509, -2100.509], mean action: 2.000 [2.000, 2.000],  loss: 2828690.000000, mae: 4261.218750, mean_q: -2594.244141
 2494/5000: episode: 2494, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -7888.854, mean reward: -7888.854 [-7888.854, -7888.854], mean action: 2.000 [2.000, 2.000],  loss: 3238234.500000, mae: 4177.622070, mean_q: -2597.243896
 2495/5000: episode: 2495, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7949.985, mean reward: -7949.985 [-7949.985, -7949.985], mean action: 2.000 [2.000, 2.000],  loss: 2732987.500000, mae: 4305.873047, mean_q: -2597.117188
 2496/5000: episode: 2496, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4809.524, mean reward: -4809.524 [-4809.524, -4809.524], mean action: 2.000 [2.000, 2.000],  loss: 2828454.750000, mae: 4222.317383, mean_q: -2591.992188
 2497/5000: episode: 2497, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3272.541, mean reward: -3272.541 [-3272.541, -3272.541], mean action: 2.000 [2.000, 2.000],  loss: 1988256.750000, mae: 4208.065430, mean_q: -2590.402832
 2498/5000: episode: 2498, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -327.079, mean reward: -327.079 [-327.079, -327.079], mean action: 2.000 [2.000, 2.000],  loss: 5347527.000000, mae: 4412.222656, mean_q: -2581.759766
 2499/5000: episode: 2499, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1534.292, mean reward: -1534.292 [-1534.292, -1534.292], mean action: 2.000 [2.000, 2.000],  loss: 2181193.750000, mae: 4281.333008, mean_q: -2596.855957
 2500/5000: episode: 2500, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1954.462, mean reward: -1954.462 [-1954.462, -1954.462], mean action: 2.000 [2.000, 2.000],  loss: 1945715.625000, mae: 4162.511230, mean_q: -2604.622070
 2501/5000: episode: 2501, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -309.633, mean reward: -309.633 [-309.633, -309.633], mean action: 2.000 [2.000, 2.000],  loss: 2927664.250000, mae: 4146.863281, mean_q: -2611.964844
 2502/5000: episode: 2502, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -797.382, mean reward: -797.382 [-797.382, -797.382], mean action: 2.000 [2.000, 2.000],  loss: 2278242.000000, mae: 4235.212891, mean_q: -2599.463867
 2503/5000: episode: 2503, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -332.979, mean reward: -332.979 [-332.979, -332.979], mean action: 1.000 [1.000, 1.000],  loss: 1756278.500000, mae: 4267.531250, mean_q: -2602.335205
 2504/5000: episode: 2504, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -458.944, mean reward: -458.944 [-458.944, -458.944], mean action: 2.000 [2.000, 2.000],  loss: 2625057.000000, mae: 4172.597656, mean_q: -2587.551514
 2505/5000: episode: 2505, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4612.178, mean reward: -4612.178 [-4612.178, -4612.178], mean action: 2.000 [2.000, 2.000],  loss: 2202155.000000, mae: 4210.498047, mean_q: -2597.358398
 2506/5000: episode: 2506, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -690.588, mean reward: -690.588 [-690.588, -690.588], mean action: 2.000 [2.000, 2.000],  loss: 3133196.000000, mae: 4237.555664, mean_q: -2582.159424
 2507/5000: episode: 2507, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -165.580, mean reward: -165.580 [-165.580, -165.580], mean action: 2.000 [2.000, 2.000],  loss: 3070768.750000, mae: 4256.263672, mean_q: -2589.489258
 2508/5000: episode: 2508, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -751.672, mean reward: -751.672 [-751.672, -751.672], mean action: 2.000 [2.000, 2.000],  loss: 3926674.250000, mae: 4323.100098, mean_q: -2593.228027
 2509/5000: episode: 2509, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1290.310, mean reward: -1290.310 [-1290.310, -1290.310], mean action: 2.000 [2.000, 2.000],  loss: 4018788.000000, mae: 4335.516602, mean_q: -2580.422119
 2510/5000: episode: 2510, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -621.645, mean reward: -621.645 [-621.645, -621.645], mean action: 2.000 [2.000, 2.000],  loss: 2074985.750000, mae: 4165.801758, mean_q: -2569.047852
 2511/5000: episode: 2511, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -759.959, mean reward: -759.959 [-759.959, -759.959], mean action: 2.000 [2.000, 2.000],  loss: 3384333.500000, mae: 4213.808594, mean_q: -2576.503662
 2512/5000: episode: 2512, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -380.336, mean reward: -380.336 [-380.336, -380.336], mean action: 2.000 [2.000, 2.000],  loss: 2328484.750000, mae: 4185.254395, mean_q: -2566.597900
 2513/5000: episode: 2513, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7360.734, mean reward: -7360.734 [-7360.734, -7360.734], mean action: 2.000 [2.000, 2.000],  loss: 2560023.750000, mae: 4187.695312, mean_q: -2558.030273
 2514/5000: episode: 2514, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5524.046, mean reward: -5524.046 [-5524.046, -5524.046], mean action: 1.000 [1.000, 1.000],  loss: 3197377.500000, mae: 4266.683105, mean_q: -2555.861816
 2515/5000: episode: 2515, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2740.440, mean reward: -2740.440 [-2740.440, -2740.440], mean action: 2.000 [2.000, 2.000],  loss: 2425507.500000, mae: 4187.311035, mean_q: -2546.685791
 2516/5000: episode: 2516, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2566.991, mean reward: -2566.991 [-2566.991, -2566.991], mean action: 2.000 [2.000, 2.000],  loss: 2028128.250000, mae: 4170.616211, mean_q: -2533.236328
 2517/5000: episode: 2517, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1164.087, mean reward: -1164.087 [-1164.087, -1164.087], mean action: 1.000 [1.000, 1.000],  loss: 1788489.250000, mae: 4138.305664, mean_q: -2531.135986
 2518/5000: episode: 2518, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1771.051, mean reward: -1771.051 [-1771.051, -1771.051], mean action: 2.000 [2.000, 2.000],  loss: 2898519.500000, mae: 4142.719238, mean_q: -2528.180664
 2519/5000: episode: 2519, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3073.043, mean reward: -3073.043 [-3073.043, -3073.043], mean action: 2.000 [2.000, 2.000],  loss: 2431381.500000, mae: 4122.605469, mean_q: -2533.478027
 2520/5000: episode: 2520, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -249.312, mean reward: -249.312 [-249.312, -249.312], mean action: 2.000 [2.000, 2.000],  loss: 3309485.000000, mae: 4169.196289, mean_q: -2528.722900
 2521/5000: episode: 2521, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -9710.580, mean reward: -9710.580 [-9710.580, -9710.580], mean action: 0.000 [0.000, 0.000],  loss: 3742089.500000, mae: 4202.583496, mean_q: -2533.008301
 2522/5000: episode: 2522, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2273.085, mean reward: -2273.085 [-2273.085, -2273.085], mean action: 2.000 [2.000, 2.000],  loss: 2789945.500000, mae: 4207.368164, mean_q: -2528.136719
 2523/5000: episode: 2523, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1903.134, mean reward: -1903.134 [-1903.134, -1903.134], mean action: 2.000 [2.000, 2.000],  loss: 2248023.000000, mae: 4229.019531, mean_q: -2534.629150
 2524/5000: episode: 2524, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1013.619, mean reward: -1013.619 [-1013.619, -1013.619], mean action: 2.000 [2.000, 2.000],  loss: 3253987.500000, mae: 4267.744141, mean_q: -2532.952881
 2525/5000: episode: 2525, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -86.916, mean reward: -86.916 [-86.916, -86.916], mean action: 2.000 [2.000, 2.000],  loss: 2644113.000000, mae: 4179.752441, mean_q: -2515.473145
 2526/5000: episode: 2526, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1863.900, mean reward: -1863.900 [-1863.900, -1863.900], mean action: 2.000 [2.000, 2.000],  loss: 2725409.000000, mae: 4218.710938, mean_q: -2527.564941
 2527/5000: episode: 2527, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1812.349, mean reward: -1812.349 [-1812.349, -1812.349], mean action: 2.000 [2.000, 2.000],  loss: 2484538.000000, mae: 4056.729492, mean_q: -2512.336914
 2528/5000: episode: 2528, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1120.310, mean reward: -1120.310 [-1120.310, -1120.310], mean action: 2.000 [2.000, 2.000],  loss: 3007235.250000, mae: 4170.670410, mean_q: -2518.384033
 2529/5000: episode: 2529, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1396.152, mean reward: -1396.152 [-1396.152, -1396.152], mean action: 2.000 [2.000, 2.000],  loss: 3513545.000000, mae: 4289.821289, mean_q: -2520.115234
 2530/5000: episode: 2530, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -708.554, mean reward: -708.554 [-708.554, -708.554], mean action: 2.000 [2.000, 2.000],  loss: 1835263.625000, mae: 4127.818359, mean_q: -2527.936035
 2531/5000: episode: 2531, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1415.813, mean reward: -1415.813 [-1415.813, -1415.813], mean action: 2.000 [2.000, 2.000],  loss: 4468494.000000, mae: 4267.252930, mean_q: -2524.411377
 2532/5000: episode: 2532, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7443.794, mean reward: -7443.794 [-7443.794, -7443.794], mean action: 2.000 [2.000, 2.000],  loss: 2226408.500000, mae: 4159.911133, mean_q: -2541.403809
 2533/5000: episode: 2533, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4953.464, mean reward: -4953.464 [-4953.464, -4953.464], mean action: 2.000 [2.000, 2.000],  loss: 2038692.625000, mae: 4159.824219, mean_q: -2542.890625
 2534/5000: episode: 2534, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -91.238, mean reward: -91.238 [-91.238, -91.238], mean action: 2.000 [2.000, 2.000],  loss: 2956756.250000, mae: 4270.544922, mean_q: -2551.145996
 2535/5000: episode: 2535, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -317.033, mean reward: -317.033 [-317.033, -317.033], mean action: 2.000 [2.000, 2.000],  loss: 2385101.500000, mae: 4192.880371, mean_q: -2547.463379
 2536/5000: episode: 2536, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4134.547, mean reward: -4134.547 [-4134.547, -4134.547], mean action: 2.000 [2.000, 2.000],  loss: 1171442.250000, mae: 3979.940918, mean_q: -2552.655518
 2537/5000: episode: 2537, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2466.060, mean reward: -2466.060 [-2466.060, -2466.060], mean action: 2.000 [2.000, 2.000],  loss: 1434937.250000, mae: 4120.450195, mean_q: -2557.565430
 2538/5000: episode: 2538, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -3933.585, mean reward: -3933.585 [-3933.585, -3933.585], mean action: 2.000 [2.000, 2.000],  loss: 1268755.500000, mae: 4064.722168, mean_q: -2540.957520
 2539/5000: episode: 2539, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5533.717, mean reward: -5533.717 [-5533.717, -5533.717], mean action: 2.000 [2.000, 2.000],  loss: 3675862.250000, mae: 4266.041992, mean_q: -2540.484375
 2540/5000: episode: 2540, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3674.027, mean reward: -3674.027 [-3674.027, -3674.027], mean action: 2.000 [2.000, 2.000],  loss: 2745808.000000, mae: 4201.609863, mean_q: -2537.374023
 2541/5000: episode: 2541, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6549.748, mean reward: -6549.748 [-6549.748, -6549.748], mean action: 2.000 [2.000, 2.000],  loss: 4703970.000000, mae: 4305.381836, mean_q: -2529.158203
 2542/5000: episode: 2542, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1046.993, mean reward: -1046.993 [-1046.993, -1046.993], mean action: 2.000 [2.000, 2.000],  loss: 4233568.500000, mae: 4242.887695, mean_q: -2538.302734
 2543/5000: episode: 2543, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2346.557, mean reward: -2346.557 [-2346.557, -2346.557], mean action: 2.000 [2.000, 2.000],  loss: 2507310.000000, mae: 4116.219727, mean_q: -2526.484375
 2544/5000: episode: 2544, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2526.292, mean reward: -2526.292 [-2526.292, -2526.292], mean action: 2.000 [2.000, 2.000],  loss: 3429019.500000, mae: 4224.573242, mean_q: -2522.488281
 2545/5000: episode: 2545, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -902.758, mean reward: -902.758 [-902.758, -902.758], mean action: 2.000 [2.000, 2.000],  loss: 3986127.500000, mae: 4188.240234, mean_q: -2512.477783
 2546/5000: episode: 2546, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3085.035, mean reward: -3085.035 [-3085.035, -3085.035], mean action: 2.000 [2.000, 2.000],  loss: 2473021.000000, mae: 4182.645508, mean_q: -2507.200195
 2547/5000: episode: 2547, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2127.896, mean reward: -2127.896 [-2127.896, -2127.896], mean action: 2.000 [2.000, 2.000],  loss: 3064757.000000, mae: 4162.977051, mean_q: -2517.652832
 2548/5000: episode: 2548, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -3871.143, mean reward: -3871.143 [-3871.143, -3871.143], mean action: 2.000 [2.000, 2.000],  loss: 1822571.500000, mae: 4103.864746, mean_q: -2526.826660
 2549/5000: episode: 2549, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -744.674, mean reward: -744.674 [-744.674, -744.674], mean action: 2.000 [2.000, 2.000],  loss: 2768783.500000, mae: 4226.547852, mean_q: -2521.562500
 2550/5000: episode: 2550, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3889.524, mean reward: -3889.524 [-3889.524, -3889.524], mean action: 2.000 [2.000, 2.000],  loss: 3228885.500000, mae: 4174.378418, mean_q: -2535.239746
 2551/5000: episode: 2551, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1527.757, mean reward: -1527.757 [-1527.757, -1527.757], mean action: 2.000 [2.000, 2.000],  loss: 2119815.000000, mae: 4101.334961, mean_q: -2557.598633
 2552/5000: episode: 2552, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2042.808, mean reward: -2042.808 [-2042.808, -2042.808], mean action: 2.000 [2.000, 2.000],  loss: 2201177.750000, mae: 4155.370605, mean_q: -2554.275391
 2553/5000: episode: 2553, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6459.011, mean reward: -6459.011 [-6459.011, -6459.011], mean action: 2.000 [2.000, 2.000],  loss: 2556325.500000, mae: 4223.185547, mean_q: -2560.896484
 2554/5000: episode: 2554, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -779.485, mean reward: -779.485 [-779.485, -779.485], mean action: 2.000 [2.000, 2.000],  loss: 3302218.250000, mae: 4264.747070, mean_q: -2572.636230
 2555/5000: episode: 2555, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5707.933, mean reward: -5707.933 [-5707.933, -5707.933], mean action: 2.000 [2.000, 2.000],  loss: 3032355.500000, mae: 4267.509277, mean_q: -2576.713867
 2556/5000: episode: 2556, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -885.956, mean reward: -885.956 [-885.956, -885.956], mean action: 2.000 [2.000, 2.000],  loss: 3629064.500000, mae: 4328.035156, mean_q: -2589.687744
 2557/5000: episode: 2557, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4341.071, mean reward: -4341.071 [-4341.071, -4341.071], mean action: 2.000 [2.000, 2.000],  loss: 2400860.500000, mae: 4191.822266, mean_q: -2600.505859
 2558/5000: episode: 2558, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1118.509, mean reward: -1118.509 [-1118.509, -1118.509], mean action: 2.000 [2.000, 2.000],  loss: 3933328.750000, mae: 4302.838867, mean_q: -2590.403809
 2559/5000: episode: 2559, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -492.021, mean reward: -492.021 [-492.021, -492.021], mean action: 2.000 [2.000, 2.000],  loss: 3442124.250000, mae: 4292.304688, mean_q: -2592.195801
 2560/5000: episode: 2560, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1532.570, mean reward: -1532.570 [-1532.570, -1532.570], mean action: 2.000 [2.000, 2.000],  loss: 2267307.500000, mae: 4192.005859, mean_q: -2575.984131
 2561/5000: episode: 2561, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2140.527, mean reward: -2140.527 [-2140.527, -2140.527], mean action: 2.000 [2.000, 2.000],  loss: 2448222.250000, mae: 4194.044922, mean_q: -2562.720703
 2562/5000: episode: 2562, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -260.662, mean reward: -260.662 [-260.662, -260.662], mean action: 2.000 [2.000, 2.000],  loss: 2741172.500000, mae: 4303.241211, mean_q: -2564.849609
 2563/5000: episode: 2563, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -740.589, mean reward: -740.589 [-740.589, -740.589], mean action: 2.000 [2.000, 2.000],  loss: 2157572.500000, mae: 4111.960938, mean_q: -2553.732910
 2564/5000: episode: 2564, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -2679.411, mean reward: -2679.411 [-2679.411, -2679.411], mean action: 2.000 [2.000, 2.000],  loss: 3819917.500000, mae: 4255.533691, mean_q: -2541.717285
 2565/5000: episode: 2565, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -44.127, mean reward: -44.127 [-44.127, -44.127], mean action: 2.000 [2.000, 2.000],  loss: 2210943.750000, mae: 4201.211426, mean_q: -2537.048828
 2566/5000: episode: 2566, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4052.008, mean reward: -4052.008 [-4052.008, -4052.008], mean action: 2.000 [2.000, 2.000],  loss: 2542684.000000, mae: 4232.758789, mean_q: -2530.880371
 2567/5000: episode: 2567, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1677.829, mean reward: -1677.829 [-1677.829, -1677.829], mean action: 2.000 [2.000, 2.000],  loss: 2448640.250000, mae: 4183.147949, mean_q: -2517.186523
 2568/5000: episode: 2568, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -199.493, mean reward: -199.493 [-199.493, -199.493], mean action: 2.000 [2.000, 2.000],  loss: 2052779.250000, mae: 4146.486328, mean_q: -2498.493164
 2569/5000: episode: 2569, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3635.112, mean reward: -3635.112 [-3635.112, -3635.112], mean action: 2.000 [2.000, 2.000],  loss: 3009163.750000, mae: 4069.577637, mean_q: -2484.776855
 2570/5000: episode: 2570, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -144.781, mean reward: -144.781 [-144.781, -144.781], mean action: 2.000 [2.000, 2.000],  loss: 2852183.000000, mae: 4185.959473, mean_q: -2466.517090
 2571/5000: episode: 2571, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3443.728, mean reward: -3443.728 [-3443.728, -3443.728], mean action: 2.000 [2.000, 2.000],  loss: 2451812.500000, mae: 4137.626953, mean_q: -2463.259277
 2572/5000: episode: 2572, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2234.844, mean reward: -2234.844 [-2234.844, -2234.844], mean action: 2.000 [2.000, 2.000],  loss: 3395470.250000, mae: 4108.428711, mean_q: -2453.943359
 2573/5000: episode: 2573, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1623.230, mean reward: -1623.230 [-1623.230, -1623.230], mean action: 2.000 [2.000, 2.000],  loss: 4312556.000000, mae: 4283.285645, mean_q: -2476.547852
 2574/5000: episode: 2574, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5180.155, mean reward: -5180.155 [-5180.155, -5180.155], mean action: 2.000 [2.000, 2.000],  loss: 2646687.500000, mae: 4201.424316, mean_q: -2485.605225
 2575/5000: episode: 2575, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4750.552, mean reward: -4750.552 [-4750.552, -4750.552], mean action: 2.000 [2.000, 2.000],  loss: 3606055.000000, mae: 4179.692383, mean_q: -2479.997070
 2576/5000: episode: 2576, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1784.381, mean reward: -1784.381 [-1784.381, -1784.381], mean action: 2.000 [2.000, 2.000],  loss: 3891928.500000, mae: 4254.951172, mean_q: -2501.375732
 2577/5000: episode: 2577, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1655.907, mean reward: -1655.907 [-1655.907, -1655.907], mean action: 2.000 [2.000, 2.000],  loss: 3000005.750000, mae: 4151.176758, mean_q: -2494.047119
 2578/5000: episode: 2578, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5332.084, mean reward: -5332.084 [-5332.084, -5332.084], mean action: 2.000 [2.000, 2.000],  loss: 3198799.250000, mae: 4175.337891, mean_q: -2495.558105
 2579/5000: episode: 2579, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3905.264, mean reward: -3905.264 [-3905.264, -3905.264], mean action: 2.000 [2.000, 2.000],  loss: 1862203.625000, mae: 4127.807617, mean_q: -2491.455566
 2580/5000: episode: 2580, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -303.714, mean reward: -303.714 [-303.714, -303.714], mean action: 2.000 [2.000, 2.000],  loss: 2648851.250000, mae: 4056.207520, mean_q: -2502.514160
 2581/5000: episode: 2581, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1467.037, mean reward: -1467.037 [-1467.037, -1467.037], mean action: 2.000 [2.000, 2.000],  loss: 3827942.750000, mae: 4250.510742, mean_q: -2500.153320
 2582/5000: episode: 2582, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -113.791, mean reward: -113.791 [-113.791, -113.791], mean action: 2.000 [2.000, 2.000],  loss: 2471773.500000, mae: 4181.549805, mean_q: -2488.350586
 2583/5000: episode: 2583, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2347.283, mean reward: -2347.283 [-2347.283, -2347.283], mean action: 2.000 [2.000, 2.000],  loss: 2220624.750000, mae: 4079.345215, mean_q: -2477.286133
 2584/5000: episode: 2584, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -35.217, mean reward: -35.217 [-35.217, -35.217], mean action: 2.000 [2.000, 2.000],  loss: 2419474.000000, mae: 4173.099609, mean_q: -2488.048828
 2585/5000: episode: 2585, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5751.105, mean reward: -5751.105 [-5751.105, -5751.105], mean action: 2.000 [2.000, 2.000],  loss: 3347059.000000, mae: 4168.909180, mean_q: -2484.955322
 2586/5000: episode: 2586, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2600.459, mean reward: -2600.459 [-2600.459, -2600.459], mean action: 2.000 [2.000, 2.000],  loss: 3187720.750000, mae: 4161.994141, mean_q: -2498.055664
 2587/5000: episode: 2587, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1437.183, mean reward: -1437.183 [-1437.183, -1437.183], mean action: 2.000 [2.000, 2.000],  loss: 4151564.250000, mae: 4331.102051, mean_q: -2491.749512
 2588/5000: episode: 2588, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4527.182, mean reward: -4527.182 [-4527.182, -4527.182], mean action: 2.000 [2.000, 2.000],  loss: 3528354.500000, mae: 4178.649902, mean_q: -2494.999512
 2589/5000: episode: 2589, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -7656.242, mean reward: -7656.242 [-7656.242, -7656.242], mean action: 2.000 [2.000, 2.000],  loss: 2645270.750000, mae: 4094.714355, mean_q: -2511.491699
 2590/5000: episode: 2590, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1701.950, mean reward: -1701.950 [-1701.950, -1701.950], mean action: 2.000 [2.000, 2.000],  loss: 3203749.750000, mae: 4291.370117, mean_q: -2520.155762
 2591/5000: episode: 2591, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2798.237, mean reward: -2798.237 [-2798.237, -2798.237], mean action: 2.000 [2.000, 2.000],  loss: 3326436.250000, mae: 4238.971680, mean_q: -2534.777344
 2592/5000: episode: 2592, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1946.187, mean reward: -1946.187 [-1946.187, -1946.187], mean action: 2.000 [2.000, 2.000],  loss: 2624255.250000, mae: 4216.526367, mean_q: -2536.977051
 2593/5000: episode: 2593, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -493.876, mean reward: -493.876 [-493.876, -493.876], mean action: 2.000 [2.000, 2.000],  loss: 1985060.125000, mae: 4114.127441, mean_q: -2531.108398
 2594/5000: episode: 2594, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -527.264, mean reward: -527.264 [-527.264, -527.264], mean action: 2.000 [2.000, 2.000],  loss: 2228717.250000, mae: 4169.894531, mean_q: -2542.928955
 2595/5000: episode: 2595, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -53.003, mean reward: -53.003 [-53.003, -53.003], mean action: 2.000 [2.000, 2.000],  loss: 2537061.000000, mae: 4219.838867, mean_q: -2551.853027
 2596/5000: episode: 2596, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2709.453, mean reward: -2709.453 [-2709.453, -2709.453], mean action: 2.000 [2.000, 2.000],  loss: 3337300.000000, mae: 4220.392578, mean_q: -2561.255859
 2597/5000: episode: 2597, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -960.676, mean reward: -960.676 [-960.676, -960.676], mean action: 2.000 [2.000, 2.000],  loss: 3058290.000000, mae: 4188.647949, mean_q: -2560.243164
 2598/5000: episode: 2598, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3252.720, mean reward: -3252.720 [-3252.720, -3252.720], mean action: 2.000 [2.000, 2.000],  loss: 2793133.500000, mae: 4191.240234, mean_q: -2567.121826
 2599/5000: episode: 2599, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -717.597, mean reward: -717.597 [-717.597, -717.597], mean action: 2.000 [2.000, 2.000],  loss: 2287407.500000, mae: 4201.796875, mean_q: -2579.218994
 2600/5000: episode: 2600, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5356.413, mean reward: -5356.413 [-5356.413, -5356.413], mean action: 2.000 [2.000, 2.000],  loss: 2128760.000000, mae: 4175.520020, mean_q: -2584.678223
 2601/5000: episode: 2601, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -812.550, mean reward: -812.550 [-812.550, -812.550], mean action: 2.000 [2.000, 2.000],  loss: 1618872.500000, mae: 4126.098633, mean_q: -2591.403076
 2602/5000: episode: 2602, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -446.218, mean reward: -446.218 [-446.218, -446.218], mean action: 2.000 [2.000, 2.000],  loss: 4193110.750000, mae: 4274.430176, mean_q: -2600.416504
 2603/5000: episode: 2603, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1493.901, mean reward: -1493.901 [-1493.901, -1493.901], mean action: 2.000 [2.000, 2.000],  loss: 2355928.750000, mae: 4194.001953, mean_q: -2612.901855
 2604/5000: episode: 2604, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9878.584, mean reward: -9878.584 [-9878.584, -9878.584], mean action: 2.000 [2.000, 2.000],  loss: 2682613.000000, mae: 4177.078125, mean_q: -2615.778076
 2605/5000: episode: 2605, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -441.298, mean reward: -441.298 [-441.298, -441.298], mean action: 2.000 [2.000, 2.000],  loss: 3132034.000000, mae: 4157.467773, mean_q: -2616.310059
 2606/5000: episode: 2606, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2651.633, mean reward: -2651.633 [-2651.633, -2651.633], mean action: 2.000 [2.000, 2.000],  loss: 3342790.250000, mae: 4286.915039, mean_q: -2602.384766
 2607/5000: episode: 2607, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1378.231, mean reward: -1378.231 [-1378.231, -1378.231], mean action: 2.000 [2.000, 2.000],  loss: 2464166.750000, mae: 4238.673828, mean_q: -2595.347656
 2608/5000: episode: 2608, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6035.079, mean reward: -6035.079 [-6035.079, -6035.079], mean action: 2.000 [2.000, 2.000],  loss: 3177091.000000, mae: 4286.224121, mean_q: -2610.043457
 2609/5000: episode: 2609, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -673.335, mean reward: -673.335 [-673.335, -673.335], mean action: 2.000 [2.000, 2.000],  loss: 3763044.500000, mae: 4281.002930, mean_q: -2594.550537
 2610/5000: episode: 2610, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3487.673, mean reward: -3487.673 [-3487.673, -3487.673], mean action: 2.000 [2.000, 2.000],  loss: 1911709.875000, mae: 4137.721680, mean_q: -2587.965576
 2611/5000: episode: 2611, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -3812.349, mean reward: -3812.349 [-3812.349, -3812.349], mean action: 2.000 [2.000, 2.000],  loss: 2695064.750000, mae: 4256.163574, mean_q: -2616.900391
 2612/5000: episode: 2612, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -6820.957, mean reward: -6820.957 [-6820.957, -6820.957], mean action: 2.000 [2.000, 2.000],  loss: 4206387.000000, mae: 4242.456543, mean_q: -2612.301758
 2613/5000: episode: 2613, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3055.821, mean reward: -3055.821 [-3055.821, -3055.821], mean action: 2.000 [2.000, 2.000],  loss: 2223954.250000, mae: 4257.112305, mean_q: -2607.916260
 2614/5000: episode: 2614, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -750.161, mean reward: -750.161 [-750.161, -750.161], mean action: 2.000 [2.000, 2.000],  loss: 2095806.500000, mae: 4195.766113, mean_q: -2606.784668
 2615/5000: episode: 2615, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -10010.946, mean reward: -10010.946 [-10010.946, -10010.946], mean action: 0.000 [0.000, 0.000],  loss: 2632575.000000, mae: 4274.967773, mean_q: -2594.999756
 2616/5000: episode: 2616, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2075.569, mean reward: -2075.569 [-2075.569, -2075.569], mean action: 2.000 [2.000, 2.000],  loss: 2689477.000000, mae: 4224.133789, mean_q: -2594.519043
 2617/5000: episode: 2617, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -2538.655, mean reward: -2538.655 [-2538.655, -2538.655], mean action: 2.000 [2.000, 2.000],  loss: 3996708.750000, mae: 4293.021973, mean_q: -2573.640625
 2618/5000: episode: 2618, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -7737.601, mean reward: -7737.601 [-7737.601, -7737.601], mean action: 2.000 [2.000, 2.000],  loss: 2938943.000000, mae: 4300.442383, mean_q: -2577.474854
 2619/5000: episode: 2619, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -69.392, mean reward: -69.392 [-69.392, -69.392], mean action: 2.000 [2.000, 2.000],  loss: 2154144.000000, mae: 4222.466309, mean_q: -2579.163086
 2620/5000: episode: 2620, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5.741, mean reward: -5.741 [-5.741, -5.741], mean action: 2.000 [2.000, 2.000],  loss: 2396008.250000, mae: 4183.050781, mean_q: -2565.107910
 2621/5000: episode: 2621, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4779.639, mean reward: -4779.639 [-4779.639, -4779.639], mean action: 2.000 [2.000, 2.000],  loss: 2520148.500000, mae: 4192.342773, mean_q: -2564.959961
 2622/5000: episode: 2622, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -7817.565, mean reward: -7817.565 [-7817.565, -7817.565], mean action: 2.000 [2.000, 2.000],  loss: 1807388.500000, mae: 4134.377930, mean_q: -2546.319336
 2623/5000: episode: 2623, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -3096.097, mean reward: -3096.097 [-3096.097, -3096.097], mean action: 2.000 [2.000, 2.000],  loss: 5564696.500000, mae: 4405.889648, mean_q: -2532.785156
 2624/5000: episode: 2624, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1745.521, mean reward: -1745.521 [-1745.521, -1745.521], mean action: 2.000 [2.000, 2.000],  loss: 2377680.000000, mae: 4246.020996, mean_q: -2513.346191
 2625/5000: episode: 2625, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -953.884, mean reward: -953.884 [-953.884, -953.884], mean action: 2.000 [2.000, 2.000],  loss: 2307856.750000, mae: 4103.766602, mean_q: -2518.060547
 2626/5000: episode: 2626, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -601.534, mean reward: -601.534 [-601.534, -601.534], mean action: 2.000 [2.000, 2.000],  loss: 2492699.000000, mae: 4148.421875, mean_q: -2501.800781
 2627/5000: episode: 2627, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3623.832, mean reward: -3623.832 [-3623.832, -3623.832], mean action: 2.000 [2.000, 2.000],  loss: 2090182.625000, mae: 4048.292969, mean_q: -2506.375000
 2628/5000: episode: 2628, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4463.978, mean reward: -4463.978 [-4463.978, -4463.978], mean action: 2.000 [2.000, 2.000],  loss: 4117219.250000, mae: 4296.861328, mean_q: -2488.929199
 2629/5000: episode: 2629, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3008.605, mean reward: -3008.605 [-3008.605, -3008.605], mean action: 2.000 [2.000, 2.000],  loss: 3579912.000000, mae: 4215.311523, mean_q: -2468.301514
 2630/5000: episode: 2630, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1305.946, mean reward: -1305.946 [-1305.946, -1305.946], mean action: 2.000 [2.000, 2.000],  loss: 3206635.250000, mae: 4089.796875, mean_q: -2459.894531
 2631/5000: episode: 2631, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2230.348, mean reward: -2230.348 [-2230.348, -2230.348], mean action: 2.000 [2.000, 2.000],  loss: 3276761.000000, mae: 4222.993652, mean_q: -2466.825684
 2632/5000: episode: 2632, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3121.649, mean reward: -3121.649 [-3121.649, -3121.649], mean action: 2.000 [2.000, 2.000],  loss: 3528886.750000, mae: 4171.737305, mean_q: -2462.789062
 2633/5000: episode: 2633, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3767.096, mean reward: -3767.096 [-3767.096, -3767.096], mean action: 2.000 [2.000, 2.000],  loss: 3767899.750000, mae: 4279.825195, mean_q: -2460.264160
 2634/5000: episode: 2634, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3237.723, mean reward: -3237.723 [-3237.723, -3237.723], mean action: 2.000 [2.000, 2.000],  loss: 3400164.000000, mae: 4205.842773, mean_q: -2458.090820
 2635/5000: episode: 2635, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2889.322, mean reward: -2889.322 [-2889.322, -2889.322], mean action: 2.000 [2.000, 2.000],  loss: 3020417.500000, mae: 4163.046387, mean_q: -2462.238281
 2636/5000: episode: 2636, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1861.475, mean reward: -1861.475 [-1861.475, -1861.475], mean action: 2.000 [2.000, 2.000],  loss: 4453296.500000, mae: 4239.366211, mean_q: -2468.943604
 2637/5000: episode: 2637, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4881.365, mean reward: -4881.365 [-4881.365, -4881.365], mean action: 2.000 [2.000, 2.000],  loss: 2515705.000000, mae: 4159.447266, mean_q: -2480.042969
 2638/5000: episode: 2638, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -185.818, mean reward: -185.818 [-185.818, -185.818], mean action: 2.000 [2.000, 2.000],  loss: 3006668.500000, mae: 4131.474609, mean_q: -2478.750000
 2639/5000: episode: 2639, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3669.331, mean reward: -3669.331 [-3669.331, -3669.331], mean action: 2.000 [2.000, 2.000],  loss: 3784542.250000, mae: 4173.843750, mean_q: -2480.198486
 2640/5000: episode: 2640, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -62.361, mean reward: -62.361 [-62.361, -62.361], mean action: 2.000 [2.000, 2.000],  loss: 2779542.250000, mae: 4247.543457, mean_q: -2473.039062
 2641/5000: episode: 2641, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -1568.720, mean reward: -1568.720 [-1568.720, -1568.720], mean action: 2.000 [2.000, 2.000],  loss: 4018246.000000, mae: 4265.028809, mean_q: -2478.866211
 2642/5000: episode: 2642, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1318.552, mean reward: -1318.552 [-1318.552, -1318.552], mean action: 2.000 [2.000, 2.000],  loss: 1963920.000000, mae: 4123.672852, mean_q: -2490.044434
 2643/5000: episode: 2643, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -864.343, mean reward: -864.343 [-864.343, -864.343], mean action: 2.000 [2.000, 2.000],  loss: 2657973.750000, mae: 4162.908203, mean_q: -2485.508301
 2644/5000: episode: 2644, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4510.360, mean reward: -4510.360 [-4510.360, -4510.360], mean action: 2.000 [2.000, 2.000],  loss: 2512963.000000, mae: 4160.947754, mean_q: -2514.021484
 2645/5000: episode: 2645, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -1218.448, mean reward: -1218.448 [-1218.448, -1218.448], mean action: 2.000 [2.000, 2.000],  loss: 3347126.250000, mae: 4121.050293, mean_q: -2526.177490
 2646/5000: episode: 2646, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7125.696, mean reward: -7125.696 [-7125.696, -7125.696], mean action: 2.000 [2.000, 2.000],  loss: 3004763.500000, mae: 4192.710938, mean_q: -2547.820557
 2647/5000: episode: 2647, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3988.708, mean reward: -3988.708 [-3988.708, -3988.708], mean action: 2.000 [2.000, 2.000],  loss: 3498729.250000, mae: 4225.820312, mean_q: -2559.143066
 2648/5000: episode: 2648, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1760.645, mean reward: -1760.645 [-1760.645, -1760.645], mean action: 2.000 [2.000, 2.000],  loss: 3470978.500000, mae: 4395.237305, mean_q: -2567.894043
 2649/5000: episode: 2649, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -2710.677, mean reward: -2710.677 [-2710.677, -2710.677], mean action: 2.000 [2.000, 2.000],  loss: 2660724.750000, mae: 4236.694336, mean_q: -2595.019043
 2650/5000: episode: 2650, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -193.007, mean reward: -193.007 [-193.007, -193.007], mean action: 2.000 [2.000, 2.000],  loss: 2061112.500000, mae: 4209.305664, mean_q: -2603.390869
 2651/5000: episode: 2651, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -631.525, mean reward: -631.525 [-631.525, -631.525], mean action: 2.000 [2.000, 2.000],  loss: 1969379.000000, mae: 4205.548828, mean_q: -2619.533691
 2652/5000: episode: 2652, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -169.594, mean reward: -169.594 [-169.594, -169.594], mean action: 2.000 [2.000, 2.000],  loss: 3038013.750000, mae: 4281.749023, mean_q: -2605.783203
 2653/5000: episode: 2653, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5.779, mean reward: -5.779 [-5.779, -5.779], mean action: 3.000 [3.000, 3.000],  loss: 2184445.250000, mae: 4235.743164, mean_q: -2614.903320
 2654/5000: episode: 2654, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8140.029, mean reward: -8140.029 [-8140.029, -8140.029], mean action: 2.000 [2.000, 2.000],  loss: 3762914.750000, mae: 4315.104492, mean_q: -2629.237305
 2655/5000: episode: 2655, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3158.649, mean reward: -3158.649 [-3158.649, -3158.649], mean action: 3.000 [3.000, 3.000],  loss: 1482558.875000, mae: 4250.597656, mean_q: -2636.344727
 2656/5000: episode: 2656, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3244.793, mean reward: -3244.793 [-3244.793, -3244.793], mean action: 2.000 [2.000, 2.000],  loss: 3944704.750000, mae: 4345.457031, mean_q: -2638.520508
 2657/5000: episode: 2657, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1538.129, mean reward: -1538.129 [-1538.129, -1538.129], mean action: 2.000 [2.000, 2.000],  loss: 2642993.500000, mae: 4219.647461, mean_q: -2657.517334
 2658/5000: episode: 2658, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -874.401, mean reward: -874.401 [-874.401, -874.401], mean action: 2.000 [2.000, 2.000],  loss: 3148778.000000, mae: 4239.586914, mean_q: -2651.298096
 2659/5000: episode: 2659, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -671.589, mean reward: -671.589 [-671.589, -671.589], mean action: 2.000 [2.000, 2.000],  loss: 3308104.750000, mae: 4308.723145, mean_q: -2661.428223
 2660/5000: episode: 2660, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -8408.734, mean reward: -8408.734 [-8408.734, -8408.734], mean action: 2.000 [2.000, 2.000],  loss: 1684458.750000, mae: 4090.708252, mean_q: -2663.924316
 2661/5000: episode: 2661, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2694.274, mean reward: -2694.274 [-2694.274, -2694.274], mean action: 2.000 [2.000, 2.000],  loss: 2730773.750000, mae: 4335.053711, mean_q: -2687.891357
 2662/5000: episode: 2662, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1841.265, mean reward: -1841.265 [-1841.265, -1841.265], mean action: 2.000 [2.000, 2.000],  loss: 3256870.500000, mae: 4366.264648, mean_q: -2685.591064
 2663/5000: episode: 2663, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6482.447, mean reward: -6482.447 [-6482.447, -6482.447], mean action: 2.000 [2.000, 2.000],  loss: 3450122.500000, mae: 4379.832031, mean_q: -2689.309814
 2664/5000: episode: 2664, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2320.314, mean reward: -2320.314 [-2320.314, -2320.314], mean action: 2.000 [2.000, 2.000],  loss: 2451088.500000, mae: 4140.031250, mean_q: -2675.218262
 2665/5000: episode: 2665, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3280.903, mean reward: -3280.903 [-3280.903, -3280.903], mean action: 2.000 [2.000, 2.000],  loss: 2428597.750000, mae: 4245.197266, mean_q: -2675.875000
 2666/5000: episode: 2666, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4404.675, mean reward: -4404.675 [-4404.675, -4404.675], mean action: 2.000 [2.000, 2.000],  loss: 3376277.500000, mae: 4364.597656, mean_q: -2683.216309
 2667/5000: episode: 2667, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -452.614, mean reward: -452.614 [-452.614, -452.614], mean action: 3.000 [3.000, 3.000],  loss: 3490755.500000, mae: 4378.430176, mean_q: -2670.147461
 2668/5000: episode: 2668, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -640.287, mean reward: -640.287 [-640.287, -640.287], mean action: 2.000 [2.000, 2.000],  loss: 2013621.875000, mae: 4210.208984, mean_q: -2661.605469
 2669/5000: episode: 2669, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8145.845, mean reward: -8145.845 [-8145.845, -8145.845], mean action: 2.000 [2.000, 2.000],  loss: 2662353.500000, mae: 4199.055664, mean_q: -2653.665039
 2670/5000: episode: 2670, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5838.335, mean reward: -5838.335 [-5838.335, -5838.335], mean action: 2.000 [2.000, 2.000],  loss: 2521065.750000, mae: 4157.924805, mean_q: -2645.436523
 2671/5000: episode: 2671, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1294.884, mean reward: -1294.884 [-1294.884, -1294.884], mean action: 2.000 [2.000, 2.000],  loss: 3601916.250000, mae: 4305.074219, mean_q: -2638.120850
 2672/5000: episode: 2672, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4934.232, mean reward: -4934.232 [-4934.232, -4934.232], mean action: 2.000 [2.000, 2.000],  loss: 2546934.000000, mae: 4189.909180, mean_q: -2649.000977
 2673/5000: episode: 2673, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -232.996, mean reward: -232.996 [-232.996, -232.996], mean action: 2.000 [2.000, 2.000],  loss: 2824179.000000, mae: 4320.870605, mean_q: -2628.067627
 2674/5000: episode: 2674, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3480.589, mean reward: -3480.589 [-3480.589, -3480.589], mean action: 0.000 [0.000, 0.000],  loss: 4452392.500000, mae: 4300.603027, mean_q: -2630.006104
 2675/5000: episode: 2675, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -291.401, mean reward: -291.401 [-291.401, -291.401], mean action: 2.000 [2.000, 2.000],  loss: 3782596.250000, mae: 4291.640625, mean_q: -2621.560059
 2676/5000: episode: 2676, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2123.415, mean reward: -2123.415 [-2123.415, -2123.415], mean action: 2.000 [2.000, 2.000],  loss: 2853683.250000, mae: 4277.963867, mean_q: -2634.336670
 2677/5000: episode: 2677, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6137.936, mean reward: -6137.936 [-6137.936, -6137.936], mean action: 2.000 [2.000, 2.000],  loss: 3242555.750000, mae: 4300.314941, mean_q: -2628.097656
 2678/5000: episode: 2678, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1118.426, mean reward: -1118.426 [-1118.426, -1118.426], mean action: 0.000 [0.000, 0.000],  loss: 1882227.375000, mae: 4178.121094, mean_q: -2616.148193
 2679/5000: episode: 2679, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4277.018, mean reward: -4277.018 [-4277.018, -4277.018], mean action: 2.000 [2.000, 2.000],  loss: 2785494.750000, mae: 4256.611328, mean_q: -2609.379150
 2680/5000: episode: 2680, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2735.406, mean reward: -2735.406 [-2735.406, -2735.406], mean action: 2.000 [2.000, 2.000],  loss: 2551871.000000, mae: 4194.618652, mean_q: -2601.132568
 2681/5000: episode: 2681, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2721.194, mean reward: -2721.194 [-2721.194, -2721.194], mean action: 2.000 [2.000, 2.000],  loss: 2728091.000000, mae: 4103.566406, mean_q: -2601.758057
 2682/5000: episode: 2682, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4526.380, mean reward: -4526.380 [-4526.380, -4526.380], mean action: 2.000 [2.000, 2.000],  loss: 3305960.500000, mae: 4238.885254, mean_q: -2593.684082
 2683/5000: episode: 2683, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1703.700, mean reward: -1703.700 [-1703.700, -1703.700], mean action: 2.000 [2.000, 2.000],  loss: 3937652.250000, mae: 4195.427734, mean_q: -2598.015625
 2684/5000: episode: 2684, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5614.792, mean reward: -5614.792 [-5614.792, -5614.792], mean action: 3.000 [3.000, 3.000],  loss: 3076159.000000, mae: 4205.828125, mean_q: -2618.858398
 2685/5000: episode: 2685, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -936.136, mean reward: -936.136 [-936.136, -936.136], mean action: 2.000 [2.000, 2.000],  loss: 1957718.500000, mae: 4147.612305, mean_q: -2636.746826
 2686/5000: episode: 2686, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1443.668, mean reward: -1443.668 [-1443.668, -1443.668], mean action: 2.000 [2.000, 2.000],  loss: 3726479.000000, mae: 4250.625000, mean_q: -2628.764648
 2687/5000: episode: 2687, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1991.062, mean reward: -1991.062 [-1991.062, -1991.062], mean action: 2.000 [2.000, 2.000],  loss: 3444186.000000, mae: 4264.657715, mean_q: -2631.250488
 2688/5000: episode: 2688, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2076.099, mean reward: -2076.099 [-2076.099, -2076.099], mean action: 2.000 [2.000, 2.000],  loss: 2171810.000000, mae: 4187.120605, mean_q: -2643.253662
 2689/5000: episode: 2689, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -553.671, mean reward: -553.671 [-553.671, -553.671], mean action: 2.000 [2.000, 2.000],  loss: 2432402.500000, mae: 4138.351074, mean_q: -2648.583496
 2690/5000: episode: 2690, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3406.370, mean reward: -3406.370 [-3406.370, -3406.370], mean action: 2.000 [2.000, 2.000],  loss: 4486154.000000, mae: 4405.614746, mean_q: -2652.231689
 2691/5000: episode: 2691, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3528.661, mean reward: -3528.661 [-3528.661, -3528.661], mean action: 2.000 [2.000, 2.000],  loss: 3029018.500000, mae: 4266.670898, mean_q: -2662.080078
 2692/5000: episode: 2692, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -151.568, mean reward: -151.568 [-151.568, -151.568], mean action: 2.000 [2.000, 2.000],  loss: 2105400.500000, mae: 4138.062500, mean_q: -2662.326172
 2693/5000: episode: 2693, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4957.676, mean reward: -4957.676 [-4957.676, -4957.676], mean action: 2.000 [2.000, 2.000],  loss: 2892378.500000, mae: 4220.936523, mean_q: -2673.138672
 2694/5000: episode: 2694, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2354.705, mean reward: -2354.705 [-2354.705, -2354.705], mean action: 2.000 [2.000, 2.000],  loss: 3583304.500000, mae: 4385.276367, mean_q: -2669.525879
 2695/5000: episode: 2695, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3625.018, mean reward: -3625.018 [-3625.018, -3625.018], mean action: 2.000 [2.000, 2.000],  loss: 3347970.500000, mae: 4294.577148, mean_q: -2656.196533
 2696/5000: episode: 2696, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2239.483, mean reward: -2239.483 [-2239.483, -2239.483], mean action: 2.000 [2.000, 2.000],  loss: 3054412.500000, mae: 4337.084473, mean_q: -2674.867188
 2697/5000: episode: 2697, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4797.189, mean reward: -4797.189 [-4797.189, -4797.189], mean action: 2.000 [2.000, 2.000],  loss: 5534890.000000, mae: 4441.734863, mean_q: -2662.893066
 2698/5000: episode: 2698, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5886.379, mean reward: -5886.379 [-5886.379, -5886.379], mean action: 2.000 [2.000, 2.000],  loss: 2315771.750000, mae: 4282.027344, mean_q: -2668.576904
 2699/5000: episode: 2699, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -415.583, mean reward: -415.583 [-415.583, -415.583], mean action: 2.000 [2.000, 2.000],  loss: 1854532.375000, mae: 4180.423340, mean_q: -2662.896484
 2700/5000: episode: 2700, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3394.105, mean reward: -3394.105 [-3394.105, -3394.105], mean action: 2.000 [2.000, 2.000],  loss: 3233939.750000, mae: 4403.994141, mean_q: -2658.463379
 2701/5000: episode: 2701, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4639.745, mean reward: -4639.745 [-4639.745, -4639.745], mean action: 2.000 [2.000, 2.000],  loss: 2388196.000000, mae: 4321.908691, mean_q: -2642.940918
 2702/5000: episode: 2702, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9.053, mean reward: -9.053 [-9.053, -9.053], mean action: 2.000 [2.000, 2.000],  loss: 2597137.000000, mae: 4373.962891, mean_q: -2650.916260
 2703/5000: episode: 2703, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1678.279, mean reward: -1678.279 [-1678.279, -1678.279], mean action: 2.000 [2.000, 2.000],  loss: 2815646.500000, mae: 4299.248047, mean_q: -2636.020508
 2704/5000: episode: 2704, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5.651, mean reward: -5.651 [-5.651, -5.651], mean action: 2.000 [2.000, 2.000],  loss: 3179498.250000, mae: 4438.887695, mean_q: -2650.100830
 2705/5000: episode: 2705, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4701.602, mean reward: -4701.602 [-4701.602, -4701.602], mean action: 2.000 [2.000, 2.000],  loss: 3297878.750000, mae: 4330.648438, mean_q: -2647.637695
 2706/5000: episode: 2706, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -2844.227, mean reward: -2844.227 [-2844.227, -2844.227], mean action: 2.000 [2.000, 2.000],  loss: 2167336.000000, mae: 4285.695801, mean_q: -2644.051270
 2707/5000: episode: 2707, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -365.953, mean reward: -365.953 [-365.953, -365.953], mean action: 2.000 [2.000, 2.000],  loss: 1795823.500000, mae: 4329.387695, mean_q: -2648.546143
 2708/5000: episode: 2708, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7131.948, mean reward: -7131.948 [-7131.948, -7131.948], mean action: 1.000 [1.000, 1.000],  loss: 1623090.000000, mae: 4230.964844, mean_q: -2630.770752
 2709/5000: episode: 2709, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7836.192, mean reward: -7836.192 [-7836.192, -7836.192], mean action: 2.000 [2.000, 2.000],  loss: 2420431.000000, mae: 4262.538086, mean_q: -2637.570801
 2710/5000: episode: 2710, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5114.712, mean reward: -5114.712 [-5114.712, -5114.712], mean action: 2.000 [2.000, 2.000],  loss: 2344785.000000, mae: 4337.354492, mean_q: -2630.696777
 2711/5000: episode: 2711, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7086.614, mean reward: -7086.614 [-7086.614, -7086.614], mean action: 2.000 [2.000, 2.000],  loss: 3271613.000000, mae: 4368.958984, mean_q: -2624.178955
 2712/5000: episode: 2712, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1274.168, mean reward: -1274.168 [-1274.168, -1274.168], mean action: 2.000 [2.000, 2.000],  loss: 2950034.000000, mae: 4326.701660, mean_q: -2638.212402
 2713/5000: episode: 2713, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -656.472, mean reward: -656.472 [-656.472, -656.472], mean action: 2.000 [2.000, 2.000],  loss: 3612100.000000, mae: 4412.957031, mean_q: -2635.839600
 2714/5000: episode: 2714, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1409.241, mean reward: -1409.241 [-1409.241, -1409.241], mean action: 2.000 [2.000, 2.000],  loss: 2910623.500000, mae: 4350.612305, mean_q: -2633.392090
 2715/5000: episode: 2715, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -4244.121, mean reward: -4244.121 [-4244.121, -4244.121], mean action: 2.000 [2.000, 2.000],  loss: 2850961.250000, mae: 4361.193359, mean_q: -2623.841553
 2716/5000: episode: 2716, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -9174.207, mean reward: -9174.207 [-9174.207, -9174.207], mean action: 2.000 [2.000, 2.000],  loss: 2909445.500000, mae: 4354.673340, mean_q: -2639.107666
 2717/5000: episode: 2717, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3284.356, mean reward: -3284.356 [-3284.356, -3284.356], mean action: 2.000 [2.000, 2.000],  loss: 2671472.750000, mae: 4299.844727, mean_q: -2635.183105
 2718/5000: episode: 2718, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4024.407, mean reward: -4024.407 [-4024.407, -4024.407], mean action: 2.000 [2.000, 2.000],  loss: 1843856.625000, mae: 4247.163086, mean_q: -2632.294922
 2719/5000: episode: 2719, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2285.958, mean reward: -2285.958 [-2285.958, -2285.958], mean action: 2.000 [2.000, 2.000],  loss: 3954284.000000, mae: 4297.261230, mean_q: -2621.678467
 2720/5000: episode: 2720, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4642.271, mean reward: -4642.271 [-4642.271, -4642.271], mean action: 0.000 [0.000, 0.000],  loss: 3824946.000000, mae: 4390.843750, mean_q: -2615.388916
 2721/5000: episode: 2721, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7453.973, mean reward: -7453.973 [-7453.973, -7453.973], mean action: 3.000 [3.000, 3.000],  loss: 3054497.250000, mae: 4241.516602, mean_q: -2617.747559
 2722/5000: episode: 2722, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1018.686, mean reward: -1018.686 [-1018.686, -1018.686], mean action: 2.000 [2.000, 2.000],  loss: 5856429.000000, mae: 4427.839844, mean_q: -2603.559570
 2723/5000: episode: 2723, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -250.075, mean reward: -250.075 [-250.075, -250.075], mean action: 2.000 [2.000, 2.000],  loss: 1375776.000000, mae: 4286.238281, mean_q: -2629.596191
 2724/5000: episode: 2724, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9397.234, mean reward: -9397.234 [-9397.234, -9397.234], mean action: 2.000 [2.000, 2.000],  loss: 2017531.250000, mae: 4286.939453, mean_q: -2610.563721
 2725/5000: episode: 2725, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2793.465, mean reward: -2793.465 [-2793.465, -2793.465], mean action: 2.000 [2.000, 2.000],  loss: 3976606.000000, mae: 4389.297363, mean_q: -2609.363281
 2726/5000: episode: 2726, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -300.906, mean reward: -300.906 [-300.906, -300.906], mean action: 2.000 [2.000, 2.000],  loss: 2645889.250000, mae: 4308.331543, mean_q: -2611.973633
 2727/5000: episode: 2727, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2016.352, mean reward: -2016.352 [-2016.352, -2016.352], mean action: 2.000 [2.000, 2.000],  loss: 2089424.250000, mae: 4273.462891, mean_q: -2608.339111
 2728/5000: episode: 2728, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4429.158, mean reward: -4429.158 [-4429.158, -4429.158], mean action: 2.000 [2.000, 2.000],  loss: 2807373.000000, mae: 4328.988281, mean_q: -2615.041504
 2729/5000: episode: 2729, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -721.211, mean reward: -721.211 [-721.211, -721.211], mean action: 2.000 [2.000, 2.000],  loss: 1894097.750000, mae: 4264.710938, mean_q: -2621.057129
 2730/5000: episode: 2730, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -3187.035, mean reward: -3187.035 [-3187.035, -3187.035], mean action: 2.000 [2.000, 2.000],  loss: 2059715.000000, mae: 4265.427734, mean_q: -2609.844482
 2731/5000: episode: 2731, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3429.880, mean reward: -3429.880 [-3429.880, -3429.880], mean action: 3.000 [3.000, 3.000],  loss: 3133483.250000, mae: 4349.828613, mean_q: -2606.929199
 2732/5000: episode: 2732, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1842.725, mean reward: -1842.725 [-1842.725, -1842.725], mean action: 2.000 [2.000, 2.000],  loss: 1796819.875000, mae: 4313.233887, mean_q: -2610.007812
 2733/5000: episode: 2733, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4771.453, mean reward: -4771.453 [-4771.453, -4771.453], mean action: 2.000 [2.000, 2.000],  loss: 2339424.250000, mae: 4259.514648, mean_q: -2590.568359
 2734/5000: episode: 2734, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1986.208, mean reward: -1986.208 [-1986.208, -1986.208], mean action: 2.000 [2.000, 2.000],  loss: 2907443.500000, mae: 4347.641602, mean_q: -2612.997070
 2735/5000: episode: 2735, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5523.434, mean reward: -5523.434 [-5523.434, -5523.434], mean action: 2.000 [2.000, 2.000],  loss: 3825312.750000, mae: 4294.615234, mean_q: -2605.376709
 2736/5000: episode: 2736, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -947.748, mean reward: -947.748 [-947.748, -947.748], mean action: 2.000 [2.000, 2.000],  loss: 2796825.250000, mae: 4296.985840, mean_q: -2615.615479
 2737/5000: episode: 2737, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2728.416, mean reward: -2728.416 [-2728.416, -2728.416], mean action: 2.000 [2.000, 2.000],  loss: 5091551.000000, mae: 4352.133789, mean_q: -2604.762939
 2738/5000: episode: 2738, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5055.937, mean reward: -5055.937 [-5055.937, -5055.937], mean action: 2.000 [2.000, 2.000],  loss: 2862263.500000, mae: 4299.712891, mean_q: -2629.113037
 2739/5000: episode: 2739, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1528.923, mean reward: -1528.923 [-1528.923, -1528.923], mean action: 2.000 [2.000, 2.000],  loss: 3406985.000000, mae: 4297.042480, mean_q: -2627.910156
 2740/5000: episode: 2740, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3945.363, mean reward: -3945.363 [-3945.363, -3945.363], mean action: 2.000 [2.000, 2.000],  loss: 2072166.625000, mae: 4218.971191, mean_q: -2637.251953
 2741/5000: episode: 2741, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3560.878, mean reward: -3560.878 [-3560.878, -3560.878], mean action: 2.000 [2.000, 2.000],  loss: 3241574.250000, mae: 4335.225098, mean_q: -2649.313477
 2742/5000: episode: 2742, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1949.716, mean reward: -1949.716 [-1949.716, -1949.716], mean action: 2.000 [2.000, 2.000],  loss: 2593502.000000, mae: 4263.328613, mean_q: -2659.845703
 2743/5000: episode: 2743, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4136.676, mean reward: -4136.676 [-4136.676, -4136.676], mean action: 2.000 [2.000, 2.000],  loss: 2073286.250000, mae: 4274.268555, mean_q: -2666.899170
 2744/5000: episode: 2744, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3020.302, mean reward: -3020.302 [-3020.302, -3020.302], mean action: 2.000 [2.000, 2.000],  loss: 2407740.750000, mae: 4202.770508, mean_q: -2682.081055
 2745/5000: episode: 2745, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3579.309, mean reward: -3579.309 [-3579.309, -3579.309], mean action: 2.000 [2.000, 2.000],  loss: 3014729.250000, mae: 4306.101562, mean_q: -2675.729248
 2746/5000: episode: 2746, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3420.096, mean reward: -3420.096 [-3420.096, -3420.096], mean action: 2.000 [2.000, 2.000],  loss: 2866901.500000, mae: 4268.149414, mean_q: -2661.399414
 2747/5000: episode: 2747, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3761.699, mean reward: -3761.699 [-3761.699, -3761.699], mean action: 2.000 [2.000, 2.000],  loss: 3422262.500000, mae: 4201.254883, mean_q: -2664.837891
 2748/5000: episode: 2748, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -3689.415, mean reward: -3689.415 [-3689.415, -3689.415], mean action: 2.000 [2.000, 2.000],  loss: 2980953.000000, mae: 4267.970215, mean_q: -2674.926758
 2749/5000: episode: 2749, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -3022.427, mean reward: -3022.427 [-3022.427, -3022.427], mean action: 2.000 [2.000, 2.000],  loss: 3353859.500000, mae: 4217.117188, mean_q: -2671.594482
 2750/5000: episode: 2750, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -131.849, mean reward: -131.849 [-131.849, -131.849], mean action: 2.000 [2.000, 2.000],  loss: 2788238.250000, mae: 4241.265625, mean_q: -2675.232422
 2751/5000: episode: 2751, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3006.827, mean reward: -3006.827 [-3006.827, -3006.827], mean action: 3.000 [3.000, 3.000],  loss: 4844272.500000, mae: 4349.592773, mean_q: -2661.687012
 2752/5000: episode: 2752, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -153.248, mean reward: -153.248 [-153.248, -153.248], mean action: 2.000 [2.000, 2.000],  loss: 4045351.250000, mae: 4306.977051, mean_q: -2671.651367
 2753/5000: episode: 2753, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1555.772, mean reward: -1555.772 [-1555.772, -1555.772], mean action: 2.000 [2.000, 2.000],  loss: 3009055.250000, mae: 4230.271484, mean_q: -2661.847168
 2754/5000: episode: 2754, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5901.680, mean reward: -5901.680 [-5901.680, -5901.680], mean action: 2.000 [2.000, 2.000],  loss: 4416117.500000, mae: 4308.024414, mean_q: -2643.395752
 2755/5000: episode: 2755, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1626.605, mean reward: -1626.605 [-1626.605, -1626.605], mean action: 2.000 [2.000, 2.000],  loss: 1821465.500000, mae: 4130.616699, mean_q: -2650.618896
 2756/5000: episode: 2756, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -291.728, mean reward: -291.728 [-291.728, -291.728], mean action: 2.000 [2.000, 2.000],  loss: 2640790.250000, mae: 4172.154297, mean_q: -2649.885498
 2757/5000: episode: 2757, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1691.004, mean reward: -1691.004 [-1691.004, -1691.004], mean action: 2.000 [2.000, 2.000],  loss: 2607727.500000, mae: 4130.669434, mean_q: -2658.123047
 2758/5000: episode: 2758, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2330.206, mean reward: -2330.206 [-2330.206, -2330.206], mean action: 2.000 [2.000, 2.000],  loss: 2609295.000000, mae: 4137.803711, mean_q: -2650.012207
 2759/5000: episode: 2759, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4227.343, mean reward: -4227.343 [-4227.343, -4227.343], mean action: 2.000 [2.000, 2.000],  loss: 4361565.000000, mae: 4219.093750, mean_q: -2650.098633
 2760/5000: episode: 2760, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -3386.593, mean reward: -3386.593 [-3386.593, -3386.593], mean action: 2.000 [2.000, 2.000],  loss: 3949936.500000, mae: 4260.216797, mean_q: -2656.272461
 2761/5000: episode: 2761, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1140.730, mean reward: -1140.730 [-1140.730, -1140.730], mean action: 2.000 [2.000, 2.000],  loss: 3042531.000000, mae: 4211.303711, mean_q: -2679.612549
 2762/5000: episode: 2762, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2361.954, mean reward: -2361.954 [-2361.954, -2361.954], mean action: 2.000 [2.000, 2.000],  loss: 3023932.500000, mae: 4133.063477, mean_q: -2672.069824
 2763/5000: episode: 2763, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -258.873, mean reward: -258.873 [-258.873, -258.873], mean action: 2.000 [2.000, 2.000],  loss: 2959524.750000, mae: 4194.383789, mean_q: -2686.502441
 2764/5000: episode: 2764, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7076.667, mean reward: -7076.667 [-7076.667, -7076.667], mean action: 2.000 [2.000, 2.000],  loss: 1751321.750000, mae: 4098.236328, mean_q: -2689.199219
 2765/5000: episode: 2765, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5022.454, mean reward: -5022.454 [-5022.454, -5022.454], mean action: 2.000 [2.000, 2.000],  loss: 1408739.500000, mae: 3982.062744, mean_q: -2696.928711
 2766/5000: episode: 2766, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4557.468, mean reward: -4557.468 [-4557.468, -4557.468], mean action: 2.000 [2.000, 2.000],  loss: 3680967.000000, mae: 4211.937988, mean_q: -2694.778320
 2767/5000: episode: 2767, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1202.894, mean reward: -1202.894 [-1202.894, -1202.894], mean action: 2.000 [2.000, 2.000],  loss: 3177543.750000, mae: 4095.729980, mean_q: -2701.201660
 2768/5000: episode: 2768, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3421.911, mean reward: -3421.911 [-3421.911, -3421.911], mean action: 2.000 [2.000, 2.000],  loss: 3237624.500000, mae: 4203.442383, mean_q: -2668.174805
 2769/5000: episode: 2769, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1558.872, mean reward: -1558.872 [-1558.872, -1558.872], mean action: 2.000 [2.000, 2.000],  loss: 3769137.250000, mae: 4238.619141, mean_q: -2681.119141
 2770/5000: episode: 2770, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -986.722, mean reward: -986.722 [-986.722, -986.722], mean action: 2.000 [2.000, 2.000],  loss: 3682199.750000, mae: 4185.914062, mean_q: -2678.356934
 2771/5000: episode: 2771, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2731.093, mean reward: -2731.093 [-2731.093, -2731.093], mean action: 2.000 [2.000, 2.000],  loss: 3820825.500000, mae: 4125.131836, mean_q: -2666.076172
 2772/5000: episode: 2772, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -786.037, mean reward: -786.037 [-786.037, -786.037], mean action: 2.000 [2.000, 2.000],  loss: 3849719.750000, mae: 4125.214844, mean_q: -2666.540039
 2773/5000: episode: 2773, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2832.476, mean reward: -2832.476 [-2832.476, -2832.476], mean action: 2.000 [2.000, 2.000],  loss: 2395279.750000, mae: 4119.642578, mean_q: -2667.692139
 2774/5000: episode: 2774, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -544.136, mean reward: -544.136 [-544.136, -544.136], mean action: 2.000 [2.000, 2.000],  loss: 3011740.500000, mae: 4113.822266, mean_q: -2663.184814
 2775/5000: episode: 2775, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -4598.128, mean reward: -4598.128 [-4598.128, -4598.128], mean action: 2.000 [2.000, 2.000],  loss: 2603582.000000, mae: 4110.704102, mean_q: -2660.718750
 2776/5000: episode: 2776, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -897.796, mean reward: -897.796 [-897.796, -897.796], mean action: 2.000 [2.000, 2.000],  loss: 2851845.250000, mae: 4027.295410, mean_q: -2639.750000
 2777/5000: episode: 2777, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4059.300, mean reward: -4059.300 [-4059.300, -4059.300], mean action: 2.000 [2.000, 2.000],  loss: 3550019.500000, mae: 4152.994141, mean_q: -2662.558594
 2778/5000: episode: 2778, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1584.639, mean reward: -1584.639 [-1584.639, -1584.639], mean action: 3.000 [3.000, 3.000],  loss: 2644195.750000, mae: 4035.635742, mean_q: -2653.437500
 2779/5000: episode: 2779, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -421.478, mean reward: -421.478 [-421.478, -421.478], mean action: 2.000 [2.000, 2.000],  loss: 2663223.000000, mae: 3989.122070, mean_q: -2654.928223
 2780/5000: episode: 2780, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1660.085, mean reward: -1660.085 [-1660.085, -1660.085], mean action: 2.000 [2.000, 2.000],  loss: 3190185.000000, mae: 4127.770020, mean_q: -2658.485596
 2781/5000: episode: 2781, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2529.820, mean reward: -2529.820 [-2529.820, -2529.820], mean action: 2.000 [2.000, 2.000],  loss: 1836042.500000, mae: 3968.438965, mean_q: -2643.237305
 2782/5000: episode: 2782, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1101.881, mean reward: -1101.881 [-1101.881, -1101.881], mean action: 2.000 [2.000, 2.000],  loss: 3772749.500000, mae: 4198.874023, mean_q: -2632.740723
 2783/5000: episode: 2783, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -980.790, mean reward: -980.790 [-980.790, -980.790], mean action: 2.000 [2.000, 2.000],  loss: 3625285.000000, mae: 4161.155762, mean_q: -2629.455811
 2784/5000: episode: 2784, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -476.756, mean reward: -476.756 [-476.756, -476.756], mean action: 2.000 [2.000, 2.000],  loss: 3535621.750000, mae: 4228.618652, mean_q: -2639.650391
 2785/5000: episode: 2785, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7946.136, mean reward: -7946.136 [-7946.136, -7946.136], mean action: 1.000 [1.000, 1.000],  loss: 4865323.500000, mae: 4092.477051, mean_q: -2624.006348
 2786/5000: episode: 2786, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -157.826, mean reward: -157.826 [-157.826, -157.826], mean action: 2.000 [2.000, 2.000],  loss: 2633241.500000, mae: 4069.538818, mean_q: -2638.818604
 2787/5000: episode: 2787, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4164.954, mean reward: -4164.954 [-4164.954, -4164.954], mean action: 2.000 [2.000, 2.000],  loss: 3119338.500000, mae: 4112.035645, mean_q: -2619.575684
 2788/5000: episode: 2788, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -65.997, mean reward: -65.997 [-65.997, -65.997], mean action: 2.000 [2.000, 2.000],  loss: 2742068.500000, mae: 4046.191895, mean_q: -2626.964355
 2789/5000: episode: 2789, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -883.645, mean reward: -883.645 [-883.645, -883.645], mean action: 2.000 [2.000, 2.000],  loss: 3508129.500000, mae: 4087.021729, mean_q: -2626.985107
 2790/5000: episode: 2790, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3314.528, mean reward: -3314.528 [-3314.528, -3314.528], mean action: 2.000 [2.000, 2.000],  loss: 2398782.500000, mae: 4078.608887, mean_q: -2641.185059
 2791/5000: episode: 2791, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1426.594, mean reward: -1426.594 [-1426.594, -1426.594], mean action: 2.000 [2.000, 2.000],  loss: 2024589.750000, mae: 4055.864746, mean_q: -2627.993652
 2792/5000: episode: 2792, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2586.994, mean reward: -2586.994 [-2586.994, -2586.994], mean action: 2.000 [2.000, 2.000],  loss: 2993185.250000, mae: 4131.059570, mean_q: -2640.479004
 2793/5000: episode: 2793, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4867.786, mean reward: -4867.786 [-4867.786, -4867.786], mean action: 2.000 [2.000, 2.000],  loss: 2859954.000000, mae: 4197.464844, mean_q: -2629.700684
 2794/5000: episode: 2794, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7448.921, mean reward: -7448.921 [-7448.921, -7448.921], mean action: 2.000 [2.000, 2.000],  loss: 3457461.000000, mae: 4162.062012, mean_q: -2621.772949
 2795/5000: episode: 2795, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4317.397, mean reward: -4317.397 [-4317.397, -4317.397], mean action: 2.000 [2.000, 2.000],  loss: 1778149.125000, mae: 4030.492676, mean_q: -2607.732422
 2796/5000: episode: 2796, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -218.153, mean reward: -218.153 [-218.153, -218.153], mean action: 2.000 [2.000, 2.000],  loss: 3991697.000000, mae: 4157.157227, mean_q: -2606.659912
 2797/5000: episode: 2797, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2201.506, mean reward: -2201.506 [-2201.506, -2201.506], mean action: 2.000 [2.000, 2.000],  loss: 2791131.500000, mae: 4147.809570, mean_q: -2611.475586
 2798/5000: episode: 2798, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3464.729, mean reward: -3464.729 [-3464.729, -3464.729], mean action: 2.000 [2.000, 2.000],  loss: 2287822.000000, mae: 4167.463867, mean_q: -2602.973145
 2799/5000: episode: 2799, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3460.376, mean reward: -3460.376 [-3460.376, -3460.376], mean action: 2.000 [2.000, 2.000],  loss: 1955578.000000, mae: 4057.492188, mean_q: -2593.739014
 2800/5000: episode: 2800, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -983.576, mean reward: -983.576 [-983.576, -983.576], mean action: 2.000 [2.000, 2.000],  loss: 3670370.500000, mae: 4146.695312, mean_q: -2572.677734
 2801/5000: episode: 2801, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6920.695, mean reward: -6920.695 [-6920.695, -6920.695], mean action: 2.000 [2.000, 2.000],  loss: 2635763.500000, mae: 4056.286621, mean_q: -2583.222412
 2802/5000: episode: 2802, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4349.747, mean reward: -4349.747 [-4349.747, -4349.747], mean action: 2.000 [2.000, 2.000],  loss: 1798717.500000, mae: 4042.754883, mean_q: -2566.520264
 2803/5000: episode: 2803, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -379.828, mean reward: -379.828 [-379.828, -379.828], mean action: 2.000 [2.000, 2.000],  loss: 2914353.500000, mae: 4014.493896, mean_q: -2559.307129
 2804/5000: episode: 2804, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -2877.805, mean reward: -2877.805 [-2877.805, -2877.805], mean action: 2.000 [2.000, 2.000],  loss: 3233888.000000, mae: 4102.800781, mean_q: -2549.777588
 2805/5000: episode: 2805, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -580.357, mean reward: -580.357 [-580.357, -580.357], mean action: 2.000 [2.000, 2.000],  loss: 3344477.750000, mae: 4115.358887, mean_q: -2543.692139
 2806/5000: episode: 2806, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -187.463, mean reward: -187.463 [-187.463, -187.463], mean action: 2.000 [2.000, 2.000],  loss: 1820792.000000, mae: 4037.435059, mean_q: -2560.482666
 2807/5000: episode: 2807, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7586.054, mean reward: -7586.054 [-7586.054, -7586.054], mean action: 1.000 [1.000, 1.000],  loss: 2803099.000000, mae: 4019.137939, mean_q: -2549.915527
 2808/5000: episode: 2808, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -966.295, mean reward: -966.295 [-966.295, -966.295], mean action: 2.000 [2.000, 2.000],  loss: 3033291.500000, mae: 4079.333740, mean_q: -2532.060791
 2809/5000: episode: 2809, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2642.099, mean reward: -2642.099 [-2642.099, -2642.099], mean action: 2.000 [2.000, 2.000],  loss: 2458442.000000, mae: 4102.971680, mean_q: -2530.926025
 2810/5000: episode: 2810, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2722.278, mean reward: -2722.278 [-2722.278, -2722.278], mean action: 2.000 [2.000, 2.000],  loss: 4151190.000000, mae: 4187.230469, mean_q: -2523.364746
 2811/5000: episode: 2811, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1177.225, mean reward: -1177.225 [-1177.225, -1177.225], mean action: 2.000 [2.000, 2.000],  loss: 2518711.000000, mae: 4189.539062, mean_q: -2529.975098
 2812/5000: episode: 2812, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1340.090, mean reward: -1340.090 [-1340.090, -1340.090], mean action: 2.000 [2.000, 2.000],  loss: 1874152.000000, mae: 4068.130859, mean_q: -2509.013428
 2813/5000: episode: 2813, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5.606, mean reward: -5.606 [-5.606, -5.606], mean action: 2.000 [2.000, 2.000],  loss: 2642930.250000, mae: 4139.753418, mean_q: -2524.542969
 2814/5000: episode: 2814, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2609.145, mean reward: -2609.145 [-2609.145, -2609.145], mean action: 2.000 [2.000, 2.000],  loss: 2494803.500000, mae: 4121.268555, mean_q: -2509.867188
 2815/5000: episode: 2815, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -117.374, mean reward: -117.374 [-117.374, -117.374], mean action: 2.000 [2.000, 2.000],  loss: 2928416.500000, mae: 4128.982422, mean_q: -2526.947998
 2816/5000: episode: 2816, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -99.610, mean reward: -99.610 [-99.610, -99.610], mean action: 2.000 [2.000, 2.000],  loss: 2260367.500000, mae: 4086.346924, mean_q: -2534.065918
 2817/5000: episode: 2817, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1111.357, mean reward: -1111.357 [-1111.357, -1111.357], mean action: 2.000 [2.000, 2.000],  loss: 3686105.500000, mae: 4154.657715, mean_q: -2535.008301
 2818/5000: episode: 2818, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1770.512, mean reward: -1770.512 [-1770.512, -1770.512], mean action: 2.000 [2.000, 2.000],  loss: 2485213.500000, mae: 4146.907227, mean_q: -2551.911621
 2819/5000: episode: 2819, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1259.568, mean reward: -1259.568 [-1259.568, -1259.568], mean action: 2.000 [2.000, 2.000],  loss: 4791492.000000, mae: 4137.523438, mean_q: -2549.355957
 2820/5000: episode: 2820, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2758.145, mean reward: -2758.145 [-2758.145, -2758.145], mean action: 2.000 [2.000, 2.000],  loss: 3374528.750000, mae: 4220.377930, mean_q: -2552.655762
 2821/5000: episode: 2821, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7544.311, mean reward: -7544.311 [-7544.311, -7544.311], mean action: 2.000 [2.000, 2.000],  loss: 3618151.000000, mae: 4152.909668, mean_q: -2551.405518
 2822/5000: episode: 2822, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4298.636, mean reward: -4298.636 [-4298.636, -4298.636], mean action: 2.000 [2.000, 2.000],  loss: 3148216.000000, mae: 4228.957031, mean_q: -2571.959473
 2823/5000: episode: 2823, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3093.150, mean reward: -3093.150 [-3093.150, -3093.150], mean action: 2.000 [2.000, 2.000],  loss: 2147688.500000, mae: 4061.689941, mean_q: -2570.989258
 2824/5000: episode: 2824, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -930.485, mean reward: -930.485 [-930.485, -930.485], mean action: 2.000 [2.000, 2.000],  loss: 2093435.500000, mae: 4143.737305, mean_q: -2584.990723
 2825/5000: episode: 2825, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3557.651, mean reward: -3557.651 [-3557.651, -3557.651], mean action: 2.000 [2.000, 2.000],  loss: 3032240.000000, mae: 4227.700195, mean_q: -2570.977539
 2826/5000: episode: 2826, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3056.666, mean reward: -3056.666 [-3056.666, -3056.666], mean action: 2.000 [2.000, 2.000],  loss: 3037490.750000, mae: 4183.923828, mean_q: -2573.370117
 2827/5000: episode: 2827, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1096.190, mean reward: -1096.190 [-1096.190, -1096.190], mean action: 2.000 [2.000, 2.000],  loss: 3113700.000000, mae: 4178.403320, mean_q: -2571.888672
 2828/5000: episode: 2828, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6651.807, mean reward: -6651.807 [-6651.807, -6651.807], mean action: 2.000 [2.000, 2.000],  loss: 1532543.250000, mae: 4113.002930, mean_q: -2552.034912
 2829/5000: episode: 2829, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3483.764, mean reward: -3483.764 [-3483.764, -3483.764], mean action: 2.000 [2.000, 2.000],  loss: 3264974.750000, mae: 4164.931152, mean_q: -2558.851562
 2830/5000: episode: 2830, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -2188.715, mean reward: -2188.715 [-2188.715, -2188.715], mean action: 2.000 [2.000, 2.000],  loss: 2382812.500000, mae: 4094.015869, mean_q: -2555.449219
 2831/5000: episode: 2831, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4060.692, mean reward: -4060.692 [-4060.692, -4060.692], mean action: 2.000 [2.000, 2.000],  loss: 1922434.625000, mae: 4050.216064, mean_q: -2560.105957
 2832/5000: episode: 2832, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7631.292, mean reward: -7631.292 [-7631.292, -7631.292], mean action: 2.000 [2.000, 2.000],  loss: 1418779.000000, mae: 3958.548340, mean_q: -2565.395752
 2833/5000: episode: 2833, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5227.742, mean reward: -5227.742 [-5227.742, -5227.742], mean action: 2.000 [2.000, 2.000],  loss: 1623728.000000, mae: 4111.871582, mean_q: -2570.965332
 2834/5000: episode: 2834, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6217.433, mean reward: -6217.433 [-6217.433, -6217.433], mean action: 2.000 [2.000, 2.000],  loss: 3008189.000000, mae: 4136.682617, mean_q: -2565.917480
 2835/5000: episode: 2835, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2937.457, mean reward: -2937.457 [-2937.457, -2937.457], mean action: 2.000 [2.000, 2.000],  loss: 3701396.250000, mae: 4162.555664, mean_q: -2591.209961
 2836/5000: episode: 2836, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -545.336, mean reward: -545.336 [-545.336, -545.336], mean action: 2.000 [2.000, 2.000],  loss: 3210857.000000, mae: 4217.942383, mean_q: -2592.567627
 2837/5000: episode: 2837, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3635.693, mean reward: -3635.693 [-3635.693, -3635.693], mean action: 2.000 [2.000, 2.000],  loss: 3061087.000000, mae: 4234.327148, mean_q: -2596.455322
 2838/5000: episode: 2838, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7546.913, mean reward: -7546.913 [-7546.913, -7546.913], mean action: 2.000 [2.000, 2.000],  loss: 5382548.000000, mae: 4252.793457, mean_q: -2593.837891
 2839/5000: episode: 2839, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -332.156, mean reward: -332.156 [-332.156, -332.156], mean action: 2.000 [2.000, 2.000],  loss: 2543241.500000, mae: 4119.552734, mean_q: -2587.174561
 2840/5000: episode: 2840, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9150.726, mean reward: -9150.726 [-9150.726, -9150.726], mean action: 2.000 [2.000, 2.000],  loss: 3508656.000000, mae: 4162.314453, mean_q: -2589.271484
 2841/5000: episode: 2841, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4230.774, mean reward: -4230.774 [-4230.774, -4230.774], mean action: 2.000 [2.000, 2.000],  loss: 3197687.000000, mae: 4194.687500, mean_q: -2585.161621
 2842/5000: episode: 2842, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4791.967, mean reward: -4791.967 [-4791.967, -4791.967], mean action: 3.000 [3.000, 3.000],  loss: 3009724.750000, mae: 4130.515137, mean_q: -2588.841309
 2843/5000: episode: 2843, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5850.420, mean reward: -5850.420 [-5850.420, -5850.420], mean action: 3.000 [3.000, 3.000],  loss: 3364265.500000, mae: 4129.441895, mean_q: -2589.614746
 2844/5000: episode: 2844, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -14.538, mean reward: -14.538 [-14.538, -14.538], mean action: 3.000 [3.000, 3.000],  loss: 2647220.500000, mae: 4208.757812, mean_q: -2609.166016
 2845/5000: episode: 2845, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -685.685, mean reward: -685.685 [-685.685, -685.685], mean action: 3.000 [3.000, 3.000],  loss: 1930038.750000, mae: 4087.804443, mean_q: -2610.030762
 2846/5000: episode: 2846, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -4235.136, mean reward: -4235.136 [-4235.136, -4235.136], mean action: 2.000 [2.000, 2.000],  loss: 2219298.250000, mae: 4168.298828, mean_q: -2624.063477
 2847/5000: episode: 2847, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -40.003, mean reward: -40.003 [-40.003, -40.003], mean action: 2.000 [2.000, 2.000],  loss: 4225308.000000, mae: 4261.912109, mean_q: -2620.635498
 2848/5000: episode: 2848, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -12.933, mean reward: -12.933 [-12.933, -12.933], mean action: 2.000 [2.000, 2.000],  loss: 2198474.250000, mae: 4111.427734, mean_q: -2622.999512
 2849/5000: episode: 2849, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3878.030, mean reward: -3878.030 [-3878.030, -3878.030], mean action: 2.000 [2.000, 2.000],  loss: 2686096.000000, mae: 4132.068359, mean_q: -2617.762695
 2850/5000: episode: 2850, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5655.476, mean reward: -5655.476 [-5655.476, -5655.476], mean action: 2.000 [2.000, 2.000],  loss: 3145651.000000, mae: 4214.355469, mean_q: -2609.447266
 2851/5000: episode: 2851, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1968.048, mean reward: -1968.048 [-1968.048, -1968.048], mean action: 2.000 [2.000, 2.000],  loss: 2594596.000000, mae: 4235.428711, mean_q: -2616.561035
 2852/5000: episode: 2852, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2930.187, mean reward: -2930.187 [-2930.187, -2930.187], mean action: 2.000 [2.000, 2.000],  loss: 1878918.750000, mae: 4105.563477, mean_q: -2607.019043
 2853/5000: episode: 2853, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -202.626, mean reward: -202.626 [-202.626, -202.626], mean action: 2.000 [2.000, 2.000],  loss: 2622924.000000, mae: 4272.996582, mean_q: -2624.680908
 2854/5000: episode: 2854, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2534.376, mean reward: -2534.376 [-2534.376, -2534.376], mean action: 2.000 [2.000, 2.000],  loss: 1296458.125000, mae: 4079.977539, mean_q: -2610.289062
 2855/5000: episode: 2855, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1605.887, mean reward: -1605.887 [-1605.887, -1605.887], mean action: 3.000 [3.000, 3.000],  loss: 4676623.500000, mae: 4348.437500, mean_q: -2619.744873
 2856/5000: episode: 2856, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -536.274, mean reward: -536.274 [-536.274, -536.274], mean action: 2.000 [2.000, 2.000],  loss: 3365282.000000, mae: 4232.509277, mean_q: -2620.522461
 2857/5000: episode: 2857, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4498.537, mean reward: -4498.537 [-4498.537, -4498.537], mean action: 2.000 [2.000, 2.000],  loss: 1404964.625000, mae: 3980.379883, mean_q: -2626.907715
 2858/5000: episode: 2858, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -108.785, mean reward: -108.785 [-108.785, -108.785], mean action: 3.000 [3.000, 3.000],  loss: 3002986.750000, mae: 4251.250488, mean_q: -2638.986328
 2859/5000: episode: 2859, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2231.849, mean reward: -2231.849 [-2231.849, -2231.849], mean action: 2.000 [2.000, 2.000],  loss: 3595724.750000, mae: 4304.211914, mean_q: -2657.700439
 2860/5000: episode: 2860, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3141.315, mean reward: -3141.315 [-3141.315, -3141.315], mean action: 2.000 [2.000, 2.000],  loss: 3474887.500000, mae: 4233.644043, mean_q: -2682.439453
 2861/5000: episode: 2861, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1816.795, mean reward: -1816.795 [-1816.795, -1816.795], mean action: 2.000 [2.000, 2.000],  loss: 3757202.750000, mae: 4338.075195, mean_q: -2689.174316
 2862/5000: episode: 2862, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1264.879, mean reward: -1264.879 [-1264.879, -1264.879], mean action: 2.000 [2.000, 2.000],  loss: 2813758.500000, mae: 4130.790039, mean_q: -2689.111084
 2863/5000: episode: 2863, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3169.422, mean reward: -3169.422 [-3169.422, -3169.422], mean action: 2.000 [2.000, 2.000],  loss: 3052635.000000, mae: 4252.424316, mean_q: -2696.388184
 2864/5000: episode: 2864, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3299.275, mean reward: -3299.275 [-3299.275, -3299.275], mean action: 2.000 [2.000, 2.000],  loss: 1951512.875000, mae: 4151.258789, mean_q: -2712.609375
 2865/5000: episode: 2865, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1255.487, mean reward: -1255.487 [-1255.487, -1255.487], mean action: 2.000 [2.000, 2.000],  loss: 3718524.250000, mae: 4346.630859, mean_q: -2720.513184
 2866/5000: episode: 2866, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4710.963, mean reward: -4710.963 [-4710.963, -4710.963], mean action: 2.000 [2.000, 2.000],  loss: 3888788.250000, mae: 4373.108398, mean_q: -2739.769287
 2867/5000: episode: 2867, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1230.323, mean reward: -1230.323 [-1230.323, -1230.323], mean action: 2.000 [2.000, 2.000],  loss: 2879569.750000, mae: 4349.916016, mean_q: -2730.868408
 2868/5000: episode: 2868, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -932.384, mean reward: -932.384 [-932.384, -932.384], mean action: 2.000 [2.000, 2.000],  loss: 2316900.000000, mae: 4218.959961, mean_q: -2736.274170
 2869/5000: episode: 2869, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -24.859, mean reward: -24.859 [-24.859, -24.859], mean action: 2.000 [2.000, 2.000],  loss: 2369397.000000, mae: 4175.734863, mean_q: -2741.781494
 2870/5000: episode: 2870, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3794.375, mean reward: -3794.375 [-3794.375, -3794.375], mean action: 2.000 [2.000, 2.000],  loss: 2733715.500000, mae: 4220.421875, mean_q: -2735.613037
 2871/5000: episode: 2871, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -446.171, mean reward: -446.171 [-446.171, -446.171], mean action: 2.000 [2.000, 2.000],  loss: 3441346.500000, mae: 4308.818359, mean_q: -2731.590820
 2872/5000: episode: 2872, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -934.958, mean reward: -934.958 [-934.958, -934.958], mean action: 2.000 [2.000, 2.000],  loss: 2640967.500000, mae: 4259.428711, mean_q: -2747.344238
 2873/5000: episode: 2873, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3782.841, mean reward: -3782.841 [-3782.841, -3782.841], mean action: 2.000 [2.000, 2.000],  loss: 2501821.500000, mae: 4313.693359, mean_q: -2767.645752
 2874/5000: episode: 2874, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1500.546, mean reward: -1500.546 [-1500.546, -1500.546], mean action: 3.000 [3.000, 3.000],  loss: 2995988.500000, mae: 4329.801270, mean_q: -2773.288818
 2875/5000: episode: 2875, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -4366.169, mean reward: -4366.169 [-4366.169, -4366.169], mean action: 2.000 [2.000, 2.000],  loss: 2257187.500000, mae: 4322.572754, mean_q: -2772.692627
 2876/5000: episode: 2876, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2246.622, mean reward: -2246.622 [-2246.622, -2246.622], mean action: 1.000 [1.000, 1.000],  loss: 1454804.500000, mae: 4265.523926, mean_q: -2775.820801
 2877/5000: episode: 2877, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1525.387, mean reward: -1525.387 [-1525.387, -1525.387], mean action: 2.000 [2.000, 2.000],  loss: 1472290.750000, mae: 4243.512695, mean_q: -2788.076172
 2878/5000: episode: 2878, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -656.539, mean reward: -656.539 [-656.539, -656.539], mean action: 2.000 [2.000, 2.000],  loss: 3315725.750000, mae: 4334.418945, mean_q: -2778.640137
 2879/5000: episode: 2879, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -707.926, mean reward: -707.926 [-707.926, -707.926], mean action: 2.000 [2.000, 2.000],  loss: 3398469.500000, mae: 4393.289551, mean_q: -2764.370361
 2880/5000: episode: 2880, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2110.155, mean reward: -2110.155 [-2110.155, -2110.155], mean action: 2.000 [2.000, 2.000],  loss: 2091831.875000, mae: 4236.035156, mean_q: -2751.916992
 2881/5000: episode: 2881, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -391.448, mean reward: -391.448 [-391.448, -391.448], mean action: 2.000 [2.000, 2.000],  loss: 2862102.500000, mae: 4302.783691, mean_q: -2761.051758
 2882/5000: episode: 2882, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2696.317, mean reward: -2696.317 [-2696.317, -2696.317], mean action: 2.000 [2.000, 2.000],  loss: 1953945.000000, mae: 4249.423828, mean_q: -2767.822754
 2883/5000: episode: 2883, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6573.673, mean reward: -6573.673 [-6573.673, -6573.673], mean action: 2.000 [2.000, 2.000],  loss: 3008503.500000, mae: 4329.129883, mean_q: -2741.365967
 2884/5000: episode: 2884, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2780.474, mean reward: -2780.474 [-2780.474, -2780.474], mean action: 2.000 [2.000, 2.000],  loss: 2423586.500000, mae: 4295.749023, mean_q: -2742.393799
 2885/5000: episode: 2885, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1122.017, mean reward: -1122.017 [-1122.017, -1122.017], mean action: 2.000 [2.000, 2.000],  loss: 2465837.000000, mae: 4298.057129, mean_q: -2732.695068
 2886/5000: episode: 2886, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1055.326, mean reward: -1055.326 [-1055.326, -1055.326], mean action: 2.000 [2.000, 2.000],  loss: 3652258.000000, mae: 4395.092285, mean_q: -2705.574219
 2887/5000: episode: 2887, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -246.808, mean reward: -246.808 [-246.808, -246.808], mean action: 2.000 [2.000, 2.000],  loss: 3597398.750000, mae: 4401.411133, mean_q: -2693.215576
 2888/5000: episode: 2888, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1315.104, mean reward: -1315.104 [-1315.104, -1315.104], mean action: 2.000 [2.000, 2.000],  loss: 2275666.000000, mae: 4300.590820, mean_q: -2687.167969
 2889/5000: episode: 2889, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -500.757, mean reward: -500.757 [-500.757, -500.757], mean action: 2.000 [2.000, 2.000],  loss: 2470124.250000, mae: 4270.035156, mean_q: -2671.946777
 2890/5000: episode: 2890, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6894.302, mean reward: -6894.302 [-6894.302, -6894.302], mean action: 2.000 [2.000, 2.000],  loss: 1633016.625000, mae: 4231.156738, mean_q: -2657.415039
 2891/5000: episode: 2891, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4176.676, mean reward: -4176.676 [-4176.676, -4176.676], mean action: 2.000 [2.000, 2.000],  loss: 2329356.500000, mae: 4267.061035, mean_q: -2654.050781
 2892/5000: episode: 2892, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3523.440, mean reward: -3523.440 [-3523.440, -3523.440], mean action: 2.000 [2.000, 2.000],  loss: 2505812.500000, mae: 4276.449219, mean_q: -2622.292969
 2893/5000: episode: 2893, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -76.721, mean reward: -76.721 [-76.721, -76.721], mean action: 2.000 [2.000, 2.000],  loss: 3059364.500000, mae: 4355.013184, mean_q: -2630.750488
 2894/5000: episode: 2894, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4427.096, mean reward: -4427.096 [-4427.096, -4427.096], mean action: 2.000 [2.000, 2.000],  loss: 2360583.750000, mae: 4233.130859, mean_q: -2625.290039
 2895/5000: episode: 2895, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1316.770, mean reward: -1316.770 [-1316.770, -1316.770], mean action: 2.000 [2.000, 2.000],  loss: 1733931.500000, mae: 4212.315430, mean_q: -2624.839600
 2896/5000: episode: 2896, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1338.911, mean reward: -1338.911 [-1338.911, -1338.911], mean action: 2.000 [2.000, 2.000],  loss: 3043625.750000, mae: 4284.547852, mean_q: -2632.937744
 2897/5000: episode: 2897, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4186.750, mean reward: -4186.750 [-4186.750, -4186.750], mean action: 2.000 [2.000, 2.000],  loss: 2695470.750000, mae: 4299.539551, mean_q: -2634.828369
 2898/5000: episode: 2898, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2400.441, mean reward: -2400.441 [-2400.441, -2400.441], mean action: 2.000 [2.000, 2.000],  loss: 2627023.500000, mae: 4146.157715, mean_q: -2649.325439
 2899/5000: episode: 2899, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2248.370, mean reward: -2248.370 [-2248.370, -2248.370], mean action: 2.000 [2.000, 2.000],  loss: 2019267.500000, mae: 4245.336914, mean_q: -2634.929688
 2900/5000: episode: 2900, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -366.333, mean reward: -366.333 [-366.333, -366.333], mean action: 2.000 [2.000, 2.000],  loss: 4051097.250000, mae: 4354.949707, mean_q: -2635.710938
 2901/5000: episode: 2901, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2205.907, mean reward: -2205.907 [-2205.907, -2205.907], mean action: 0.000 [0.000, 0.000],  loss: 2221824.500000, mae: 4259.173828, mean_q: -2644.062500
 2902/5000: episode: 2902, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -9356.726, mean reward: -9356.726 [-9356.726, -9356.726], mean action: 2.000 [2.000, 2.000],  loss: 2460070.000000, mae: 4297.162109, mean_q: -2665.971191
 2903/5000: episode: 2903, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -8723.031, mean reward: -8723.031 [-8723.031, -8723.031], mean action: 1.000 [1.000, 1.000],  loss: 1725139.875000, mae: 4196.415039, mean_q: -2676.057129
 2904/5000: episode: 2904, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2702.431, mean reward: -2702.431 [-2702.431, -2702.431], mean action: 2.000 [2.000, 2.000],  loss: 1702851.000000, mae: 4238.271484, mean_q: -2664.278809
 2905/5000: episode: 2905, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4075.118, mean reward: -4075.118 [-4075.118, -4075.118], mean action: 2.000 [2.000, 2.000],  loss: 3338608.500000, mae: 4351.893066, mean_q: -2682.681641
 2906/5000: episode: 2906, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5331.131, mean reward: -5331.131 [-5331.131, -5331.131], mean action: 2.000 [2.000, 2.000],  loss: 2218988.250000, mae: 4320.120117, mean_q: -2695.564453
 2907/5000: episode: 2907, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -771.043, mean reward: -771.043 [-771.043, -771.043], mean action: 2.000 [2.000, 2.000],  loss: 3799579.500000, mae: 4400.915039, mean_q: -2691.151367
 2908/5000: episode: 2908, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2791.915, mean reward: -2791.915 [-2791.915, -2791.915], mean action: 2.000 [2.000, 2.000],  loss: 3739510.000000, mae: 4339.421875, mean_q: -2705.625977
 2909/5000: episode: 2909, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3342.698, mean reward: -3342.698 [-3342.698, -3342.698], mean action: 2.000 [2.000, 2.000],  loss: 2086949.250000, mae: 4290.927734, mean_q: -2702.792969
 2910/5000: episode: 2910, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6150.125, mean reward: -6150.125 [-6150.125, -6150.125], mean action: 3.000 [3.000, 3.000],  loss: 2358583.500000, mae: 4280.457031, mean_q: -2711.269043
 2911/5000: episode: 2911, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2304.925, mean reward: -2304.925 [-2304.925, -2304.925], mean action: 1.000 [1.000, 1.000],  loss: 4682280.000000, mae: 4456.892578, mean_q: -2708.508301
 2912/5000: episode: 2912, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2880.556, mean reward: -2880.556 [-2880.556, -2880.556], mean action: 2.000 [2.000, 2.000],  loss: 3021474.500000, mae: 4324.808105, mean_q: -2714.043457
 2913/5000: episode: 2913, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1035.494, mean reward: -1035.494 [-1035.494, -1035.494], mean action: 2.000 [2.000, 2.000],  loss: 3727428.250000, mae: 4443.511719, mean_q: -2703.576416
 2914/5000: episode: 2914, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -156.854, mean reward: -156.854 [-156.854, -156.854], mean action: 2.000 [2.000, 2.000],  loss: 2711684.250000, mae: 4344.062500, mean_q: -2700.595703
 2915/5000: episode: 2915, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4501.440, mean reward: -4501.440 [-4501.440, -4501.440], mean action: 2.000 [2.000, 2.000],  loss: 1283141.000000, mae: 4256.850586, mean_q: -2693.082764
 2916/5000: episode: 2916, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5959.951, mean reward: -5959.951 [-5959.951, -5959.951], mean action: 2.000 [2.000, 2.000],  loss: 2643792.000000, mae: 4408.116211, mean_q: -2694.581055
 2917/5000: episode: 2917, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1709.376, mean reward: -1709.376 [-1709.376, -1709.376], mean action: 2.000 [2.000, 2.000],  loss: 2817410.000000, mae: 4312.276367, mean_q: -2687.376953
 2918/5000: episode: 2918, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -3421.466, mean reward: -3421.466 [-3421.466, -3421.466], mean action: 2.000 [2.000, 2.000],  loss: 2854983.250000, mae: 4182.863281, mean_q: -2683.899414
 2919/5000: episode: 2919, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -1269.913, mean reward: -1269.913 [-1269.913, -1269.913], mean action: 2.000 [2.000, 2.000],  loss: 2888287.000000, mae: 4328.259766, mean_q: -2689.607178
 2920/5000: episode: 2920, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1575.444, mean reward: -1575.444 [-1575.444, -1575.444], mean action: 2.000 [2.000, 2.000],  loss: 2798182.750000, mae: 4317.720703, mean_q: -2711.592773
 2921/5000: episode: 2921, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5460.709, mean reward: -5460.709 [-5460.709, -5460.709], mean action: 3.000 [3.000, 3.000],  loss: 3511085.000000, mae: 4384.049805, mean_q: -2709.185547
 2922/5000: episode: 2922, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3455.265, mean reward: -3455.265 [-3455.265, -3455.265], mean action: 2.000 [2.000, 2.000],  loss: 3374025.750000, mae: 4342.708984, mean_q: -2712.034668
 2923/5000: episode: 2923, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -4457.451, mean reward: -4457.451 [-4457.451, -4457.451], mean action: 2.000 [2.000, 2.000],  loss: 3100409.000000, mae: 4344.891113, mean_q: -2729.311279
 2924/5000: episode: 2924, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1082.296, mean reward: -1082.296 [-1082.296, -1082.296], mean action: 2.000 [2.000, 2.000],  loss: 2757644.500000, mae: 4326.873535, mean_q: -2727.623535
 2925/5000: episode: 2925, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -12959.198, mean reward: -12959.198 [-12959.198, -12959.198], mean action: 3.000 [3.000, 3.000],  loss: 4732082.500000, mae: 4379.358887, mean_q: -2741.966797
 2926/5000: episode: 2926, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -225.314, mean reward: -225.314 [-225.314, -225.314], mean action: 2.000 [2.000, 2.000],  loss: 2452774.750000, mae: 4336.419922, mean_q: -2753.685547
 2927/5000: episode: 2927, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1115.784, mean reward: -1115.784 [-1115.784, -1115.784], mean action: 2.000 [2.000, 2.000],  loss: 2964798.750000, mae: 4280.936523, mean_q: -2752.213379
 2928/5000: episode: 2928, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -160.494, mean reward: -160.494 [-160.494, -160.494], mean action: 2.000 [2.000, 2.000],  loss: 3166094.500000, mae: 4397.890625, mean_q: -2758.636475
 2929/5000: episode: 2929, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -9119.443, mean reward: -9119.443 [-9119.443, -9119.443], mean action: 2.000 [2.000, 2.000],  loss: 2200046.750000, mae: 4244.690918, mean_q: -2747.553955
 2930/5000: episode: 2930, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4636.725, mean reward: -4636.725 [-4636.725, -4636.725], mean action: 2.000 [2.000, 2.000],  loss: 3127912.500000, mae: 4271.842285, mean_q: -2740.517578
 2931/5000: episode: 2931, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2352.904, mean reward: -2352.904 [-2352.904, -2352.904], mean action: 2.000 [2.000, 2.000],  loss: 2803063.500000, mae: 4349.907715, mean_q: -2745.267090
 2932/5000: episode: 2932, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1745.447, mean reward: -1745.447 [-1745.447, -1745.447], mean action: 2.000 [2.000, 2.000],  loss: 1930967.500000, mae: 4247.903320, mean_q: -2720.245605
 2933/5000: episode: 2933, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -410.383, mean reward: -410.383 [-410.383, -410.383], mean action: 2.000 [2.000, 2.000],  loss: 2595331.750000, mae: 4261.017578, mean_q: -2721.035889
 2934/5000: episode: 2934, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9393.422, mean reward: -9393.422 [-9393.422, -9393.422], mean action: 0.000 [0.000, 0.000],  loss: 2667120.500000, mae: 4349.362793, mean_q: -2726.865723
 2935/5000: episode: 2935, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1077.235, mean reward: -1077.235 [-1077.235, -1077.235], mean action: 2.000 [2.000, 2.000],  loss: 2634980.000000, mae: 4275.419922, mean_q: -2704.951904
 2936/5000: episode: 2936, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -7023.852, mean reward: -7023.852 [-7023.852, -7023.852], mean action: 2.000 [2.000, 2.000],  loss: 2248473.750000, mae: 4267.237793, mean_q: -2701.055664
 2937/5000: episode: 2937, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -10076.128, mean reward: -10076.128 [-10076.128, -10076.128], mean action: 2.000 [2.000, 2.000],  loss: 2054096.500000, mae: 4238.124023, mean_q: -2701.845703
 2938/5000: episode: 2938, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -549.786, mean reward: -549.786 [-549.786, -549.786], mean action: 2.000 [2.000, 2.000],  loss: 3778856.500000, mae: 4427.786133, mean_q: -2688.738770
 2939/5000: episode: 2939, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2235.842, mean reward: -2235.842 [-2235.842, -2235.842], mean action: 2.000 [2.000, 2.000],  loss: 2432488.750000, mae: 4362.814453, mean_q: -2684.394287
 2940/5000: episode: 2940, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1277.265, mean reward: -1277.265 [-1277.265, -1277.265], mean action: 2.000 [2.000, 2.000],  loss: 3290579.000000, mae: 4293.029297, mean_q: -2684.144775
 2941/5000: episode: 2941, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6160.598, mean reward: -6160.598 [-6160.598, -6160.598], mean action: 2.000 [2.000, 2.000],  loss: 3849025.000000, mae: 4378.204590, mean_q: -2663.677734
 2942/5000: episode: 2942, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -785.177, mean reward: -785.177 [-785.177, -785.177], mean action: 2.000 [2.000, 2.000],  loss: 1767030.875000, mae: 4253.883789, mean_q: -2661.792480
 2943/5000: episode: 2943, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -831.780, mean reward: -831.780 [-831.780, -831.780], mean action: 2.000 [2.000, 2.000],  loss: 3649730.000000, mae: 4320.443359, mean_q: -2658.430664
 2944/5000: episode: 2944, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -1576.605, mean reward: -1576.605 [-1576.605, -1576.605], mean action: 2.000 [2.000, 2.000],  loss: 1647824.500000, mae: 4279.317871, mean_q: -2643.768066
 2945/5000: episode: 2945, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1505.121, mean reward: -1505.121 [-1505.121, -1505.121], mean action: 2.000 [2.000, 2.000],  loss: 2151480.750000, mae: 4324.116211, mean_q: -2630.630127
 2946/5000: episode: 2946, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -7675.422, mean reward: -7675.422 [-7675.422, -7675.422], mean action: 2.000 [2.000, 2.000],  loss: 2996925.750000, mae: 4416.083008, mean_q: -2620.526123
 2947/5000: episode: 2947, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1407.962, mean reward: -1407.962 [-1407.962, -1407.962], mean action: 2.000 [2.000, 2.000],  loss: 3636774.500000, mae: 4411.433594, mean_q: -2609.204590
 2948/5000: episode: 2948, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -951.271, mean reward: -951.271 [-951.271, -951.271], mean action: 2.000 [2.000, 2.000],  loss: 4162149.500000, mae: 4431.117188, mean_q: -2596.827637
 2949/5000: episode: 2949, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3930.562, mean reward: -3930.562 [-3930.562, -3930.562], mean action: 2.000 [2.000, 2.000],  loss: 2964051.250000, mae: 4315.292969, mean_q: -2583.458008
 2950/5000: episode: 2950, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -906.311, mean reward: -906.311 [-906.311, -906.311], mean action: 2.000 [2.000, 2.000],  loss: 2972421.750000, mae: 4352.165039, mean_q: -2587.076660
 2951/5000: episode: 2951, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4814.259, mean reward: -4814.259 [-4814.259, -4814.259], mean action: 2.000 [2.000, 2.000],  loss: 3187736.000000, mae: 4397.718750, mean_q: -2592.074219
 2952/5000: episode: 2952, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1368.081, mean reward: -1368.081 [-1368.081, -1368.081], mean action: 2.000 [2.000, 2.000],  loss: 3863876.750000, mae: 4472.060059, mean_q: -2606.339600
 2953/5000: episode: 2953, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1806.538, mean reward: -1806.538 [-1806.538, -1806.538], mean action: 2.000 [2.000, 2.000],  loss: 2483026.250000, mae: 4305.679199, mean_q: -2611.794922
 2954/5000: episode: 2954, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -88.937, mean reward: -88.937 [-88.937, -88.937], mean action: 2.000 [2.000, 2.000],  loss: 3183676.500000, mae: 4305.414551, mean_q: -2607.595703
 2955/5000: episode: 2955, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3578.382, mean reward: -3578.382 [-3578.382, -3578.382], mean action: 2.000 [2.000, 2.000],  loss: 4131580.500000, mae: 4293.278320, mean_q: -2611.845215
 2956/5000: episode: 2956, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4742.579, mean reward: -4742.579 [-4742.579, -4742.579], mean action: 2.000 [2.000, 2.000],  loss: 1963039.250000, mae: 4260.653320, mean_q: -2621.770508
 2957/5000: episode: 2957, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4633.032, mean reward: -4633.032 [-4633.032, -4633.032], mean action: 2.000 [2.000, 2.000],  loss: 1811918.500000, mae: 4316.066406, mean_q: -2616.534180
 2958/5000: episode: 2958, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1640.401, mean reward: -1640.401 [-1640.401, -1640.401], mean action: 2.000 [2.000, 2.000],  loss: 2344460.000000, mae: 4315.528320, mean_q: -2633.977051
 2959/5000: episode: 2959, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1667.410, mean reward: -1667.410 [-1667.410, -1667.410], mean action: 2.000 [2.000, 2.000],  loss: 2387333.500000, mae: 4302.798828, mean_q: -2615.151611
 2960/5000: episode: 2960, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -8085.068, mean reward: -8085.068 [-8085.068, -8085.068], mean action: 0.000 [0.000, 0.000],  loss: 3211739.500000, mae: 4413.341797, mean_q: -2610.635742
 2961/5000: episode: 2961, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3041.955, mean reward: -3041.955 [-3041.955, -3041.955], mean action: 2.000 [2.000, 2.000],  loss: 2206115.750000, mae: 4332.190430, mean_q: -2618.208740
 2962/5000: episode: 2962, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1746.128, mean reward: -1746.128 [-1746.128, -1746.128], mean action: 2.000 [2.000, 2.000],  loss: 3542904.250000, mae: 4332.185547, mean_q: -2622.570801
 2963/5000: episode: 2963, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4079.303, mean reward: -4079.303 [-4079.303, -4079.303], mean action: 2.000 [2.000, 2.000],  loss: 3438556.750000, mae: 4403.289551, mean_q: -2641.960449
 2964/5000: episode: 2964, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -542.269, mean reward: -542.269 [-542.269, -542.269], mean action: 2.000 [2.000, 2.000],  loss: 2638787.750000, mae: 4239.117188, mean_q: -2625.369629
 2965/5000: episode: 2965, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -10435.976, mean reward: -10435.976 [-10435.976, -10435.976], mean action: 2.000 [2.000, 2.000],  loss: 2920232.000000, mae: 4296.764160, mean_q: -2649.385986
 2966/5000: episode: 2966, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -927.850, mean reward: -927.850 [-927.850, -927.850], mean action: 2.000 [2.000, 2.000],  loss: 3933872.250000, mae: 4445.406250, mean_q: -2650.353271
 2967/5000: episode: 2967, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2423.614, mean reward: -2423.614 [-2423.614, -2423.614], mean action: 2.000 [2.000, 2.000],  loss: 2924525.000000, mae: 4348.532227, mean_q: -2647.326660
 2968/5000: episode: 2968, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5423.773, mean reward: -5423.773 [-5423.773, -5423.773], mean action: 2.000 [2.000, 2.000],  loss: 4063137.500000, mae: 4348.726562, mean_q: -2647.661621
 2969/5000: episode: 2969, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -39.730, mean reward: -39.730 [-39.730, -39.730], mean action: 2.000 [2.000, 2.000],  loss: 2505031.500000, mae: 4258.238281, mean_q: -2638.560791
 2970/5000: episode: 2970, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -923.856, mean reward: -923.856 [-923.856, -923.856], mean action: 2.000 [2.000, 2.000],  loss: 1772851.750000, mae: 4255.655273, mean_q: -2642.739746
 2971/5000: episode: 2971, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5222.845, mean reward: -5222.845 [-5222.845, -5222.845], mean action: 2.000 [2.000, 2.000],  loss: 2783113.750000, mae: 4294.549805, mean_q: -2635.270264
 2972/5000: episode: 2972, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -981.369, mean reward: -981.369 [-981.369, -981.369], mean action: 2.000 [2.000, 2.000],  loss: 1930862.625000, mae: 4246.783691, mean_q: -2640.538574
 2973/5000: episode: 2973, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3867.981, mean reward: -3867.981 [-3867.981, -3867.981], mean action: 2.000 [2.000, 2.000],  loss: 1682400.375000, mae: 4241.266113, mean_q: -2610.599609
 2974/5000: episode: 2974, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2838.532, mean reward: -2838.532 [-2838.532, -2838.532], mean action: 2.000 [2.000, 2.000],  loss: 3188528.500000, mae: 4379.815430, mean_q: -2611.501953
 2975/5000: episode: 2975, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -218.643, mean reward: -218.643 [-218.643, -218.643], mean action: 2.000 [2.000, 2.000],  loss: 2201721.000000, mae: 4304.884766, mean_q: -2600.237793
 2976/5000: episode: 2976, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -67.443, mean reward: -67.443 [-67.443, -67.443], mean action: 2.000 [2.000, 2.000],  loss: 2278744.750000, mae: 4227.985352, mean_q: -2587.066406
 2977/5000: episode: 2977, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2085.421, mean reward: -2085.421 [-2085.421, -2085.421], mean action: 2.000 [2.000, 2.000],  loss: 1588117.000000, mae: 4225.923340, mean_q: -2589.336914
 2978/5000: episode: 2978, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5194.164, mean reward: -5194.164 [-5194.164, -5194.164], mean action: 2.000 [2.000, 2.000],  loss: 4079780.750000, mae: 4435.927734, mean_q: -2580.934326
 2979/5000: episode: 2979, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -1099.612, mean reward: -1099.612 [-1099.612, -1099.612], mean action: 2.000 [2.000, 2.000],  loss: 3406453.500000, mae: 4329.887695, mean_q: -2582.243896
 2980/5000: episode: 2980, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2694.521, mean reward: -2694.521 [-2694.521, -2694.521], mean action: 2.000 [2.000, 2.000],  loss: 3019750.500000, mae: 4279.843750, mean_q: -2591.994629
 2981/5000: episode: 2981, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -991.371, mean reward: -991.371 [-991.371, -991.371], mean action: 2.000 [2.000, 2.000],  loss: 2773896.500000, mae: 4381.603516, mean_q: -2596.492676
 2982/5000: episode: 2982, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4435.764, mean reward: -4435.764 [-4435.764, -4435.764], mean action: 2.000 [2.000, 2.000],  loss: 4242627.000000, mae: 4377.777344, mean_q: -2605.144531
 2983/5000: episode: 2983, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2486.192, mean reward: -2486.192 [-2486.192, -2486.192], mean action: 2.000 [2.000, 2.000],  loss: 3167563.250000, mae: 4283.997070, mean_q: -2613.793457
 2984/5000: episode: 2984, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -606.438, mean reward: -606.438 [-606.438, -606.438], mean action: 1.000 [1.000, 1.000],  loss: 3994837.000000, mae: 4514.251953, mean_q: -2631.356445
 2985/5000: episode: 2985, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -771.201, mean reward: -771.201 [-771.201, -771.201], mean action: 2.000 [2.000, 2.000],  loss: 2813144.750000, mae: 4361.483398, mean_q: -2636.637207
 2986/5000: episode: 2986, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2861.030, mean reward: -2861.030 [-2861.030, -2861.030], mean action: 1.000 [1.000, 1.000],  loss: 2716755.250000, mae: 4301.040039, mean_q: -2627.490723
 2987/5000: episode: 2987, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -190.599, mean reward: -190.599 [-190.599, -190.599], mean action: 1.000 [1.000, 1.000],  loss: 2820457.500000, mae: 4331.831055, mean_q: -2636.282959
 2988/5000: episode: 2988, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3268.403, mean reward: -3268.403 [-3268.403, -3268.403], mean action: 2.000 [2.000, 2.000],  loss: 4012796.750000, mae: 4364.896484, mean_q: -2620.510254
 2989/5000: episode: 2989, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1665.808, mean reward: -1665.808 [-1665.808, -1665.808], mean action: 2.000 [2.000, 2.000],  loss: 1898383.750000, mae: 4233.684570, mean_q: -2628.279785
 2990/5000: episode: 2990, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1937.962, mean reward: -1937.962 [-1937.962, -1937.962], mean action: 2.000 [2.000, 2.000],  loss: 2481623.000000, mae: 4243.840820, mean_q: -2601.794922
 2991/5000: episode: 2991, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -2014.508, mean reward: -2014.508 [-2014.508, -2014.508], mean action: 2.000 [2.000, 2.000],  loss: 4877691.000000, mae: 4429.518066, mean_q: -2620.306396
 2992/5000: episode: 2992, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -152.012, mean reward: -152.012 [-152.012, -152.012], mean action: 2.000 [2.000, 2.000],  loss: 4279430.000000, mae: 4432.948730, mean_q: -2629.094238
 2993/5000: episode: 2993, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -151.023, mean reward: -151.023 [-151.023, -151.023], mean action: 2.000 [2.000, 2.000],  loss: 2148335.000000, mae: 4187.711426, mean_q: -2620.984863
 2994/5000: episode: 2994, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1651.893, mean reward: -1651.893 [-1651.893, -1651.893], mean action: 2.000 [2.000, 2.000],  loss: 2756029.750000, mae: 4295.269531, mean_q: -2639.918213
 2995/5000: episode: 2995, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1234.146, mean reward: -1234.146 [-1234.146, -1234.146], mean action: 2.000 [2.000, 2.000],  loss: 3530693.000000, mae: 4194.562500, mean_q: -2642.811523
 2996/5000: episode: 2996, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4855.688, mean reward: -4855.688 [-4855.688, -4855.688], mean action: 2.000 [2.000, 2.000],  loss: 3214206.500000, mae: 4334.700195, mean_q: -2646.114502
 2997/5000: episode: 2997, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1614.195, mean reward: -1614.195 [-1614.195, -1614.195], mean action: 2.000 [2.000, 2.000],  loss: 2682897.500000, mae: 4242.597168, mean_q: -2659.027832
 2998/5000: episode: 2998, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3389.634, mean reward: -3389.634 [-3389.634, -3389.634], mean action: 2.000 [2.000, 2.000],  loss: 4687273.000000, mae: 4337.648438, mean_q: -2672.056396
 2999/5000: episode: 2999, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2515.858, mean reward: -2515.858 [-2515.858, -2515.858], mean action: 2.000 [2.000, 2.000],  loss: 2404166.750000, mae: 4167.485352, mean_q: -2664.697266
 3000/5000: episode: 3000, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4126.634, mean reward: -4126.634 [-4126.634, -4126.634], mean action: 2.000 [2.000, 2.000],  loss: 2904678.500000, mae: 4215.303711, mean_q: -2665.086426
 3001/5000: episode: 3001, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4219.076, mean reward: -4219.076 [-4219.076, -4219.076], mean action: 2.000 [2.000, 2.000],  loss: 2685684.750000, mae: 4289.651367, mean_q: -2685.259033
 3002/5000: episode: 3002, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1348.078, mean reward: -1348.078 [-1348.078, -1348.078], mean action: 2.000 [2.000, 2.000],  loss: 2599066.500000, mae: 4175.350586, mean_q: -2675.255859
 3003/5000: episode: 3003, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1055.536, mean reward: -1055.536 [-1055.536, -1055.536], mean action: 1.000 [1.000, 1.000],  loss: 2396944.000000, mae: 4213.430664, mean_q: -2656.689453
 3004/5000: episode: 3004, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3190.342, mean reward: -3190.342 [-3190.342, -3190.342], mean action: 2.000 [2.000, 2.000],  loss: 2942004.000000, mae: 4143.022949, mean_q: -2652.366455
 3005/5000: episode: 3005, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1303.835, mean reward: -1303.835 [-1303.835, -1303.835], mean action: 2.000 [2.000, 2.000],  loss: 2090954.250000, mae: 4085.950195, mean_q: -2639.989502
 3006/5000: episode: 3006, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -4367.332, mean reward: -4367.332 [-4367.332, -4367.332], mean action: 2.000 [2.000, 2.000],  loss: 3223091.000000, mae: 4195.395508, mean_q: -2628.177734
 3007/5000: episode: 3007, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -461.410, mean reward: -461.410 [-461.410, -461.410], mean action: 2.000 [2.000, 2.000],  loss: 3568567.250000, mae: 4209.139160, mean_q: -2619.318115
 3008/5000: episode: 3008, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3069.305, mean reward: -3069.305 [-3069.305, -3069.305], mean action: 2.000 [2.000, 2.000],  loss: 2725729.750000, mae: 4188.956543, mean_q: -2614.619629
 3009/5000: episode: 3009, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2994.001, mean reward: -2994.001 [-2994.001, -2994.001], mean action: 2.000 [2.000, 2.000],  loss: 3174095.250000, mae: 4190.737305, mean_q: -2609.425537
 3010/5000: episode: 3010, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1033.241, mean reward: -1033.241 [-1033.241, -1033.241], mean action: 2.000 [2.000, 2.000],  loss: 4333070.000000, mae: 4185.449219, mean_q: -2604.430420
 3011/5000: episode: 3011, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -12213.425, mean reward: -12213.425 [-12213.425, -12213.425], mean action: 0.000 [0.000, 0.000],  loss: 2040777.000000, mae: 4122.005371, mean_q: -2584.380127
 3012/5000: episode: 3012, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4412.393, mean reward: -4412.393 [-4412.393, -4412.393], mean action: 2.000 [2.000, 2.000],  loss: 3000263.500000, mae: 4119.937500, mean_q: -2602.262451
 3013/5000: episode: 3013, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1641.077, mean reward: -1641.077 [-1641.077, -1641.077], mean action: 2.000 [2.000, 2.000],  loss: 3000420.000000, mae: 4187.444336, mean_q: -2573.501465
 3014/5000: episode: 3014, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2091.821, mean reward: -2091.821 [-2091.821, -2091.821], mean action: 2.000 [2.000, 2.000],  loss: 1926086.375000, mae: 4124.872559, mean_q: -2573.275879
 3015/5000: episode: 3015, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4178.081, mean reward: -4178.081 [-4178.081, -4178.081], mean action: 2.000 [2.000, 2.000],  loss: 2730081.250000, mae: 4007.173340, mean_q: -2578.419922
 3016/5000: episode: 3016, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4401.654, mean reward: -4401.654 [-4401.654, -4401.654], mean action: 2.000 [2.000, 2.000],  loss: 2049184.250000, mae: 4085.494629, mean_q: -2580.928467
 3017/5000: episode: 3017, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2515.527, mean reward: -2515.527 [-2515.527, -2515.527], mean action: 2.000 [2.000, 2.000],  loss: 2431733.250000, mae: 3971.148682, mean_q: -2566.345215
 3018/5000: episode: 3018, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2722.353, mean reward: -2722.353 [-2722.353, -2722.353], mean action: 2.000 [2.000, 2.000],  loss: 2775505.000000, mae: 4058.379150, mean_q: -2570.541748
 3019/5000: episode: 3019, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2609.376, mean reward: -2609.376 [-2609.376, -2609.376], mean action: 2.000 [2.000, 2.000],  loss: 3556620.500000, mae: 4138.752930, mean_q: -2574.467285
 3020/5000: episode: 3020, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -81.879, mean reward: -81.879 [-81.879, -81.879], mean action: 2.000 [2.000, 2.000],  loss: 3093312.500000, mae: 4109.233398, mean_q: -2565.102539
 3021/5000: episode: 3021, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2696.028, mean reward: -2696.028 [-2696.028, -2696.028], mean action: 2.000 [2.000, 2.000],  loss: 2689416.000000, mae: 4038.660645, mean_q: -2577.694336
 3022/5000: episode: 3022, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5133.523, mean reward: -5133.523 [-5133.523, -5133.523], mean action: 2.000 [2.000, 2.000],  loss: 4847327.500000, mae: 4225.893555, mean_q: -2585.894043
 3023/5000: episode: 3023, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5327.803, mean reward: -5327.803 [-5327.803, -5327.803], mean action: 2.000 [2.000, 2.000],  loss: 2417084.500000, mae: 4127.465332, mean_q: -2587.468994
 3024/5000: episode: 3024, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1287.622, mean reward: -1287.622 [-1287.622, -1287.622], mean action: 2.000 [2.000, 2.000],  loss: 2327595.500000, mae: 4095.442627, mean_q: -2574.421387
 3025/5000: episode: 3025, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5715.566, mean reward: -5715.566 [-5715.566, -5715.566], mean action: 2.000 [2.000, 2.000],  loss: 2600099.250000, mae: 4129.399414, mean_q: -2568.146484
 3026/5000: episode: 3026, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1095.871, mean reward: -1095.871 [-1095.871, -1095.871], mean action: 3.000 [3.000, 3.000],  loss: 3596171.000000, mae: 4125.117676, mean_q: -2575.750000
 3027/5000: episode: 3027, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3459.460, mean reward: -3459.460 [-3459.460, -3459.460], mean action: 2.000 [2.000, 2.000],  loss: 2196798.500000, mae: 4100.182129, mean_q: -2592.027832
 3028/5000: episode: 3028, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4740.708, mean reward: -4740.708 [-4740.708, -4740.708], mean action: 2.000 [2.000, 2.000],  loss: 2577236.000000, mae: 4138.333008, mean_q: -2571.486328
 3029/5000: episode: 3029, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3429.307, mean reward: -3429.307 [-3429.307, -3429.307], mean action: 2.000 [2.000, 2.000],  loss: 2578472.000000, mae: 4114.662598, mean_q: -2572.271973
 3030/5000: episode: 3030, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1686.336, mean reward: -1686.336 [-1686.336, -1686.336], mean action: 2.000 [2.000, 2.000],  loss: 2210039.000000, mae: 4117.875977, mean_q: -2578.055420
 3031/5000: episode: 3031, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5294.968, mean reward: -5294.968 [-5294.968, -5294.968], mean action: 2.000 [2.000, 2.000],  loss: 3411300.750000, mae: 4181.852051, mean_q: -2567.218994
 3032/5000: episode: 3032, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -111.846, mean reward: -111.846 [-111.846, -111.846], mean action: 2.000 [2.000, 2.000],  loss: 2026921.250000, mae: 4051.743164, mean_q: -2571.063232
 3033/5000: episode: 3033, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -807.759, mean reward: -807.759 [-807.759, -807.759], mean action: 2.000 [2.000, 2.000],  loss: 1406306.500000, mae: 4050.302246, mean_q: -2556.004395
 3034/5000: episode: 3034, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1401.885, mean reward: -1401.885 [-1401.885, -1401.885], mean action: 2.000 [2.000, 2.000],  loss: 3520591.250000, mae: 4138.157227, mean_q: -2557.707764
 3035/5000: episode: 3035, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -799.275, mean reward: -799.275 [-799.275, -799.275], mean action: 2.000 [2.000, 2.000],  loss: 2793521.000000, mae: 4106.019531, mean_q: -2550.951660
 3036/5000: episode: 3036, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -300.655, mean reward: -300.655 [-300.655, -300.655], mean action: 2.000 [2.000, 2.000],  loss: 1979697.750000, mae: 4132.768555, mean_q: -2548.507324
 3037/5000: episode: 3037, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -16.071, mean reward: -16.071 [-16.071, -16.071], mean action: 2.000 [2.000, 2.000],  loss: 2313872.000000, mae: 4144.094727, mean_q: -2548.184570
 3038/5000: episode: 3038, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1280.030, mean reward: -1280.030 [-1280.030, -1280.030], mean action: 2.000 [2.000, 2.000],  loss: 2205610.000000, mae: 4144.327148, mean_q: -2548.358398
 3039/5000: episode: 3039, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9209.540, mean reward: -9209.540 [-9209.540, -9209.540], mean action: 2.000 [2.000, 2.000],  loss: 2553213.000000, mae: 4056.041992, mean_q: -2539.339355
 3040/5000: episode: 3040, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2307.818, mean reward: -2307.818 [-2307.818, -2307.818], mean action: 2.000 [2.000, 2.000],  loss: 2939938.000000, mae: 4198.545898, mean_q: -2541.779785
 3041/5000: episode: 3041, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6002.904, mean reward: -6002.904 [-6002.904, -6002.904], mean action: 0.000 [0.000, 0.000],  loss: 5355116.500000, mae: 4259.934570, mean_q: -2530.939453
 3042/5000: episode: 3042, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -983.221, mean reward: -983.221 [-983.221, -983.221], mean action: 2.000 [2.000, 2.000],  loss: 2084902.875000, mae: 4150.300781, mean_q: -2556.384277
 3043/5000: episode: 3043, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2722.582, mean reward: -2722.582 [-2722.582, -2722.582], mean action: 3.000 [3.000, 3.000],  loss: 3851800.500000, mae: 4320.969238, mean_q: -2557.538574
 3044/5000: episode: 3044, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -46.201, mean reward: -46.201 [-46.201, -46.201], mean action: 2.000 [2.000, 2.000],  loss: 3460047.500000, mae: 4225.151367, mean_q: -2576.201904
 3045/5000: episode: 3045, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2239.815, mean reward: -2239.815 [-2239.815, -2239.815], mean action: 2.000 [2.000, 2.000],  loss: 3029179.500000, mae: 4224.822266, mean_q: -2582.428955
 3046/5000: episode: 3046, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -27.381, mean reward: -27.381 [-27.381, -27.381], mean action: 2.000 [2.000, 2.000],  loss: 3086620.000000, mae: 4213.659180, mean_q: -2601.606201
 3047/5000: episode: 3047, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3605.881, mean reward: -3605.881 [-3605.881, -3605.881], mean action: 2.000 [2.000, 2.000],  loss: 3748140.500000, mae: 4168.881836, mean_q: -2582.461914
 3048/5000: episode: 3048, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2539.891, mean reward: -2539.891 [-2539.891, -2539.891], mean action: 2.000 [2.000, 2.000],  loss: 3166170.750000, mae: 4129.920898, mean_q: -2605.174316
 3049/5000: episode: 3049, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7040.969, mean reward: -7040.969 [-7040.969, -7040.969], mean action: 2.000 [2.000, 2.000],  loss: 4023447.000000, mae: 4262.994141, mean_q: -2608.361816
 3050/5000: episode: 3050, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -415.813, mean reward: -415.813 [-415.813, -415.813], mean action: 2.000 [2.000, 2.000],  loss: 2460398.000000, mae: 4273.450195, mean_q: -2633.166260
 3051/5000: episode: 3051, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1505.191, mean reward: -1505.191 [-1505.191, -1505.191], mean action: 3.000 [3.000, 3.000],  loss: 2613682.000000, mae: 4176.738770, mean_q: -2615.520264
 3052/5000: episode: 3052, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -3377.455, mean reward: -3377.455 [-3377.455, -3377.455], mean action: 2.000 [2.000, 2.000],  loss: 1772171.750000, mae: 4172.178711, mean_q: -2623.321289
 3053/5000: episode: 3053, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -8582.520, mean reward: -8582.520 [-8582.520, -8582.520], mean action: 2.000 [2.000, 2.000],  loss: 1849000.250000, mae: 4194.866699, mean_q: -2630.857666
 3054/5000: episode: 3054, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -7195.306, mean reward: -7195.306 [-7195.306, -7195.306], mean action: 2.000 [2.000, 2.000],  loss: 2550782.000000, mae: 4228.765137, mean_q: -2640.283203
 3055/5000: episode: 3055, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2094.855, mean reward: -2094.855 [-2094.855, -2094.855], mean action: 2.000 [2.000, 2.000],  loss: 3360938.750000, mae: 4249.497070, mean_q: -2635.562988
 3056/5000: episode: 3056, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4984.943, mean reward: -4984.943 [-4984.943, -4984.943], mean action: 2.000 [2.000, 2.000],  loss: 2899136.000000, mae: 4284.554199, mean_q: -2645.318604
 3057/5000: episode: 3057, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1285.037, mean reward: -1285.037 [-1285.037, -1285.037], mean action: 2.000 [2.000, 2.000],  loss: 1894246.625000, mae: 4149.613281, mean_q: -2652.092285
 3058/5000: episode: 3058, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1586.748, mean reward: -1586.748 [-1586.748, -1586.748], mean action: 2.000 [2.000, 2.000],  loss: 1782549.250000, mae: 4156.237305, mean_q: -2637.173340
 3059/5000: episode: 3059, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1371.406, mean reward: -1371.406 [-1371.406, -1371.406], mean action: 2.000 [2.000, 2.000],  loss: 3520870.000000, mae: 4321.774414, mean_q: -2654.085938
 3060/5000: episode: 3060, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4903.489, mean reward: -4903.489 [-4903.489, -4903.489], mean action: 2.000 [2.000, 2.000],  loss: 1676537.375000, mae: 4143.584473, mean_q: -2631.916260
 3061/5000: episode: 3061, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2761.063, mean reward: -2761.063 [-2761.063, -2761.063], mean action: 2.000 [2.000, 2.000],  loss: 3778368.000000, mae: 4283.887695, mean_q: -2631.319580
 3062/5000: episode: 3062, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -11.895, mean reward: -11.895 [-11.895, -11.895], mean action: 2.000 [2.000, 2.000],  loss: 3844474.000000, mae: 4241.914551, mean_q: -2619.902344
 3063/5000: episode: 3063, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2723.333, mean reward: -2723.333 [-2723.333, -2723.333], mean action: 2.000 [2.000, 2.000],  loss: 3219504.750000, mae: 4224.529297, mean_q: -2615.128418
 3064/5000: episode: 3064, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1066.031, mean reward: -1066.031 [-1066.031, -1066.031], mean action: 2.000 [2.000, 2.000],  loss: 3990834.500000, mae: 4238.748047, mean_q: -2630.126953
 3065/5000: episode: 3065, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -489.156, mean reward: -489.156 [-489.156, -489.156], mean action: 2.000 [2.000, 2.000],  loss: 2694924.500000, mae: 4177.166016, mean_q: -2616.562988
 3066/5000: episode: 3066, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1736.899, mean reward: -1736.899 [-1736.899, -1736.899], mean action: 2.000 [2.000, 2.000],  loss: 3048930.500000, mae: 4096.166992, mean_q: -2619.795654
 3067/5000: episode: 3067, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5683.383, mean reward: -5683.383 [-5683.383, -5683.383], mean action: 2.000 [2.000, 2.000],  loss: 2351010.000000, mae: 4217.378418, mean_q: -2610.684326
 3068/5000: episode: 3068, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3054.007, mean reward: -3054.007 [-3054.007, -3054.007], mean action: 2.000 [2.000, 2.000],  loss: 1876387.500000, mae: 4099.900391, mean_q: -2592.482422
 3069/5000: episode: 3069, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5614.879, mean reward: -5614.879 [-5614.879, -5614.879], mean action: 2.000 [2.000, 2.000],  loss: 2997729.500000, mae: 4221.509766, mean_q: -2583.117676
 3070/5000: episode: 3070, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -398.883, mean reward: -398.883 [-398.883, -398.883], mean action: 2.000 [2.000, 2.000],  loss: 2956159.500000, mae: 4083.137695, mean_q: -2566.750488
 3071/5000: episode: 3071, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4101.859, mean reward: -4101.859 [-4101.859, -4101.859], mean action: 2.000 [2.000, 2.000],  loss: 2617385.000000, mae: 4119.100586, mean_q: -2583.321777
 3072/5000: episode: 3072, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -10480.066, mean reward: -10480.066 [-10480.066, -10480.066], mean action: 2.000 [2.000, 2.000],  loss: 1799502.125000, mae: 4026.455811, mean_q: -2572.906982
 3073/5000: episode: 3073, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5043.232, mean reward: -5043.232 [-5043.232, -5043.232], mean action: 2.000 [2.000, 2.000],  loss: 3055898.000000, mae: 4154.289062, mean_q: -2599.791992
 3074/5000: episode: 3074, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -6908.839, mean reward: -6908.839 [-6908.839, -6908.839], mean action: 2.000 [2.000, 2.000],  loss: 3630128.750000, mae: 4250.399414, mean_q: -2606.871338
 3075/5000: episode: 3075, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1536.976, mean reward: -1536.976 [-1536.976, -1536.976], mean action: 2.000 [2.000, 2.000],  loss: 2738990.250000, mae: 4186.685547, mean_q: -2605.590332
 3076/5000: episode: 3076, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1061.052, mean reward: -1061.052 [-1061.052, -1061.052], mean action: 2.000 [2.000, 2.000],  loss: 3665349.000000, mae: 4215.327637, mean_q: -2615.218750
 3077/5000: episode: 3077, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1625.029, mean reward: -1625.029 [-1625.029, -1625.029], mean action: 2.000 [2.000, 2.000],  loss: 2675681.250000, mae: 4132.660645, mean_q: -2630.036621
 3078/5000: episode: 3078, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1923.593, mean reward: -1923.593 [-1923.593, -1923.593], mean action: 2.000 [2.000, 2.000],  loss: 4287527.500000, mae: 4267.494141, mean_q: -2642.577637
 3079/5000: episode: 3079, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -835.539, mean reward: -835.539 [-835.539, -835.539], mean action: 2.000 [2.000, 2.000],  loss: 2976464.000000, mae: 4140.931641, mean_q: -2635.736084
 3080/5000: episode: 3080, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2215.690, mean reward: -2215.690 [-2215.690, -2215.690], mean action: 2.000 [2.000, 2.000],  loss: 2451994.250000, mae: 4203.492188, mean_q: -2647.761719
 3081/5000: episode: 3081, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1486.612, mean reward: -1486.612 [-1486.612, -1486.612], mean action: 2.000 [2.000, 2.000],  loss: 1279323.250000, mae: 4057.822754, mean_q: -2631.564453
 3082/5000: episode: 3082, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8706.448, mean reward: -8706.448 [-8706.448, -8706.448], mean action: 2.000 [2.000, 2.000],  loss: 1599254.375000, mae: 4098.108398, mean_q: -2627.553223
 3083/5000: episode: 3083, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2757.384, mean reward: -2757.384 [-2757.384, -2757.384], mean action: 2.000 [2.000, 2.000],  loss: 3428951.750000, mae: 4328.903320, mean_q: -2643.717041
 3084/5000: episode: 3084, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7306.848, mean reward: -7306.848 [-7306.848, -7306.848], mean action: 0.000 [0.000, 0.000],  loss: 3051720.000000, mae: 4253.274414, mean_q: -2632.249512
 3085/5000: episode: 3085, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1323.185, mean reward: -1323.185 [-1323.185, -1323.185], mean action: 2.000 [2.000, 2.000],  loss: 2934766.500000, mae: 4181.347656, mean_q: -2614.732178
 3086/5000: episode: 3086, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -81.590, mean reward: -81.590 [-81.590, -81.590], mean action: 2.000 [2.000, 2.000],  loss: 2413613.500000, mae: 4134.602539, mean_q: -2621.397949
 3087/5000: episode: 3087, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -268.252, mean reward: -268.252 [-268.252, -268.252], mean action: 2.000 [2.000, 2.000],  loss: 2734281.000000, mae: 4201.895508, mean_q: -2628.283936
 3088/5000: episode: 3088, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -8758.044, mean reward: -8758.044 [-8758.044, -8758.044], mean action: 1.000 [1.000, 1.000],  loss: 3600223.250000, mae: 4240.134766, mean_q: -2615.728027
 3089/5000: episode: 3089, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3608.959, mean reward: -3608.959 [-3608.959, -3608.959], mean action: 2.000 [2.000, 2.000],  loss: 2240426.750000, mae: 4133.129395, mean_q: -2619.873047
 3090/5000: episode: 3090, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2595.580, mean reward: -2595.580 [-2595.580, -2595.580], mean action: 2.000 [2.000, 2.000],  loss: 2044195.250000, mae: 4117.137695, mean_q: -2624.725098
 3091/5000: episode: 3091, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5424.713, mean reward: -5424.713 [-5424.713, -5424.713], mean action: 2.000 [2.000, 2.000],  loss: 2150509.500000, mae: 4075.450195, mean_q: -2613.560547
 3092/5000: episode: 3092, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7874.753, mean reward: -7874.753 [-7874.753, -7874.753], mean action: 2.000 [2.000, 2.000],  loss: 2527277.250000, mae: 4180.377930, mean_q: -2626.945312
 3093/5000: episode: 3093, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1216.454, mean reward: -1216.454 [-1216.454, -1216.454], mean action: 2.000 [2.000, 2.000],  loss: 3643397.750000, mae: 4228.403809, mean_q: -2615.434570
 3094/5000: episode: 3094, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4955.065, mean reward: -4955.065 [-4955.065, -4955.065], mean action: 2.000 [2.000, 2.000],  loss: 3135051.500000, mae: 4166.625977, mean_q: -2610.057373
 3095/5000: episode: 3095, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -903.310, mean reward: -903.310 [-903.310, -903.310], mean action: 2.000 [2.000, 2.000],  loss: 2444573.250000, mae: 4176.321289, mean_q: -2622.571777
 3096/5000: episode: 3096, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10431.194, mean reward: -10431.194 [-10431.194, -10431.194], mean action: 0.000 [0.000, 0.000],  loss: 1969451.625000, mae: 4070.466309, mean_q: -2620.951660
 3097/5000: episode: 3097, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6009.809, mean reward: -6009.809 [-6009.809, -6009.809], mean action: 2.000 [2.000, 2.000],  loss: 3448452.250000, mae: 4163.009766, mean_q: -2627.164551
 3098/5000: episode: 3098, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -670.829, mean reward: -670.829 [-670.829, -670.829], mean action: 3.000 [3.000, 3.000],  loss: 3072440.250000, mae: 4229.958984, mean_q: -2626.423828
 3099/5000: episode: 3099, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2900.431, mean reward: -2900.431 [-2900.431, -2900.431], mean action: 2.000 [2.000, 2.000],  loss: 1647285.625000, mae: 4005.327637, mean_q: -2634.802246
 3100/5000: episode: 3100, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3075.881, mean reward: -3075.881 [-3075.881, -3075.881], mean action: 2.000 [2.000, 2.000],  loss: 4293967.000000, mae: 4233.982422, mean_q: -2643.283203
 3101/5000: episode: 3101, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -160.116, mean reward: -160.116 [-160.116, -160.116], mean action: 2.000 [2.000, 2.000],  loss: 1450230.500000, mae: 4040.067383, mean_q: -2639.643555
 3102/5000: episode: 3102, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2600.313, mean reward: -2600.313 [-2600.313, -2600.313], mean action: 2.000 [2.000, 2.000],  loss: 2393885.750000, mae: 4110.397949, mean_q: -2652.323730
 3103/5000: episode: 3103, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4035.647, mean reward: -4035.647 [-4035.647, -4035.647], mean action: 2.000 [2.000, 2.000],  loss: 4154304.000000, mae: 4259.799316, mean_q: -2655.228516
 3104/5000: episode: 3104, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6233.293, mean reward: -6233.293 [-6233.293, -6233.293], mean action: 2.000 [2.000, 2.000],  loss: 2415401.500000, mae: 4203.032227, mean_q: -2643.158447
 3105/5000: episode: 3105, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1378.963, mean reward: -1378.963 [-1378.963, -1378.963], mean action: 2.000 [2.000, 2.000],  loss: 2855112.250000, mae: 4245.948730, mean_q: -2664.011963
 3106/5000: episode: 3106, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1417.732, mean reward: -1417.732 [-1417.732, -1417.732], mean action: 2.000 [2.000, 2.000],  loss: 3104152.000000, mae: 4090.695312, mean_q: -2646.432129
 3107/5000: episode: 3107, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3466.923, mean reward: -3466.923 [-3466.923, -3466.923], mean action: 2.000 [2.000, 2.000],  loss: 2193387.000000, mae: 4096.214844, mean_q: -2665.135254
 3108/5000: episode: 3108, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2249.906, mean reward: -2249.906 [-2249.906, -2249.906], mean action: 2.000 [2.000, 2.000],  loss: 2314806.000000, mae: 4063.081055, mean_q: -2672.275879
 3109/5000: episode: 3109, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3661.945, mean reward: -3661.945 [-3661.945, -3661.945], mean action: 2.000 [2.000, 2.000],  loss: 2166822.500000, mae: 4110.915039, mean_q: -2673.058594
 3110/5000: episode: 3110, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1863.866, mean reward: -1863.866 [-1863.866, -1863.866], mean action: 2.000 [2.000, 2.000],  loss: 2561257.500000, mae: 4180.968750, mean_q: -2667.394043
 3111/5000: episode: 3111, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1596.199, mean reward: -1596.199 [-1596.199, -1596.199], mean action: 2.000 [2.000, 2.000],  loss: 3163675.500000, mae: 4120.555664, mean_q: -2675.286133
 3112/5000: episode: 3112, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -585.908, mean reward: -585.908 [-585.908, -585.908], mean action: 2.000 [2.000, 2.000],  loss: 2242617.500000, mae: 4139.571777, mean_q: -2663.777832
 3113/5000: episode: 3113, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2695.315, mean reward: -2695.315 [-2695.315, -2695.315], mean action: 0.000 [0.000, 0.000],  loss: 3177472.000000, mae: 4231.595703, mean_q: -2650.398926
 3114/5000: episode: 3114, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1237.462, mean reward: -1237.462 [-1237.462, -1237.462], mean action: 2.000 [2.000, 2.000],  loss: 3267283.500000, mae: 4218.472168, mean_q: -2660.832764
 3115/5000: episode: 3115, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3279.211, mean reward: -3279.211 [-3279.211, -3279.211], mean action: 2.000 [2.000, 2.000],  loss: 3240775.500000, mae: 4175.489258, mean_q: -2659.411133
 3116/5000: episode: 3116, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1317.789, mean reward: -1317.789 [-1317.789, -1317.789], mean action: 2.000 [2.000, 2.000],  loss: 3187215.750000, mae: 4217.960449, mean_q: -2663.236816
 3117/5000: episode: 3117, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2935.247, mean reward: -2935.247 [-2935.247, -2935.247], mean action: 2.000 [2.000, 2.000],  loss: 2124677.250000, mae: 4153.859375, mean_q: -2672.917236
 3118/5000: episode: 3118, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -207.276, mean reward: -207.276 [-207.276, -207.276], mean action: 2.000 [2.000, 2.000],  loss: 1697676.125000, mae: 4110.072266, mean_q: -2669.621582
 3119/5000: episode: 3119, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3768.704, mean reward: -3768.704 [-3768.704, -3768.704], mean action: 2.000 [2.000, 2.000],  loss: 3813980.500000, mae: 4243.516113, mean_q: -2648.500977
 3120/5000: episode: 3120, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4595.632, mean reward: -4595.632 [-4595.632, -4595.632], mean action: 2.000 [2.000, 2.000],  loss: 2130900.750000, mae: 4176.871094, mean_q: -2644.186768
 3121/5000: episode: 3121, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4642.296, mean reward: -4642.296 [-4642.296, -4642.296], mean action: 2.000 [2.000, 2.000],  loss: 2513785.500000, mae: 4217.535156, mean_q: -2641.766602
 3122/5000: episode: 3122, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -45.982, mean reward: -45.982 [-45.982, -45.982], mean action: 2.000 [2.000, 2.000],  loss: 2029450.000000, mae: 4188.670898, mean_q: -2627.100342
 3123/5000: episode: 3123, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4951.502, mean reward: -4951.502 [-4951.502, -4951.502], mean action: 2.000 [2.000, 2.000],  loss: 2837065.250000, mae: 4193.792969, mean_q: -2619.062500
 3124/5000: episode: 3124, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2455.507, mean reward: -2455.507 [-2455.507, -2455.507], mean action: 2.000 [2.000, 2.000],  loss: 2487156.250000, mae: 4192.026367, mean_q: -2619.557373
 3125/5000: episode: 3125, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -64.508, mean reward: -64.508 [-64.508, -64.508], mean action: 2.000 [2.000, 2.000],  loss: 3416468.000000, mae: 4188.945312, mean_q: -2588.502930
 3126/5000: episode: 3126, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -180.204, mean reward: -180.204 [-180.204, -180.204], mean action: 2.000 [2.000, 2.000],  loss: 2159565.000000, mae: 4176.972656, mean_q: -2601.213135
 3127/5000: episode: 3127, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -497.773, mean reward: -497.773 [-497.773, -497.773], mean action: 2.000 [2.000, 2.000],  loss: 2358535.500000, mae: 4159.678711, mean_q: -2611.996826
 3128/5000: episode: 3128, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3904.876, mean reward: -3904.876 [-3904.876, -3904.876], mean action: 2.000 [2.000, 2.000],  loss: 2930800.750000, mae: 4130.081055, mean_q: -2608.428711
 3129/5000: episode: 3129, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2407.958, mean reward: -2407.958 [-2407.958, -2407.958], mean action: 2.000 [2.000, 2.000],  loss: 3019269.000000, mae: 4172.938965, mean_q: -2595.070068
 3130/5000: episode: 3130, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -568.574, mean reward: -568.574 [-568.574, -568.574], mean action: 2.000 [2.000, 2.000],  loss: 3108305.500000, mae: 4165.822266, mean_q: -2588.544678
 3131/5000: episode: 3131, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7598.800, mean reward: -7598.800 [-7598.800, -7598.800], mean action: 3.000 [3.000, 3.000],  loss: 5698887.000000, mae: 4169.877930, mean_q: -2594.637207
 3132/5000: episode: 3132, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6705.293, mean reward: -6705.293 [-6705.293, -6705.293], mean action: 3.000 [3.000, 3.000],  loss: 2223549.500000, mae: 4087.443848, mean_q: -2577.551758
 3133/5000: episode: 3133, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8.763, mean reward: -8.763 [-8.763, -8.763], mean action: 2.000 [2.000, 2.000],  loss: 2881750.000000, mae: 4045.197266, mean_q: -2575.728516
 3134/5000: episode: 3134, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2828.095, mean reward: -2828.095 [-2828.095, -2828.095], mean action: 2.000 [2.000, 2.000],  loss: 2048490.125000, mae: 4101.919922, mean_q: -2601.291016
 3135/5000: episode: 3135, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -110.965, mean reward: -110.965 [-110.965, -110.965], mean action: 2.000 [2.000, 2.000],  loss: 2106882.250000, mae: 4115.992188, mean_q: -2594.366455
 3136/5000: episode: 3136, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -460.448, mean reward: -460.448 [-460.448, -460.448], mean action: 2.000 [2.000, 2.000],  loss: 1865074.625000, mae: 4140.597168, mean_q: -2604.416016
 3137/5000: episode: 3137, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2717.306, mean reward: -2717.306 [-2717.306, -2717.306], mean action: 0.000 [0.000, 0.000],  loss: 2668749.500000, mae: 4160.021484, mean_q: -2608.038574
 3138/5000: episode: 3138, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2463.065, mean reward: -2463.065 [-2463.065, -2463.065], mean action: 2.000 [2.000, 2.000],  loss: 3214141.750000, mae: 4149.535645, mean_q: -2606.600830
 3139/5000: episode: 3139, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1162.107, mean reward: -1162.107 [-1162.107, -1162.107], mean action: 2.000 [2.000, 2.000],  loss: 2479066.500000, mae: 4119.548828, mean_q: -2604.419189
 3140/5000: episode: 3140, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3670.265, mean reward: -3670.265 [-3670.265, -3670.265], mean action: 2.000 [2.000, 2.000],  loss: 2390502.250000, mae: 4017.867188, mean_q: -2588.352051
 3141/5000: episode: 3141, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1799.046, mean reward: -1799.046 [-1799.046, -1799.046], mean action: 2.000 [2.000, 2.000],  loss: 3008079.500000, mae: 4161.717773, mean_q: -2611.693359
 3142/5000: episode: 3142, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -7051.531, mean reward: -7051.531 [-7051.531, -7051.531], mean action: 2.000 [2.000, 2.000],  loss: 1968115.500000, mae: 4074.972656, mean_q: -2605.928223
 3143/5000: episode: 3143, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1632.125, mean reward: -1632.125 [-1632.125, -1632.125], mean action: 2.000 [2.000, 2.000],  loss: 1484649.250000, mae: 4059.602051, mean_q: -2603.016357
 3144/5000: episode: 3144, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5804.967, mean reward: -5804.967 [-5804.967, -5804.967], mean action: 2.000 [2.000, 2.000],  loss: 2989942.250000, mae: 4072.862793, mean_q: -2604.099121
 3145/5000: episode: 3145, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3230.250, mean reward: -3230.250 [-3230.250, -3230.250], mean action: 2.000 [2.000, 2.000],  loss: 3677978.500000, mae: 4229.615234, mean_q: -2621.201172
 3146/5000: episode: 3146, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3438.023, mean reward: -3438.023 [-3438.023, -3438.023], mean action: 2.000 [2.000, 2.000],  loss: 3425539.250000, mae: 4060.193848, mean_q: -2615.697754
 3147/5000: episode: 3147, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1404.126, mean reward: -1404.126 [-1404.126, -1404.126], mean action: 2.000 [2.000, 2.000],  loss: 3168140.750000, mae: 4245.754883, mean_q: -2643.099609
 3148/5000: episode: 3148, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7645.656, mean reward: -7645.656 [-7645.656, -7645.656], mean action: 1.000 [1.000, 1.000],  loss: 2286609.500000, mae: 3985.868652, mean_q: -2623.911865
 3149/5000: episode: 3149, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1204.524, mean reward: -1204.524 [-1204.524, -1204.524], mean action: 2.000 [2.000, 2.000],  loss: 3568323.500000, mae: 4232.506836, mean_q: -2646.330322
 3150/5000: episode: 3150, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -233.134, mean reward: -233.134 [-233.134, -233.134], mean action: 2.000 [2.000, 2.000],  loss: 3422574.500000, mae: 4198.870605, mean_q: -2643.613770
 3151/5000: episode: 3151, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2435.272, mean reward: -2435.272 [-2435.272, -2435.272], mean action: 2.000 [2.000, 2.000],  loss: 3490271.750000, mae: 4163.860352, mean_q: -2648.752197
 3152/5000: episode: 3152, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2744.132, mean reward: -2744.132 [-2744.132, -2744.132], mean action: 2.000 [2.000, 2.000],  loss: 2118128.250000, mae: 4174.580078, mean_q: -2682.352539
 3153/5000: episode: 3153, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6867.871, mean reward: -6867.871 [-6867.871, -6867.871], mean action: 2.000 [2.000, 2.000],  loss: 1838799.000000, mae: 4024.601074, mean_q: -2667.948975
 3154/5000: episode: 3154, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2865.085, mean reward: -2865.085 [-2865.085, -2865.085], mean action: 2.000 [2.000, 2.000],  loss: 2391401.000000, mae: 4182.787598, mean_q: -2669.103271
 3155/5000: episode: 3155, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3291.577, mean reward: -3291.577 [-3291.577, -3291.577], mean action: 2.000 [2.000, 2.000],  loss: 2800732.000000, mae: 4242.576660, mean_q: -2670.535889
 3156/5000: episode: 3156, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -399.917, mean reward: -399.917 [-399.917, -399.917], mean action: 2.000 [2.000, 2.000],  loss: 4471028.500000, mae: 4273.712402, mean_q: -2666.415283
 3157/5000: episode: 3157, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2080.173, mean reward: -2080.173 [-2080.173, -2080.173], mean action: 2.000 [2.000, 2.000],  loss: 3387347.500000, mae: 4227.958984, mean_q: -2670.102051
 3158/5000: episode: 3158, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -749.169, mean reward: -749.169 [-749.169, -749.169], mean action: 2.000 [2.000, 2.000],  loss: 3281146.000000, mae: 4223.805664, mean_q: -2657.676270
 3159/5000: episode: 3159, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -259.881, mean reward: -259.881 [-259.881, -259.881], mean action: 2.000 [2.000, 2.000],  loss: 2123751.500000, mae: 4136.696777, mean_q: -2643.880859
 3160/5000: episode: 3160, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1179.381, mean reward: -1179.381 [-1179.381, -1179.381], mean action: 2.000 [2.000, 2.000],  loss: 2530623.500000, mae: 4146.309570, mean_q: -2658.193848
 3161/5000: episode: 3161, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5.944, mean reward: -5.944 [-5.944, -5.944], mean action: 2.000 [2.000, 2.000],  loss: 2045343.875000, mae: 4218.690918, mean_q: -2657.099609
 3162/5000: episode: 3162, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -936.843, mean reward: -936.843 [-936.843, -936.843], mean action: 2.000 [2.000, 2.000],  loss: 3576036.000000, mae: 4312.941406, mean_q: -2653.392578
 3163/5000: episode: 3163, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1823.909, mean reward: -1823.909 [-1823.909, -1823.909], mean action: 2.000 [2.000, 2.000],  loss: 4632813.000000, mae: 4341.710938, mean_q: -2650.824707
 3164/5000: episode: 3164, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2266.381, mean reward: -2266.381 [-2266.381, -2266.381], mean action: 2.000 [2.000, 2.000],  loss: 2200030.750000, mae: 4194.329102, mean_q: -2654.583496
 3165/5000: episode: 3165, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2165.844, mean reward: -2165.844 [-2165.844, -2165.844], mean action: 2.000 [2.000, 2.000],  loss: 2404074.500000, mae: 4182.498047, mean_q: -2636.543457
 3166/5000: episode: 3166, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1530.078, mean reward: -1530.078 [-1530.078, -1530.078], mean action: 2.000 [2.000, 2.000],  loss: 2206828.250000, mae: 4160.677734, mean_q: -2651.502930
 3167/5000: episode: 3167, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -647.458, mean reward: -647.458 [-647.458, -647.458], mean action: 2.000 [2.000, 2.000],  loss: 3507124.500000, mae: 4235.238770, mean_q: -2641.353516
 3168/5000: episode: 3168, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -659.869, mean reward: -659.869 [-659.869, -659.869], mean action: 2.000 [2.000, 2.000],  loss: 4657019.000000, mae: 4243.731445, mean_q: -2641.232422
 3169/5000: episode: 3169, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1896.281, mean reward: -1896.281 [-1896.281, -1896.281], mean action: 2.000 [2.000, 2.000],  loss: 2635953.000000, mae: 4177.667969, mean_q: -2628.036621
 3170/5000: episode: 3170, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2976.770, mean reward: -2976.770 [-2976.770, -2976.770], mean action: 2.000 [2.000, 2.000],  loss: 1599581.625000, mae: 4069.064941, mean_q: -2620.602783
 3171/5000: episode: 3171, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4573.509, mean reward: -4573.509 [-4573.509, -4573.509], mean action: 2.000 [2.000, 2.000],  loss: 3697259.500000, mae: 4307.783203, mean_q: -2625.172363
 3172/5000: episode: 3172, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9666.039, mean reward: -9666.039 [-9666.039, -9666.039], mean action: 2.000 [2.000, 2.000],  loss: 2527224.750000, mae: 4174.727051, mean_q: -2634.866943
 3173/5000: episode: 3173, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2565.305, mean reward: -2565.305 [-2565.305, -2565.305], mean action: 2.000 [2.000, 2.000],  loss: 2504942.500000, mae: 4162.380859, mean_q: -2619.912598
 3174/5000: episode: 3174, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3010.666, mean reward: -3010.666 [-3010.666, -3010.666], mean action: 2.000 [2.000, 2.000],  loss: 2720555.750000, mae: 4147.171387, mean_q: -2610.225098
 3175/5000: episode: 3175, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2007.705, mean reward: -2007.705 [-2007.705, -2007.705], mean action: 2.000 [2.000, 2.000],  loss: 4000101.500000, mae: 4270.680664, mean_q: -2606.876709
 3176/5000: episode: 3176, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5533.004, mean reward: -5533.004 [-5533.004, -5533.004], mean action: 2.000 [2.000, 2.000],  loss: 2178141.500000, mae: 4125.577148, mean_q: -2599.738770
 3177/5000: episode: 3177, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3786.032, mean reward: -3786.032 [-3786.032, -3786.032], mean action: 2.000 [2.000, 2.000],  loss: 2315724.250000, mae: 4098.541992, mean_q: -2608.946045
 3178/5000: episode: 3178, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4216.050, mean reward: -4216.050 [-4216.050, -4216.050], mean action: 2.000 [2.000, 2.000],  loss: 2843845.500000, mae: 4160.958008, mean_q: -2616.494629
 3179/5000: episode: 3179, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1954.024, mean reward: -1954.024 [-1954.024, -1954.024], mean action: 2.000 [2.000, 2.000],  loss: 2276483.000000, mae: 4092.299561, mean_q: -2596.303223
 3180/5000: episode: 3180, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2033.673, mean reward: -2033.673 [-2033.673, -2033.673], mean action: 2.000 [2.000, 2.000],  loss: 2045420.250000, mae: 4154.708008, mean_q: -2618.993164
 3181/5000: episode: 3181, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -4611.838, mean reward: -4611.838 [-4611.838, -4611.838], mean action: 2.000 [2.000, 2.000],  loss: 3393376.000000, mae: 4257.078613, mean_q: -2610.906982
 3182/5000: episode: 3182, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4413.248, mean reward: -4413.248 [-4413.248, -4413.248], mean action: 2.000 [2.000, 2.000],  loss: 2492659.500000, mae: 4151.413086, mean_q: -2621.992188
 3183/5000: episode: 3183, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4025.848, mean reward: -4025.848 [-4025.848, -4025.848], mean action: 2.000 [2.000, 2.000],  loss: 3032785.500000, mae: 4221.048828, mean_q: -2624.395508
 3184/5000: episode: 3184, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2700.188, mean reward: -2700.188 [-2700.188, -2700.188], mean action: 2.000 [2.000, 2.000],  loss: 3560683.000000, mae: 4264.810547, mean_q: -2629.453857
 3185/5000: episode: 3185, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3215.154, mean reward: -3215.154 [-3215.154, -3215.154], mean action: 2.000 [2.000, 2.000],  loss: 2295486.500000, mae: 4238.275391, mean_q: -2630.438477
 3186/5000: episode: 3186, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3036.108, mean reward: -3036.108 [-3036.108, -3036.108], mean action: 2.000 [2.000, 2.000],  loss: 2843485.500000, mae: 4269.914062, mean_q: -2625.792969
 3187/5000: episode: 3187, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -915.451, mean reward: -915.451 [-915.451, -915.451], mean action: 2.000 [2.000, 2.000],  loss: 2754507.000000, mae: 4276.626953, mean_q: -2621.608154
 3188/5000: episode: 3188, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3722.762, mean reward: -3722.762 [-3722.762, -3722.762], mean action: 2.000 [2.000, 2.000],  loss: 3362309.500000, mae: 4271.820312, mean_q: -2626.674072
 3189/5000: episode: 3189, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1931.477, mean reward: -1931.477 [-1931.477, -1931.477], mean action: 1.000 [1.000, 1.000],  loss: 2724507.250000, mae: 4217.333984, mean_q: -2638.627441
 3190/5000: episode: 3190, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1559.123, mean reward: -1559.123 [-1559.123, -1559.123], mean action: 2.000 [2.000, 2.000],  loss: 3495045.000000, mae: 4165.301758, mean_q: -2641.312012
 3191/5000: episode: 3191, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4132.199, mean reward: -4132.199 [-4132.199, -4132.199], mean action: 2.000 [2.000, 2.000],  loss: 2686230.500000, mae: 4228.319336, mean_q: -2652.946777
 3192/5000: episode: 3192, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3374.738, mean reward: -3374.738 [-3374.738, -3374.738], mean action: 2.000 [2.000, 2.000],  loss: 2002295.750000, mae: 4199.426758, mean_q: -2659.024658
 3193/5000: episode: 3193, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -674.788, mean reward: -674.788 [-674.788, -674.788], mean action: 2.000 [2.000, 2.000],  loss: 2405285.750000, mae: 4210.712891, mean_q: -2676.223145
 3194/5000: episode: 3194, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6097.214, mean reward: -6097.214 [-6097.214, -6097.214], mean action: 2.000 [2.000, 2.000],  loss: 1770888.750000, mae: 4264.298828, mean_q: -2683.212891
 3195/5000: episode: 3195, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -696.026, mean reward: -696.026 [-696.026, -696.026], mean action: 2.000 [2.000, 2.000],  loss: 2092243.000000, mae: 4255.567871, mean_q: -2669.908203
 3196/5000: episode: 3196, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6763.639, mean reward: -6763.639 [-6763.639, -6763.639], mean action: 2.000 [2.000, 2.000],  loss: 1924375.500000, mae: 4208.643555, mean_q: -2681.062988
 3197/5000: episode: 3197, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -390.632, mean reward: -390.632 [-390.632, -390.632], mean action: 2.000 [2.000, 2.000],  loss: 2851755.750000, mae: 4232.750977, mean_q: -2674.815430
 3198/5000: episode: 3198, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1726.962, mean reward: -1726.962 [-1726.962, -1726.962], mean action: 2.000 [2.000, 2.000],  loss: 3444693.500000, mae: 4359.878906, mean_q: -2667.187988
 3199/5000: episode: 3199, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -8776.504, mean reward: -8776.504 [-8776.504, -8776.504], mean action: 0.000 [0.000, 0.000],  loss: 3484201.000000, mae: 4360.807617, mean_q: -2672.744141
 3200/5000: episode: 3200, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1531.035, mean reward: -1531.035 [-1531.035, -1531.035], mean action: 2.000 [2.000, 2.000],  loss: 5234535.000000, mae: 4440.310547, mean_q: -2693.006104
 3201/5000: episode: 3201, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1766.571, mean reward: -1766.571 [-1766.571, -1766.571], mean action: 2.000 [2.000, 2.000],  loss: 3204521.500000, mae: 4348.225098, mean_q: -2681.396484
 3202/5000: episode: 3202, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1906.274, mean reward: -1906.274 [-1906.274, -1906.274], mean action: 2.000 [2.000, 2.000],  loss: 3994425.000000, mae: 4339.095703, mean_q: -2681.012207
 3203/5000: episode: 3203, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -999.377, mean reward: -999.377 [-999.377, -999.377], mean action: 2.000 [2.000, 2.000],  loss: 2181701.500000, mae: 4217.224121, mean_q: -2687.692871
 3204/5000: episode: 3204, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1489.840, mean reward: -1489.840 [-1489.840, -1489.840], mean action: 2.000 [2.000, 2.000],  loss: 1594527.000000, mae: 4166.819336, mean_q: -2694.430908
 3205/5000: episode: 3205, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8371.060, mean reward: -8371.060 [-8371.060, -8371.060], mean action: 2.000 [2.000, 2.000],  loss: 1948372.500000, mae: 4133.477539, mean_q: -2678.762695
 3206/5000: episode: 3206, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2839.205, mean reward: -2839.205 [-2839.205, -2839.205], mean action: 2.000 [2.000, 2.000],  loss: 2522288.250000, mae: 4161.778320, mean_q: -2690.077393
 3207/5000: episode: 3207, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1271.848, mean reward: -1271.848 [-1271.848, -1271.848], mean action: 2.000 [2.000, 2.000],  loss: 2684513.500000, mae: 4312.298828, mean_q: -2683.881592
 3208/5000: episode: 3208, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3276.862, mean reward: -3276.862 [-3276.862, -3276.862], mean action: 2.000 [2.000, 2.000],  loss: 5394402.000000, mae: 4311.839844, mean_q: -2690.403320
 3209/5000: episode: 3209, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3832.284, mean reward: -3832.284 [-3832.284, -3832.284], mean action: 2.000 [2.000, 2.000],  loss: 2259095.750000, mae: 4215.024414, mean_q: -2691.075195
 3210/5000: episode: 3210, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1178.348, mean reward: -1178.348 [-1178.348, -1178.348], mean action: 2.000 [2.000, 2.000],  loss: 2994984.750000, mae: 4266.995605, mean_q: -2682.169678
 3211/5000: episode: 3211, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -52.974, mean reward: -52.974 [-52.974, -52.974], mean action: 2.000 [2.000, 2.000],  loss: 2357671.000000, mae: 4166.680176, mean_q: -2670.384766
 3212/5000: episode: 3212, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2300.623, mean reward: -2300.623 [-2300.623, -2300.623], mean action: 2.000 [2.000, 2.000],  loss: 2676491.500000, mae: 4155.180664, mean_q: -2667.053223
 3213/5000: episode: 3213, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6141.355, mean reward: -6141.355 [-6141.355, -6141.355], mean action: 2.000 [2.000, 2.000],  loss: 2772365.500000, mae: 4137.007812, mean_q: -2671.627197
 3214/5000: episode: 3214, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3763.702, mean reward: -3763.702 [-3763.702, -3763.702], mean action: 2.000 [2.000, 2.000],  loss: 3238371.000000, mae: 4165.306152, mean_q: -2670.325684
 3215/5000: episode: 3215, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -298.997, mean reward: -298.997 [-298.997, -298.997], mean action: 2.000 [2.000, 2.000],  loss: 2182821.250000, mae: 4197.259277, mean_q: -2680.819336
 3216/5000: episode: 3216, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1281.616, mean reward: -1281.616 [-1281.616, -1281.616], mean action: 2.000 [2.000, 2.000],  loss: 1445844.625000, mae: 4119.619141, mean_q: -2673.620605
 3217/5000: episode: 3217, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3940.362, mean reward: -3940.362 [-3940.362, -3940.362], mean action: 2.000 [2.000, 2.000],  loss: 1761823.625000, mae: 4145.489258, mean_q: -2680.661133
 3218/5000: episode: 3218, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -296.501, mean reward: -296.501 [-296.501, -296.501], mean action: 2.000 [2.000, 2.000],  loss: 1997269.500000, mae: 4220.203613, mean_q: -2680.711914
 3219/5000: episode: 3219, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2187.603, mean reward: -2187.603 [-2187.603, -2187.603], mean action: 2.000 [2.000, 2.000],  loss: 2660729.750000, mae: 4275.503906, mean_q: -2672.891113
 3220/5000: episode: 3220, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1857.274, mean reward: -1857.274 [-1857.274, -1857.274], mean action: 2.000 [2.000, 2.000],  loss: 2756374.750000, mae: 4184.812988, mean_q: -2691.906250
 3221/5000: episode: 3221, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -325.016, mean reward: -325.016 [-325.016, -325.016], mean action: 2.000 [2.000, 2.000],  loss: 4011839.000000, mae: 4346.104492, mean_q: -2694.993652
 3222/5000: episode: 3222, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2985.856, mean reward: -2985.856 [-2985.856, -2985.856], mean action: 2.000 [2.000, 2.000],  loss: 4079759.000000, mae: 4317.088867, mean_q: -2685.532959
 3223/5000: episode: 3223, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6483.667, mean reward: -6483.667 [-6483.667, -6483.667], mean action: 2.000 [2.000, 2.000],  loss: 4094233.000000, mae: 4336.068359, mean_q: -2685.364258
 3224/5000: episode: 3224, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2255.346, mean reward: -2255.346 [-2255.346, -2255.346], mean action: 2.000 [2.000, 2.000],  loss: 3084555.750000, mae: 4227.314453, mean_q: -2679.594971
 3225/5000: episode: 3225, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5377.660, mean reward: -5377.660 [-5377.660, -5377.660], mean action: 2.000 [2.000, 2.000],  loss: 2529046.000000, mae: 4252.364258, mean_q: -2688.823975
 3226/5000: episode: 3226, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3630.406, mean reward: -3630.406 [-3630.406, -3630.406], mean action: 2.000 [2.000, 2.000],  loss: 4306383.000000, mae: 4404.151367, mean_q: -2705.554932
 3227/5000: episode: 3227, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2782.665, mean reward: -2782.665 [-2782.665, -2782.665], mean action: 2.000 [2.000, 2.000],  loss: 4207196.000000, mae: 4405.772461, mean_q: -2685.898926
 3228/5000: episode: 3228, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -508.643, mean reward: -508.643 [-508.643, -508.643], mean action: 2.000 [2.000, 2.000],  loss: 2309767.000000, mae: 4203.992676, mean_q: -2699.032227
 3229/5000: episode: 3229, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -102.967, mean reward: -102.967 [-102.967, -102.967], mean action: 2.000 [2.000, 2.000],  loss: 1876006.750000, mae: 4272.485352, mean_q: -2707.644287
 3230/5000: episode: 3230, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6003.150, mean reward: -6003.150 [-6003.150, -6003.150], mean action: 2.000 [2.000, 2.000],  loss: 2753714.750000, mae: 4221.708496, mean_q: -2696.124512
 3231/5000: episode: 3231, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -12.823, mean reward: -12.823 [-12.823, -12.823], mean action: 2.000 [2.000, 2.000],  loss: 2607178.500000, mae: 4170.253418, mean_q: -2680.399902
 3232/5000: episode: 3232, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2699.206, mean reward: -2699.206 [-2699.206, -2699.206], mean action: 2.000 [2.000, 2.000],  loss: 939000.875000, mae: 4048.333008, mean_q: -2687.660889
 3233/5000: episode: 3233, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2234.082, mean reward: -2234.082 [-2234.082, -2234.082], mean action: 2.000 [2.000, 2.000],  loss: 1982008.625000, mae: 4228.911621, mean_q: -2670.999512
 3234/5000: episode: 3234, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4709.551, mean reward: -4709.551 [-4709.551, -4709.551], mean action: 2.000 [2.000, 2.000],  loss: 4068275.500000, mae: 4423.750000, mean_q: -2673.410156
 3235/5000: episode: 3235, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1928.445, mean reward: -1928.445 [-1928.445, -1928.445], mean action: 2.000 [2.000, 2.000],  loss: 2914832.750000, mae: 4256.647461, mean_q: -2673.919922
 3236/5000: episode: 3236, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1146.780, mean reward: -1146.780 [-1146.780, -1146.780], mean action: 2.000 [2.000, 2.000],  loss: 1944812.500000, mae: 4134.386719, mean_q: -2660.518799
 3237/5000: episode: 3237, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5061.513, mean reward: -5061.513 [-5061.513, -5061.513], mean action: 2.000 [2.000, 2.000],  loss: 1807915.750000, mae: 4163.286621, mean_q: -2661.676758
 3238/5000: episode: 3238, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4171.483, mean reward: -4171.483 [-4171.483, -4171.483], mean action: 2.000 [2.000, 2.000],  loss: 2951659.500000, mae: 4177.562988, mean_q: -2652.051270
 3239/5000: episode: 3239, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3853.520, mean reward: -3853.520 [-3853.520, -3853.520], mean action: 2.000 [2.000, 2.000],  loss: 3014711.000000, mae: 4179.191895, mean_q: -2630.159180
 3240/5000: episode: 3240, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2500.077, mean reward: -2500.077 [-2500.077, -2500.077], mean action: 2.000 [2.000, 2.000],  loss: 3253133.000000, mae: 4215.649414, mean_q: -2614.367676
 3241/5000: episode: 3241, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1359.541, mean reward: -1359.541 [-1359.541, -1359.541], mean action: 2.000 [2.000, 2.000],  loss: 3054489.250000, mae: 4219.504883, mean_q: -2607.214355
 3242/5000: episode: 3242, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -716.950, mean reward: -716.950 [-716.950, -716.950], mean action: 2.000 [2.000, 2.000],  loss: 2744322.000000, mae: 4192.337891, mean_q: -2611.192871
 3243/5000: episode: 3243, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1617.208, mean reward: -1617.208 [-1617.208, -1617.208], mean action: 2.000 [2.000, 2.000],  loss: 4187687.000000, mae: 4251.791016, mean_q: -2611.406982
 3244/5000: episode: 3244, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1809.082, mean reward: -1809.082 [-1809.082, -1809.082], mean action: 2.000 [2.000, 2.000],  loss: 2322963.750000, mae: 4186.196777, mean_q: -2610.935303
 3245/5000: episode: 3245, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3253.843, mean reward: -3253.843 [-3253.843, -3253.843], mean action: 2.000 [2.000, 2.000],  loss: 4667430.000000, mae: 4290.152832, mean_q: -2635.033936
 3246/5000: episode: 3246, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3413.096, mean reward: -3413.096 [-3413.096, -3413.096], mean action: 2.000 [2.000, 2.000],  loss: 2354971.250000, mae: 4208.141113, mean_q: -2630.895996
 3247/5000: episode: 3247, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2351.143, mean reward: -2351.143 [-2351.143, -2351.143], mean action: 2.000 [2.000, 2.000],  loss: 3355100.750000, mae: 4206.267090, mean_q: -2618.072266
 3248/5000: episode: 3248, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6092.197, mean reward: -6092.197 [-6092.197, -6092.197], mean action: 2.000 [2.000, 2.000],  loss: 1818265.750000, mae: 4163.484863, mean_q: -2629.148926
 3249/5000: episode: 3249, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2262.386, mean reward: -2262.386 [-2262.386, -2262.386], mean action: 2.000 [2.000, 2.000],  loss: 1802637.750000, mae: 4140.725098, mean_q: -2632.696289
 3250/5000: episode: 3250, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -554.158, mean reward: -554.158 [-554.158, -554.158], mean action: 2.000 [2.000, 2.000],  loss: 3203725.500000, mae: 4167.193848, mean_q: -2628.467285
 3251/5000: episode: 3251, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9888.054, mean reward: -9888.054 [-9888.054, -9888.054], mean action: 0.000 [0.000, 0.000],  loss: 4752016.000000, mae: 4358.725586, mean_q: -2634.674805
 3252/5000: episode: 3252, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2220.133, mean reward: -2220.133 [-2220.133, -2220.133], mean action: 2.000 [2.000, 2.000],  loss: 2415209.500000, mae: 4189.318359, mean_q: -2651.320557
 3253/5000: episode: 3253, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -2931.616, mean reward: -2931.616 [-2931.616, -2931.616], mean action: 2.000 [2.000, 2.000],  loss: 2419286.500000, mae: 4161.089844, mean_q: -2632.813965
 3254/5000: episode: 3254, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -16791.254, mean reward: -16791.254 [-16791.254, -16791.254], mean action: 0.000 [0.000, 0.000],  loss: 3387855.000000, mae: 4312.208984, mean_q: -2646.938477
 3255/5000: episode: 3255, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2553.471, mean reward: -2553.471 [-2553.471, -2553.471], mean action: 2.000 [2.000, 2.000],  loss: 2868611.500000, mae: 4285.302734, mean_q: -2660.514160
 3256/5000: episode: 3256, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1427.958, mean reward: -1427.958 [-1427.958, -1427.958], mean action: 2.000 [2.000, 2.000],  loss: 2681885.000000, mae: 4177.484863, mean_q: -2651.216797
 3257/5000: episode: 3257, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -173.307, mean reward: -173.307 [-173.307, -173.307], mean action: 2.000 [2.000, 2.000],  loss: 2930689.250000, mae: 4307.208008, mean_q: -2675.424316
 3258/5000: episode: 3258, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3893.763, mean reward: -3893.763 [-3893.763, -3893.763], mean action: 2.000 [2.000, 2.000],  loss: 3028033.750000, mae: 4268.609375, mean_q: -2674.604004
 3259/5000: episode: 3259, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1774.997, mean reward: -1774.997 [-1774.997, -1774.997], mean action: 0.000 [0.000, 0.000],  loss: 2406114.000000, mae: 4224.718750, mean_q: -2700.712402
 3260/5000: episode: 3260, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -100.879, mean reward: -100.879 [-100.879, -100.879], mean action: 2.000 [2.000, 2.000],  loss: 3422035.000000, mae: 4285.113281, mean_q: -2702.192627
 3261/5000: episode: 3261, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -177.280, mean reward: -177.280 [-177.280, -177.280], mean action: 2.000 [2.000, 2.000],  loss: 3312744.750000, mae: 4277.880859, mean_q: -2718.244385
 3262/5000: episode: 3262, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -4943.409, mean reward: -4943.409 [-4943.409, -4943.409], mean action: 0.000 [0.000, 0.000],  loss: 2462707.500000, mae: 4066.678223, mean_q: -2708.316650
 3263/5000: episode: 3263, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3123.208, mean reward: -3123.208 [-3123.208, -3123.208], mean action: 2.000 [2.000, 2.000],  loss: 2570424.500000, mae: 4245.570312, mean_q: -2718.723633
 3264/5000: episode: 3264, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -3584.772, mean reward: -3584.772 [-3584.772, -3584.772], mean action: 2.000 [2.000, 2.000],  loss: 3039168.500000, mae: 4302.373535, mean_q: -2731.134766
 3265/5000: episode: 3265, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -447.192, mean reward: -447.192 [-447.192, -447.192], mean action: 2.000 [2.000, 2.000],  loss: 1776628.000000, mae: 4229.201660, mean_q: -2722.428955
 3266/5000: episode: 3266, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2672.759, mean reward: -2672.759 [-2672.759, -2672.759], mean action: 2.000 [2.000, 2.000],  loss: 2575566.500000, mae: 4275.343262, mean_q: -2724.549805
 3267/5000: episode: 3267, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -367.903, mean reward: -367.903 [-367.903, -367.903], mean action: 2.000 [2.000, 2.000],  loss: 2792785.750000, mae: 4240.086914, mean_q: -2728.014648
 3268/5000: episode: 3268, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2345.665, mean reward: -2345.665 [-2345.665, -2345.665], mean action: 2.000 [2.000, 2.000],  loss: 3009543.250000, mae: 4243.237305, mean_q: -2709.612305
 3269/5000: episode: 3269, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5537.067, mean reward: -5537.067 [-5537.067, -5537.067], mean action: 2.000 [2.000, 2.000],  loss: 3188276.000000, mae: 4296.013672, mean_q: -2699.463867
 3270/5000: episode: 3270, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2711.013, mean reward: -2711.013 [-2711.013, -2711.013], mean action: 2.000 [2.000, 2.000],  loss: 1991361.000000, mae: 4182.609375, mean_q: -2711.134277
 3271/5000: episode: 3271, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4525.237, mean reward: -4525.237 [-4525.237, -4525.237], mean action: 2.000 [2.000, 2.000],  loss: 2341666.000000, mae: 4291.691406, mean_q: -2711.784180
 3272/5000: episode: 3272, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -478.592, mean reward: -478.592 [-478.592, -478.592], mean action: 2.000 [2.000, 2.000],  loss: 3292487.500000, mae: 4204.521973, mean_q: -2702.592041
 3273/5000: episode: 3273, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2068.191, mean reward: -2068.191 [-2068.191, -2068.191], mean action: 2.000 [2.000, 2.000],  loss: 3113169.500000, mae: 4302.987305, mean_q: -2717.176758
 3274/5000: episode: 3274, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5048.947, mean reward: -5048.947 [-5048.947, -5048.947], mean action: 0.000 [0.000, 0.000],  loss: 3044136.000000, mae: 4293.595703, mean_q: -2719.454590
 3275/5000: episode: 3275, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2983.335, mean reward: -2983.335 [-2983.335, -2983.335], mean action: 2.000 [2.000, 2.000],  loss: 2686889.500000, mae: 4242.527832, mean_q: -2729.016846
 3276/5000: episode: 3276, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2488.978, mean reward: -2488.978 [-2488.978, -2488.978], mean action: 2.000 [2.000, 2.000],  loss: 3583544.000000, mae: 4275.943359, mean_q: -2738.442139
 3277/5000: episode: 3277, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -995.406, mean reward: -995.406 [-995.406, -995.406], mean action: 2.000 [2.000, 2.000],  loss: 2911485.500000, mae: 4304.638672, mean_q: -2730.632324
 3278/5000: episode: 3278, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4214.028, mean reward: -4214.028 [-4214.028, -4214.028], mean action: 2.000 [2.000, 2.000],  loss: 2138397.750000, mae: 4297.747070, mean_q: -2736.642822
 3279/5000: episode: 3279, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3680.475, mean reward: -3680.475 [-3680.475, -3680.475], mean action: 3.000 [3.000, 3.000],  loss: 2898916.000000, mae: 4268.385254, mean_q: -2744.779297
 3280/5000: episode: 3280, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2231.066, mean reward: -2231.066 [-2231.066, -2231.066], mean action: 2.000 [2.000, 2.000],  loss: 2191723.500000, mae: 4290.394043, mean_q: -2733.320801
 3281/5000: episode: 3281, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1831.859, mean reward: -1831.859 [-1831.859, -1831.859], mean action: 2.000 [2.000, 2.000],  loss: 2156969.500000, mae: 4243.787598, mean_q: -2742.706543
 3282/5000: episode: 3282, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1676.555, mean reward: -1676.555 [-1676.555, -1676.555], mean action: 2.000 [2.000, 2.000],  loss: 2726534.250000, mae: 4304.337891, mean_q: -2738.216309
 3283/5000: episode: 3283, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1877.993, mean reward: -1877.993 [-1877.993, -1877.993], mean action: 2.000 [2.000, 2.000],  loss: 2575735.750000, mae: 4370.886719, mean_q: -2736.038574
 3284/5000: episode: 3284, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2102.692, mean reward: -2102.692 [-2102.692, -2102.692], mean action: 2.000 [2.000, 2.000],  loss: 2487005.750000, mae: 4291.691406, mean_q: -2705.594727
 3285/5000: episode: 3285, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -31.748, mean reward: -31.748 [-31.748, -31.748], mean action: 2.000 [2.000, 2.000],  loss: 3231304.000000, mae: 4404.055664, mean_q: -2725.034180
 3286/5000: episode: 3286, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -225.326, mean reward: -225.326 [-225.326, -225.326], mean action: 2.000 [2.000, 2.000],  loss: 2149037.000000, mae: 4234.698242, mean_q: -2705.283691
 3287/5000: episode: 3287, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2040.412, mean reward: -2040.412 [-2040.412, -2040.412], mean action: 2.000 [2.000, 2.000],  loss: 2878921.500000, mae: 4354.152344, mean_q: -2696.484619
 3288/5000: episode: 3288, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -631.794, mean reward: -631.794 [-631.794, -631.794], mean action: 2.000 [2.000, 2.000],  loss: 2459486.000000, mae: 4339.416504, mean_q: -2694.070312
 3289/5000: episode: 3289, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -2395.767, mean reward: -2395.767 [-2395.767, -2395.767], mean action: 2.000 [2.000, 2.000],  loss: 4752472.000000, mae: 4400.752930, mean_q: -2692.658203
 3290/5000: episode: 3290, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -3334.820, mean reward: -3334.820 [-3334.820, -3334.820], mean action: 2.000 [2.000, 2.000],  loss: 2870901.000000, mae: 4324.803711, mean_q: -2695.535645
 3291/5000: episode: 3291, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -760.672, mean reward: -760.672 [-760.672, -760.672], mean action: 1.000 [1.000, 1.000],  loss: 2541584.500000, mae: 4187.994141, mean_q: -2689.473877
 3292/5000: episode: 3292, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -1312.460, mean reward: -1312.460 [-1312.460, -1312.460], mean action: 2.000 [2.000, 2.000],  loss: 2716909.750000, mae: 4375.855469, mean_q: -2693.990234
 3293/5000: episode: 3293, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -4312.695, mean reward: -4312.695 [-4312.695, -4312.695], mean action: 2.000 [2.000, 2.000],  loss: 2506126.500000, mae: 4272.040527, mean_q: -2686.884766
 3294/5000: episode: 3294, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -2429.438, mean reward: -2429.438 [-2429.438, -2429.438], mean action: 2.000 [2.000, 2.000],  loss: 2835116.500000, mae: 4303.558594, mean_q: -2693.059570
 3295/5000: episode: 3295, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6372.863, mean reward: -6372.863 [-6372.863, -6372.863], mean action: 2.000 [2.000, 2.000],  loss: 2362081.750000, mae: 4301.151367, mean_q: -2680.488281
 3296/5000: episode: 3296, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -2734.117, mean reward: -2734.117 [-2734.117, -2734.117], mean action: 2.000 [2.000, 2.000],  loss: 3499043.000000, mae: 4408.692871, mean_q: -2667.256836
 3297/5000: episode: 3297, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -3826.527, mean reward: -3826.527 [-3826.527, -3826.527], mean action: 2.000 [2.000, 2.000],  loss: 3730883.250000, mae: 4389.213867, mean_q: -2651.773682
 3298/5000: episode: 3298, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -3165.529, mean reward: -3165.529 [-3165.529, -3165.529], mean action: 2.000 [2.000, 2.000],  loss: 2765393.500000, mae: 4378.314453, mean_q: -2649.727051
 3299/5000: episode: 3299, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -1771.935, mean reward: -1771.935 [-1771.935, -1771.935], mean action: 2.000 [2.000, 2.000],  loss: 2670748.000000, mae: 4243.953613, mean_q: -2637.352051
 3300/5000: episode: 3300, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -1031.207, mean reward: -1031.207 [-1031.207, -1031.207], mean action: 2.000 [2.000, 2.000],  loss: 2430557.500000, mae: 4221.634766, mean_q: -2628.125000
 3301/5000: episode: 3301, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2584.256, mean reward: -2584.256 [-2584.256, -2584.256], mean action: 2.000 [2.000, 2.000],  loss: 2791461.750000, mae: 4295.691406, mean_q: -2643.208008
 3302/5000: episode: 3302, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -97.601, mean reward: -97.601 [-97.601, -97.601], mean action: 2.000 [2.000, 2.000],  loss: 2213488.000000, mae: 4294.726562, mean_q: -2645.242188
 3303/5000: episode: 3303, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -1615.770, mean reward: -1615.770 [-1615.770, -1615.770], mean action: 2.000 [2.000, 2.000],  loss: 1890303.000000, mae: 4213.702637, mean_q: -2625.854004
 3304/5000: episode: 3304, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -1442.105, mean reward: -1442.105 [-1442.105, -1442.105], mean action: 2.000 [2.000, 2.000],  loss: 2380721.250000, mae: 4228.452148, mean_q: -2628.561035
 3305/5000: episode: 3305, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -1275.578, mean reward: -1275.578 [-1275.578, -1275.578], mean action: 2.000 [2.000, 2.000],  loss: 3400329.000000, mae: 4352.274902, mean_q: -2622.139648
 3306/5000: episode: 3306, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2994.722, mean reward: -2994.722 [-2994.722, -2994.722], mean action: 2.000 [2.000, 2.000],  loss: 3627039.000000, mae: 4219.123535, mean_q: -2606.572754
 3307/5000: episode: 3307, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -3379.325, mean reward: -3379.325 [-3379.325, -3379.325], mean action: 2.000 [2.000, 2.000],  loss: 2413716.000000, mae: 4228.133789, mean_q: -2592.592041
 3308/5000: episode: 3308, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -716.690, mean reward: -716.690 [-716.690, -716.690], mean action: 2.000 [2.000, 2.000],  loss: 2615102.500000, mae: 4114.267578, mean_q: -2575.414551
 3309/5000: episode: 3309, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2648.090, mean reward: -2648.090 [-2648.090, -2648.090], mean action: 2.000 [2.000, 2.000],  loss: 2311334.250000, mae: 4240.932617, mean_q: -2578.313232
 3310/5000: episode: 3310, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -1203.553, mean reward: -1203.553 [-1203.553, -1203.553], mean action: 2.000 [2.000, 2.000],  loss: 3028842.000000, mae: 4253.045898, mean_q: -2578.768799
 3311/5000: episode: 3311, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2979.000, mean reward: -2979.000 [-2979.000, -2979.000], mean action: 2.000 [2.000, 2.000],  loss: 1801259.625000, mae: 4135.629883, mean_q: -2593.521729
 3312/5000: episode: 3312, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -3184.250, mean reward: -3184.250 [-3184.250, -3184.250], mean action: 2.000 [2.000, 2.000],  loss: 3284117.000000, mae: 4398.311035, mean_q: -2593.272461
 3313/5000: episode: 3313, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -1344.184, mean reward: -1344.184 [-1344.184, -1344.184], mean action: 2.000 [2.000, 2.000],  loss: 3351181.500000, mae: 4261.028320, mean_q: -2585.494629
 3314/5000: episode: 3314, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2311.899, mean reward: -2311.899 [-2311.899, -2311.899], mean action: 2.000 [2.000, 2.000],  loss: 2050630.000000, mae: 4154.299805, mean_q: -2572.362549
 3315/5000: episode: 3315, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -220.335, mean reward: -220.335 [-220.335, -220.335], mean action: 2.000 [2.000, 2.000],  loss: 3765514.500000, mae: 4345.601562, mean_q: -2573.987793
 3316/5000: episode: 3316, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5382.534, mean reward: -5382.534 [-5382.534, -5382.534], mean action: 2.000 [2.000, 2.000],  loss: 2625416.000000, mae: 4277.774414, mean_q: -2573.137207
 3317/5000: episode: 3317, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1952.149, mean reward: -1952.149 [-1952.149, -1952.149], mean action: 3.000 [3.000, 3.000],  loss: 2638090.000000, mae: 4187.739258, mean_q: -2562.792480
 3318/5000: episode: 3318, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1965.244, mean reward: -1965.244 [-1965.244, -1965.244], mean action: 2.000 [2.000, 2.000],  loss: 1678969.250000, mae: 4151.888672, mean_q: -2560.561523
 3319/5000: episode: 3319, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5736.825, mean reward: -5736.825 [-5736.825, -5736.825], mean action: 2.000 [2.000, 2.000],  loss: 3084512.500000, mae: 4131.617676, mean_q: -2548.475098
 3320/5000: episode: 3320, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -639.185, mean reward: -639.185 [-639.185, -639.185], mean action: 2.000 [2.000, 2.000],  loss: 2389943.000000, mae: 4216.601562, mean_q: -2527.469238
 3321/5000: episode: 3321, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2146.890, mean reward: -2146.890 [-2146.890, -2146.890], mean action: 2.000 [2.000, 2.000],  loss: 2224221.500000, mae: 4105.984375, mean_q: -2521.908203
 3322/5000: episode: 3322, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -3762.880, mean reward: -3762.880 [-3762.880, -3762.880], mean action: 2.000 [2.000, 2.000],  loss: 3456959.250000, mae: 4149.692383, mean_q: -2516.352051
 3323/5000: episode: 3323, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1552.708, mean reward: -1552.708 [-1552.708, -1552.708], mean action: 2.000 [2.000, 2.000],  loss: 3027926.000000, mae: 4171.785156, mean_q: -2508.952637
 3324/5000: episode: 3324, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1165.962, mean reward: -1165.962 [-1165.962, -1165.962], mean action: 3.000 [3.000, 3.000],  loss: 3773568.000000, mae: 4255.506836, mean_q: -2510.475830
 3325/5000: episode: 3325, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1006.729, mean reward: -1006.729 [-1006.729, -1006.729], mean action: 2.000 [2.000, 2.000],  loss: 4431068.500000, mae: 4256.916016, mean_q: -2536.980957
 3326/5000: episode: 3326, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1954.392, mean reward: -1954.392 [-1954.392, -1954.392], mean action: 2.000 [2.000, 2.000],  loss: 3462731.000000, mae: 4155.018066, mean_q: -2538.139160
 3327/5000: episode: 3327, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -693.838, mean reward: -693.838 [-693.838, -693.838], mean action: 2.000 [2.000, 2.000],  loss: 2584700.250000, mae: 4124.933105, mean_q: -2558.649414
 3328/5000: episode: 3328, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1015.852, mean reward: -1015.852 [-1015.852, -1015.852], mean action: 2.000 [2.000, 2.000],  loss: 3120318.000000, mae: 4186.567871, mean_q: -2586.107422
 3329/5000: episode: 3329, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2953.158, mean reward: -2953.158 [-2953.158, -2953.158], mean action: 2.000 [2.000, 2.000],  loss: 2836181.750000, mae: 4101.171387, mean_q: -2589.911133
 3330/5000: episode: 3330, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -970.419, mean reward: -970.419 [-970.419, -970.419], mean action: 2.000 [2.000, 2.000],  loss: 3238207.500000, mae: 4171.482910, mean_q: -2604.276611
 3331/5000: episode: 3331, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5218.130, mean reward: -5218.130 [-5218.130, -5218.130], mean action: 2.000 [2.000, 2.000],  loss: 1942731.500000, mae: 4100.215820, mean_q: -2600.247070
 3332/5000: episode: 3332, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2551.546, mean reward: -2551.546 [-2551.546, -2551.546], mean action: 2.000 [2.000, 2.000],  loss: 1750626.250000, mae: 4060.426514, mean_q: -2620.836914
 3333/5000: episode: 3333, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -975.194, mean reward: -975.194 [-975.194, -975.194], mean action: 2.000 [2.000, 2.000],  loss: 4857020.500000, mae: 4285.941895, mean_q: -2628.284180
 3334/5000: episode: 3334, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1304.217, mean reward: -1304.217 [-1304.217, -1304.217], mean action: 1.000 [1.000, 1.000],  loss: 2236085.500000, mae: 4075.966309, mean_q: -2631.153076
 3335/5000: episode: 3335, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1535.946, mean reward: -1535.946 [-1535.946, -1535.946], mean action: 2.000 [2.000, 2.000],  loss: 2298022.000000, mae: 4113.846680, mean_q: -2637.981934
 3336/5000: episode: 3336, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2865.123, mean reward: -2865.123 [-2865.123, -2865.123], mean action: 3.000 [3.000, 3.000],  loss: 2868450.000000, mae: 4032.628906, mean_q: -2631.177490
 3337/5000: episode: 3337, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2521.044, mean reward: -2521.044 [-2521.044, -2521.044], mean action: 2.000 [2.000, 2.000],  loss: 1574226.375000, mae: 3995.019043, mean_q: -2634.197754
 3338/5000: episode: 3338, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3937.225, mean reward: -3937.225 [-3937.225, -3937.225], mean action: 2.000 [2.000, 2.000],  loss: 2598439.000000, mae: 4134.356934, mean_q: -2644.900879
 3339/5000: episode: 3339, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -274.545, mean reward: -274.545 [-274.545, -274.545], mean action: 2.000 [2.000, 2.000],  loss: 4254046.500000, mae: 4134.198730, mean_q: -2645.368408
 3340/5000: episode: 3340, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3692.753, mean reward: -3692.753 [-3692.753, -3692.753], mean action: 2.000 [2.000, 2.000],  loss: 2210717.500000, mae: 4089.009277, mean_q: -2661.135254
 3341/5000: episode: 3341, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9032.825, mean reward: -9032.825 [-9032.825, -9032.825], mean action: 2.000 [2.000, 2.000],  loss: 1978170.750000, mae: 4088.613037, mean_q: -2657.406738
 3342/5000: episode: 3342, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1270.860, mean reward: -1270.860 [-1270.860, -1270.860], mean action: 2.000 [2.000, 2.000],  loss: 5728965.500000, mae: 4367.125977, mean_q: -2662.480469
 3343/5000: episode: 3343, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3027.080, mean reward: -3027.080 [-3027.080, -3027.080], mean action: 3.000 [3.000, 3.000],  loss: 2824529.500000, mae: 4084.194336, mean_q: -2660.694336
 3344/5000: episode: 3344, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1201.061, mean reward: -1201.061 [-1201.061, -1201.061], mean action: 2.000 [2.000, 2.000],  loss: 4768177.500000, mae: 4274.908203, mean_q: -2660.764648
 3345/5000: episode: 3345, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -396.983, mean reward: -396.983 [-396.983, -396.983], mean action: 2.000 [2.000, 2.000],  loss: 2874222.000000, mae: 4094.188721, mean_q: -2658.861816
 3346/5000: episode: 3346, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1471.464, mean reward: -1471.464 [-1471.464, -1471.464], mean action: 2.000 [2.000, 2.000],  loss: 3896836.000000, mae: 4195.491211, mean_q: -2664.925781
 3347/5000: episode: 3347, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -4807.833, mean reward: -4807.833 [-4807.833, -4807.833], mean action: 2.000 [2.000, 2.000],  loss: 2515995.000000, mae: 4112.228027, mean_q: -2675.795410
 3348/5000: episode: 3348, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -933.192, mean reward: -933.192 [-933.192, -933.192], mean action: 2.000 [2.000, 2.000],  loss: 2233111.000000, mae: 4003.971191, mean_q: -2667.785156
 3349/5000: episode: 3349, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -476.860, mean reward: -476.860 [-476.860, -476.860], mean action: 2.000 [2.000, 2.000],  loss: 3240411.250000, mae: 4098.044922, mean_q: -2652.596436
 3350/5000: episode: 3350, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2578.876, mean reward: -2578.876 [-2578.876, -2578.876], mean action: 2.000 [2.000, 2.000],  loss: 2487588.750000, mae: 4049.989990, mean_q: -2661.530762
 3351/5000: episode: 3351, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1174.824, mean reward: -1174.824 [-1174.824, -1174.824], mean action: 2.000 [2.000, 2.000],  loss: 3358684.500000, mae: 4072.834961, mean_q: -2669.801270
 3352/5000: episode: 3352, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1829.653, mean reward: -1829.653 [-1829.653, -1829.653], mean action: 2.000 [2.000, 2.000],  loss: 3723362.500000, mae: 4181.647461, mean_q: -2670.794189
 3353/5000: episode: 3353, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3199.262, mean reward: -3199.262 [-3199.262, -3199.262], mean action: 2.000 [2.000, 2.000],  loss: 2672104.750000, mae: 4083.661621, mean_q: -2658.557617
 3354/5000: episode: 3354, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4293.840, mean reward: -4293.840 [-4293.840, -4293.840], mean action: 0.000 [0.000, 0.000],  loss: 3340811.000000, mae: 4173.179688, mean_q: -2673.246338
 3355/5000: episode: 3355, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -157.372, mean reward: -157.372 [-157.372, -157.372], mean action: 2.000 [2.000, 2.000],  loss: 3063500.500000, mae: 4108.813965, mean_q: -2674.285645
 3356/5000: episode: 3356, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1551.138, mean reward: -1551.138 [-1551.138, -1551.138], mean action: 2.000 [2.000, 2.000],  loss: 2359822.000000, mae: 4068.416504, mean_q: -2660.034424
 3357/5000: episode: 3357, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1952.803, mean reward: -1952.803 [-1952.803, -1952.803], mean action: 2.000 [2.000, 2.000],  loss: 3873074.500000, mae: 4111.775879, mean_q: -2660.889160
 3358/5000: episode: 3358, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -378.377, mean reward: -378.377 [-378.377, -378.377], mean action: 2.000 [2.000, 2.000],  loss: 1892213.750000, mae: 4048.789795, mean_q: -2656.348877
 3359/5000: episode: 3359, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5759.386, mean reward: -5759.386 [-5759.386, -5759.386], mean action: 2.000 [2.000, 2.000],  loss: 4094050.250000, mae: 4194.722656, mean_q: -2653.289307
 3360/5000: episode: 3360, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -348.963, mean reward: -348.963 [-348.963, -348.963], mean action: 2.000 [2.000, 2.000],  loss: 2488236.500000, mae: 4098.737305, mean_q: -2664.199219
 3361/5000: episode: 3361, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2887.723, mean reward: -2887.723 [-2887.723, -2887.723], mean action: 2.000 [2.000, 2.000],  loss: 2616117.750000, mae: 4089.976807, mean_q: -2642.995117
 3362/5000: episode: 3362, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3086.104, mean reward: -3086.104 [-3086.104, -3086.104], mean action: 2.000 [2.000, 2.000],  loss: 4348764.500000, mae: 4274.738281, mean_q: -2659.852051
 3363/5000: episode: 3363, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1281.911, mean reward: -1281.911 [-1281.911, -1281.911], mean action: 3.000 [3.000, 3.000],  loss: 1989905.000000, mae: 4054.187012, mean_q: -2650.442139
 3364/5000: episode: 3364, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -698.786, mean reward: -698.786 [-698.786, -698.786], mean action: 2.000 [2.000, 2.000],  loss: 1562798.250000, mae: 4083.443848, mean_q: -2638.197266
 3365/5000: episode: 3365, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1000.119, mean reward: -1000.119 [-1000.119, -1000.119], mean action: 2.000 [2.000, 2.000],  loss: 2156175.000000, mae: 4064.419434, mean_q: -2639.724609
 3366/5000: episode: 3366, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1021.326, mean reward: -1021.326 [-1021.326, -1021.326], mean action: 2.000 [2.000, 2.000],  loss: 3353689.500000, mae: 4192.095703, mean_q: -2630.766602
 3367/5000: episode: 3367, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5229.106, mean reward: -5229.106 [-5229.106, -5229.106], mean action: 2.000 [2.000, 2.000],  loss: 2716095.250000, mae: 4126.199707, mean_q: -2617.509766
 3368/5000: episode: 3368, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3541.709, mean reward: -3541.709 [-3541.709, -3541.709], mean action: 2.000 [2.000, 2.000],  loss: 3617728.000000, mae: 4129.476562, mean_q: -2601.794434
 3369/5000: episode: 3369, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -40.801, mean reward: -40.801 [-40.801, -40.801], mean action: 2.000 [2.000, 2.000],  loss: 2905546.750000, mae: 4172.614258, mean_q: -2593.798828
 3370/5000: episode: 3370, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -3541.183, mean reward: -3541.183 [-3541.183, -3541.183], mean action: 2.000 [2.000, 2.000],  loss: 3185120.500000, mae: 4066.959961, mean_q: -2598.115967
 3371/5000: episode: 3371, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1445.276, mean reward: -1445.276 [-1445.276, -1445.276], mean action: 2.000 [2.000, 2.000],  loss: 1893815.500000, mae: 4065.858887, mean_q: -2597.786621
 3372/5000: episode: 3372, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2453.734, mean reward: -2453.734 [-2453.734, -2453.734], mean action: 2.000 [2.000, 2.000],  loss: 2449820.250000, mae: 3997.831055, mean_q: -2592.062500
 3373/5000: episode: 3373, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -142.117, mean reward: -142.117 [-142.117, -142.117], mean action: 2.000 [2.000, 2.000],  loss: 1668030.750000, mae: 3960.216064, mean_q: -2579.968018
 3374/5000: episode: 3374, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5815.988, mean reward: -5815.988 [-5815.988, -5815.988], mean action: 2.000 [2.000, 2.000],  loss: 1533545.625000, mae: 3917.178223, mean_q: -2561.696777
 3375/5000: episode: 3375, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6120.976, mean reward: -6120.976 [-6120.976, -6120.976], mean action: 2.000 [2.000, 2.000],  loss: 3234118.500000, mae: 4118.583008, mean_q: -2572.442871
 3376/5000: episode: 3376, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1361.781, mean reward: -1361.781 [-1361.781, -1361.781], mean action: 2.000 [2.000, 2.000],  loss: 3373601.500000, mae: 4148.347656, mean_q: -2570.668701
 3377/5000: episode: 3377, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2393.286, mean reward: -2393.286 [-2393.286, -2393.286], mean action: 2.000 [2.000, 2.000],  loss: 2929621.500000, mae: 4124.554688, mean_q: -2561.657715
 3378/5000: episode: 3378, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1793.786, mean reward: -1793.786 [-1793.786, -1793.786], mean action: 2.000 [2.000, 2.000],  loss: 3050737.500000, mae: 4147.569336, mean_q: -2553.863037
 3379/5000: episode: 3379, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5266.973, mean reward: -5266.973 [-5266.973, -5266.973], mean action: 2.000 [2.000, 2.000],  loss: 4026920.000000, mae: 4187.332031, mean_q: -2566.292725
 3380/5000: episode: 3380, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -9872.851, mean reward: -9872.851 [-9872.851, -9872.851], mean action: 2.000 [2.000, 2.000],  loss: 3733815.000000, mae: 4188.884277, mean_q: -2571.278320
 3381/5000: episode: 3381, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2590.169, mean reward: -2590.169 [-2590.169, -2590.169], mean action: 2.000 [2.000, 2.000],  loss: 2143818.000000, mae: 4061.851074, mean_q: -2574.856445
 3382/5000: episode: 3382, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -93.563, mean reward: -93.563 [-93.563, -93.563], mean action: 2.000 [2.000, 2.000],  loss: 3246208.000000, mae: 4138.921387, mean_q: -2583.235840
 3383/5000: episode: 3383, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -729.935, mean reward: -729.935 [-729.935, -729.935], mean action: 1.000 [1.000, 1.000],  loss: 2286225.250000, mae: 4075.772461, mean_q: -2594.104248
 3384/5000: episode: 3384, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -811.753, mean reward: -811.753 [-811.753, -811.753], mean action: 2.000 [2.000, 2.000],  loss: 1356527.000000, mae: 4016.527344, mean_q: -2602.279541
 3385/5000: episode: 3385, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4187.834, mean reward: -4187.834 [-4187.834, -4187.834], mean action: 2.000 [2.000, 2.000],  loss: 3548024.250000, mae: 4194.031738, mean_q: -2613.231934
 3386/5000: episode: 3386, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -431.366, mean reward: -431.366 [-431.366, -431.366], mean action: 2.000 [2.000, 2.000],  loss: 3142203.500000, mae: 4059.432373, mean_q: -2612.246094
 3387/5000: episode: 3387, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1456.056, mean reward: -1456.056 [-1456.056, -1456.056], mean action: 2.000 [2.000, 2.000],  loss: 2220441.250000, mae: 4100.594727, mean_q: -2615.817871
 3388/5000: episode: 3388, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -48.037, mean reward: -48.037 [-48.037, -48.037], mean action: 2.000 [2.000, 2.000],  loss: 3035487.000000, mae: 4199.437012, mean_q: -2625.477539
 3389/5000: episode: 3389, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2112.192, mean reward: -2112.192 [-2112.192, -2112.192], mean action: 2.000 [2.000, 2.000],  loss: 3381770.000000, mae: 4123.822266, mean_q: -2619.770996
 3390/5000: episode: 3390, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3957.448, mean reward: -3957.448 [-3957.448, -3957.448], mean action: 2.000 [2.000, 2.000],  loss: 2869952.750000, mae: 4092.098633, mean_q: -2633.113770
 3391/5000: episode: 3391, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4686.645, mean reward: -4686.645 [-4686.645, -4686.645], mean action: 2.000 [2.000, 2.000],  loss: 2114791.000000, mae: 4111.170410, mean_q: -2621.737549
 3392/5000: episode: 3392, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6219.597, mean reward: -6219.597 [-6219.597, -6219.597], mean action: 0.000 [0.000, 0.000],  loss: 2002397.625000, mae: 4082.014160, mean_q: -2619.280762
 3393/5000: episode: 3393, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -10076.149, mean reward: -10076.149 [-10076.149, -10076.149], mean action: 1.000 [1.000, 1.000],  loss: 2364322.250000, mae: 4083.499023, mean_q: -2609.015625
 3394/5000: episode: 3394, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1390.910, mean reward: -1390.910 [-1390.910, -1390.910], mean action: 2.000 [2.000, 2.000],  loss: 2174663.000000, mae: 4120.770508, mean_q: -2628.161865
 3395/5000: episode: 3395, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -700.783, mean reward: -700.783 [-700.783, -700.783], mean action: 2.000 [2.000, 2.000],  loss: 1193659.125000, mae: 3919.225586, mean_q: -2602.876953
 3396/5000: episode: 3396, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -66.099, mean reward: -66.099 [-66.099, -66.099], mean action: 2.000 [2.000, 2.000],  loss: 2222396.000000, mae: 4121.907227, mean_q: -2589.848633
 3397/5000: episode: 3397, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2477.799, mean reward: -2477.799 [-2477.799, -2477.799], mean action: 2.000 [2.000, 2.000],  loss: 4918281.000000, mae: 4244.347656, mean_q: -2595.208496
 3398/5000: episode: 3398, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1918.471, mean reward: -1918.471 [-1918.471, -1918.471], mean action: 2.000 [2.000, 2.000],  loss: 2464777.750000, mae: 4117.841797, mean_q: -2590.327637
 3399/5000: episode: 3399, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -167.020, mean reward: -167.020 [-167.020, -167.020], mean action: 2.000 [2.000, 2.000],  loss: 4503214.500000, mae: 4251.971680, mean_q: -2587.499023
 3400/5000: episode: 3400, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -155.395, mean reward: -155.395 [-155.395, -155.395], mean action: 2.000 [2.000, 2.000],  loss: 2624521.750000, mae: 4178.836914, mean_q: -2596.983887
 3401/5000: episode: 3401, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -6425.474, mean reward: -6425.474 [-6425.474, -6425.474], mean action: 2.000 [2.000, 2.000],  loss: 2766078.500000, mae: 4223.256348, mean_q: -2593.545898
 3402/5000: episode: 3402, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -925.302, mean reward: -925.302 [-925.302, -925.302], mean action: 2.000 [2.000, 2.000],  loss: 3705871.000000, mae: 4216.882324, mean_q: -2576.258789
 3403/5000: episode: 3403, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -8353.020, mean reward: -8353.020 [-8353.020, -8353.020], mean action: 2.000 [2.000, 2.000],  loss: 2464500.750000, mae: 4160.815430, mean_q: -2608.649902
 3404/5000: episode: 3404, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -985.610, mean reward: -985.610 [-985.610, -985.610], mean action: 2.000 [2.000, 2.000],  loss: 3173943.000000, mae: 4123.047363, mean_q: -2611.351807
 3405/5000: episode: 3405, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6386.650, mean reward: -6386.650 [-6386.650, -6386.650], mean action: 2.000 [2.000, 2.000],  loss: 2524010.500000, mae: 4077.660645, mean_q: -2629.967285
 3406/5000: episode: 3406, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -3108.385, mean reward: -3108.385 [-3108.385, -3108.385], mean action: 2.000 [2.000, 2.000],  loss: 2234442.500000, mae: 4131.079590, mean_q: -2633.785645
 3407/5000: episode: 3407, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6577.613, mean reward: -6577.613 [-6577.613, -6577.613], mean action: 2.000 [2.000, 2.000],  loss: 2878106.500000, mae: 4075.089600, mean_q: -2622.840576
 3408/5000: episode: 3408, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3685.918, mean reward: -3685.918 [-3685.918, -3685.918], mean action: 0.000 [0.000, 0.000],  loss: 2951040.250000, mae: 4158.195312, mean_q: -2635.427490
 3409/5000: episode: 3409, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4177.020, mean reward: -4177.020 [-4177.020, -4177.020], mean action: 2.000 [2.000, 2.000],  loss: 3071668.500000, mae: 4253.044922, mean_q: -2645.046875
 3410/5000: episode: 3410, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1213.031, mean reward: -1213.031 [-1213.031, -1213.031], mean action: 2.000 [2.000, 2.000],  loss: 4258483.000000, mae: 4259.551758, mean_q: -2657.220459
 3411/5000: episode: 3411, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -3091.179, mean reward: -3091.179 [-3091.179, -3091.179], mean action: 2.000 [2.000, 2.000],  loss: 3277093.750000, mae: 4160.481445, mean_q: -2657.002197
 3412/5000: episode: 3412, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2411.425, mean reward: -2411.425 [-2411.425, -2411.425], mean action: 2.000 [2.000, 2.000],  loss: 2805810.500000, mae: 4218.691406, mean_q: -2658.587402
 3413/5000: episode: 3413, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2449.976, mean reward: -2449.976 [-2449.976, -2449.976], mean action: 2.000 [2.000, 2.000],  loss: 2198612.500000, mae: 4085.739502, mean_q: -2646.113281
 3414/5000: episode: 3414, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1903.688, mean reward: -1903.688 [-1903.688, -1903.688], mean action: 2.000 [2.000, 2.000],  loss: 3999168.250000, mae: 4264.869141, mean_q: -2649.291016
 3415/5000: episode: 3415, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2872.254, mean reward: -2872.254 [-2872.254, -2872.254], mean action: 2.000 [2.000, 2.000],  loss: 1808964.625000, mae: 4090.087891, mean_q: -2643.805420
 3416/5000: episode: 3416, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -484.982, mean reward: -484.982 [-484.982, -484.982], mean action: 3.000 [3.000, 3.000],  loss: 2755811.500000, mae: 4188.654297, mean_q: -2643.218262
 3417/5000: episode: 3417, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4309.570, mean reward: -4309.570 [-4309.570, -4309.570], mean action: 2.000 [2.000, 2.000],  loss: 3728124.750000, mae: 4139.993164, mean_q: -2627.808105
 3418/5000: episode: 3418, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4952.232, mean reward: -4952.232 [-4952.232, -4952.232], mean action: 0.000 [0.000, 0.000],  loss: 2654579.500000, mae: 4084.341309, mean_q: -2638.260254
 3419/5000: episode: 3419, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1433.063, mean reward: -1433.063 [-1433.063, -1433.063], mean action: 2.000 [2.000, 2.000],  loss: 3448923.000000, mae: 4185.565918, mean_q: -2637.403809
 3420/5000: episode: 3420, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -394.548, mean reward: -394.548 [-394.548, -394.548], mean action: 2.000 [2.000, 2.000],  loss: 1209855.250000, mae: 4011.609375, mean_q: -2628.003662
 3421/5000: episode: 3421, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4598.505, mean reward: -4598.505 [-4598.505, -4598.505], mean action: 2.000 [2.000, 2.000],  loss: 2471532.000000, mae: 4101.569336, mean_q: -2635.558594
 3422/5000: episode: 3422, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3972.420, mean reward: -3972.420 [-3972.420, -3972.420], mean action: 2.000 [2.000, 2.000],  loss: 2922520.000000, mae: 4189.783691, mean_q: -2635.980469
 3423/5000: episode: 3423, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1140.550, mean reward: -1140.550 [-1140.550, -1140.550], mean action: 2.000 [2.000, 2.000],  loss: 2612075.500000, mae: 4177.810547, mean_q: -2640.461670
 3424/5000: episode: 3424, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1905.107, mean reward: -1905.107 [-1905.107, -1905.107], mean action: 2.000 [2.000, 2.000],  loss: 1957342.500000, mae: 4130.896484, mean_q: -2640.736084
 3425/5000: episode: 3425, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1119.972, mean reward: -1119.972 [-1119.972, -1119.972], mean action: 2.000 [2.000, 2.000],  loss: 2896559.250000, mae: 4016.109619, mean_q: -2637.454102
 3426/5000: episode: 3426, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1144.115, mean reward: -1144.115 [-1144.115, -1144.115], mean action: 2.000 [2.000, 2.000],  loss: 3349234.500000, mae: 4166.459961, mean_q: -2640.311768
 3427/5000: episode: 3427, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4169.926, mean reward: -4169.926 [-4169.926, -4169.926], mean action: 2.000 [2.000, 2.000],  loss: 2214128.500000, mae: 4110.248535, mean_q: -2643.867432
 3428/5000: episode: 3428, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2552.784, mean reward: -2552.784 [-2552.784, -2552.784], mean action: 2.000 [2.000, 2.000],  loss: 3285017.500000, mae: 4130.160645, mean_q: -2623.005615
 3429/5000: episode: 3429, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1684.118, mean reward: -1684.118 [-1684.118, -1684.118], mean action: 2.000 [2.000, 2.000],  loss: 2700017.750000, mae: 4156.372070, mean_q: -2654.781494
 3430/5000: episode: 3430, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1246.270, mean reward: -1246.270 [-1246.270, -1246.270], mean action: 2.000 [2.000, 2.000],  loss: 1907117.750000, mae: 4108.662109, mean_q: -2640.418945
 3431/5000: episode: 3431, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5433.108, mean reward: -5433.108 [-5433.108, -5433.108], mean action: 2.000 [2.000, 2.000],  loss: 2311361.500000, mae: 4173.406738, mean_q: -2654.434570
 3432/5000: episode: 3432, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1289.586, mean reward: -1289.586 [-1289.586, -1289.586], mean action: 2.000 [2.000, 2.000],  loss: 2894617.750000, mae: 4182.034180, mean_q: -2639.747314
 3433/5000: episode: 3433, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5546.679, mean reward: -5546.679 [-5546.679, -5546.679], mean action: 2.000 [2.000, 2.000],  loss: 2923012.250000, mae: 4075.845215, mean_q: -2641.579590
 3434/5000: episode: 3434, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1439.008, mean reward: -1439.008 [-1439.008, -1439.008], mean action: 2.000 [2.000, 2.000],  loss: 3362799.500000, mae: 4167.499512, mean_q: -2639.899414
 3435/5000: episode: 3435, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -18171.653, mean reward: -18171.653 [-18171.653, -18171.653], mean action: 0.000 [0.000, 0.000],  loss: 3394328.000000, mae: 4201.682617, mean_q: -2656.896484
 3436/5000: episode: 3436, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2160.926, mean reward: -2160.926 [-2160.926, -2160.926], mean action: 2.000 [2.000, 2.000],  loss: 4721118.000000, mae: 4282.639648, mean_q: -2642.631348
 3437/5000: episode: 3437, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2280.571, mean reward: -2280.571 [-2280.571, -2280.571], mean action: 2.000 [2.000, 2.000],  loss: 2783752.500000, mae: 4219.019531, mean_q: -2649.482422
 3438/5000: episode: 3438, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5543.729, mean reward: -5543.729 [-5543.729, -5543.729], mean action: 2.000 [2.000, 2.000],  loss: 3757894.500000, mae: 4328.581055, mean_q: -2648.351074
 3439/5000: episode: 3439, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2157.977, mean reward: -2157.977 [-2157.977, -2157.977], mean action: 2.000 [2.000, 2.000],  loss: 3317668.000000, mae: 4228.073242, mean_q: -2647.034424
 3440/5000: episode: 3440, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4669.254, mean reward: -4669.254 [-4669.254, -4669.254], mean action: 2.000 [2.000, 2.000],  loss: 4590537.000000, mae: 4298.458984, mean_q: -2669.111816
 3441/5000: episode: 3441, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -568.341, mean reward: -568.341 [-568.341, -568.341], mean action: 2.000 [2.000, 2.000],  loss: 1777068.500000, mae: 4164.138672, mean_q: -2650.625000
 3442/5000: episode: 3442, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2709.036, mean reward: -2709.036 [-2709.036, -2709.036], mean action: 2.000 [2.000, 2.000],  loss: 3932283.000000, mae: 4284.924805, mean_q: -2666.873291
 3443/5000: episode: 3443, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -10523.905, mean reward: -10523.905 [-10523.905, -10523.905], mean action: 2.000 [2.000, 2.000],  loss: 2876595.750000, mae: 4216.385742, mean_q: -2655.045410
 3444/5000: episode: 3444, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -320.240, mean reward: -320.240 [-320.240, -320.240], mean action: 2.000 [2.000, 2.000],  loss: 1212117.000000, mae: 4098.804199, mean_q: -2664.473633
 3445/5000: episode: 3445, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -2304.659, mean reward: -2304.659 [-2304.659, -2304.659], mean action: 2.000 [2.000, 2.000],  loss: 1879373.000000, mae: 4073.050049, mean_q: -2651.439209
 3446/5000: episode: 3446, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3378.615, mean reward: -3378.615 [-3378.615, -3378.615], mean action: 2.000 [2.000, 2.000],  loss: 2474163.250000, mae: 4170.530273, mean_q: -2639.758301
 3447/5000: episode: 3447, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -662.292, mean reward: -662.292 [-662.292, -662.292], mean action: 2.000 [2.000, 2.000],  loss: 3777167.750000, mae: 4236.907227, mean_q: -2638.020996
 3448/5000: episode: 3448, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2023.735, mean reward: -2023.735 [-2023.735, -2023.735], mean action: 2.000 [2.000, 2.000],  loss: 2468926.000000, mae: 4053.340332, mean_q: -2612.349854
 3449/5000: episode: 3449, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1349.245, mean reward: -1349.245 [-1349.245, -1349.245], mean action: 2.000 [2.000, 2.000],  loss: 5366423.000000, mae: 4291.013184, mean_q: -2614.570801
 3450/5000: episode: 3450, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -928.390, mean reward: -928.390 [-928.390, -928.390], mean action: 2.000 [2.000, 2.000],  loss: 3599094.750000, mae: 4121.745605, mean_q: -2597.263672
 3451/5000: episode: 3451, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2095.403, mean reward: -2095.403 [-2095.403, -2095.403], mean action: 2.000 [2.000, 2.000],  loss: 1992074.125000, mae: 4122.034668, mean_q: -2604.405029
 3452/5000: episode: 3452, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5340.295, mean reward: -5340.295 [-5340.295, -5340.295], mean action: 2.000 [2.000, 2.000],  loss: 2832679.000000, mae: 4236.907227, mean_q: -2614.019531
 3453/5000: episode: 3453, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3152.048, mean reward: -3152.048 [-3152.048, -3152.048], mean action: 2.000 [2.000, 2.000],  loss: 2141274.500000, mae: 4071.503906, mean_q: -2604.793213
 3454/5000: episode: 3454, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3195.470, mean reward: -3195.470 [-3195.470, -3195.470], mean action: 2.000 [2.000, 2.000],  loss: 4781480.000000, mae: 4240.233398, mean_q: -2601.253174
 3455/5000: episode: 3455, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1024.953, mean reward: -1024.953 [-1024.953, -1024.953], mean action: 2.000 [2.000, 2.000],  loss: 2608359.000000, mae: 4102.275391, mean_q: -2616.918945
 3456/5000: episode: 3456, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1385.779, mean reward: -1385.779 [-1385.779, -1385.779], mean action: 2.000 [2.000, 2.000],  loss: 4991650.000000, mae: 4350.353516, mean_q: -2618.946289
 3457/5000: episode: 3457, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9378.339, mean reward: -9378.339 [-9378.339, -9378.339], mean action: 2.000 [2.000, 2.000],  loss: 3381491.250000, mae: 4130.620117, mean_q: -2617.713623
 3458/5000: episode: 3458, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1035.025, mean reward: -1035.025 [-1035.025, -1035.025], mean action: 1.000 [1.000, 1.000],  loss: 1966515.750000, mae: 4100.698730, mean_q: -2626.041260
 3459/5000: episode: 3459, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1800.398, mean reward: -1800.398 [-1800.398, -1800.398], mean action: 2.000 [2.000, 2.000],  loss: 3066239.500000, mae: 4218.064453, mean_q: -2635.772461
 3460/5000: episode: 3460, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6654.127, mean reward: -6654.127 [-6654.127, -6654.127], mean action: 2.000 [2.000, 2.000],  loss: 1649603.375000, mae: 4117.617188, mean_q: -2638.669922
 3461/5000: episode: 3461, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6743.166, mean reward: -6743.166 [-6743.166, -6743.166], mean action: 2.000 [2.000, 2.000],  loss: 2986088.000000, mae: 4186.023926, mean_q: -2634.199707
 3462/5000: episode: 3462, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2824.982, mean reward: -2824.982 [-2824.982, -2824.982], mean action: 2.000 [2.000, 2.000],  loss: 3015877.500000, mae: 4230.961914, mean_q: -2640.203857
 3463/5000: episode: 3463, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2220.684, mean reward: -2220.684 [-2220.684, -2220.684], mean action: 2.000 [2.000, 2.000],  loss: 2718347.500000, mae: 4131.885254, mean_q: -2635.620850
 3464/5000: episode: 3464, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2860.667, mean reward: -2860.667 [-2860.667, -2860.667], mean action: 2.000 [2.000, 2.000],  loss: 2792464.000000, mae: 4191.546875, mean_q: -2672.603516
 3465/5000: episode: 3465, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5085.283, mean reward: -5085.283 [-5085.283, -5085.283], mean action: 2.000 [2.000, 2.000],  loss: 1747623.000000, mae: 3994.178711, mean_q: -2660.185059
 3466/5000: episode: 3466, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2680.128, mean reward: -2680.128 [-2680.128, -2680.128], mean action: 2.000 [2.000, 2.000],  loss: 2421757.000000, mae: 4112.921875, mean_q: -2671.718262
 3467/5000: episode: 3467, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3368.326, mean reward: -3368.326 [-3368.326, -3368.326], mean action: 2.000 [2.000, 2.000],  loss: 2352873.750000, mae: 4132.283691, mean_q: -2677.695801
 3468/5000: episode: 3468, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2363.982, mean reward: -2363.982 [-2363.982, -2363.982], mean action: 2.000 [2.000, 2.000],  loss: 2058624.625000, mae: 4154.518555, mean_q: -2682.492920
 3469/5000: episode: 3469, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4221.671, mean reward: -4221.671 [-4221.671, -4221.671], mean action: 2.000 [2.000, 2.000],  loss: 1496715.875000, mae: 4015.227539, mean_q: -2665.273438
 3470/5000: episode: 3470, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1649.976, mean reward: -1649.976 [-1649.976, -1649.976], mean action: 2.000 [2.000, 2.000],  loss: 1539230.125000, mae: 3982.320312, mean_q: -2692.111816
 3471/5000: episode: 3471, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6820.082, mean reward: -6820.082 [-6820.082, -6820.082], mean action: 2.000 [2.000, 2.000],  loss: 3573505.000000, mae: 4203.114258, mean_q: -2661.508301
 3472/5000: episode: 3472, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2396.047, mean reward: -2396.047 [-2396.047, -2396.047], mean action: 2.000 [2.000, 2.000],  loss: 2507536.000000, mae: 4066.993652, mean_q: -2657.510986
 3473/5000: episode: 3473, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3218.647, mean reward: -3218.647 [-3218.647, -3218.647], mean action: 2.000 [2.000, 2.000],  loss: 2646168.500000, mae: 4173.509766, mean_q: -2654.658691
 3474/5000: episode: 3474, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1779.383, mean reward: -1779.383 [-1779.383, -1779.383], mean action: 2.000 [2.000, 2.000],  loss: 4084869.500000, mae: 4203.162109, mean_q: -2658.126709
 3475/5000: episode: 3475, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -176.628, mean reward: -176.628 [-176.628, -176.628], mean action: 2.000 [2.000, 2.000],  loss: 2765227.000000, mae: 4136.893066, mean_q: -2658.356445
 3476/5000: episode: 3476, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1526.379, mean reward: -1526.379 [-1526.379, -1526.379], mean action: 2.000 [2.000, 2.000],  loss: 2933481.750000, mae: 4178.037598, mean_q: -2652.531006
 3477/5000: episode: 3477, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9056.303, mean reward: -9056.303 [-9056.303, -9056.303], mean action: 2.000 [2.000, 2.000],  loss: 3155515.000000, mae: 4227.193359, mean_q: -2660.915771
 3478/5000: episode: 3478, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -10116.733, mean reward: -10116.733 [-10116.733, -10116.733], mean action: 2.000 [2.000, 2.000],  loss: 4526624.000000, mae: 4143.664062, mean_q: -2659.071777
 3479/5000: episode: 3479, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -1472.221, mean reward: -1472.221 [-1472.221, -1472.221], mean action: 2.000 [2.000, 2.000],  loss: 2384382.500000, mae: 4100.064453, mean_q: -2665.727783
 3480/5000: episode: 3480, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1909.275, mean reward: -1909.275 [-1909.275, -1909.275], mean action: 2.000 [2.000, 2.000],  loss: 2354489.000000, mae: 4090.517822, mean_q: -2692.777588
 3481/5000: episode: 3481, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -707.732, mean reward: -707.732 [-707.732, -707.732], mean action: 2.000 [2.000, 2.000],  loss: 1812449.500000, mae: 3992.383301, mean_q: -2676.466797
 3482/5000: episode: 3482, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4041.504, mean reward: -4041.504 [-4041.504, -4041.504], mean action: 2.000 [2.000, 2.000],  loss: 2672675.750000, mae: 4148.855469, mean_q: -2670.848389
 3483/5000: episode: 3483, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1009.785, mean reward: -1009.785 [-1009.785, -1009.785], mean action: 2.000 [2.000, 2.000],  loss: 3179486.750000, mae: 4094.786621, mean_q: -2675.006348
 3484/5000: episode: 3484, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2725.236, mean reward: -2725.236 [-2725.236, -2725.236], mean action: 2.000 [2.000, 2.000],  loss: 1554649.000000, mae: 3990.095703, mean_q: -2679.493652
 3485/5000: episode: 3485, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3156.864, mean reward: -3156.864 [-3156.864, -3156.864], mean action: 2.000 [2.000, 2.000],  loss: 3406619.000000, mae: 4209.561523, mean_q: -2673.804688
 3486/5000: episode: 3486, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1127.514, mean reward: -1127.514 [-1127.514, -1127.514], mean action: 2.000 [2.000, 2.000],  loss: 1921993.375000, mae: 4062.945068, mean_q: -2668.443115
 3487/5000: episode: 3487, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4169.927, mean reward: -4169.927 [-4169.927, -4169.927], mean action: 2.000 [2.000, 2.000],  loss: 1862310.250000, mae: 4025.519043, mean_q: -2668.345459
 3488/5000: episode: 3488, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4354.674, mean reward: -4354.674 [-4354.674, -4354.674], mean action: 2.000 [2.000, 2.000],  loss: 1678347.375000, mae: 4001.784912, mean_q: -2661.955322
 3489/5000: episode: 3489, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -737.056, mean reward: -737.056 [-737.056, -737.056], mean action: 2.000 [2.000, 2.000],  loss: 2606790.500000, mae: 4147.472656, mean_q: -2661.390381
 3490/5000: episode: 3490, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1834.137, mean reward: -1834.137 [-1834.137, -1834.137], mean action: 2.000 [2.000, 2.000],  loss: 1364622.500000, mae: 3908.052734, mean_q: -2654.594727
 3491/5000: episode: 3491, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1495.461, mean reward: -1495.461 [-1495.461, -1495.461], mean action: 2.000 [2.000, 2.000],  loss: 2507387.500000, mae: 4008.690186, mean_q: -2635.517822
 3492/5000: episode: 3492, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -863.257, mean reward: -863.257 [-863.257, -863.257], mean action: 2.000 [2.000, 2.000],  loss: 2598057.000000, mae: 4030.605469, mean_q: -2630.257324
 3493/5000: episode: 3493, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2635.746, mean reward: -2635.746 [-2635.746, -2635.746], mean action: 2.000 [2.000, 2.000],  loss: 2812219.000000, mae: 4133.924805, mean_q: -2638.387207
 3494/5000: episode: 3494, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -416.578, mean reward: -416.578 [-416.578, -416.578], mean action: 2.000 [2.000, 2.000],  loss: 2851003.250000, mae: 4074.703613, mean_q: -2636.498779
 3495/5000: episode: 3495, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -368.642, mean reward: -368.642 [-368.642, -368.642], mean action: 2.000 [2.000, 2.000],  loss: 2084363.625000, mae: 4012.027832, mean_q: -2631.766602
 3496/5000: episode: 3496, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9904.903, mean reward: -9904.903 [-9904.903, -9904.903], mean action: 2.000 [2.000, 2.000],  loss: 1366158.250000, mae: 3982.036621, mean_q: -2617.841064
 3497/5000: episode: 3497, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5435.637, mean reward: -5435.637 [-5435.637, -5435.637], mean action: 2.000 [2.000, 2.000],  loss: 3696934.000000, mae: 4200.614258, mean_q: -2618.429443
 3498/5000: episode: 3498, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3822.202, mean reward: -3822.202 [-3822.202, -3822.202], mean action: 2.000 [2.000, 2.000],  loss: 3042075.000000, mae: 4103.502930, mean_q: -2625.909668
 3499/5000: episode: 3499, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -23.465, mean reward: -23.465 [-23.465, -23.465], mean action: 2.000 [2.000, 2.000],  loss: 4981226.000000, mae: 4310.475098, mean_q: -2621.923828
 3500/5000: episode: 3500, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6220.211, mean reward: -6220.211 [-6220.211, -6220.211], mean action: 2.000 [2.000, 2.000],  loss: 2542345.000000, mae: 4073.573486, mean_q: -2602.951660
 3501/5000: episode: 3501, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1607.450, mean reward: -1607.450 [-1607.450, -1607.450], mean action: 2.000 [2.000, 2.000],  loss: 2739702.500000, mae: 4132.607910, mean_q: -2603.978027
 3502/5000: episode: 3502, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -194.484, mean reward: -194.484 [-194.484, -194.484], mean action: 2.000 [2.000, 2.000],  loss: 3331303.000000, mae: 4033.542480, mean_q: -2599.286377
 3503/5000: episode: 3503, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3168.549, mean reward: -3168.549 [-3168.549, -3168.549], mean action: 1.000 [1.000, 1.000],  loss: 3595575.000000, mae: 4196.217285, mean_q: -2609.655273
 3504/5000: episode: 3504, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -4746.119, mean reward: -4746.119 [-4746.119, -4746.119], mean action: 2.000 [2.000, 2.000],  loss: 3597211.250000, mae: 4111.107422, mean_q: -2605.522949
 3505/5000: episode: 3505, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1294.635, mean reward: -1294.635 [-1294.635, -1294.635], mean action: 2.000 [2.000, 2.000],  loss: 3611857.000000, mae: 4120.166016, mean_q: -2602.739746
 3506/5000: episode: 3506, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1403.995, mean reward: -1403.995 [-1403.995, -1403.995], mean action: 2.000 [2.000, 2.000],  loss: 3206012.750000, mae: 4103.830078, mean_q: -2592.875977
 3507/5000: episode: 3507, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2450.568, mean reward: -2450.568 [-2450.568, -2450.568], mean action: 2.000 [2.000, 2.000],  loss: 3391098.500000, mae: 4051.642578, mean_q: -2588.336182
 3508/5000: episode: 3508, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5392.528, mean reward: -5392.528 [-5392.528, -5392.528], mean action: 2.000 [2.000, 2.000],  loss: 3069016.250000, mae: 4094.312012, mean_q: -2579.152100
 3509/5000: episode: 3509, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5893.679, mean reward: -5893.679 [-5893.679, -5893.679], mean action: 2.000 [2.000, 2.000],  loss: 2117754.250000, mae: 4096.516602, mean_q: -2589.311523
 3510/5000: episode: 3510, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1610.824, mean reward: -1610.824 [-1610.824, -1610.824], mean action: 2.000 [2.000, 2.000],  loss: 5115337.000000, mae: 4223.249023, mean_q: -2567.525879
 3511/5000: episode: 3511, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1882.959, mean reward: -1882.959 [-1882.959, -1882.959], mean action: 2.000 [2.000, 2.000],  loss: 3456263.750000, mae: 4196.031250, mean_q: -2570.315674
 3512/5000: episode: 3512, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1790.212, mean reward: -1790.212 [-1790.212, -1790.212], mean action: 2.000 [2.000, 2.000],  loss: 3756456.500000, mae: 4196.232910, mean_q: -2557.038086
 3513/5000: episode: 3513, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -243.520, mean reward: -243.520 [-243.520, -243.520], mean action: 2.000 [2.000, 2.000],  loss: 2582245.000000, mae: 4067.460938, mean_q: -2563.788818
 3514/5000: episode: 3514, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -95.409, mean reward: -95.409 [-95.409, -95.409], mean action: 2.000 [2.000, 2.000],  loss: 1248591.000000, mae: 3991.946045, mean_q: -2566.033936
 3515/5000: episode: 3515, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2554.840, mean reward: -2554.840 [-2554.840, -2554.840], mean action: 2.000 [2.000, 2.000],  loss: 2988498.000000, mae: 4141.826172, mean_q: -2568.671387
 3516/5000: episode: 3516, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4046.991, mean reward: -4046.991 [-4046.991, -4046.991], mean action: 2.000 [2.000, 2.000],  loss: 3062013.750000, mae: 4019.855469, mean_q: -2573.768555
 3517/5000: episode: 3517, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -2585.873, mean reward: -2585.873 [-2585.873, -2585.873], mean action: 2.000 [2.000, 2.000],  loss: 2339692.750000, mae: 4108.518555, mean_q: -2585.030762
 3518/5000: episode: 3518, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4382.904, mean reward: -4382.904 [-4382.904, -4382.904], mean action: 2.000 [2.000, 2.000],  loss: 2546024.750000, mae: 4026.991699, mean_q: -2567.879883
 3519/5000: episode: 3519, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3442.117, mean reward: -3442.117 [-3442.117, -3442.117], mean action: 2.000 [2.000, 2.000],  loss: 1583255.250000, mae: 4043.748535, mean_q: -2569.662598
 3520/5000: episode: 3520, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3498.145, mean reward: -3498.145 [-3498.145, -3498.145], mean action: 1.000 [1.000, 1.000],  loss: 3159073.500000, mae: 4083.563477, mean_q: -2588.146729
 3521/5000: episode: 3521, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3581.490, mean reward: -3581.490 [-3581.490, -3581.490], mean action: 2.000 [2.000, 2.000],  loss: 2415150.000000, mae: 4113.337402, mean_q: -2577.812988
 3522/5000: episode: 3522, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -245.848, mean reward: -245.848 [-245.848, -245.848], mean action: 2.000 [2.000, 2.000],  loss: 2575766.250000, mae: 4097.819336, mean_q: -2605.038818
 3523/5000: episode: 3523, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3344.951, mean reward: -3344.951 [-3344.951, -3344.951], mean action: 1.000 [1.000, 1.000],  loss: 2638992.750000, mae: 4124.561523, mean_q: -2598.876709
 3524/5000: episode: 3524, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4914.220, mean reward: -4914.220 [-4914.220, -4914.220], mean action: 2.000 [2.000, 2.000],  loss: 3425756.750000, mae: 4200.497070, mean_q: -2603.692139
 3525/5000: episode: 3525, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9053.076, mean reward: -9053.076 [-9053.076, -9053.076], mean action: 2.000 [2.000, 2.000],  loss: 4583495.000000, mae: 4249.058105, mean_q: -2612.813477
 3526/5000: episode: 3526, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -402.222, mean reward: -402.222 [-402.222, -402.222], mean action: 2.000 [2.000, 2.000],  loss: 2888072.000000, mae: 4212.444336, mean_q: -2628.765381
 3527/5000: episode: 3527, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -366.433, mean reward: -366.433 [-366.433, -366.433], mean action: 2.000 [2.000, 2.000],  loss: 2213018.250000, mae: 4117.586426, mean_q: -2652.454834
 3528/5000: episode: 3528, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -824.372, mean reward: -824.372 [-824.372, -824.372], mean action: 2.000 [2.000, 2.000],  loss: 2539893.000000, mae: 4135.455566, mean_q: -2638.437012
 3529/5000: episode: 3529, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -453.871, mean reward: -453.871 [-453.871, -453.871], mean action: 2.000 [2.000, 2.000],  loss: 2853187.000000, mae: 4110.971191, mean_q: -2649.359863
 3530/5000: episode: 3530, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -217.965, mean reward: -217.965 [-217.965, -217.965], mean action: 2.000 [2.000, 2.000],  loss: 2911481.000000, mae: 4126.132812, mean_q: -2642.712402
 3531/5000: episode: 3531, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -81.799, mean reward: -81.799 [-81.799, -81.799], mean action: 2.000 [2.000, 2.000],  loss: 3385478.750000, mae: 4082.733398, mean_q: -2620.661377
 3532/5000: episode: 3532, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -116.227, mean reward: -116.227 [-116.227, -116.227], mean action: 2.000 [2.000, 2.000],  loss: 2537208.000000, mae: 4125.059570, mean_q: -2642.876953
 3533/5000: episode: 3533, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1536.348, mean reward: -1536.348 [-1536.348, -1536.348], mean action: 2.000 [2.000, 2.000],  loss: 1217776.250000, mae: 4004.970215, mean_q: -2646.773926
 3534/5000: episode: 3534, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3935.852, mean reward: -3935.852 [-3935.852, -3935.852], mean action: 2.000 [2.000, 2.000],  loss: 3629336.750000, mae: 4137.343750, mean_q: -2625.501953
 3535/5000: episode: 3535, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5784.353, mean reward: -5784.353 [-5784.353, -5784.353], mean action: 0.000 [0.000, 0.000],  loss: 2858000.500000, mae: 4076.275391, mean_q: -2647.682861
 3536/5000: episode: 3536, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1928.073, mean reward: -1928.073 [-1928.073, -1928.073], mean action: 2.000 [2.000, 2.000],  loss: 2500101.250000, mae: 4048.397461, mean_q: -2661.241211
 3537/5000: episode: 3537, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1600.715, mean reward: -1600.715 [-1600.715, -1600.715], mean action: 2.000 [2.000, 2.000],  loss: 2144555.500000, mae: 4141.937988, mean_q: -2642.002441
 3538/5000: episode: 3538, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1939.660, mean reward: -1939.660 [-1939.660, -1939.660], mean action: 2.000 [2.000, 2.000],  loss: 4421440.000000, mae: 4243.438965, mean_q: -2651.900879
 3539/5000: episode: 3539, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -379.536, mean reward: -379.536 [-379.536, -379.536], mean action: 2.000 [2.000, 2.000],  loss: 2108925.000000, mae: 4093.340088, mean_q: -2646.790039
 3540/5000: episode: 3540, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3367.040, mean reward: -3367.040 [-3367.040, -3367.040], mean action: 2.000 [2.000, 2.000],  loss: 3956774.500000, mae: 4200.600586, mean_q: -2654.278076
 3541/5000: episode: 3541, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4785.226, mean reward: -4785.226 [-4785.226, -4785.226], mean action: 2.000 [2.000, 2.000],  loss: 2893208.500000, mae: 4080.494873, mean_q: -2631.662109
 3542/5000: episode: 3542, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3076.098, mean reward: -3076.098 [-3076.098, -3076.098], mean action: 2.000 [2.000, 2.000],  loss: 2578800.500000, mae: 4091.583496, mean_q: -2616.175293
 3543/5000: episode: 3543, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9993.742, mean reward: -9993.742 [-9993.742, -9993.742], mean action: 2.000 [2.000, 2.000],  loss: 2760736.500000, mae: 3986.807617, mean_q: -2616.847168
 3544/5000: episode: 3544, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3946.413, mean reward: -3946.413 [-3946.413, -3946.413], mean action: 2.000 [2.000, 2.000],  loss: 4116396.250000, mae: 4146.007324, mean_q: -2606.208740
 3545/5000: episode: 3545, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1327.557, mean reward: -1327.557 [-1327.557, -1327.557], mean action: 2.000 [2.000, 2.000],  loss: 4969312.000000, mae: 4129.681641, mean_q: -2602.243896
 3546/5000: episode: 3546, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3366.068, mean reward: -3366.068 [-3366.068, -3366.068], mean action: 2.000 [2.000, 2.000],  loss: 3462593.500000, mae: 4056.670898, mean_q: -2612.149414
 3547/5000: episode: 3547, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1690.176, mean reward: -1690.176 [-1690.176, -1690.176], mean action: 2.000 [2.000, 2.000],  loss: 3807625.000000, mae: 4167.647949, mean_q: -2603.856445
 3548/5000: episode: 3548, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -604.110, mean reward: -604.110 [-604.110, -604.110], mean action: 2.000 [2.000, 2.000],  loss: 3786528.500000, mae: 4179.573242, mean_q: -2614.207520
 3549/5000: episode: 3549, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -503.158, mean reward: -503.158 [-503.158, -503.158], mean action: 2.000 [2.000, 2.000],  loss: 2182929.500000, mae: 4069.126953, mean_q: -2615.515625
 3550/5000: episode: 3550, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -6270.342, mean reward: -6270.342 [-6270.342, -6270.342], mean action: 2.000 [2.000, 2.000],  loss: 4536699.000000, mae: 4162.590820, mean_q: -2622.865234
 3551/5000: episode: 3551, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2873.213, mean reward: -2873.213 [-2873.213, -2873.213], mean action: 2.000 [2.000, 2.000],  loss: 1632977.875000, mae: 4007.812012, mean_q: -2621.578613
 3552/5000: episode: 3552, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2058.160, mean reward: -2058.160 [-2058.160, -2058.160], mean action: 2.000 [2.000, 2.000],  loss: 3990581.750000, mae: 4185.044922, mean_q: -2633.770264
 3553/5000: episode: 3553, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -215.121, mean reward: -215.121 [-215.121, -215.121], mean action: 2.000 [2.000, 2.000],  loss: 3207361.000000, mae: 4181.323242, mean_q: -2645.230469
 3554/5000: episode: 3554, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6612.776, mean reward: -6612.776 [-6612.776, -6612.776], mean action: 2.000 [2.000, 2.000],  loss: 2317670.000000, mae: 4075.537109, mean_q: -2640.233398
 3555/5000: episode: 3555, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2557.086, mean reward: -2557.086 [-2557.086, -2557.086], mean action: 2.000 [2.000, 2.000],  loss: 3512376.000000, mae: 4172.001953, mean_q: -2647.931641
 3556/5000: episode: 3556, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1355.218, mean reward: -1355.218 [-1355.218, -1355.218], mean action: 2.000 [2.000, 2.000],  loss: 4092014.500000, mae: 4203.262207, mean_q: -2650.110352
 3557/5000: episode: 3557, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2806.465, mean reward: -2806.465 [-2806.465, -2806.465], mean action: 2.000 [2.000, 2.000],  loss: 2421056.000000, mae: 4031.626709, mean_q: -2638.994385
 3558/5000: episode: 3558, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2345.623, mean reward: -2345.623 [-2345.623, -2345.623], mean action: 2.000 [2.000, 2.000],  loss: 3377138.500000, mae: 4172.985352, mean_q: -2637.467773
 3559/5000: episode: 3559, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2371.864, mean reward: -2371.864 [-2371.864, -2371.864], mean action: 2.000 [2.000, 2.000],  loss: 2673711.000000, mae: 4157.979492, mean_q: -2639.872559
 3560/5000: episode: 3560, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -684.271, mean reward: -684.271 [-684.271, -684.271], mean action: 2.000 [2.000, 2.000],  loss: 4577453.000000, mae: 4273.377930, mean_q: -2644.538818
 3561/5000: episode: 3561, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4597.076, mean reward: -4597.076 [-4597.076, -4597.076], mean action: 1.000 [1.000, 1.000],  loss: 2100711.250000, mae: 4083.340576, mean_q: -2642.821289
 3562/5000: episode: 3562, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2977.054, mean reward: -2977.054 [-2977.054, -2977.054], mean action: 2.000 [2.000, 2.000],  loss: 2123270.000000, mae: 4099.985352, mean_q: -2639.609375
 3563/5000: episode: 3563, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -522.779, mean reward: -522.779 [-522.779, -522.779], mean action: 2.000 [2.000, 2.000],  loss: 2153416.000000, mae: 4119.273438, mean_q: -2653.678711
 3564/5000: episode: 3564, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1356.482, mean reward: -1356.482 [-1356.482, -1356.482], mean action: 2.000 [2.000, 2.000],  loss: 3722148.000000, mae: 4250.844727, mean_q: -2643.567383
 3565/5000: episode: 3565, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3923.649, mean reward: -3923.649 [-3923.649, -3923.649], mean action: 1.000 [1.000, 1.000],  loss: 2103108.750000, mae: 4068.759766, mean_q: -2639.752930
 3566/5000: episode: 3566, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -149.980, mean reward: -149.980 [-149.980, -149.980], mean action: 2.000 [2.000, 2.000],  loss: 4289018.000000, mae: 4273.666504, mean_q: -2650.228516
 3567/5000: episode: 3567, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3491.688, mean reward: -3491.688 [-3491.688, -3491.688], mean action: 2.000 [2.000, 2.000],  loss: 3263386.000000, mae: 4090.322266, mean_q: -2646.984863
 3568/5000: episode: 3568, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -166.803, mean reward: -166.803 [-166.803, -166.803], mean action: 2.000 [2.000, 2.000],  loss: 1845462.375000, mae: 4088.057129, mean_q: -2674.387207
 3569/5000: episode: 3569, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3383.380, mean reward: -3383.380 [-3383.380, -3383.380], mean action: 2.000 [2.000, 2.000],  loss: 2815516.750000, mae: 4229.686035, mean_q: -2662.331055
 3570/5000: episode: 3570, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2053.082, mean reward: -2053.082 [-2053.082, -2053.082], mean action: 2.000 [2.000, 2.000],  loss: 2740373.000000, mae: 4123.463867, mean_q: -2663.150879
 3571/5000: episode: 3571, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3322.661, mean reward: -3322.661 [-3322.661, -3322.661], mean action: 2.000 [2.000, 2.000],  loss: 4716547.500000, mae: 4240.427246, mean_q: -2665.873047
 3572/5000: episode: 3572, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -690.548, mean reward: -690.548 [-690.548, -690.548], mean action: 2.000 [2.000, 2.000],  loss: 2055719.000000, mae: 4094.557861, mean_q: -2687.002441
 3573/5000: episode: 3573, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -310.035, mean reward: -310.035 [-310.035, -310.035], mean action: 2.000 [2.000, 2.000],  loss: 2643087.500000, mae: 4173.268066, mean_q: -2690.419922
 3574/5000: episode: 3574, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -2620.311, mean reward: -2620.311 [-2620.311, -2620.311], mean action: 2.000 [2.000, 2.000],  loss: 3405290.750000, mae: 4178.755859, mean_q: -2706.210693
 3575/5000: episode: 3575, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3327.691, mean reward: -3327.691 [-3327.691, -3327.691], mean action: 2.000 [2.000, 2.000],  loss: 2116495.500000, mae: 4189.285156, mean_q: -2697.390381
 3576/5000: episode: 3576, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1318.947, mean reward: -1318.947 [-1318.947, -1318.947], mean action: 2.000 [2.000, 2.000],  loss: 4266472.500000, mae: 4224.104980, mean_q: -2690.915527
 3577/5000: episode: 3577, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3293.529, mean reward: -3293.529 [-3293.529, -3293.529], mean action: 2.000 [2.000, 2.000],  loss: 2346869.500000, mae: 4170.065430, mean_q: -2674.104004
 3578/5000: episode: 3578, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -751.654, mean reward: -751.654 [-751.654, -751.654], mean action: 2.000 [2.000, 2.000],  loss: 2173782.000000, mae: 4133.133789, mean_q: -2693.861816
 3579/5000: episode: 3579, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2629.531, mean reward: -2629.531 [-2629.531, -2629.531], mean action: 2.000 [2.000, 2.000],  loss: 2526305.250000, mae: 4205.796875, mean_q: -2696.108154
 3580/5000: episode: 3580, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -325.131, mean reward: -325.131 [-325.131, -325.131], mean action: 2.000 [2.000, 2.000],  loss: 3691049.750000, mae: 4250.989746, mean_q: -2688.675049
 3581/5000: episode: 3581, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2880.671, mean reward: -2880.671 [-2880.671, -2880.671], mean action: 2.000 [2.000, 2.000],  loss: 2073690.250000, mae: 4010.394043, mean_q: -2662.064941
 3582/5000: episode: 3582, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1373.228, mean reward: -1373.228 [-1373.228, -1373.228], mean action: 2.000 [2.000, 2.000],  loss: 4452172.500000, mae: 4247.540039, mean_q: -2657.760254
 3583/5000: episode: 3583, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -7608.772, mean reward: -7608.772 [-7608.772, -7608.772], mean action: 2.000 [2.000, 2.000],  loss: 2363725.250000, mae: 4166.061523, mean_q: -2664.598877
 3584/5000: episode: 3584, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2686.996, mean reward: -2686.996 [-2686.996, -2686.996], mean action: 0.000 [0.000, 0.000],  loss: 1806284.500000, mae: 4118.445801, mean_q: -2659.018066
 3585/5000: episode: 3585, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3061.637, mean reward: -3061.637 [-3061.637, -3061.637], mean action: 2.000 [2.000, 2.000],  loss: 2499954.500000, mae: 4096.993652, mean_q: -2646.460205
 3586/5000: episode: 3586, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6468.427, mean reward: -6468.427 [-6468.427, -6468.427], mean action: 2.000 [2.000, 2.000],  loss: 3974788.000000, mae: 4099.622070, mean_q: -2638.941162
 3587/5000: episode: 3587, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4430.793, mean reward: -4430.793 [-4430.793, -4430.793], mean action: 2.000 [2.000, 2.000],  loss: 2775718.000000, mae: 4096.798340, mean_q: -2626.216064
 3588/5000: episode: 3588, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -943.250, mean reward: -943.250 [-943.250, -943.250], mean action: 2.000 [2.000, 2.000],  loss: 3055160.750000, mae: 4164.061035, mean_q: -2626.339111
 3589/5000: episode: 3589, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1948.848, mean reward: -1948.848 [-1948.848, -1948.848], mean action: 2.000 [2.000, 2.000],  loss: 4880239.000000, mae: 4192.795898, mean_q: -2618.554199
 3590/5000: episode: 3590, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3142.958, mean reward: -3142.958 [-3142.958, -3142.958], mean action: 1.000 [1.000, 1.000],  loss: 2154873.000000, mae: 4076.216797, mean_q: -2619.940918
 3591/5000: episode: 3591, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8.196, mean reward: -8.196 [-8.196, -8.196], mean action: 2.000 [2.000, 2.000],  loss: 5029214.000000, mae: 4310.225586, mean_q: -2627.558594
 3592/5000: episode: 3592, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -631.764, mean reward: -631.764 [-631.764, -631.764], mean action: 2.000 [2.000, 2.000],  loss: 3144844.500000, mae: 4156.875977, mean_q: -2621.540771
 3593/5000: episode: 3593, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3841.795, mean reward: -3841.795 [-3841.795, -3841.795], mean action: 2.000 [2.000, 2.000],  loss: 2387698.750000, mae: 4086.404785, mean_q: -2633.669922
 3594/5000: episode: 3594, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3209.028, mean reward: -3209.028 [-3209.028, -3209.028], mean action: 2.000 [2.000, 2.000],  loss: 5312260.500000, mae: 4254.584961, mean_q: -2632.031250
 3595/5000: episode: 3595, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6603.528, mean reward: -6603.528 [-6603.528, -6603.528], mean action: 2.000 [2.000, 2.000],  loss: 2149793.000000, mae: 4212.810059, mean_q: -2627.321289
 3596/5000: episode: 3596, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2790.870, mean reward: -2790.870 [-2790.870, -2790.870], mean action: 2.000 [2.000, 2.000],  loss: 2268347.500000, mae: 4097.248535, mean_q: -2651.296875
 3597/5000: episode: 3597, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5010.518, mean reward: -5010.518 [-5010.518, -5010.518], mean action: 2.000 [2.000, 2.000],  loss: 1872626.250000, mae: 4142.364258, mean_q: -2654.767090
 3598/5000: episode: 3598, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2100.756, mean reward: -2100.756 [-2100.756, -2100.756], mean action: 3.000 [3.000, 3.000],  loss: 3854299.250000, mae: 4173.942383, mean_q: -2669.194824
 3599/5000: episode: 3599, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3324.041, mean reward: -3324.041 [-3324.041, -3324.041], mean action: 1.000 [1.000, 1.000],  loss: 2512629.500000, mae: 4127.140137, mean_q: -2667.574463
 3600/5000: episode: 3600, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1244.064, mean reward: -1244.064 [-1244.064, -1244.064], mean action: 2.000 [2.000, 2.000],  loss: 4418743.500000, mae: 4208.738281, mean_q: -2674.485107
 3601/5000: episode: 3601, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3569.402, mean reward: -3569.402 [-3569.402, -3569.402], mean action: 2.000 [2.000, 2.000],  loss: 2246293.000000, mae: 4201.133789, mean_q: -2682.300293
 3602/5000: episode: 3602, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1849.629, mean reward: -1849.629 [-1849.629, -1849.629], mean action: 2.000 [2.000, 2.000],  loss: 2955434.000000, mae: 4254.400391, mean_q: -2683.125488
 3603/5000: episode: 3603, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -746.017, mean reward: -746.017 [-746.017, -746.017], mean action: 2.000 [2.000, 2.000],  loss: 2415615.000000, mae: 4143.112305, mean_q: -2682.685059
 3604/5000: episode: 3604, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -931.727, mean reward: -931.727 [-931.727, -931.727], mean action: 2.000 [2.000, 2.000],  loss: 3303306.500000, mae: 4258.161621, mean_q: -2678.115234
 3605/5000: episode: 3605, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6809.417, mean reward: -6809.417 [-6809.417, -6809.417], mean action: 2.000 [2.000, 2.000],  loss: 1044608.375000, mae: 4050.656250, mean_q: -2695.490234
 3606/5000: episode: 3606, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1091.671, mean reward: -1091.671 [-1091.671, -1091.671], mean action: 2.000 [2.000, 2.000],  loss: 2003004.625000, mae: 4141.132324, mean_q: -2679.799805
 3607/5000: episode: 3607, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2775.769, mean reward: -2775.769 [-2775.769, -2775.769], mean action: 2.000 [2.000, 2.000],  loss: 2055180.250000, mae: 4161.319336, mean_q: -2689.065430
 3608/5000: episode: 3608, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4996.543, mean reward: -4996.543 [-4996.543, -4996.543], mean action: 2.000 [2.000, 2.000],  loss: 1599252.250000, mae: 4069.184082, mean_q: -2682.648926
 3609/5000: episode: 3609, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -362.280, mean reward: -362.280 [-362.280, -362.280], mean action: 2.000 [2.000, 2.000],  loss: 2150673.000000, mae: 4205.867188, mean_q: -2676.761230
 3610/5000: episode: 3610, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2062.592, mean reward: -2062.592 [-2062.592, -2062.592], mean action: 2.000 [2.000, 2.000],  loss: 1804303.250000, mae: 4190.950195, mean_q: -2683.993896
 3611/5000: episode: 3611, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3866.000, mean reward: -3866.000 [-3866.000, -3866.000], mean action: 2.000 [2.000, 2.000],  loss: 3060391.250000, mae: 4090.342773, mean_q: -2660.084229
 3612/5000: episode: 3612, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -947.399, mean reward: -947.399 [-947.399, -947.399], mean action: 2.000 [2.000, 2.000],  loss: 2858317.000000, mae: 4128.625977, mean_q: -2646.683594
 3613/5000: episode: 3613, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -7714.623, mean reward: -7714.623 [-7714.623, -7714.623], mean action: 3.000 [3.000, 3.000],  loss: 3195679.500000, mae: 4183.281738, mean_q: -2655.265137
 3614/5000: episode: 3614, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4756.709, mean reward: -4756.709 [-4756.709, -4756.709], mean action: 2.000 [2.000, 2.000],  loss: 3265533.500000, mae: 4299.132812, mean_q: -2657.164551
 3615/5000: episode: 3615, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1774.339, mean reward: -1774.339 [-1774.339, -1774.339], mean action: 2.000 [2.000, 2.000],  loss: 2625581.250000, mae: 4146.368164, mean_q: -2642.563477
 3616/5000: episode: 3616, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3539.878, mean reward: -3539.878 [-3539.878, -3539.878], mean action: 2.000 [2.000, 2.000],  loss: 2725188.000000, mae: 4136.371094, mean_q: -2645.566406
 3617/5000: episode: 3617, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3363.369, mean reward: -3363.369 [-3363.369, -3363.369], mean action: 2.000 [2.000, 2.000],  loss: 2250584.500000, mae: 4131.362305, mean_q: -2623.311768
 3618/5000: episode: 3618, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -834.238, mean reward: -834.238 [-834.238, -834.238], mean action: 2.000 [2.000, 2.000],  loss: 2405863.000000, mae: 4058.714111, mean_q: -2613.083008
 3619/5000: episode: 3619, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3129.034, mean reward: -3129.034 [-3129.034, -3129.034], mean action: 2.000 [2.000, 2.000],  loss: 2640251.250000, mae: 4164.020508, mean_q: -2625.305176
 3620/5000: episode: 3620, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -543.291, mean reward: -543.291 [-543.291, -543.291], mean action: 2.000 [2.000, 2.000],  loss: 2021285.625000, mae: 4081.166992, mean_q: -2614.767578
 3621/5000: episode: 3621, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6572.825, mean reward: -6572.825 [-6572.825, -6572.825], mean action: 2.000 [2.000, 2.000],  loss: 2422860.000000, mae: 4187.927734, mean_q: -2612.333984
 3622/5000: episode: 3622, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3477.472, mean reward: -3477.472 [-3477.472, -3477.472], mean action: 2.000 [2.000, 2.000],  loss: 3581940.000000, mae: 4212.442383, mean_q: -2607.168945
 3623/5000: episode: 3623, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -3538.440, mean reward: -3538.440 [-3538.440, -3538.440], mean action: 2.000 [2.000, 2.000],  loss: 3063081.000000, mae: 4164.689453, mean_q: -2598.016113
 3624/5000: episode: 3624, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -912.947, mean reward: -912.947 [-912.947, -912.947], mean action: 2.000 [2.000, 2.000],  loss: 3582419.250000, mae: 4179.217285, mean_q: -2601.853271
 3625/5000: episode: 3625, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5.638, mean reward: -5.638 [-5.638, -5.638], mean action: 2.000 [2.000, 2.000],  loss: 3794291.000000, mae: 4277.467773, mean_q: -2586.509277
 3626/5000: episode: 3626, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1017.462, mean reward: -1017.462 [-1017.462, -1017.462], mean action: 2.000 [2.000, 2.000],  loss: 2597613.500000, mae: 4131.857422, mean_q: -2594.795166
 3627/5000: episode: 3627, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -278.223, mean reward: -278.223 [-278.223, -278.223], mean action: 2.000 [2.000, 2.000],  loss: 2847640.500000, mae: 4142.643066, mean_q: -2586.764404
 3628/5000: episode: 3628, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1282.458, mean reward: -1282.458 [-1282.458, -1282.458], mean action: 3.000 [3.000, 3.000],  loss: 3956766.250000, mae: 4226.803711, mean_q: -2579.789062
 3629/5000: episode: 3629, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -377.626, mean reward: -377.626 [-377.626, -377.626], mean action: 2.000 [2.000, 2.000],  loss: 3572277.000000, mae: 4135.969727, mean_q: -2598.452148
 3630/5000: episode: 3630, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1580.534, mean reward: -1580.534 [-1580.534, -1580.534], mean action: 2.000 [2.000, 2.000],  loss: 1997843.750000, mae: 4116.525391, mean_q: -2581.395020
 3631/5000: episode: 3631, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -7299.973, mean reward: -7299.973 [-7299.973, -7299.973], mean action: 2.000 [2.000, 2.000],  loss: 4002460.500000, mae: 4255.056152, mean_q: -2597.220703
 3632/5000: episode: 3632, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1893.001, mean reward: -1893.001 [-1893.001, -1893.001], mean action: 2.000 [2.000, 2.000],  loss: 3591766.500000, mae: 4158.727051, mean_q: -2581.848877
 3633/5000: episode: 3633, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2258.558, mean reward: -2258.558 [-2258.558, -2258.558], mean action: 2.000 [2.000, 2.000],  loss: 2208994.750000, mae: 4109.112305, mean_q: -2602.124023
 3634/5000: episode: 3634, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8509.670, mean reward: -8509.670 [-8509.670, -8509.670], mean action: 2.000 [2.000, 2.000],  loss: 2422661.750000, mae: 4062.891602, mean_q: -2597.577148
 3635/5000: episode: 3635, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2077.967, mean reward: -2077.967 [-2077.967, -2077.967], mean action: 2.000 [2.000, 2.000],  loss: 2877718.500000, mae: 4086.453613, mean_q: -2614.403809
 3636/5000: episode: 3636, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6985.057, mean reward: -6985.057 [-6985.057, -6985.057], mean action: 2.000 [2.000, 2.000],  loss: 1709691.500000, mae: 4095.396484, mean_q: -2625.514160
 3637/5000: episode: 3637, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2944.319, mean reward: -2944.319 [-2944.319, -2944.319], mean action: 2.000 [2.000, 2.000],  loss: 3921708.000000, mae: 4205.400391, mean_q: -2629.671387
 3638/5000: episode: 3638, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1469.116, mean reward: -1469.116 [-1469.116, -1469.116], mean action: 2.000 [2.000, 2.000],  loss: 2684512.250000, mae: 4154.673828, mean_q: -2636.642578
 3639/5000: episode: 3639, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2199.227, mean reward: -2199.227 [-2199.227, -2199.227], mean action: 2.000 [2.000, 2.000],  loss: 1634352.250000, mae: 4081.833008, mean_q: -2654.605225
 3640/5000: episode: 3640, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1315.327, mean reward: -1315.327 [-1315.327, -1315.327], mean action: 2.000 [2.000, 2.000],  loss: 2473635.000000, mae: 4145.584473, mean_q: -2654.775146
 3641/5000: episode: 3641, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -967.996, mean reward: -967.996 [-967.996, -967.996], mean action: 2.000 [2.000, 2.000],  loss: 2659557.500000, mae: 4161.541504, mean_q: -2668.519531
 3642/5000: episode: 3642, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3241.973, mean reward: -3241.973 [-3241.973, -3241.973], mean action: 2.000 [2.000, 2.000],  loss: 2570308.500000, mae: 4143.442871, mean_q: -2666.155518
 3643/5000: episode: 3643, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -334.510, mean reward: -334.510 [-334.510, -334.510], mean action: 2.000 [2.000, 2.000],  loss: 2274958.500000, mae: 4179.408203, mean_q: -2659.424805
 3644/5000: episode: 3644, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2696.725, mean reward: -2696.725 [-2696.725, -2696.725], mean action: 2.000 [2.000, 2.000],  loss: 4094704.750000, mae: 4265.439453, mean_q: -2659.559814
 3645/5000: episode: 3645, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8760.872, mean reward: -8760.872 [-8760.872, -8760.872], mean action: 2.000 [2.000, 2.000],  loss: 1755662.125000, mae: 4076.230469, mean_q: -2657.157471
 3646/5000: episode: 3646, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -470.022, mean reward: -470.022 [-470.022, -470.022], mean action: 2.000 [2.000, 2.000],  loss: 3508284.000000, mae: 4213.208008, mean_q: -2666.238525
 3647/5000: episode: 3647, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1057.665, mean reward: -1057.665 [-1057.665, -1057.665], mean action: 2.000 [2.000, 2.000],  loss: 3347048.500000, mae: 4152.151367, mean_q: -2661.986816
 3648/5000: episode: 3648, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -423.934, mean reward: -423.934 [-423.934, -423.934], mean action: 3.000 [3.000, 3.000],  loss: 2621282.000000, mae: 4079.693115, mean_q: -2674.461426
 3649/5000: episode: 3649, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10.333, mean reward: -10.333 [-10.333, -10.333], mean action: 2.000 [2.000, 2.000],  loss: 3809947.750000, mae: 4200.743652, mean_q: -2674.294922
 3650/5000: episode: 3650, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1099.094, mean reward: -1099.094 [-1099.094, -1099.094], mean action: 2.000 [2.000, 2.000],  loss: 2774517.250000, mae: 4152.368164, mean_q: -2669.900879
 3651/5000: episode: 3651, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1887.003, mean reward: -1887.003 [-1887.003, -1887.003], mean action: 2.000 [2.000, 2.000],  loss: 2721626.500000, mae: 4169.617676, mean_q: -2687.318359
 3652/5000: episode: 3652, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1907.389, mean reward: -1907.389 [-1907.389, -1907.389], mean action: 2.000 [2.000, 2.000],  loss: 2750086.500000, mae: 4044.405273, mean_q: -2680.362305
 3653/5000: episode: 3653, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1530.817, mean reward: -1530.817 [-1530.817, -1530.817], mean action: 2.000 [2.000, 2.000],  loss: 3646948.500000, mae: 4170.335449, mean_q: -2694.962402
 3654/5000: episode: 3654, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2243.933, mean reward: -2243.933 [-2243.933, -2243.933], mean action: 2.000 [2.000, 2.000],  loss: 4359520.500000, mae: 4268.373535, mean_q: -2685.112305
 3655/5000: episode: 3655, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6552.966, mean reward: -6552.966 [-6552.966, -6552.966], mean action: 2.000 [2.000, 2.000],  loss: 2712284.250000, mae: 4158.073242, mean_q: -2700.892334
 3656/5000: episode: 3656, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5057.039, mean reward: -5057.039 [-5057.039, -5057.039], mean action: 2.000 [2.000, 2.000],  loss: 2630298.000000, mae: 4133.915039, mean_q: -2705.523926
 3657/5000: episode: 3657, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3237.318, mean reward: -3237.318 [-3237.318, -3237.318], mean action: 2.000 [2.000, 2.000],  loss: 2968692.000000, mae: 4164.467773, mean_q: -2679.965332
 3658/5000: episode: 3658, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4722.429, mean reward: -4722.429 [-4722.429, -4722.429], mean action: 2.000 [2.000, 2.000],  loss: 4503074.500000, mae: 4263.402344, mean_q: -2692.402832
 3659/5000: episode: 3659, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2124.873, mean reward: -2124.873 [-2124.873, -2124.873], mean action: 2.000 [2.000, 2.000],  loss: 1476747.125000, mae: 4039.170410, mean_q: -2680.132812
 3660/5000: episode: 3660, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2917.226, mean reward: -2917.226 [-2917.226, -2917.226], mean action: 2.000 [2.000, 2.000],  loss: 2472714.500000, mae: 4095.401611, mean_q: -2664.641602
 3661/5000: episode: 3661, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1928.894, mean reward: -1928.894 [-1928.894, -1928.894], mean action: 2.000 [2.000, 2.000],  loss: 3559209.000000, mae: 4214.169434, mean_q: -2681.080811
 3662/5000: episode: 3662, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8822.918, mean reward: -8822.918 [-8822.918, -8822.918], mean action: 2.000 [2.000, 2.000],  loss: 3341282.000000, mae: 4172.064453, mean_q: -2683.617920
 3663/5000: episode: 3663, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1738.007, mean reward: -1738.007 [-1738.007, -1738.007], mean action: 2.000 [2.000, 2.000],  loss: 4026727.500000, mae: 4213.858398, mean_q: -2673.977051
 3664/5000: episode: 3664, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2106.911, mean reward: -2106.911 [-2106.911, -2106.911], mean action: 2.000 [2.000, 2.000],  loss: 2971484.500000, mae: 4152.421875, mean_q: -2670.756836
 3665/5000: episode: 3665, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5642.808, mean reward: -5642.808 [-5642.808, -5642.808], mean action: 2.000 [2.000, 2.000],  loss: 3787919.250000, mae: 4214.648438, mean_q: -2681.569580
 3666/5000: episode: 3666, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -945.272, mean reward: -945.272 [-945.272, -945.272], mean action: 2.000 [2.000, 2.000],  loss: 2026522.250000, mae: 4056.901611, mean_q: -2697.239746
 3667/5000: episode: 3667, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2743.201, mean reward: -2743.201 [-2743.201, -2743.201], mean action: 2.000 [2.000, 2.000],  loss: 3443818.500000, mae: 4209.672852, mean_q: -2704.960938
 3668/5000: episode: 3668, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1902.429, mean reward: -1902.429 [-1902.429, -1902.429], mean action: 2.000 [2.000, 2.000],  loss: 2767964.500000, mae: 4096.646484, mean_q: -2706.047363
 3669/5000: episode: 3669, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -446.772, mean reward: -446.772 [-446.772, -446.772], mean action: 2.000 [2.000, 2.000],  loss: 3583709.750000, mae: 4158.546875, mean_q: -2714.602783
 3670/5000: episode: 3670, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8906.426, mean reward: -8906.426 [-8906.426, -8906.426], mean action: 2.000 [2.000, 2.000],  loss: 2374844.500000, mae: 4058.096924, mean_q: -2732.605713
 3671/5000: episode: 3671, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1698.027, mean reward: -1698.027 [-1698.027, -1698.027], mean action: 2.000 [2.000, 2.000],  loss: 2288924.500000, mae: 4092.625488, mean_q: -2735.802734
 3672/5000: episode: 3672, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3460.885, mean reward: -3460.885 [-3460.885, -3460.885], mean action: 2.000 [2.000, 2.000],  loss: 3972019.500000, mae: 4229.290039, mean_q: -2730.266113
 3673/5000: episode: 3673, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -434.274, mean reward: -434.274 [-434.274, -434.274], mean action: 2.000 [2.000, 2.000],  loss: 3876607.750000, mae: 4158.832031, mean_q: -2720.111328
 3674/5000: episode: 3674, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -762.015, mean reward: -762.015 [-762.015, -762.015], mean action: 2.000 [2.000, 2.000],  loss: 4491908.000000, mae: 4172.352539, mean_q: -2737.419922
 3675/5000: episode: 3675, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3480.305, mean reward: -3480.305 [-3480.305, -3480.305], mean action: 2.000 [2.000, 2.000],  loss: 3229195.250000, mae: 4107.178711, mean_q: -2718.061279
 3676/5000: episode: 3676, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -783.780, mean reward: -783.780 [-783.780, -783.780], mean action: 2.000 [2.000, 2.000],  loss: 1682251.750000, mae: 4085.852539, mean_q: -2732.977295
 3677/5000: episode: 3677, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -9587.285, mean reward: -9587.285 [-9587.285, -9587.285], mean action: 2.000 [2.000, 2.000],  loss: 3433140.750000, mae: 4190.363281, mean_q: -2738.032227
 3678/5000: episode: 3678, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -444.184, mean reward: -444.184 [-444.184, -444.184], mean action: 2.000 [2.000, 2.000],  loss: 3459746.250000, mae: 4208.695312, mean_q: -2724.728516
 3679/5000: episode: 3679, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5260.206, mean reward: -5260.206 [-5260.206, -5260.206], mean action: 2.000 [2.000, 2.000],  loss: 1907569.750000, mae: 4016.355957, mean_q: -2713.535156
 3680/5000: episode: 3680, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -602.313, mean reward: -602.313 [-602.313, -602.313], mean action: 2.000 [2.000, 2.000],  loss: 2673418.250000, mae: 4128.260742, mean_q: -2702.751709
 3681/5000: episode: 3681, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -101.334, mean reward: -101.334 [-101.334, -101.334], mean action: 2.000 [2.000, 2.000],  loss: 2931282.500000, mae: 4086.526123, mean_q: -2703.390625
 3682/5000: episode: 3682, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1180.031, mean reward: -1180.031 [-1180.031, -1180.031], mean action: 2.000 [2.000, 2.000],  loss: 3104725.500000, mae: 4147.982910, mean_q: -2714.486572
 3683/5000: episode: 3683, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1507.812, mean reward: -1507.812 [-1507.812, -1507.812], mean action: 2.000 [2.000, 2.000],  loss: 1693190.750000, mae: 3971.320801, mean_q: -2705.007324
 3684/5000: episode: 3684, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1556.541, mean reward: -1556.541 [-1556.541, -1556.541], mean action: 2.000 [2.000, 2.000],  loss: 2423984.750000, mae: 4141.313477, mean_q: -2722.121338
 3685/5000: episode: 3685, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2790.718, mean reward: -2790.718 [-2790.718, -2790.718], mean action: 2.000 [2.000, 2.000],  loss: 1924688.375000, mae: 4065.400146, mean_q: -2707.273682
 3686/5000: episode: 3686, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7343.582, mean reward: -7343.582 [-7343.582, -7343.582], mean action: 2.000 [2.000, 2.000],  loss: 2859324.500000, mae: 4062.388184, mean_q: -2692.367432
 3687/5000: episode: 3687, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1440.418, mean reward: -1440.418 [-1440.418, -1440.418], mean action: 2.000 [2.000, 2.000],  loss: 2109911.500000, mae: 4131.611816, mean_q: -2703.394531
 3688/5000: episode: 3688, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6306.076, mean reward: -6306.076 [-6306.076, -6306.076], mean action: 0.000 [0.000, 0.000],  loss: 3436419.750000, mae: 4179.958496, mean_q: -2679.905273
 3689/5000: episode: 3689, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2379.217, mean reward: -2379.217 [-2379.217, -2379.217], mean action: 2.000 [2.000, 2.000],  loss: 1793986.875000, mae: 4020.981934, mean_q: -2681.969727
 3690/5000: episode: 3690, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -353.301, mean reward: -353.301 [-353.301, -353.301], mean action: 2.000 [2.000, 2.000],  loss: 2454572.500000, mae: 3981.615723, mean_q: -2684.498535
 3691/5000: episode: 3691, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -740.762, mean reward: -740.762 [-740.762, -740.762], mean action: 2.000 [2.000, 2.000],  loss: 3684277.250000, mae: 4180.897461, mean_q: -2672.526855
 3692/5000: episode: 3692, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1765.659, mean reward: -1765.659 [-1765.659, -1765.659], mean action: 2.000 [2.000, 2.000],  loss: 1796674.750000, mae: 4056.947021, mean_q: -2674.791016
 3693/5000: episode: 3693, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1347.613, mean reward: -1347.613 [-1347.613, -1347.613], mean action: 2.000 [2.000, 2.000],  loss: 3783227.000000, mae: 4125.117676, mean_q: -2662.379883
 3694/5000: episode: 3694, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6273.977, mean reward: -6273.977 [-6273.977, -6273.977], mean action: 2.000 [2.000, 2.000],  loss: 3908957.500000, mae: 4191.224121, mean_q: -2660.413330
 3695/5000: episode: 3695, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -479.844, mean reward: -479.844 [-479.844, -479.844], mean action: 2.000 [2.000, 2.000],  loss: 1973766.875000, mae: 4026.786377, mean_q: -2655.706787
 3696/5000: episode: 3696, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7969.915, mean reward: -7969.915 [-7969.915, -7969.915], mean action: 2.000 [2.000, 2.000],  loss: 2733053.000000, mae: 4156.208496, mean_q: -2664.840332
 3697/5000: episode: 3697, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -144.950, mean reward: -144.950 [-144.950, -144.950], mean action: 2.000 [2.000, 2.000],  loss: 2342320.750000, mae: 4003.000488, mean_q: -2651.673828
 3698/5000: episode: 3698, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3735.083, mean reward: -3735.083 [-3735.083, -3735.083], mean action: 2.000 [2.000, 2.000],  loss: 2623679.000000, mae: 4127.660645, mean_q: -2665.932129
 3699/5000: episode: 3699, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5789.893, mean reward: -5789.893 [-5789.893, -5789.893], mean action: 2.000 [2.000, 2.000],  loss: 3532448.500000, mae: 4114.475586, mean_q: -2654.893066
 3700/5000: episode: 3700, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -989.023, mean reward: -989.023 [-989.023, -989.023], mean action: 2.000 [2.000, 2.000],  loss: 2700345.000000, mae: 4018.258789, mean_q: -2627.594238
 3701/5000: episode: 3701, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2283.491, mean reward: -2283.491 [-2283.491, -2283.491], mean action: 2.000 [2.000, 2.000],  loss: 4107480.500000, mae: 4078.639893, mean_q: -2637.816895
 3702/5000: episode: 3702, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3478.006, mean reward: -3478.006 [-3478.006, -3478.006], mean action: 2.000 [2.000, 2.000],  loss: 2616295.000000, mae: 4059.457520, mean_q: -2618.699707
 3703/5000: episode: 3703, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -586.642, mean reward: -586.642 [-586.642, -586.642], mean action: 2.000 [2.000, 2.000],  loss: 3149328.500000, mae: 4083.008057, mean_q: -2607.971191
 3704/5000: episode: 3704, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1488.254, mean reward: -1488.254 [-1488.254, -1488.254], mean action: 2.000 [2.000, 2.000],  loss: 1861142.500000, mae: 4059.921875, mean_q: -2621.154297
 3705/5000: episode: 3705, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1485.978, mean reward: -1485.978 [-1485.978, -1485.978], mean action: 2.000 [2.000, 2.000],  loss: 4737255.500000, mae: 4151.811035, mean_q: -2590.864990
 3706/5000: episode: 3706, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3896.908, mean reward: -3896.908 [-3896.908, -3896.908], mean action: 2.000 [2.000, 2.000],  loss: 2443153.000000, mae: 4080.504150, mean_q: -2611.252441
 3707/5000: episode: 3707, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2890.408, mean reward: -2890.408 [-2890.408, -2890.408], mean action: 2.000 [2.000, 2.000],  loss: 1652606.500000, mae: 3998.508789, mean_q: -2632.977295
 3708/5000: episode: 3708, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5782.894, mean reward: -5782.894 [-5782.894, -5782.894], mean action: 2.000 [2.000, 2.000],  loss: 3958237.250000, mae: 4156.764648, mean_q: -2636.489746
 3709/5000: episode: 3709, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3802.215, mean reward: -3802.215 [-3802.215, -3802.215], mean action: 2.000 [2.000, 2.000],  loss: 1733115.625000, mae: 4022.824951, mean_q: -2637.336914
 3710/5000: episode: 3710, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -213.623, mean reward: -213.623 [-213.623, -213.623], mean action: 2.000 [2.000, 2.000],  loss: 1333017.000000, mae: 4010.628418, mean_q: -2647.467041
 3711/5000: episode: 3711, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -606.857, mean reward: -606.857 [-606.857, -606.857], mean action: 2.000 [2.000, 2.000],  loss: 2933593.000000, mae: 4152.160645, mean_q: -2662.739258
 3712/5000: episode: 3712, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4623.558, mean reward: -4623.558 [-4623.558, -4623.558], mean action: 2.000 [2.000, 2.000],  loss: 4894466.500000, mae: 4082.482422, mean_q: -2657.733398
 3713/5000: episode: 3713, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8174.793, mean reward: -8174.793 [-8174.793, -8174.793], mean action: 2.000 [2.000, 2.000],  loss: 6085750.500000, mae: 4316.556641, mean_q: -2674.612793
 3714/5000: episode: 3714, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2003.791, mean reward: -2003.791 [-2003.791, -2003.791], mean action: 2.000 [2.000, 2.000],  loss: 3243005.000000, mae: 4170.541504, mean_q: -2686.918945
 3715/5000: episode: 3715, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -7424.054, mean reward: -7424.054 [-7424.054, -7424.054], mean action: 2.000 [2.000, 2.000],  loss: 2668498.750000, mae: 4114.690430, mean_q: -2689.390137
 3716/5000: episode: 3716, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2864.796, mean reward: -2864.796 [-2864.796, -2864.796], mean action: 2.000 [2.000, 2.000],  loss: 2903531.000000, mae: 4145.062988, mean_q: -2697.781006
 3717/5000: episode: 3717, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -568.680, mean reward: -568.680 [-568.680, -568.680], mean action: 2.000 [2.000, 2.000],  loss: 2294925.250000, mae: 4184.928711, mean_q: -2709.321289
 3718/5000: episode: 3718, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1984.000, mean reward: -1984.000 [-1984.000, -1984.000], mean action: 2.000 [2.000, 2.000],  loss: 2711922.750000, mae: 4162.935059, mean_q: -2707.992676
 3719/5000: episode: 3719, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2103.585, mean reward: -2103.585 [-2103.585, -2103.585], mean action: 2.000 [2.000, 2.000],  loss: 1699686.000000, mae: 4077.421143, mean_q: -2689.467041
 3720/5000: episode: 3720, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2931.638, mean reward: -2931.638 [-2931.638, -2931.638], mean action: 2.000 [2.000, 2.000],  loss: 2865224.750000, mae: 4130.354980, mean_q: -2706.931641
 3721/5000: episode: 3721, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -1215.666, mean reward: -1215.666 [-1215.666, -1215.666], mean action: 2.000 [2.000, 2.000],  loss: 2554034.500000, mae: 4017.985107, mean_q: -2712.097168
 3722/5000: episode: 3722, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -841.134, mean reward: -841.134 [-841.134, -841.134], mean action: 2.000 [2.000, 2.000],  loss: 2800308.000000, mae: 4087.547363, mean_q: -2697.078369
 3723/5000: episode: 3723, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1625.809, mean reward: -1625.809 [-1625.809, -1625.809], mean action: 2.000 [2.000, 2.000],  loss: 2834562.000000, mae: 4190.594238, mean_q: -2712.028809
 3724/5000: episode: 3724, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2669.437, mean reward: -2669.437 [-2669.437, -2669.437], mean action: 2.000 [2.000, 2.000],  loss: 2505186.500000, mae: 4006.658691, mean_q: -2706.107178
 3725/5000: episode: 3725, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1699.342, mean reward: -1699.342 [-1699.342, -1699.342], mean action: 2.000 [2.000, 2.000],  loss: 3201668.000000, mae: 4329.531250, mean_q: -2719.217041
 3726/5000: episode: 3726, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5060.851, mean reward: -5060.851 [-5060.851, -5060.851], mean action: 2.000 [2.000, 2.000],  loss: 1932454.500000, mae: 4100.543945, mean_q: -2707.881836
 3727/5000: episode: 3727, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1201.971, mean reward: -1201.971 [-1201.971, -1201.971], mean action: 2.000 [2.000, 2.000],  loss: 1942858.625000, mae: 4127.705078, mean_q: -2692.935547
 3728/5000: episode: 3728, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2912.889, mean reward: -2912.889 [-2912.889, -2912.889], mean action: 2.000 [2.000, 2.000],  loss: 2077460.250000, mae: 4133.413086, mean_q: -2682.689941
 3729/5000: episode: 3729, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3033.086, mean reward: -3033.086 [-3033.086, -3033.086], mean action: 2.000 [2.000, 2.000],  loss: 1611663.125000, mae: 4015.628906, mean_q: -2664.249023
 3730/5000: episode: 3730, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -1709.258, mean reward: -1709.258 [-1709.258, -1709.258], mean action: 2.000 [2.000, 2.000],  loss: 3328559.000000, mae: 4107.412598, mean_q: -2655.069824
 3731/5000: episode: 3731, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7997.367, mean reward: -7997.367 [-7997.367, -7997.367], mean action: 2.000 [2.000, 2.000],  loss: 2636127.500000, mae: 4191.149902, mean_q: -2667.425049
 3732/5000: episode: 3732, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1263.658, mean reward: -1263.658 [-1263.658, -1263.658], mean action: 2.000 [2.000, 2.000],  loss: 2615574.500000, mae: 4073.307129, mean_q: -2666.990479
 3733/5000: episode: 3733, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2240.420, mean reward: -2240.420 [-2240.420, -2240.420], mean action: 2.000 [2.000, 2.000],  loss: 3133601.250000, mae: 4132.902344, mean_q: -2671.943848
 3734/5000: episode: 3734, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3721.229, mean reward: -3721.229 [-3721.229, -3721.229], mean action: 2.000 [2.000, 2.000],  loss: 4669206.000000, mae: 4254.405273, mean_q: -2671.204102
 3735/5000: episode: 3735, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -791.789, mean reward: -791.789 [-791.789, -791.789], mean action: 2.000 [2.000, 2.000],  loss: 4733208.000000, mae: 4262.707520, mean_q: -2655.039307
 3736/5000: episode: 3736, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -516.297, mean reward: -516.297 [-516.297, -516.297], mean action: 2.000 [2.000, 2.000],  loss: 4165442.000000, mae: 4286.177734, mean_q: -2648.171875
 3737/5000: episode: 3737, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -735.763, mean reward: -735.763 [-735.763, -735.763], mean action: 2.000 [2.000, 2.000],  loss: 3081303.000000, mae: 4180.338379, mean_q: -2659.820312
 3738/5000: episode: 3738, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -481.095, mean reward: -481.095 [-481.095, -481.095], mean action: 2.000 [2.000, 2.000],  loss: 2042452.000000, mae: 4030.884277, mean_q: -2637.337891
 3739/5000: episode: 3739, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -7227.969, mean reward: -7227.969 [-7227.969, -7227.969], mean action: 2.000 [2.000, 2.000],  loss: 3036572.000000, mae: 4055.779297, mean_q: -2640.613770
 3740/5000: episode: 3740, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1691.565, mean reward: -1691.565 [-1691.565, -1691.565], mean action: 2.000 [2.000, 2.000],  loss: 3204799.500000, mae: 4146.750000, mean_q: -2648.687744
 3741/5000: episode: 3741, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3736.084, mean reward: -3736.084 [-3736.084, -3736.084], mean action: 2.000 [2.000, 2.000],  loss: 1533972.000000, mae: 4029.250000, mean_q: -2617.371094
 3742/5000: episode: 3742, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4523.545, mean reward: -4523.545 [-4523.545, -4523.545], mean action: 2.000 [2.000, 2.000],  loss: 3274257.500000, mae: 4156.510742, mean_q: -2645.413330
 3743/5000: episode: 3743, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3786.658, mean reward: -3786.658 [-3786.658, -3786.658], mean action: 2.000 [2.000, 2.000],  loss: 1956620.500000, mae: 4109.750000, mean_q: -2641.382324
 3744/5000: episode: 3744, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2404.577, mean reward: -2404.577 [-2404.577, -2404.577], mean action: 2.000 [2.000, 2.000],  loss: 3521526.000000, mae: 4082.900391, mean_q: -2648.118652
 3745/5000: episode: 3745, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2614.542, mean reward: -2614.542 [-2614.542, -2614.542], mean action: 2.000 [2.000, 2.000],  loss: 3070758.000000, mae: 4108.786133, mean_q: -2642.816895
 3746/5000: episode: 3746, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6371.830, mean reward: -6371.830 [-6371.830, -6371.830], mean action: 2.000 [2.000, 2.000],  loss: 2794245.000000, mae: 4021.590576, mean_q: -2654.617676
 3747/5000: episode: 3747, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -501.907, mean reward: -501.907 [-501.907, -501.907], mean action: 2.000 [2.000, 2.000],  loss: 1913747.500000, mae: 4064.494629, mean_q: -2665.739746
 3748/5000: episode: 3748, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2798.495, mean reward: -2798.495 [-2798.495, -2798.495], mean action: 2.000 [2.000, 2.000],  loss: 2477293.250000, mae: 4072.348145, mean_q: -2667.097168
 3749/5000: episode: 3749, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3951.000, mean reward: -3951.000 [-3951.000, -3951.000], mean action: 2.000 [2.000, 2.000],  loss: 2227269.500000, mae: 4037.356445, mean_q: -2658.972656
 3750/5000: episode: 3750, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6380.065, mean reward: -6380.065 [-6380.065, -6380.065], mean action: 2.000 [2.000, 2.000],  loss: 1951347.000000, mae: 4093.432129, mean_q: -2665.916748
 3751/5000: episode: 3751, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1321.559, mean reward: -1321.559 [-1321.559, -1321.559], mean action: 2.000 [2.000, 2.000],  loss: 2821857.500000, mae: 4114.279297, mean_q: -2650.981445
 3752/5000: episode: 3752, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1514.874, mean reward: -1514.874 [-1514.874, -1514.874], mean action: 2.000 [2.000, 2.000],  loss: 2343398.000000, mae: 4095.097656, mean_q: -2662.058838
 3753/5000: episode: 3753, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3051.880, mean reward: -3051.880 [-3051.880, -3051.880], mean action: 2.000 [2.000, 2.000],  loss: 1545471.500000, mae: 4085.183594, mean_q: -2647.157715
 3754/5000: episode: 3754, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4271.814, mean reward: -4271.814 [-4271.814, -4271.814], mean action: 2.000 [2.000, 2.000],  loss: 2412438.750000, mae: 4119.621094, mean_q: -2647.635498
 3755/5000: episode: 3755, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3050.465, mean reward: -3050.465 [-3050.465, -3050.465], mean action: 2.000 [2.000, 2.000],  loss: 2439137.000000, mae: 4146.817383, mean_q: -2642.926514
 3756/5000: episode: 3756, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2087.918, mean reward: -2087.918 [-2087.918, -2087.918], mean action: 2.000 [2.000, 2.000],  loss: 2276757.750000, mae: 4028.521484, mean_q: -2627.438965
 3757/5000: episode: 3757, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4088.526, mean reward: -4088.526 [-4088.526, -4088.526], mean action: 2.000 [2.000, 2.000],  loss: 3560288.000000, mae: 4238.842285, mean_q: -2627.245605
 3758/5000: episode: 3758, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2202.189, mean reward: -2202.189 [-2202.189, -2202.189], mean action: 2.000 [2.000, 2.000],  loss: 3121246.500000, mae: 4158.832031, mean_q: -2630.956055
 3759/5000: episode: 3759, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -587.145, mean reward: -587.145 [-587.145, -587.145], mean action: 2.000 [2.000, 2.000],  loss: 1949972.000000, mae: 4042.734863, mean_q: -2618.372559
 3760/5000: episode: 3760, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2358.568, mean reward: -2358.568 [-2358.568, -2358.568], mean action: 2.000 [2.000, 2.000],  loss: 3154405.000000, mae: 4045.569824, mean_q: -2590.685059
 3761/5000: episode: 3761, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1464.250, mean reward: -1464.250 [-1464.250, -1464.250], mean action: 2.000 [2.000, 2.000],  loss: 2375607.500000, mae: 4102.722656, mean_q: -2589.860352
 3762/5000: episode: 3762, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -193.143, mean reward: -193.143 [-193.143, -193.143], mean action: 2.000 [2.000, 2.000],  loss: 2581359.250000, mae: 4095.917480, mean_q: -2611.920410
 3763/5000: episode: 3763, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4562.608, mean reward: -4562.608 [-4562.608, -4562.608], mean action: 2.000 [2.000, 2.000],  loss: 2330670.500000, mae: 4058.884277, mean_q: -2610.534912
 3764/5000: episode: 3764, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1572.677, mean reward: -1572.677 [-1572.677, -1572.677], mean action: 2.000 [2.000, 2.000],  loss: 1639556.000000, mae: 3902.166992, mean_q: -2594.515869
 3765/5000: episode: 3765, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2069.753, mean reward: -2069.753 [-2069.753, -2069.753], mean action: 2.000 [2.000, 2.000],  loss: 3214155.500000, mae: 4106.594727, mean_q: -2608.380859
 3766/5000: episode: 3766, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3716.526, mean reward: -3716.526 [-3716.526, -3716.526], mean action: 1.000 [1.000, 1.000],  loss: 3006761.000000, mae: 4160.120605, mean_q: -2599.530273
 3767/5000: episode: 3767, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2514.224, mean reward: -2514.224 [-2514.224, -2514.224], mean action: 2.000 [2.000, 2.000],  loss: 2052079.250000, mae: 4075.816650, mean_q: -2594.231934
 3768/5000: episode: 3768, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -37.893, mean reward: -37.893 [-37.893, -37.893], mean action: 2.000 [2.000, 2.000],  loss: 2979359.500000, mae: 4136.079102, mean_q: -2599.339600
 3769/5000: episode: 3769, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -483.904, mean reward: -483.904 [-483.904, -483.904], mean action: 2.000 [2.000, 2.000],  loss: 2530385.750000, mae: 4202.980957, mean_q: -2616.178223
 3770/5000: episode: 3770, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1538.056, mean reward: -1538.056 [-1538.056, -1538.056], mean action: 2.000 [2.000, 2.000],  loss: 2873135.000000, mae: 4143.482422, mean_q: -2610.825684
 3771/5000: episode: 3771, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -103.082, mean reward: -103.082 [-103.082, -103.082], mean action: 2.000 [2.000, 2.000],  loss: 3342714.250000, mae: 4177.407227, mean_q: -2609.335205
 3772/5000: episode: 3772, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3274.405, mean reward: -3274.405 [-3274.405, -3274.405], mean action: 2.000 [2.000, 2.000],  loss: 2674728.000000, mae: 4064.836426, mean_q: -2606.067383
 3773/5000: episode: 3773, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -542.420, mean reward: -542.420 [-542.420, -542.420], mean action: 2.000 [2.000, 2.000],  loss: 2329014.500000, mae: 4168.451172, mean_q: -2611.931641
 3774/5000: episode: 3774, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5816.529, mean reward: -5816.529 [-5816.529, -5816.529], mean action: 2.000 [2.000, 2.000],  loss: 4930636.000000, mae: 4210.027832, mean_q: -2631.282715
 3775/5000: episode: 3775, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1659.065, mean reward: -1659.065 [-1659.065, -1659.065], mean action: 2.000 [2.000, 2.000],  loss: 1950224.250000, mae: 4073.788818, mean_q: -2616.874023
 3776/5000: episode: 3776, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -320.253, mean reward: -320.253 [-320.253, -320.253], mean action: 2.000 [2.000, 2.000],  loss: 4124807.750000, mae: 4158.522461, mean_q: -2632.535645
 3777/5000: episode: 3777, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7579.818, mean reward: -7579.818 [-7579.818, -7579.818], mean action: 2.000 [2.000, 2.000],  loss: 4708600.000000, mae: 4322.091797, mean_q: -2628.748291
 3778/5000: episode: 3778, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1579.205, mean reward: -1579.205 [-1579.205, -1579.205], mean action: 2.000 [2.000, 2.000],  loss: 2248713.250000, mae: 4056.407471, mean_q: -2642.009766
 3779/5000: episode: 3779, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2032.107, mean reward: -2032.107 [-2032.107, -2032.107], mean action: 2.000 [2.000, 2.000],  loss: 2520260.500000, mae: 4174.679688, mean_q: -2653.298096
 3780/5000: episode: 3780, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -988.581, mean reward: -988.581 [-988.581, -988.581], mean action: 2.000 [2.000, 2.000],  loss: 3505430.500000, mae: 4199.875000, mean_q: -2645.328125
 3781/5000: episode: 3781, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -160.104, mean reward: -160.104 [-160.104, -160.104], mean action: 2.000 [2.000, 2.000],  loss: 3896629.000000, mae: 4265.874023, mean_q: -2654.416504
 3782/5000: episode: 3782, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3092.438, mean reward: -3092.438 [-3092.438, -3092.438], mean action: 2.000 [2.000, 2.000],  loss: 2407732.500000, mae: 4079.823242, mean_q: -2655.750000
 3783/5000: episode: 3783, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -49.284, mean reward: -49.284 [-49.284, -49.284], mean action: 2.000 [2.000, 2.000],  loss: 2399040.500000, mae: 4069.973389, mean_q: -2658.381592
 3784/5000: episode: 3784, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -109.067, mean reward: -109.067 [-109.067, -109.067], mean action: 2.000 [2.000, 2.000],  loss: 1669882.250000, mae: 3979.333496, mean_q: -2652.185791
 3785/5000: episode: 3785, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2470.865, mean reward: -2470.865 [-2470.865, -2470.865], mean action: 2.000 [2.000, 2.000],  loss: 2612083.000000, mae: 4158.482422, mean_q: -2667.114502
 3786/5000: episode: 3786, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -774.933, mean reward: -774.933 [-774.933, -774.933], mean action: 2.000 [2.000, 2.000],  loss: 2971043.250000, mae: 4163.354492, mean_q: -2650.207520
 3787/5000: episode: 3787, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -757.366, mean reward: -757.366 [-757.366, -757.366], mean action: 2.000 [2.000, 2.000],  loss: 3225868.000000, mae: 4147.040039, mean_q: -2660.401367
 3788/5000: episode: 3788, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -9446.208, mean reward: -9446.208 [-9446.208, -9446.208], mean action: 2.000 [2.000, 2.000],  loss: 3947085.500000, mae: 4054.277588, mean_q: -2664.754395
 3789/5000: episode: 3789, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2152.029, mean reward: -2152.029 [-2152.029, -2152.029], mean action: 2.000 [2.000, 2.000],  loss: 2560808.000000, mae: 4052.726562, mean_q: -2659.266602
 3790/5000: episode: 3790, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2736.366, mean reward: -2736.366 [-2736.366, -2736.366], mean action: 2.000 [2.000, 2.000],  loss: 2660916.500000, mae: 4159.723633, mean_q: -2656.363770
 3791/5000: episode: 3791, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2812.920, mean reward: -2812.920 [-2812.920, -2812.920], mean action: 2.000 [2.000, 2.000],  loss: 2269871.750000, mae: 4107.847656, mean_q: -2681.666016
 3792/5000: episode: 3792, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -742.536, mean reward: -742.536 [-742.536, -742.536], mean action: 2.000 [2.000, 2.000],  loss: 2837047.500000, mae: 4144.228516, mean_q: -2666.330566
 3793/5000: episode: 3793, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -214.922, mean reward: -214.922 [-214.922, -214.922], mean action: 2.000 [2.000, 2.000],  loss: 1675134.250000, mae: 4031.717773, mean_q: -2677.889648
 3794/5000: episode: 3794, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9300.279, mean reward: -9300.279 [-9300.279, -9300.279], mean action: 2.000 [2.000, 2.000],  loss: 2608680.750000, mae: 4164.538086, mean_q: -2683.081055
 3795/5000: episode: 3795, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5610.730, mean reward: -5610.730 [-5610.730, -5610.730], mean action: 1.000 [1.000, 1.000],  loss: 2242841.750000, mae: 4089.773193, mean_q: -2674.810059
 3796/5000: episode: 3796, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5634.992, mean reward: -5634.992 [-5634.992, -5634.992], mean action: 2.000 [2.000, 2.000],  loss: 2686223.250000, mae: 4080.685547, mean_q: -2661.270996
 3797/5000: episode: 3797, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1423.503, mean reward: -1423.503 [-1423.503, -1423.503], mean action: 2.000 [2.000, 2.000],  loss: 2222838.000000, mae: 4132.116211, mean_q: -2673.775879
 3798/5000: episode: 3798, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5228.955, mean reward: -5228.955 [-5228.955, -5228.955], mean action: 2.000 [2.000, 2.000],  loss: 3933700.500000, mae: 4210.979004, mean_q: -2658.976074
 3799/5000: episode: 3799, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4430.193, mean reward: -4430.193 [-4430.193, -4430.193], mean action: 2.000 [2.000, 2.000],  loss: 2182570.250000, mae: 4064.166992, mean_q: -2652.388184
 3800/5000: episode: 3800, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -9528.159, mean reward: -9528.159 [-9528.159, -9528.159], mean action: 2.000 [2.000, 2.000],  loss: 2538154.250000, mae: 4167.887695, mean_q: -2659.678955
 3801/5000: episode: 3801, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2336.187, mean reward: -2336.187 [-2336.187, -2336.187], mean action: 2.000 [2.000, 2.000],  loss: 2395273.500000, mae: 4082.629639, mean_q: -2651.491211
 3802/5000: episode: 3802, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -314.449, mean reward: -314.449 [-314.449, -314.449], mean action: 3.000 [3.000, 3.000],  loss: 2342545.250000, mae: 4099.242676, mean_q: -2636.201172
 3803/5000: episode: 3803, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5219.684, mean reward: -5219.684 [-5219.684, -5219.684], mean action: 2.000 [2.000, 2.000],  loss: 1929448.500000, mae: 4082.660645, mean_q: -2637.530762
 3804/5000: episode: 3804, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -193.821, mean reward: -193.821 [-193.821, -193.821], mean action: 2.000 [2.000, 2.000],  loss: 2351881.750000, mae: 4198.934570, mean_q: -2626.441895
 3805/5000: episode: 3805, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2570.932, mean reward: -2570.932 [-2570.932, -2570.932], mean action: 2.000 [2.000, 2.000],  loss: 3544229.750000, mae: 4185.689941, mean_q: -2623.579590
 3806/5000: episode: 3806, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -904.326, mean reward: -904.326 [-904.326, -904.326], mean action: 2.000 [2.000, 2.000],  loss: 3275577.000000, mae: 4114.511719, mean_q: -2618.001465
 3807/5000: episode: 3807, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -480.515, mean reward: -480.515 [-480.515, -480.515], mean action: 2.000 [2.000, 2.000],  loss: 1918564.375000, mae: 4109.173828, mean_q: -2607.121582
 3808/5000: episode: 3808, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -838.878, mean reward: -838.878 [-838.878, -838.878], mean action: 2.000 [2.000, 2.000],  loss: 4463608.500000, mae: 4186.397949, mean_q: -2616.195557
 3809/5000: episode: 3809, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4574.705, mean reward: -4574.705 [-4574.705, -4574.705], mean action: 2.000 [2.000, 2.000],  loss: 3393703.500000, mae: 4194.062012, mean_q: -2627.270996
 3810/5000: episode: 3810, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1887.682, mean reward: -1887.682 [-1887.682, -1887.682], mean action: 2.000 [2.000, 2.000],  loss: 2182124.250000, mae: 4092.883789, mean_q: -2619.489990
 3811/5000: episode: 3811, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -287.549, mean reward: -287.549 [-287.549, -287.549], mean action: 2.000 [2.000, 2.000],  loss: 1338847.000000, mae: 4004.270996, mean_q: -2622.326660
 3812/5000: episode: 3812, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -2454.383, mean reward: -2454.383 [-2454.383, -2454.383], mean action: 2.000 [2.000, 2.000],  loss: 2322541.500000, mae: 4126.888672, mean_q: -2615.772461
 3813/5000: episode: 3813, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4208.871, mean reward: -4208.871 [-4208.871, -4208.871], mean action: 2.000 [2.000, 2.000],  loss: 2188908.000000, mae: 4112.792480, mean_q: -2628.402832
 3814/5000: episode: 3814, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2609.895, mean reward: -2609.895 [-2609.895, -2609.895], mean action: 2.000 [2.000, 2.000],  loss: 3012933.500000, mae: 4133.285156, mean_q: -2623.240723
 3815/5000: episode: 3815, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3978.755, mean reward: -3978.755 [-3978.755, -3978.755], mean action: 2.000 [2.000, 2.000],  loss: 4038172.750000, mae: 4238.600098, mean_q: -2609.858887
 3816/5000: episode: 3816, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -464.450, mean reward: -464.450 [-464.450, -464.450], mean action: 2.000 [2.000, 2.000],  loss: 2442108.750000, mae: 4146.395996, mean_q: -2621.835693
 3817/5000: episode: 3817, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5334.265, mean reward: -5334.265 [-5334.265, -5334.265], mean action: 2.000 [2.000, 2.000],  loss: 2885694.000000, mae: 4174.808594, mean_q: -2609.328613
 3818/5000: episode: 3818, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1141.941, mean reward: -1141.941 [-1141.941, -1141.941], mean action: 2.000 [2.000, 2.000],  loss: 1766390.000000, mae: 4034.491211, mean_q: -2614.606201
 3819/5000: episode: 3819, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -991.235, mean reward: -991.235 [-991.235, -991.235], mean action: 2.000 [2.000, 2.000],  loss: 1898666.875000, mae: 3982.793945, mean_q: -2610.886230
 3820/5000: episode: 3820, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2232.133, mean reward: -2232.133 [-2232.133, -2232.133], mean action: 2.000 [2.000, 2.000],  loss: 1664758.375000, mae: 4018.509033, mean_q: -2607.195801
 3821/5000: episode: 3821, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6257.928, mean reward: -6257.928 [-6257.928, -6257.928], mean action: 2.000 [2.000, 2.000],  loss: 3021750.500000, mae: 4099.376953, mean_q: -2611.300537
 3822/5000: episode: 3822, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6666.387, mean reward: -6666.387 [-6666.387, -6666.387], mean action: 2.000 [2.000, 2.000],  loss: 1856504.500000, mae: 3971.679688, mean_q: -2613.097656
 3823/5000: episode: 3823, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6842.332, mean reward: -6842.332 [-6842.332, -6842.332], mean action: 2.000 [2.000, 2.000],  loss: 1678345.750000, mae: 3949.829102, mean_q: -2598.300781
 3824/5000: episode: 3824, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1062.802, mean reward: -1062.802 [-1062.802, -1062.802], mean action: 2.000 [2.000, 2.000],  loss: 1691541.500000, mae: 4045.697266, mean_q: -2615.938721
 3825/5000: episode: 3825, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -287.167, mean reward: -287.167 [-287.167, -287.167], mean action: 2.000 [2.000, 2.000],  loss: 2000623.000000, mae: 3951.848145, mean_q: -2609.405029
 3826/5000: episode: 3826, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2322.657, mean reward: -2322.657 [-2322.657, -2322.657], mean action: 2.000 [2.000, 2.000],  loss: 1821072.125000, mae: 4019.781494, mean_q: -2607.285889
 3827/5000: episode: 3827, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3709.884, mean reward: -3709.884 [-3709.884, -3709.884], mean action: 2.000 [2.000, 2.000],  loss: 2166291.250000, mae: 3995.714600, mean_q: -2600.363037
 3828/5000: episode: 3828, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -951.888, mean reward: -951.888 [-951.888, -951.888], mean action: 2.000 [2.000, 2.000],  loss: 3988765.750000, mae: 4073.452637, mean_q: -2598.131836
 3829/5000: episode: 3829, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2795.480, mean reward: -2795.480 [-2795.480, -2795.480], mean action: 2.000 [2.000, 2.000],  loss: 1796371.375000, mae: 3944.962891, mean_q: -2586.586914
 3830/5000: episode: 3830, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3036.113, mean reward: -3036.113 [-3036.113, -3036.113], mean action: 2.000 [2.000, 2.000],  loss: 3478305.000000, mae: 4094.205322, mean_q: -2582.218506
 3831/5000: episode: 3831, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3350.552, mean reward: -3350.552 [-3350.552, -3350.552], mean action: 2.000 [2.000, 2.000],  loss: 3189147.500000, mae: 4133.365723, mean_q: -2585.698486
 3832/5000: episode: 3832, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -340.285, mean reward: -340.285 [-340.285, -340.285], mean action: 2.000 [2.000, 2.000],  loss: 2536746.500000, mae: 3949.908447, mean_q: -2567.050781
 3833/5000: episode: 3833, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2402.448, mean reward: -2402.448 [-2402.448, -2402.448], mean action: 3.000 [3.000, 3.000],  loss: 2966661.750000, mae: 4101.890625, mean_q: -2562.875000
 3834/5000: episode: 3834, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -3564.851, mean reward: -3564.851 [-3564.851, -3564.851], mean action: 2.000 [2.000, 2.000],  loss: 1386432.875000, mae: 3989.614746, mean_q: -2559.924805
 3835/5000: episode: 3835, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -27.411, mean reward: -27.411 [-27.411, -27.411], mean action: 2.000 [2.000, 2.000],  loss: 2857538.000000, mae: 4003.535645, mean_q: -2563.798584
 3836/5000: episode: 3836, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5779.241, mean reward: -5779.241 [-5779.241, -5779.241], mean action: 2.000 [2.000, 2.000],  loss: 3106143.250000, mae: 3993.368164, mean_q: -2558.772461
 3837/5000: episode: 3837, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4466.739, mean reward: -4466.739 [-4466.739, -4466.739], mean action: 2.000 [2.000, 2.000],  loss: 3517393.000000, mae: 4189.231934, mean_q: -2557.645020
 3838/5000: episode: 3838, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2091.421, mean reward: -2091.421 [-2091.421, -2091.421], mean action: 2.000 [2.000, 2.000],  loss: 3809281.000000, mae: 4139.910645, mean_q: -2576.102051
 3839/5000: episode: 3839, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7683.705, mean reward: -7683.705 [-7683.705, -7683.705], mean action: 2.000 [2.000, 2.000],  loss: 3142047.000000, mae: 4115.984375, mean_q: -2583.561523
 3840/5000: episode: 3840, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5352.777, mean reward: -5352.777 [-5352.777, -5352.777], mean action: 0.000 [0.000, 0.000],  loss: 3182765.500000, mae: 4103.940430, mean_q: -2599.895020
 3841/5000: episode: 3841, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2324.479, mean reward: -2324.479 [-2324.479, -2324.479], mean action: 2.000 [2.000, 2.000],  loss: 3272412.000000, mae: 4171.348633, mean_q: -2623.366211
 3842/5000: episode: 3842, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -12.731, mean reward: -12.731 [-12.731, -12.731], mean action: 2.000 [2.000, 2.000],  loss: 2484199.750000, mae: 4030.748535, mean_q: -2651.272461
 3843/5000: episode: 3843, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -938.087, mean reward: -938.087 [-938.087, -938.087], mean action: 2.000 [2.000, 2.000],  loss: 3854896.250000, mae: 4209.092773, mean_q: -2664.714844
 3844/5000: episode: 3844, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2164.945, mean reward: -2164.945 [-2164.945, -2164.945], mean action: 2.000 [2.000, 2.000],  loss: 3735751.750000, mae: 4239.666016, mean_q: -2663.900879
 3845/5000: episode: 3845, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -371.925, mean reward: -371.925 [-371.925, -371.925], mean action: 2.000 [2.000, 2.000],  loss: 2461108.500000, mae: 4154.011719, mean_q: -2692.819580
 3846/5000: episode: 3846, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -562.976, mean reward: -562.976 [-562.976, -562.976], mean action: 2.000 [2.000, 2.000],  loss: 2404311.500000, mae: 4072.381348, mean_q: -2706.794189
 3847/5000: episode: 3847, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3765.379, mean reward: -3765.379 [-3765.379, -3765.379], mean action: 2.000 [2.000, 2.000],  loss: 2463106.500000, mae: 4122.047852, mean_q: -2702.348633
 3848/5000: episode: 3848, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3360.364, mean reward: -3360.364 [-3360.364, -3360.364], mean action: 2.000 [2.000, 2.000],  loss: 2475655.750000, mae: 4075.753418, mean_q: -2722.135498
 3849/5000: episode: 3849, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4338.457, mean reward: -4338.457 [-4338.457, -4338.457], mean action: 2.000 [2.000, 2.000],  loss: 2508022.250000, mae: 4104.504883, mean_q: -2724.001465
 3850/5000: episode: 3850, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -860.269, mean reward: -860.269 [-860.269, -860.269], mean action: 2.000 [2.000, 2.000],  loss: 1837649.750000, mae: 4084.472656, mean_q: -2726.984863
 3851/5000: episode: 3851, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7651.790, mean reward: -7651.790 [-7651.790, -7651.790], mean action: 2.000 [2.000, 2.000],  loss: 3629635.000000, mae: 4200.766113, mean_q: -2741.270996
 3852/5000: episode: 3852, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3641.432, mean reward: -3641.432 [-3641.432, -3641.432], mean action: 2.000 [2.000, 2.000],  loss: 3534120.750000, mae: 4223.678711, mean_q: -2744.784668
 3853/5000: episode: 3853, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5040.334, mean reward: -5040.334 [-5040.334, -5040.334], mean action: 2.000 [2.000, 2.000],  loss: 2250784.500000, mae: 4123.191895, mean_q: -2748.473145
 3854/5000: episode: 3854, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2920.260, mean reward: -2920.260 [-2920.260, -2920.260], mean action: 2.000 [2.000, 2.000],  loss: 2867877.750000, mae: 4160.197754, mean_q: -2735.097656
 3855/5000: episode: 3855, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5681.257, mean reward: -5681.257 [-5681.257, -5681.257], mean action: 2.000 [2.000, 2.000],  loss: 2886615.500000, mae: 4048.439453, mean_q: -2748.489258
 3856/5000: episode: 3856, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -54.230, mean reward: -54.230 [-54.230, -54.230], mean action: 2.000 [2.000, 2.000],  loss: 2010995.750000, mae: 4156.436035, mean_q: -2731.284180
 3857/5000: episode: 3857, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4098.363, mean reward: -4098.363 [-4098.363, -4098.363], mean action: 2.000 [2.000, 2.000],  loss: 3066570.000000, mae: 4291.223145, mean_q: -2747.666748
 3858/5000: episode: 3858, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -173.557, mean reward: -173.557 [-173.557, -173.557], mean action: 2.000 [2.000, 2.000],  loss: 1788592.875000, mae: 4058.925293, mean_q: -2728.573975
 3859/5000: episode: 3859, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5199.728, mean reward: -5199.728 [-5199.728, -5199.728], mean action: 2.000 [2.000, 2.000],  loss: 2395284.750000, mae: 4139.058105, mean_q: -2725.147705
 3860/5000: episode: 3860, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4928.064, mean reward: -4928.064 [-4928.064, -4928.064], mean action: 1.000 [1.000, 1.000],  loss: 3810667.750000, mae: 4251.619141, mean_q: -2698.886475
 3861/5000: episode: 3861, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2425.085, mean reward: -2425.085 [-2425.085, -2425.085], mean action: 2.000 [2.000, 2.000],  loss: 2980718.250000, mae: 4167.077637, mean_q: -2678.980469
 3862/5000: episode: 3862, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6640.980, mean reward: -6640.980 [-6640.980, -6640.980], mean action: 2.000 [2.000, 2.000],  loss: 2830074.500000, mae: 4314.755371, mean_q: -2669.478027
 3863/5000: episode: 3863, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7159.403, mean reward: -7159.403 [-7159.403, -7159.403], mean action: 2.000 [2.000, 2.000],  loss: 2729961.000000, mae: 4130.493652, mean_q: -2670.121582
 3864/5000: episode: 3864, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3244.249, mean reward: -3244.249 [-3244.249, -3244.249], mean action: 2.000 [2.000, 2.000],  loss: 3747962.500000, mae: 4232.150879, mean_q: -2672.911133
 3865/5000: episode: 3865, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1971.868, mean reward: -1971.868 [-1971.868, -1971.868], mean action: 2.000 [2.000, 2.000],  loss: 2496832.250000, mae: 4268.769531, mean_q: -2653.860107
 3866/5000: episode: 3866, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2702.762, mean reward: -2702.762 [-2702.762, -2702.762], mean action: 2.000 [2.000, 2.000],  loss: 3146654.000000, mae: 4270.549805, mean_q: -2659.298340
 3867/5000: episode: 3867, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3569.307, mean reward: -3569.307 [-3569.307, -3569.307], mean action: 2.000 [2.000, 2.000],  loss: 4127954.250000, mae: 4366.488281, mean_q: -2660.912598
 3868/5000: episode: 3868, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2299.301, mean reward: -2299.301 [-2299.301, -2299.301], mean action: 2.000 [2.000, 2.000],  loss: 1025209.500000, mae: 4062.676270, mean_q: -2661.959717
 3869/5000: episode: 3869, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -472.665, mean reward: -472.665 [-472.665, -472.665], mean action: 2.000 [2.000, 2.000],  loss: 2338193.250000, mae: 4152.985352, mean_q: -2642.851074
 3870/5000: episode: 3870, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1051.021, mean reward: -1051.021 [-1051.021, -1051.021], mean action: 2.000 [2.000, 2.000],  loss: 1541426.500000, mae: 4092.947998, mean_q: -2632.364502
 3871/5000: episode: 3871, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6178.660, mean reward: -6178.660 [-6178.660, -6178.660], mean action: 0.000 [0.000, 0.000],  loss: 5485072.500000, mae: 4323.943359, mean_q: -2621.840088
 3872/5000: episode: 3872, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4151.821, mean reward: -4151.821 [-4151.821, -4151.821], mean action: 2.000 [2.000, 2.000],  loss: 1533413.125000, mae: 4065.640869, mean_q: -2614.824219
 3873/5000: episode: 3873, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -126.403, mean reward: -126.403 [-126.403, -126.403], mean action: 2.000 [2.000, 2.000],  loss: 2733556.500000, mae: 4127.009277, mean_q: -2614.003906
 3874/5000: episode: 3874, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3187.332, mean reward: -3187.332 [-3187.332, -3187.332], mean action: 2.000 [2.000, 2.000],  loss: 2257181.000000, mae: 4008.582031, mean_q: -2599.153320
 3875/5000: episode: 3875, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1863.130, mean reward: -1863.130 [-1863.130, -1863.130], mean action: 2.000 [2.000, 2.000],  loss: 2289906.000000, mae: 4104.814453, mean_q: -2606.613770
 3876/5000: episode: 3876, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7415.473, mean reward: -7415.473 [-7415.473, -7415.473], mean action: 2.000 [2.000, 2.000],  loss: 3229858.000000, mae: 4080.499023, mean_q: -2582.919922
 3877/5000: episode: 3877, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3067.877, mean reward: -3067.877 [-3067.877, -3067.877], mean action: 2.000 [2.000, 2.000],  loss: 3359958.000000, mae: 4173.469727, mean_q: -2569.559082
 3878/5000: episode: 3878, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7578.407, mean reward: -7578.407 [-7578.407, -7578.407], mean action: 2.000 [2.000, 2.000],  loss: 2607864.500000, mae: 4148.375000, mean_q: -2584.336914
 3879/5000: episode: 3879, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2703.069, mean reward: -2703.069 [-2703.069, -2703.069], mean action: 2.000 [2.000, 2.000],  loss: 2457092.000000, mae: 4143.845703, mean_q: -2584.252930
 3880/5000: episode: 3880, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7388.350, mean reward: -7388.350 [-7388.350, -7388.350], mean action: 2.000 [2.000, 2.000],  loss: 2911816.000000, mae: 4162.508301, mean_q: -2573.085449
 3881/5000: episode: 3881, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8422.467, mean reward: -8422.467 [-8422.467, -8422.467], mean action: 2.000 [2.000, 2.000],  loss: 2571891.000000, mae: 4078.661377, mean_q: -2567.073730
 3882/5000: episode: 3882, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1311.114, mean reward: -1311.114 [-1311.114, -1311.114], mean action: 2.000 [2.000, 2.000],  loss: 3585289.000000, mae: 4202.769043, mean_q: -2567.345703
 3883/5000: episode: 3883, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -400.777, mean reward: -400.777 [-400.777, -400.777], mean action: 2.000 [2.000, 2.000],  loss: 3296393.750000, mae: 4013.887207, mean_q: -2568.667969
 3884/5000: episode: 3884, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1152.788, mean reward: -1152.788 [-1152.788, -1152.788], mean action: 2.000 [2.000, 2.000],  loss: 3654322.000000, mae: 4125.716797, mean_q: -2588.982422
 3885/5000: episode: 3885, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1670.414, mean reward: -1670.414 [-1670.414, -1670.414], mean action: 2.000 [2.000, 2.000],  loss: 1840966.125000, mae: 4003.945312, mean_q: -2583.432617
 3886/5000: episode: 3886, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -208.602, mean reward: -208.602 [-208.602, -208.602], mean action: 2.000 [2.000, 2.000],  loss: 3587932.250000, mae: 4168.028809, mean_q: -2588.419922
 3887/5000: episode: 3887, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -820.629, mean reward: -820.629 [-820.629, -820.629], mean action: 2.000 [2.000, 2.000],  loss: 2194330.000000, mae: 4131.597168, mean_q: -2608.036621
 3888/5000: episode: 3888, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6638.389, mean reward: -6638.389 [-6638.389, -6638.389], mean action: 2.000 [2.000, 2.000],  loss: 2758866.500000, mae: 4138.663574, mean_q: -2591.701172
 3889/5000: episode: 3889, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1438.088, mean reward: -1438.088 [-1438.088, -1438.088], mean action: 2.000 [2.000, 2.000],  loss: 2264462.500000, mae: 4068.363281, mean_q: -2602.506104
 3890/5000: episode: 3890, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2993.098, mean reward: -2993.098 [-2993.098, -2993.098], mean action: 2.000 [2.000, 2.000],  loss: 2812171.250000, mae: 4028.091064, mean_q: -2585.892822
 3891/5000: episode: 3891, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5270.709, mean reward: -5270.709 [-5270.709, -5270.709], mean action: 2.000 [2.000, 2.000],  loss: 2095261.250000, mae: 4064.053467, mean_q: -2606.429688
 3892/5000: episode: 3892, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5294.619, mean reward: -5294.619 [-5294.619, -5294.619], mean action: 2.000 [2.000, 2.000],  loss: 2136078.500000, mae: 4047.320312, mean_q: -2594.572266
 3893/5000: episode: 3893, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2597.118, mean reward: -2597.118 [-2597.118, -2597.118], mean action: 2.000 [2.000, 2.000],  loss: 2879015.250000, mae: 4005.854980, mean_q: -2599.955811
 3894/5000: episode: 3894, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -2999.352, mean reward: -2999.352 [-2999.352, -2999.352], mean action: 2.000 [2.000, 2.000],  loss: 1897867.875000, mae: 4039.566895, mean_q: -2597.255371
 3895/5000: episode: 3895, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1083.325, mean reward: -1083.325 [-1083.325, -1083.325], mean action: 2.000 [2.000, 2.000],  loss: 1883396.875000, mae: 3980.888672, mean_q: -2600.619141
 3896/5000: episode: 3896, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1518.339, mean reward: -1518.339 [-1518.339, -1518.339], mean action: 2.000 [2.000, 2.000],  loss: 2204013.000000, mae: 4077.547363, mean_q: -2597.133545
 3897/5000: episode: 3897, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2584.917, mean reward: -2584.917 [-2584.917, -2584.917], mean action: 2.000 [2.000, 2.000],  loss: 1805812.125000, mae: 4001.987549, mean_q: -2590.631592
 3898/5000: episode: 3898, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1330.844, mean reward: -1330.844 [-1330.844, -1330.844], mean action: 2.000 [2.000, 2.000],  loss: 2112823.500000, mae: 4096.529297, mean_q: -2583.808105
 3899/5000: episode: 3899, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -7602.905, mean reward: -7602.905 [-7602.905, -7602.905], mean action: 2.000 [2.000, 2.000],  loss: 3004603.250000, mae: 4132.286133, mean_q: -2593.812256
 3900/5000: episode: 3900, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3617.612, mean reward: -3617.612 [-3617.612, -3617.612], mean action: 2.000 [2.000, 2.000],  loss: 2154411.500000, mae: 4018.581787, mean_q: -2597.417480
 3901/5000: episode: 3901, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1785.985, mean reward: -1785.985 [-1785.985, -1785.985], mean action: 2.000 [2.000, 2.000],  loss: 1919046.500000, mae: 3998.691895, mean_q: -2611.497314
 3902/5000: episode: 3902, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7395.212, mean reward: -7395.212 [-7395.212, -7395.212], mean action: 2.000 [2.000, 2.000],  loss: 2565271.750000, mae: 4039.153809, mean_q: -2605.403809
 3903/5000: episode: 3903, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1863.785, mean reward: -1863.785 [-1863.785, -1863.785], mean action: 2.000 [2.000, 2.000],  loss: 2686041.500000, mae: 4135.975586, mean_q: -2604.242920
 3904/5000: episode: 3904, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -369.204, mean reward: -369.204 [-369.204, -369.204], mean action: 2.000 [2.000, 2.000],  loss: 3349768.750000, mae: 3965.707031, mean_q: -2602.879639
 3905/5000: episode: 3905, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1454.702, mean reward: -1454.702 [-1454.702, -1454.702], mean action: 2.000 [2.000, 2.000],  loss: 2558069.000000, mae: 4063.343018, mean_q: -2596.286621
 3906/5000: episode: 3906, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2490.980, mean reward: -2490.980 [-2490.980, -2490.980], mean action: 2.000 [2.000, 2.000],  loss: 3019738.500000, mae: 4150.170898, mean_q: -2602.842285
 3907/5000: episode: 3907, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3103.029, mean reward: -3103.029 [-3103.029, -3103.029], mean action: 2.000 [2.000, 2.000],  loss: 3081230.250000, mae: 3979.669434, mean_q: -2626.816895
 3908/5000: episode: 3908, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -153.638, mean reward: -153.638 [-153.638, -153.638], mean action: 2.000 [2.000, 2.000],  loss: 3018561.000000, mae: 4120.883789, mean_q: -2603.703857
 3909/5000: episode: 3909, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2972.221, mean reward: -2972.221 [-2972.221, -2972.221], mean action: 2.000 [2.000, 2.000],  loss: 4881509.500000, mae: 4242.607422, mean_q: -2640.069092
 3910/5000: episode: 3910, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -10811.363, mean reward: -10811.363 [-10811.363, -10811.363], mean action: 0.000 [0.000, 0.000],  loss: 2008573.500000, mae: 3992.728027, mean_q: -2623.522705
 3911/5000: episode: 3911, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4712.421, mean reward: -4712.421 [-4712.421, -4712.421], mean action: 2.000 [2.000, 2.000],  loss: 2693573.500000, mae: 4089.514404, mean_q: -2630.468262
 3912/5000: episode: 3912, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -644.651, mean reward: -644.651 [-644.651, -644.651], mean action: 2.000 [2.000, 2.000],  loss: 2185674.750000, mae: 4002.023193, mean_q: -2635.693848
 3913/5000: episode: 3913, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -656.258, mean reward: -656.258 [-656.258, -656.258], mean action: 2.000 [2.000, 2.000],  loss: 2380769.000000, mae: 4108.668945, mean_q: -2650.181396
 3914/5000: episode: 3914, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6696.179, mean reward: -6696.179 [-6696.179, -6696.179], mean action: 2.000 [2.000, 2.000],  loss: 3410503.000000, mae: 4141.990723, mean_q: -2654.319824
 3915/5000: episode: 3915, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5027.920, mean reward: -5027.920 [-5027.920, -5027.920], mean action: 2.000 [2.000, 2.000],  loss: 2666822.500000, mae: 4030.685547, mean_q: -2657.569824
 3916/5000: episode: 3916, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -619.015, mean reward: -619.015 [-619.015, -619.015], mean action: 2.000 [2.000, 2.000],  loss: 2750634.500000, mae: 4051.195801, mean_q: -2648.564453
 3917/5000: episode: 3917, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2826.370, mean reward: -2826.370 [-2826.370, -2826.370], mean action: 2.000 [2.000, 2.000],  loss: 2201429.000000, mae: 3955.369629, mean_q: -2635.121094
 3918/5000: episode: 3918, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1320.576, mean reward: -1320.576 [-1320.576, -1320.576], mean action: 2.000 [2.000, 2.000],  loss: 3588638.000000, mae: 4082.778564, mean_q: -2631.090332
 3919/5000: episode: 3919, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -13.920, mean reward: -13.920 [-13.920, -13.920], mean action: 2.000 [2.000, 2.000],  loss: 2464342.000000, mae: 4059.717041, mean_q: -2623.497559
 3920/5000: episode: 3920, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1946.630, mean reward: -1946.630 [-1946.630, -1946.630], mean action: 2.000 [2.000, 2.000],  loss: 3096539.500000, mae: 4123.603516, mean_q: -2627.671387
 3921/5000: episode: 3921, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2208.975, mean reward: -2208.975 [-2208.975, -2208.975], mean action: 2.000 [2.000, 2.000],  loss: 2042778.375000, mae: 4114.888184, mean_q: -2641.224854
 3922/5000: episode: 3922, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -784.092, mean reward: -784.092 [-784.092, -784.092], mean action: 3.000 [3.000, 3.000],  loss: 2861030.500000, mae: 4011.139893, mean_q: -2624.434570
 3923/5000: episode: 3923, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2804.116, mean reward: -2804.116 [-2804.116, -2804.116], mean action: 2.000 [2.000, 2.000],  loss: 3453319.500000, mae: 4034.775391, mean_q: -2615.441406
 3924/5000: episode: 3924, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1228.349, mean reward: -1228.349 [-1228.349, -1228.349], mean action: 2.000 [2.000, 2.000],  loss: 2875254.500000, mae: 4078.078369, mean_q: -2631.198730
 3925/5000: episode: 3925, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2095.752, mean reward: -2095.752 [-2095.752, -2095.752], mean action: 2.000 [2.000, 2.000],  loss: 2542795.500000, mae: 4095.524902, mean_q: -2621.083008
 3926/5000: episode: 3926, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -189.146, mean reward: -189.146 [-189.146, -189.146], mean action: 2.000 [2.000, 2.000],  loss: 2518812.000000, mae: 4105.373047, mean_q: -2612.155273
 3927/5000: episode: 3927, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6381.689, mean reward: -6381.689 [-6381.689, -6381.689], mean action: 1.000 [1.000, 1.000],  loss: 3030241.500000, mae: 4102.554199, mean_q: -2611.670166
 3928/5000: episode: 3928, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4614.778, mean reward: -4614.778 [-4614.778, -4614.778], mean action: 2.000 [2.000, 2.000],  loss: 3790747.000000, mae: 4198.578125, mean_q: -2593.409668
 3929/5000: episode: 3929, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3448.491, mean reward: -3448.491 [-3448.491, -3448.491], mean action: 2.000 [2.000, 2.000],  loss: 3758002.750000, mae: 4147.811523, mean_q: -2562.256348
 3930/5000: episode: 3930, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -120.461, mean reward: -120.461 [-120.461, -120.461], mean action: 2.000 [2.000, 2.000],  loss: 2412389.000000, mae: 4044.694824, mean_q: -2566.057129
 3931/5000: episode: 3931, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5864.283, mean reward: -5864.283 [-5864.283, -5864.283], mean action: 2.000 [2.000, 2.000],  loss: 2214411.500000, mae: 4089.047607, mean_q: -2553.854004
 3932/5000: episode: 3932, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -332.243, mean reward: -332.243 [-332.243, -332.243], mean action: 2.000 [2.000, 2.000],  loss: 2528353.750000, mae: 4120.729492, mean_q: -2561.947754
 3933/5000: episode: 3933, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5648.956, mean reward: -5648.956 [-5648.956, -5648.956], mean action: 2.000 [2.000, 2.000],  loss: 2488079.250000, mae: 4113.753906, mean_q: -2552.754395
 3934/5000: episode: 3934, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4313.956, mean reward: -4313.956 [-4313.956, -4313.956], mean action: 2.000 [2.000, 2.000],  loss: 2960326.250000, mae: 4056.550049, mean_q: -2532.728516
 3935/5000: episode: 3935, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3694.979, mean reward: -3694.979 [-3694.979, -3694.979], mean action: 2.000 [2.000, 2.000],  loss: 3571465.750000, mae: 4108.723633, mean_q: -2539.635498
 3936/5000: episode: 3936, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -970.660, mean reward: -970.660 [-970.660, -970.660], mean action: 2.000 [2.000, 2.000],  loss: 2844655.000000, mae: 4108.264160, mean_q: -2531.281250
 3937/5000: episode: 3937, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3234.747, mean reward: -3234.747 [-3234.747, -3234.747], mean action: 2.000 [2.000, 2.000],  loss: 2293165.750000, mae: 4105.427246, mean_q: -2557.844727
 3938/5000: episode: 3938, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1017.835, mean reward: -1017.835 [-1017.835, -1017.835], mean action: 2.000 [2.000, 2.000],  loss: 2618757.750000, mae: 4120.935547, mean_q: -2545.938477
 3939/5000: episode: 3939, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3581.013, mean reward: -3581.013 [-3581.013, -3581.013], mean action: 2.000 [2.000, 2.000],  loss: 2400624.000000, mae: 4051.417969, mean_q: -2547.177734
 3940/5000: episode: 3940, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1223.439, mean reward: -1223.439 [-1223.439, -1223.439], mean action: 2.000 [2.000, 2.000],  loss: 2282246.500000, mae: 4050.217773, mean_q: -2550.927246
 3941/5000: episode: 3941, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1726.271, mean reward: -1726.271 [-1726.271, -1726.271], mean action: 2.000 [2.000, 2.000],  loss: 4949707.000000, mae: 4309.287109, mean_q: -2566.706055
 3942/5000: episode: 3942, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -466.122, mean reward: -466.122 [-466.122, -466.122], mean action: 2.000 [2.000, 2.000],  loss: 3283005.250000, mae: 4128.130859, mean_q: -2542.863281
 3943/5000: episode: 3943, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2206.684, mean reward: -2206.684 [-2206.684, -2206.684], mean action: 2.000 [2.000, 2.000],  loss: 2589280.250000, mae: 4089.018555, mean_q: -2531.324707
 3944/5000: episode: 3944, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -378.788, mean reward: -378.788 [-378.788, -378.788], mean action: 2.000 [2.000, 2.000],  loss: 4846144.000000, mae: 4149.033691, mean_q: -2541.167969
 3945/5000: episode: 3945, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2000.097, mean reward: -2000.097 [-2000.097, -2000.097], mean action: 2.000 [2.000, 2.000],  loss: 2734113.000000, mae: 4144.463379, mean_q: -2543.226074
 3946/5000: episode: 3946, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4223.850, mean reward: -4223.850 [-4223.850, -4223.850], mean action: 2.000 [2.000, 2.000],  loss: 2473547.000000, mae: 4111.741211, mean_q: -2565.056641
 3947/5000: episode: 3947, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -11.466, mean reward: -11.466 [-11.466, -11.466], mean action: 2.000 [2.000, 2.000],  loss: 1697834.625000, mae: 4082.115234, mean_q: -2560.155029
 3948/5000: episode: 3948, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -863.538, mean reward: -863.538 [-863.538, -863.538], mean action: 2.000 [2.000, 2.000],  loss: 1563847.375000, mae: 4057.782715, mean_q: -2564.347168
 3949/5000: episode: 3949, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -187.436, mean reward: -187.436 [-187.436, -187.436], mean action: 2.000 [2.000, 2.000],  loss: 2326581.750000, mae: 4042.461182, mean_q: -2553.349121
 3950/5000: episode: 3950, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2211.411, mean reward: -2211.411 [-2211.411, -2211.411], mean action: 2.000 [2.000, 2.000],  loss: 3326102.500000, mae: 4278.173828, mean_q: -2569.058594
 3951/5000: episode: 3951, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1302.798, mean reward: -1302.798 [-1302.798, -1302.798], mean action: 2.000 [2.000, 2.000],  loss: 2556483.000000, mae: 4142.877930, mean_q: -2525.497070
 3952/5000: episode: 3952, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2447.135, mean reward: -2447.135 [-2447.135, -2447.135], mean action: 2.000 [2.000, 2.000],  loss: 1859393.875000, mae: 4082.990234, mean_q: -2559.649414
 3953/5000: episode: 3953, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -128.295, mean reward: -128.295 [-128.295, -128.295], mean action: 2.000 [2.000, 2.000],  loss: 2346706.250000, mae: 4094.689941, mean_q: -2547.653809
 3954/5000: episode: 3954, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1775.449, mean reward: -1775.449 [-1775.449, -1775.449], mean action: 2.000 [2.000, 2.000],  loss: 1636995.750000, mae: 3992.132324, mean_q: -2550.990723
 3955/5000: episode: 3955, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6572.821, mean reward: -6572.821 [-6572.821, -6572.821], mean action: 2.000 [2.000, 2.000],  loss: 4985109.000000, mae: 4206.123047, mean_q: -2568.557373
 3956/5000: episode: 3956, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1501.006, mean reward: -1501.006 [-1501.006, -1501.006], mean action: 2.000 [2.000, 2.000],  loss: 3879606.000000, mae: 4277.485352, mean_q: -2563.979736
 3957/5000: episode: 3957, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1368.644, mean reward: -1368.644 [-1368.644, -1368.644], mean action: 2.000 [2.000, 2.000],  loss: 1005918.312500, mae: 4017.509277, mean_q: -2578.102051
 3958/5000: episode: 3958, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2085.303, mean reward: -2085.303 [-2085.303, -2085.303], mean action: 2.000 [2.000, 2.000],  loss: 2794407.250000, mae: 4132.551270, mean_q: -2576.040527
 3959/5000: episode: 3959, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -6494.412, mean reward: -6494.412 [-6494.412, -6494.412], mean action: 2.000 [2.000, 2.000],  loss: 3418376.000000, mae: 4178.023438, mean_q: -2583.367432
 3960/5000: episode: 3960, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4465.021, mean reward: -4465.021 [-4465.021, -4465.021], mean action: 2.000 [2.000, 2.000],  loss: 4500097.000000, mae: 4253.001465, mean_q: -2575.979004
 3961/5000: episode: 3961, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2249.736, mean reward: -2249.736 [-2249.736, -2249.736], mean action: 3.000 [3.000, 3.000],  loss: 1657205.875000, mae: 4073.499512, mean_q: -2570.956543
 3962/5000: episode: 3962, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3439.507, mean reward: -3439.507 [-3439.507, -3439.507], mean action: 2.000 [2.000, 2.000],  loss: 1617930.250000, mae: 3970.448975, mean_q: -2554.070068
 3963/5000: episode: 3963, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2942.869, mean reward: -2942.869 [-2942.869, -2942.869], mean action: 2.000 [2.000, 2.000],  loss: 3164397.500000, mae: 4182.611328, mean_q: -2543.948242
 3964/5000: episode: 3964, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -24.715, mean reward: -24.715 [-24.715, -24.715], mean action: 2.000 [2.000, 2.000],  loss: 2859662.000000, mae: 4084.120850, mean_q: -2549.004883
 3965/5000: episode: 3965, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -990.595, mean reward: -990.595 [-990.595, -990.595], mean action: 2.000 [2.000, 2.000],  loss: 3535707.000000, mae: 4089.218018, mean_q: -2545.080078
 3966/5000: episode: 3966, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6509.077, mean reward: -6509.077 [-6509.077, -6509.077], mean action: 2.000 [2.000, 2.000],  loss: 2368205.000000, mae: 4111.698242, mean_q: -2534.433838
 3967/5000: episode: 3967, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4460.199, mean reward: -4460.199 [-4460.199, -4460.199], mean action: 2.000 [2.000, 2.000],  loss: 5134453.000000, mae: 4320.236328, mean_q: -2550.364258
 3968/5000: episode: 3968, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5325.203, mean reward: -5325.203 [-5325.203, -5325.203], mean action: 2.000 [2.000, 2.000],  loss: 3242288.250000, mae: 4131.663086, mean_q: -2536.915771
 3969/5000: episode: 3969, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1931.967, mean reward: -1931.967 [-1931.967, -1931.967], mean action: 2.000 [2.000, 2.000],  loss: 2531385.500000, mae: 4153.367188, mean_q: -2557.271484
 3970/5000: episode: 3970, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -108.239, mean reward: -108.239 [-108.239, -108.239], mean action: 2.000 [2.000, 2.000],  loss: 1754060.750000, mae: 4087.811768, mean_q: -2555.341309
 3971/5000: episode: 3971, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -256.699, mean reward: -256.699 [-256.699, -256.699], mean action: 2.000 [2.000, 2.000],  loss: 2196257.000000, mae: 4207.827148, mean_q: -2572.555664
 3972/5000: episode: 3972, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1062.630, mean reward: -1062.630 [-1062.630, -1062.630], mean action: 2.000 [2.000, 2.000],  loss: 3541006.500000, mae: 4191.141113, mean_q: -2572.936768
 3973/5000: episode: 3973, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5330.274, mean reward: -5330.274 [-5330.274, -5330.274], mean action: 2.000 [2.000, 2.000],  loss: 2684941.500000, mae: 4242.050781, mean_q: -2581.539062
 3974/5000: episode: 3974, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3800.535, mean reward: -3800.535 [-3800.535, -3800.535], mean action: 2.000 [2.000, 2.000],  loss: 3444548.500000, mae: 4221.387207, mean_q: -2602.595703
 3975/5000: episode: 3975, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -729.877, mean reward: -729.877 [-729.877, -729.877], mean action: 2.000 [2.000, 2.000],  loss: 2767055.000000, mae: 4222.816406, mean_q: -2613.685059
 3976/5000: episode: 3976, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -729.246, mean reward: -729.246 [-729.246, -729.246], mean action: 2.000 [2.000, 2.000],  loss: 3607483.500000, mae: 4196.216797, mean_q: -2626.167969
 3977/5000: episode: 3977, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7515.395, mean reward: -7515.395 [-7515.395, -7515.395], mean action: 2.000 [2.000, 2.000],  loss: 2746007.250000, mae: 4272.337402, mean_q: -2643.107666
 3978/5000: episode: 3978, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -537.176, mean reward: -537.176 [-537.176, -537.176], mean action: 2.000 [2.000, 2.000],  loss: 3023222.500000, mae: 4182.435059, mean_q: -2635.797607
 3979/5000: episode: 3979, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1498.310, mean reward: -1498.310 [-1498.310, -1498.310], mean action: 2.000 [2.000, 2.000],  loss: 1702908.000000, mae: 4134.277344, mean_q: -2639.146973
 3980/5000: episode: 3980, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3766.748, mean reward: -3766.748 [-3766.748, -3766.748], mean action: 2.000 [2.000, 2.000],  loss: 2517077.250000, mae: 4236.790527, mean_q: -2648.021484
 3981/5000: episode: 3981, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -630.023, mean reward: -630.023 [-630.023, -630.023], mean action: 2.000 [2.000, 2.000],  loss: 2059155.000000, mae: 4156.216797, mean_q: -2651.676758
 3982/5000: episode: 3982, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1781.813, mean reward: -1781.813 [-1781.813, -1781.813], mean action: 2.000 [2.000, 2.000],  loss: 2416620.500000, mae: 4237.299805, mean_q: -2639.606689
 3983/5000: episode: 3983, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9753.021, mean reward: -9753.021 [-9753.021, -9753.021], mean action: 2.000 [2.000, 2.000],  loss: 4788130.500000, mae: 4349.534180, mean_q: -2641.902832
 3984/5000: episode: 3984, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -519.998, mean reward: -519.998 [-519.998, -519.998], mean action: 2.000 [2.000, 2.000],  loss: 2553632.500000, mae: 4042.089844, mean_q: -2643.096436
 3985/5000: episode: 3985, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1762.040, mean reward: -1762.040 [-1762.040, -1762.040], mean action: 2.000 [2.000, 2.000],  loss: 4033709.750000, mae: 4273.216797, mean_q: -2659.632324
 3986/5000: episode: 3986, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3985.645, mean reward: -3985.645 [-3985.645, -3985.645], mean action: 3.000 [3.000, 3.000],  loss: 5434118.000000, mae: 4344.298828, mean_q: -2644.887695
 3987/5000: episode: 3987, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1653.715, mean reward: -1653.715 [-1653.715, -1653.715], mean action: 2.000 [2.000, 2.000],  loss: 2807902.250000, mae: 4320.019531, mean_q: -2668.272705
 3988/5000: episode: 3988, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3468.596, mean reward: -3468.596 [-3468.596, -3468.596], mean action: 2.000 [2.000, 2.000],  loss: 1732483.250000, mae: 4166.887695, mean_q: -2660.778564
 3989/5000: episode: 3989, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3326.356, mean reward: -3326.356 [-3326.356, -3326.356], mean action: 2.000 [2.000, 2.000],  loss: 3832526.000000, mae: 4410.504395, mean_q: -2675.565430
 3990/5000: episode: 3990, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4623.073, mean reward: -4623.073 [-4623.073, -4623.073], mean action: 2.000 [2.000, 2.000],  loss: 2034896.875000, mae: 4179.204102, mean_q: -2678.233398
 3991/5000: episode: 3991, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5299.547, mean reward: -5299.547 [-5299.547, -5299.547], mean action: 2.000 [2.000, 2.000],  loss: 2247254.500000, mae: 4194.004395, mean_q: -2697.907227
 3992/5000: episode: 3992, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5829.632, mean reward: -5829.632 [-5829.632, -5829.632], mean action: 2.000 [2.000, 2.000],  loss: 4302543.000000, mae: 4318.967773, mean_q: -2678.478760
 3993/5000: episode: 3993, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1613.086, mean reward: -1613.086 [-1613.086, -1613.086], mean action: 2.000 [2.000, 2.000],  loss: 4083291.000000, mae: 4326.459961, mean_q: -2671.371826
 3994/5000: episode: 3994, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -709.043, mean reward: -709.043 [-709.043, -709.043], mean action: 2.000 [2.000, 2.000],  loss: 3392412.250000, mae: 4286.816406, mean_q: -2674.346680
 3995/5000: episode: 3995, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4611.090, mean reward: -4611.090 [-4611.090, -4611.090], mean action: 2.000 [2.000, 2.000],  loss: 2208894.500000, mae: 4228.428711, mean_q: -2687.538086
 3996/5000: episode: 3996, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3035.923, mean reward: -3035.923 [-3035.923, -3035.923], mean action: 2.000 [2.000, 2.000],  loss: 2641526.500000, mae: 4292.131836, mean_q: -2694.803467
 3997/5000: episode: 3997, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1157.814, mean reward: -1157.814 [-1157.814, -1157.814], mean action: 2.000 [2.000, 2.000],  loss: 3299522.500000, mae: 4321.929688, mean_q: -2677.608887
 3998/5000: episode: 3998, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1610.482, mean reward: -1610.482 [-1610.482, -1610.482], mean action: 2.000 [2.000, 2.000],  loss: 2255001.500000, mae: 4174.641602, mean_q: -2698.012939
 3999/5000: episode: 3999, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8989.647, mean reward: -8989.647 [-8989.647, -8989.647], mean action: 2.000 [2.000, 2.000],  loss: 2309965.000000, mae: 4251.499512, mean_q: -2701.606689
 4000/5000: episode: 4000, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -109.476, mean reward: -109.476 [-109.476, -109.476], mean action: 2.000 [2.000, 2.000],  loss: 3016896.000000, mae: 4278.319824, mean_q: -2696.751953
 4001/5000: episode: 4001, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1699.897, mean reward: -1699.897 [-1699.897, -1699.897], mean action: 2.000 [2.000, 2.000],  loss: 3065215.500000, mae: 4356.201660, mean_q: -2704.735840
 4002/5000: episode: 4002, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5232.790, mean reward: -5232.790 [-5232.790, -5232.790], mean action: 1.000 [1.000, 1.000],  loss: 3062706.500000, mae: 4319.642578, mean_q: -2697.235352
 4003/5000: episode: 4003, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1091.997, mean reward: -1091.997 [-1091.997, -1091.997], mean action: 2.000 [2.000, 2.000],  loss: 1986856.250000, mae: 4125.297852, mean_q: -2692.386719
 4004/5000: episode: 4004, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1324.441, mean reward: -1324.441 [-1324.441, -1324.441], mean action: 2.000 [2.000, 2.000],  loss: 5559415.500000, mae: 4464.459961, mean_q: -2708.013184
 4005/5000: episode: 4005, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2492.401, mean reward: -2492.401 [-2492.401, -2492.401], mean action: 2.000 [2.000, 2.000],  loss: 2121510.000000, mae: 4177.207520, mean_q: -2701.171143
 4006/5000: episode: 4006, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8416.898, mean reward: -8416.898 [-8416.898, -8416.898], mean action: 0.000 [0.000, 0.000],  loss: 3593017.500000, mae: 4321.894043, mean_q: -2724.442139
 4007/5000: episode: 4007, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -264.920, mean reward: -264.920 [-264.920, -264.920], mean action: 2.000 [2.000, 2.000],  loss: 2579297.250000, mae: 4289.008789, mean_q: -2731.196533
 4008/5000: episode: 4008, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3087.070, mean reward: -3087.070 [-3087.070, -3087.070], mean action: 2.000 [2.000, 2.000],  loss: 3547713.000000, mae: 4288.832031, mean_q: -2722.687500
 4009/5000: episode: 4009, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4381.642, mean reward: -4381.642 [-4381.642, -4381.642], mean action: 3.000 [3.000, 3.000],  loss: 2111245.000000, mae: 4237.592773, mean_q: -2722.934082
 4010/5000: episode: 4010, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1260.738, mean reward: -1260.738 [-1260.738, -1260.738], mean action: 2.000 [2.000, 2.000],  loss: 3532747.250000, mae: 4352.785156, mean_q: -2734.525635
 4011/5000: episode: 4011, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3538.959, mean reward: -3538.959 [-3538.959, -3538.959], mean action: 2.000 [2.000, 2.000],  loss: 3400415.500000, mae: 4346.649414, mean_q: -2752.072021
 4012/5000: episode: 4012, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -179.238, mean reward: -179.238 [-179.238, -179.238], mean action: 2.000 [2.000, 2.000],  loss: 3065307.000000, mae: 4357.166992, mean_q: -2736.682617
 4013/5000: episode: 4013, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6662.370, mean reward: -6662.370 [-6662.370, -6662.370], mean action: 0.000 [0.000, 0.000],  loss: 2396267.500000, mae: 4238.270508, mean_q: -2746.582031
 4014/5000: episode: 4014, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -168.346, mean reward: -168.346 [-168.346, -168.346], mean action: 2.000 [2.000, 2.000],  loss: 2011765.750000, mae: 4299.787109, mean_q: -2751.015869
 4015/5000: episode: 4015, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7.674, mean reward: -7.674 [-7.674, -7.674], mean action: 2.000 [2.000, 2.000],  loss: 3440856.500000, mae: 4366.312500, mean_q: -2744.457031
 4016/5000: episode: 4016, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2173.184, mean reward: -2173.184 [-2173.184, -2173.184], mean action: 2.000 [2.000, 2.000],  loss: 1455731.375000, mae: 4154.466797, mean_q: -2740.876221
 4017/5000: episode: 4017, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1091.092, mean reward: -1091.092 [-1091.092, -1091.092], mean action: 2.000 [2.000, 2.000],  loss: 2881025.000000, mae: 4305.962891, mean_q: -2732.278809
 4018/5000: episode: 4018, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4353.330, mean reward: -4353.330 [-4353.330, -4353.330], mean action: 2.000 [2.000, 2.000],  loss: 3381866.000000, mae: 4362.251953, mean_q: -2725.212402
 4019/5000: episode: 4019, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -373.342, mean reward: -373.342 [-373.342, -373.342], mean action: 2.000 [2.000, 2.000],  loss: 3408951.000000, mae: 4274.086914, mean_q: -2719.883057
 4020/5000: episode: 4020, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7233.968, mean reward: -7233.968 [-7233.968, -7233.968], mean action: 2.000 [2.000, 2.000],  loss: 4851402.000000, mae: 4401.723633, mean_q: -2711.195801
 4021/5000: episode: 4021, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3499.270, mean reward: -3499.270 [-3499.270, -3499.270], mean action: 2.000 [2.000, 2.000],  loss: 3104548.000000, mae: 4354.232422, mean_q: -2731.616211
 4022/5000: episode: 4022, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2661.465, mean reward: -2661.465 [-2661.465, -2661.465], mean action: 2.000 [2.000, 2.000],  loss: 2935791.250000, mae: 4337.308594, mean_q: -2733.882568
 4023/5000: episode: 4023, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -8659.497, mean reward: -8659.497 [-8659.497, -8659.497], mean action: 2.000 [2.000, 2.000],  loss: 3765127.000000, mae: 4389.823242, mean_q: -2728.374512
 4024/5000: episode: 4024, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -10048.569, mean reward: -10048.569 [-10048.569, -10048.569], mean action: 2.000 [2.000, 2.000],  loss: 3874274.000000, mae: 4362.552734, mean_q: -2732.058838
 4025/5000: episode: 4025, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -860.747, mean reward: -860.747 [-860.747, -860.747], mean action: 2.000 [2.000, 2.000],  loss: 2449994.500000, mae: 4207.059570, mean_q: -2723.302734
 4026/5000: episode: 4026, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3091.052, mean reward: -3091.052 [-3091.052, -3091.052], mean action: 2.000 [2.000, 2.000],  loss: 2676955.750000, mae: 4229.625977, mean_q: -2711.583008
 4027/5000: episode: 4027, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3268.493, mean reward: -3268.493 [-3268.493, -3268.493], mean action: 2.000 [2.000, 2.000],  loss: 2633687.750000, mae: 4268.786621, mean_q: -2728.246338
 4028/5000: episode: 4028, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3155.426, mean reward: -3155.426 [-3155.426, -3155.426], mean action: 2.000 [2.000, 2.000],  loss: 2966318.750000, mae: 4313.779297, mean_q: -2720.286621
 4029/5000: episode: 4029, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2292.139, mean reward: -2292.139 [-2292.139, -2292.139], mean action: 2.000 [2.000, 2.000],  loss: 1850431.750000, mae: 4131.059570, mean_q: -2713.674072
 4030/5000: episode: 4030, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3971.446, mean reward: -3971.446 [-3971.446, -3971.446], mean action: 2.000 [2.000, 2.000],  loss: 1856376.750000, mae: 4121.063477, mean_q: -2708.238281
 4031/5000: episode: 4031, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6561.721, mean reward: -6561.721 [-6561.721, -6561.721], mean action: 2.000 [2.000, 2.000],  loss: 3155164.250000, mae: 4282.035156, mean_q: -2704.781250
 4032/5000: episode: 4032, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2616.428, mean reward: -2616.428 [-2616.428, -2616.428], mean action: 2.000 [2.000, 2.000],  loss: 2530426.750000, mae: 4244.763672, mean_q: -2674.449463
 4033/5000: episode: 4033, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1364.982, mean reward: -1364.982 [-1364.982, -1364.982], mean action: 2.000 [2.000, 2.000],  loss: 3878826.250000, mae: 4270.995117, mean_q: -2676.174316
 4034/5000: episode: 4034, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2816.309, mean reward: -2816.309 [-2816.309, -2816.309], mean action: 2.000 [2.000, 2.000],  loss: 2697570.000000, mae: 4249.563477, mean_q: -2650.123047
 4035/5000: episode: 4035, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1159.363, mean reward: -1159.363 [-1159.363, -1159.363], mean action: 2.000 [2.000, 2.000],  loss: 3447729.500000, mae: 4233.172852, mean_q: -2648.077148
 4036/5000: episode: 4036, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -2481.989, mean reward: -2481.989 [-2481.989, -2481.989], mean action: 0.000 [0.000, 0.000],  loss: 2428897.500000, mae: 4138.793457, mean_q: -2651.515869
 4037/5000: episode: 4037, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1691.890, mean reward: -1691.890 [-1691.890, -1691.890], mean action: 2.000 [2.000, 2.000],  loss: 1931362.625000, mae: 4125.058105, mean_q: -2634.694336
 4038/5000: episode: 4038, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2236.339, mean reward: -2236.339 [-2236.339, -2236.339], mean action: 2.000 [2.000, 2.000],  loss: 1793215.000000, mae: 3985.872070, mean_q: -2609.983398
 4039/5000: episode: 4039, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2459.062, mean reward: -2459.062 [-2459.062, -2459.062], mean action: 2.000 [2.000, 2.000],  loss: 2425620.000000, mae: 4164.406250, mean_q: -2616.915771
 4040/5000: episode: 4040, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -916.517, mean reward: -916.517 [-916.517, -916.517], mean action: 2.000 [2.000, 2.000],  loss: 5009835.000000, mae: 4363.041992, mean_q: -2595.660400
 4041/5000: episode: 4041, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -413.678, mean reward: -413.678 [-413.678, -413.678], mean action: 2.000 [2.000, 2.000],  loss: 3966980.500000, mae: 4221.086914, mean_q: -2608.863770
 4042/5000: episode: 4042, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6271.765, mean reward: -6271.765 [-6271.765, -6271.765], mean action: 2.000 [2.000, 2.000],  loss: 4011496.000000, mae: 4247.603027, mean_q: -2583.069824
 4043/5000: episode: 4043, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2312.076, mean reward: -2312.076 [-2312.076, -2312.076], mean action: 2.000 [2.000, 2.000],  loss: 3075232.000000, mae: 4178.525879, mean_q: -2614.270996
 4044/5000: episode: 4044, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1262.232, mean reward: -1262.232 [-1262.232, -1262.232], mean action: 2.000 [2.000, 2.000],  loss: 2331001.000000, mae: 4117.998535, mean_q: -2592.324707
 4045/5000: episode: 4045, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -261.150, mean reward: -261.150 [-261.150, -261.150], mean action: 2.000 [2.000, 2.000],  loss: 2788128.750000, mae: 4120.628906, mean_q: -2585.970215
 4046/5000: episode: 4046, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3381.796, mean reward: -3381.796 [-3381.796, -3381.796], mean action: 3.000 [3.000, 3.000],  loss: 4949204.000000, mae: 4202.632812, mean_q: -2617.381104
 4047/5000: episode: 4047, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1381.673, mean reward: -1381.673 [-1381.673, -1381.673], mean action: 2.000 [2.000, 2.000],  loss: 3384991.750000, mae: 4154.932617, mean_q: -2626.714355
 4048/5000: episode: 4048, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -3221.781, mean reward: -3221.781 [-3221.781, -3221.781], mean action: 2.000 [2.000, 2.000],  loss: 1833665.125000, mae: 4014.657959, mean_q: -2631.603271
 4049/5000: episode: 4049, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1933.448, mean reward: -1933.448 [-1933.448, -1933.448], mean action: 2.000 [2.000, 2.000],  loss: 3652856.000000, mae: 4207.375000, mean_q: -2668.935791
 4050/5000: episode: 4050, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3125.448, mean reward: -3125.448 [-3125.448, -3125.448], mean action: 3.000 [3.000, 3.000],  loss: 2512094.500000, mae: 4139.609375, mean_q: -2662.099609
 4051/5000: episode: 4051, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3030.385, mean reward: -3030.385 [-3030.385, -3030.385], mean action: 2.000 [2.000, 2.000],  loss: 2751208.500000, mae: 4140.166016, mean_q: -2691.002441
 4052/5000: episode: 4052, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5543.675, mean reward: -5543.675 [-5543.675, -5543.675], mean action: 0.000 [0.000, 0.000],  loss: 3995704.000000, mae: 4224.334961, mean_q: -2712.553223
 4053/5000: episode: 4053, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6484.306, mean reward: -6484.306 [-6484.306, -6484.306], mean action: 2.000 [2.000, 2.000],  loss: 2741349.500000, mae: 4101.459473, mean_q: -2704.497314
 4054/5000: episode: 4054, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1000.305, mean reward: -1000.305 [-1000.305, -1000.305], mean action: 2.000 [2.000, 2.000],  loss: 2435237.000000, mae: 4158.914551, mean_q: -2724.744629
 4055/5000: episode: 4055, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2433.103, mean reward: -2433.103 [-2433.103, -2433.103], mean action: 2.000 [2.000, 2.000],  loss: 3590727.000000, mae: 4270.756836, mean_q: -2743.473389
 4056/5000: episode: 4056, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -496.749, mean reward: -496.749 [-496.749, -496.749], mean action: 2.000 [2.000, 2.000],  loss: 2717379.000000, mae: 4127.875977, mean_q: -2730.223145
 4057/5000: episode: 4057, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -348.605, mean reward: -348.605 [-348.605, -348.605], mean action: 2.000 [2.000, 2.000],  loss: 2358358.750000, mae: 4093.569824, mean_q: -2724.950195
 4058/5000: episode: 4058, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2568.533, mean reward: -2568.533 [-2568.533, -2568.533], mean action: 2.000 [2.000, 2.000],  loss: 2323795.750000, mae: 4146.907715, mean_q: -2728.625244
 4059/5000: episode: 4059, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1285.091, mean reward: -1285.091 [-1285.091, -1285.091], mean action: 2.000 [2.000, 2.000],  loss: 4842999.000000, mae: 4176.278320, mean_q: -2722.527832
 4060/5000: episode: 4060, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2709.050, mean reward: -2709.050 [-2709.050, -2709.050], mean action: 2.000 [2.000, 2.000],  loss: 3059482.750000, mae: 4146.101562, mean_q: -2725.239258
 4061/5000: episode: 4061, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4021.750, mean reward: -4021.750 [-4021.750, -4021.750], mean action: 2.000 [2.000, 2.000],  loss: 2705004.250000, mae: 4148.013184, mean_q: -2701.199219
 4062/5000: episode: 4062, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2818.911, mean reward: -2818.911 [-2818.911, -2818.911], mean action: 2.000 [2.000, 2.000],  loss: 2695357.000000, mae: 4178.369629, mean_q: -2699.849365
 4063/5000: episode: 4063, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -333.792, mean reward: -333.792 [-333.792, -333.792], mean action: 2.000 [2.000, 2.000],  loss: 2744925.500000, mae: 4156.485352, mean_q: -2689.305664
 4064/5000: episode: 4064, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4505.294, mean reward: -4505.294 [-4505.294, -4505.294], mean action: 2.000 [2.000, 2.000],  loss: 3325721.750000, mae: 4173.626465, mean_q: -2669.641602
 4065/5000: episode: 4065, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -795.603, mean reward: -795.603 [-795.603, -795.603], mean action: 2.000 [2.000, 2.000],  loss: 4081702.250000, mae: 4184.312012, mean_q: -2678.341797
 4066/5000: episode: 4066, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -646.994, mean reward: -646.994 [-646.994, -646.994], mean action: 2.000 [2.000, 2.000],  loss: 2310006.500000, mae: 4131.796875, mean_q: -2640.739990
 4067/5000: episode: 4067, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -178.332, mean reward: -178.332 [-178.332, -178.332], mean action: 2.000 [2.000, 2.000],  loss: 2534339.500000, mae: 4101.363281, mean_q: -2630.804199
 4068/5000: episode: 4068, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -84.246, mean reward: -84.246 [-84.246, -84.246], mean action: 2.000 [2.000, 2.000],  loss: 3868782.750000, mae: 4150.812500, mean_q: -2648.530762
 4069/5000: episode: 4069, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2082.375, mean reward: -2082.375 [-2082.375, -2082.375], mean action: 2.000 [2.000, 2.000],  loss: 2003469.000000, mae: 4077.966797, mean_q: -2649.368164
 4070/5000: episode: 4070, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4730.637, mean reward: -4730.637 [-4730.637, -4730.637], mean action: 1.000 [1.000, 1.000],  loss: 2819015.000000, mae: 4115.527832, mean_q: -2646.938721
 4071/5000: episode: 4071, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -128.699, mean reward: -128.699 [-128.699, -128.699], mean action: 2.000 [2.000, 2.000],  loss: 2881319.000000, mae: 4184.529297, mean_q: -2631.165771
 4072/5000: episode: 4072, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -166.795, mean reward: -166.795 [-166.795, -166.795], mean action: 2.000 [2.000, 2.000],  loss: 3694989.000000, mae: 4128.919922, mean_q: -2640.130371
 4073/5000: episode: 4073, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4539.819, mean reward: -4539.819 [-4539.819, -4539.819], mean action: 2.000 [2.000, 2.000],  loss: 2819431.000000, mae: 4066.789795, mean_q: -2606.306641
 4074/5000: episode: 4074, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4942.563, mean reward: -4942.563 [-4942.563, -4942.563], mean action: 2.000 [2.000, 2.000],  loss: 2578856.750000, mae: 4098.045410, mean_q: -2601.682129
 4075/5000: episode: 4075, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -494.821, mean reward: -494.821 [-494.821, -494.821], mean action: 2.000 [2.000, 2.000],  loss: 3134140.000000, mae: 4103.374023, mean_q: -2607.570312
 4076/5000: episode: 4076, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -983.744, mean reward: -983.744 [-983.744, -983.744], mean action: 2.000 [2.000, 2.000],  loss: 2980827.500000, mae: 4134.189941, mean_q: -2596.130371
 4077/5000: episode: 4077, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2702.269, mean reward: -2702.269 [-2702.269, -2702.269], mean action: 2.000 [2.000, 2.000],  loss: 2975375.750000, mae: 4029.943359, mean_q: -2573.631836
 4078/5000: episode: 4078, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8320.704, mean reward: -8320.704 [-8320.704, -8320.704], mean action: 2.000 [2.000, 2.000],  loss: 2606572.000000, mae: 4035.667480, mean_q: -2573.488281
 4079/5000: episode: 4079, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6099.507, mean reward: -6099.507 [-6099.507, -6099.507], mean action: 2.000 [2.000, 2.000],  loss: 3436511.500000, mae: 4060.835449, mean_q: -2572.702148
 4080/5000: episode: 4080, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -302.242, mean reward: -302.242 [-302.242, -302.242], mean action: 2.000 [2.000, 2.000],  loss: 1755336.250000, mae: 4066.606445, mean_q: -2576.242676
 4081/5000: episode: 4081, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6863.737, mean reward: -6863.737 [-6863.737, -6863.737], mean action: 2.000 [2.000, 2.000],  loss: 2234885.500000, mae: 4106.925781, mean_q: -2561.533691
 4082/5000: episode: 4082, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1349.775, mean reward: -1349.775 [-1349.775, -1349.775], mean action: 2.000 [2.000, 2.000],  loss: 2528905.250000, mae: 4073.882568, mean_q: -2574.041504
 4083/5000: episode: 4083, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -710.572, mean reward: -710.572 [-710.572, -710.572], mean action: 2.000 [2.000, 2.000],  loss: 1743997.750000, mae: 3934.302734, mean_q: -2570.642090
 4084/5000: episode: 4084, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3611.138, mean reward: -3611.138 [-3611.138, -3611.138], mean action: 2.000 [2.000, 2.000],  loss: 2286004.250000, mae: 4016.765137, mean_q: -2549.947754
 4085/5000: episode: 4085, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7351.222, mean reward: -7351.222 [-7351.222, -7351.222], mean action: 2.000 [2.000, 2.000],  loss: 3810801.000000, mae: 4050.673828, mean_q: -2542.713867
 4086/5000: episode: 4086, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1024.919, mean reward: -1024.919 [-1024.919, -1024.919], mean action: 2.000 [2.000, 2.000],  loss: 3248665.250000, mae: 4091.298828, mean_q: -2570.268066
 4087/5000: episode: 4087, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1132.278, mean reward: -1132.278 [-1132.278, -1132.278], mean action: 2.000 [2.000, 2.000],  loss: 3682109.750000, mae: 4214.883789, mean_q: -2571.160400
 4088/5000: episode: 4088, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3508.224, mean reward: -3508.224 [-3508.224, -3508.224], mean action: 2.000 [2.000, 2.000],  loss: 3194271.750000, mae: 4017.704834, mean_q: -2556.810303
 4089/5000: episode: 4089, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1930.394, mean reward: -1930.394 [-1930.394, -1930.394], mean action: 2.000 [2.000, 2.000],  loss: 3237594.000000, mae: 4151.013672, mean_q: -2566.504395
 4090/5000: episode: 4090, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4593.380, mean reward: -4593.380 [-4593.380, -4593.380], mean action: 2.000 [2.000, 2.000],  loss: 3142049.750000, mae: 4111.050293, mean_q: -2562.427979
 4091/5000: episode: 4091, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2627.851, mean reward: -2627.851 [-2627.851, -2627.851], mean action: 2.000 [2.000, 2.000],  loss: 2368687.000000, mae: 4063.724609, mean_q: -2576.474609
 4092/5000: episode: 4092, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1573.380, mean reward: -1573.380 [-1573.380, -1573.380], mean action: 2.000 [2.000, 2.000],  loss: 2722534.000000, mae: 3990.195068, mean_q: -2577.806396
 4093/5000: episode: 4093, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -960.600, mean reward: -960.600 [-960.600, -960.600], mean action: 2.000 [2.000, 2.000],  loss: 3786486.250000, mae: 4171.789062, mean_q: -2587.518311
 4094/5000: episode: 4094, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -298.183, mean reward: -298.183 [-298.183, -298.183], mean action: 2.000 [2.000, 2.000],  loss: 2462365.750000, mae: 4058.583984, mean_q: -2585.139160
 4095/5000: episode: 4095, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -76.989, mean reward: -76.989 [-76.989, -76.989], mean action: 2.000 [2.000, 2.000],  loss: 2576786.000000, mae: 4075.993652, mean_q: -2591.756348
 4096/5000: episode: 4096, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4736.562, mean reward: -4736.562 [-4736.562, -4736.562], mean action: 2.000 [2.000, 2.000],  loss: 3784359.000000, mae: 4160.212891, mean_q: -2597.212891
 4097/5000: episode: 4097, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -8586.558, mean reward: -8586.558 [-8586.558, -8586.558], mean action: 2.000 [2.000, 2.000],  loss: 2846546.250000, mae: 4048.426270, mean_q: -2594.534912
 4098/5000: episode: 4098, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -346.358, mean reward: -346.358 [-346.358, -346.358], mean action: 2.000 [2.000, 2.000],  loss: 2858213.750000, mae: 4091.492432, mean_q: -2611.207520
 4099/5000: episode: 4099, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -681.942, mean reward: -681.942 [-681.942, -681.942], mean action: 2.000 [2.000, 2.000],  loss: 2675227.750000, mae: 4074.460938, mean_q: -2624.730957
 4100/5000: episode: 4100, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -483.192, mean reward: -483.192 [-483.192, -483.192], mean action: 2.000 [2.000, 2.000],  loss: 1387370.000000, mae: 3844.119873, mean_q: -2600.772705
 4101/5000: episode: 4101, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2653.431, mean reward: -2653.431 [-2653.431, -2653.431], mean action: 2.000 [2.000, 2.000],  loss: 1787962.500000, mae: 4025.355469, mean_q: -2618.630859
 4102/5000: episode: 4102, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -1064.579, mean reward: -1064.579 [-1064.579, -1064.579], mean action: 2.000 [2.000, 2.000],  loss: 2555309.500000, mae: 3974.460938, mean_q: -2639.235840
 4103/5000: episode: 4103, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -468.437, mean reward: -468.437 [-468.437, -468.437], mean action: 2.000 [2.000, 2.000],  loss: 2678795.500000, mae: 4053.339355, mean_q: -2634.460938
 4104/5000: episode: 4104, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8436.306, mean reward: -8436.306 [-8436.306, -8436.306], mean action: 2.000 [2.000, 2.000],  loss: 2664710.500000, mae: 4137.974121, mean_q: -2643.303467
 4105/5000: episode: 4105, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1594.136, mean reward: -1594.136 [-1594.136, -1594.136], mean action: 2.000 [2.000, 2.000],  loss: 2160043.000000, mae: 4114.977051, mean_q: -2639.029297
 4106/5000: episode: 4106, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1778.636, mean reward: -1778.636 [-1778.636, -1778.636], mean action: 2.000 [2.000, 2.000],  loss: 3314836.500000, mae: 4149.942383, mean_q: -2631.154541
 4107/5000: episode: 4107, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4341.958, mean reward: -4341.958 [-4341.958, -4341.958], mean action: 2.000 [2.000, 2.000],  loss: 2520315.000000, mae: 4147.190918, mean_q: -2621.174561
 4108/5000: episode: 4108, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1425.835, mean reward: -1425.835 [-1425.835, -1425.835], mean action: 2.000 [2.000, 2.000],  loss: 3467804.750000, mae: 4107.399902, mean_q: -2640.747070
 4109/5000: episode: 4109, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4236.391, mean reward: -4236.391 [-4236.391, -4236.391], mean action: 2.000 [2.000, 2.000],  loss: 2162452.750000, mae: 4044.983398, mean_q: -2613.181641
 4110/5000: episode: 4110, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4777.276, mean reward: -4777.276 [-4777.276, -4777.276], mean action: 3.000 [3.000, 3.000],  loss: 2022911.625000, mae: 4092.642578, mean_q: -2619.145508
 4111/5000: episode: 4111, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2289.039, mean reward: -2289.039 [-2289.039, -2289.039], mean action: 2.000 [2.000, 2.000],  loss: 2016981.750000, mae: 4116.934570, mean_q: -2632.960938
 4112/5000: episode: 4112, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2336.838, mean reward: -2336.838 [-2336.838, -2336.838], mean action: 2.000 [2.000, 2.000],  loss: 2264821.750000, mae: 4072.689941, mean_q: -2615.710938
 4113/5000: episode: 4113, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -558.131, mean reward: -558.131 [-558.131, -558.131], mean action: 2.000 [2.000, 2.000],  loss: 2377299.500000, mae: 4167.024902, mean_q: -2607.439941
 4114/5000: episode: 4114, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -104.975, mean reward: -104.975 [-104.975, -104.975], mean action: 2.000 [2.000, 2.000],  loss: 1688081.000000, mae: 4056.849609, mean_q: -2595.143066
 4115/5000: episode: 4115, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1317.148, mean reward: -1317.148 [-1317.148, -1317.148], mean action: 2.000 [2.000, 2.000],  loss: 2399450.000000, mae: 4127.857910, mean_q: -2571.727539
 4116/5000: episode: 4116, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -1119.916, mean reward: -1119.916 [-1119.916, -1119.916], mean action: 2.000 [2.000, 2.000],  loss: 2508698.250000, mae: 3981.319824, mean_q: -2566.249512
 4117/5000: episode: 4117, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2724.849, mean reward: -2724.849 [-2724.849, -2724.849], mean action: 2.000 [2.000, 2.000],  loss: 1957207.250000, mae: 4026.182861, mean_q: -2566.500977
 4118/5000: episode: 4118, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1649.860, mean reward: -1649.860 [-1649.860, -1649.860], mean action: 2.000 [2.000, 2.000],  loss: 2112067.250000, mae: 4108.674316, mean_q: -2535.897949
 4119/5000: episode: 4119, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2414.105, mean reward: -2414.105 [-2414.105, -2414.105], mean action: 2.000 [2.000, 2.000],  loss: 3369285.500000, mae: 4113.860352, mean_q: -2536.366699
 4120/5000: episode: 4120, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -11159.012, mean reward: -11159.012 [-11159.012, -11159.012], mean action: 2.000 [2.000, 2.000],  loss: 2304363.000000, mae: 4084.033447, mean_q: -2523.977295
 4121/5000: episode: 4121, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -426.669, mean reward: -426.669 [-426.669, -426.669], mean action: 2.000 [2.000, 2.000],  loss: 2087518.250000, mae: 4039.006836, mean_q: -2510.398438
 4122/5000: episode: 4122, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3461.892, mean reward: -3461.892 [-3461.892, -3461.892], mean action: 2.000 [2.000, 2.000],  loss: 2279243.500000, mae: 4034.063477, mean_q: -2502.864258
 4123/5000: episode: 4123, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3764.464, mean reward: -3764.464 [-3764.464, -3764.464], mean action: 2.000 [2.000, 2.000],  loss: 3000036.500000, mae: 4138.192871, mean_q: -2517.666016
 4124/5000: episode: 4124, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7393.046, mean reward: -7393.046 [-7393.046, -7393.046], mean action: 0.000 [0.000, 0.000],  loss: 1682802.000000, mae: 3930.463379, mean_q: -2516.651855
 4125/5000: episode: 4125, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -84.884, mean reward: -84.884 [-84.884, -84.884], mean action: 2.000 [2.000, 2.000],  loss: 2564283.500000, mae: 4065.894775, mean_q: -2517.492432
 4126/5000: episode: 4126, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1990.798, mean reward: -1990.798 [-1990.798, -1990.798], mean action: 2.000 [2.000, 2.000],  loss: 1965458.375000, mae: 3980.140137, mean_q: -2516.504395
 4127/5000: episode: 4127, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1505.392, mean reward: -1505.392 [-1505.392, -1505.392], mean action: 2.000 [2.000, 2.000],  loss: 3207248.000000, mae: 4085.674561, mean_q: -2526.739014
 4128/5000: episode: 4128, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1000.746, mean reward: -1000.746 [-1000.746, -1000.746], mean action: 2.000 [2.000, 2.000],  loss: 2530777.500000, mae: 4033.689453, mean_q: -2527.519775
 4129/5000: episode: 4129, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -940.519, mean reward: -940.519 [-940.519, -940.519], mean action: 2.000 [2.000, 2.000],  loss: 1243536.000000, mae: 4010.970703, mean_q: -2545.060059
 4130/5000: episode: 4130, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2379.731, mean reward: -2379.731 [-2379.731, -2379.731], mean action: 2.000 [2.000, 2.000],  loss: 2549473.000000, mae: 4037.021240, mean_q: -2542.229980
 4131/5000: episode: 4131, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -238.023, mean reward: -238.023 [-238.023, -238.023], mean action: 2.000 [2.000, 2.000],  loss: 2746265.750000, mae: 4119.492188, mean_q: -2547.580322
 4132/5000: episode: 4132, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1937.045, mean reward: -1937.045 [-1937.045, -1937.045], mean action: 2.000 [2.000, 2.000],  loss: 1684765.625000, mae: 4012.225830, mean_q: -2545.945312
 4133/5000: episode: 4133, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5037.957, mean reward: -5037.957 [-5037.957, -5037.957], mean action: 2.000 [2.000, 2.000],  loss: 2462184.500000, mae: 4060.524902, mean_q: -2539.889404
 4134/5000: episode: 4134, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1094.198, mean reward: -1094.198 [-1094.198, -1094.198], mean action: 2.000 [2.000, 2.000],  loss: 3038892.000000, mae: 4053.053955, mean_q: -2567.491699
 4135/5000: episode: 4135, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3140.437, mean reward: -3140.437 [-3140.437, -3140.437], mean action: 2.000 [2.000, 2.000],  loss: 4092412.750000, mae: 4185.923828, mean_q: -2558.713623
 4136/5000: episode: 4136, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1376.419, mean reward: -1376.419 [-1376.419, -1376.419], mean action: 2.000 [2.000, 2.000],  loss: 2111543.250000, mae: 4070.485352, mean_q: -2575.707031
 4137/5000: episode: 4137, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1186.200, mean reward: -1186.200 [-1186.200, -1186.200], mean action: 2.000 [2.000, 2.000],  loss: 2120265.750000, mae: 4109.858398, mean_q: -2595.835449
 4138/5000: episode: 4138, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1777.295, mean reward: -1777.295 [-1777.295, -1777.295], mean action: 2.000 [2.000, 2.000],  loss: 2442431.750000, mae: 4033.010742, mean_q: -2606.114014
 4139/5000: episode: 4139, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -348.321, mean reward: -348.321 [-348.321, -348.321], mean action: 2.000 [2.000, 2.000],  loss: 4052214.000000, mae: 4187.300293, mean_q: -2621.096924
 4140/5000: episode: 4140, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1904.216, mean reward: -1904.216 [-1904.216, -1904.216], mean action: 2.000 [2.000, 2.000],  loss: 3422283.000000, mae: 4222.749023, mean_q: -2598.727539
 4141/5000: episode: 4141, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1256.518, mean reward: -1256.518 [-1256.518, -1256.518], mean action: 2.000 [2.000, 2.000],  loss: 3388457.500000, mae: 4191.578125, mean_q: -2616.223145
 4142/5000: episode: 4142, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2285.913, mean reward: -2285.913 [-2285.913, -2285.913], mean action: 1.000 [1.000, 1.000],  loss: 2406243.750000, mae: 3984.581055, mean_q: -2616.538574
 4143/5000: episode: 4143, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2709.699, mean reward: -2709.699 [-2709.699, -2709.699], mean action: 2.000 [2.000, 2.000],  loss: 3969558.250000, mae: 4158.728516, mean_q: -2605.837891
 4144/5000: episode: 4144, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2150.385, mean reward: -2150.385 [-2150.385, -2150.385], mean action: 2.000 [2.000, 2.000],  loss: 3363954.000000, mae: 4138.127930, mean_q: -2616.319336
 4145/5000: episode: 4145, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5439.708, mean reward: -5439.708 [-5439.708, -5439.708], mean action: 2.000 [2.000, 2.000],  loss: 4528830.000000, mae: 4226.844727, mean_q: -2610.386963
 4146/5000: episode: 4146, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -299.231, mean reward: -299.231 [-299.231, -299.231], mean action: 2.000 [2.000, 2.000],  loss: 2747780.500000, mae: 4135.461914, mean_q: -2634.163574
 4147/5000: episode: 4147, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5061.518, mean reward: -5061.518 [-5061.518, -5061.518], mean action: 2.000 [2.000, 2.000],  loss: 1684185.250000, mae: 4040.229980, mean_q: -2639.854980
 4148/5000: episode: 4148, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1683.345, mean reward: -1683.345 [-1683.345, -1683.345], mean action: 2.000 [2.000, 2.000],  loss: 2295590.250000, mae: 4122.181641, mean_q: -2648.881348
 4149/5000: episode: 4149, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -712.883, mean reward: -712.883 [-712.883, -712.883], mean action: 2.000 [2.000, 2.000],  loss: 3491427.500000, mae: 4062.417480, mean_q: -2649.180176
 4150/5000: episode: 4150, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3461.181, mean reward: -3461.181 [-3461.181, -3461.181], mean action: 2.000 [2.000, 2.000],  loss: 2547015.500000, mae: 4210.195312, mean_q: -2687.669922
 4151/5000: episode: 4151, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2182.778, mean reward: -2182.778 [-2182.778, -2182.778], mean action: 2.000 [2.000, 2.000],  loss: 2343412.500000, mae: 4110.657715, mean_q: -2673.019043
 4152/5000: episode: 4152, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1095.195, mean reward: -1095.195 [-1095.195, -1095.195], mean action: 2.000 [2.000, 2.000],  loss: 3979861.500000, mae: 4304.610352, mean_q: -2709.045898
 4153/5000: episode: 4153, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3198.790, mean reward: -3198.790 [-3198.790, -3198.790], mean action: 2.000 [2.000, 2.000],  loss: 2996719.750000, mae: 4153.985352, mean_q: -2716.760254
 4154/5000: episode: 4154, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -157.643, mean reward: -157.643 [-157.643, -157.643], mean action: 2.000 [2.000, 2.000],  loss: 1267430.625000, mae: 4036.687500, mean_q: -2699.208496
 4155/5000: episode: 4155, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2577.350, mean reward: -2577.350 [-2577.350, -2577.350], mean action: 2.000 [2.000, 2.000],  loss: 2555887.500000, mae: 4091.238037, mean_q: -2736.590088
 4156/5000: episode: 4156, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2928.162, mean reward: -2928.162 [-2928.162, -2928.162], mean action: 2.000 [2.000, 2.000],  loss: 3243023.500000, mae: 4280.932129, mean_q: -2729.189941
 4157/5000: episode: 4157, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -19.421, mean reward: -19.421 [-19.421, -19.421], mean action: 2.000 [2.000, 2.000],  loss: 1489786.250000, mae: 4122.047852, mean_q: -2737.443848
 4158/5000: episode: 4158, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3138.042, mean reward: -3138.042 [-3138.042, -3138.042], mean action: 2.000 [2.000, 2.000],  loss: 2031029.000000, mae: 4111.976074, mean_q: -2718.959473
 4159/5000: episode: 4159, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1757.092, mean reward: -1757.092 [-1757.092, -1757.092], mean action: 2.000 [2.000, 2.000],  loss: 2833496.750000, mae: 4208.318359, mean_q: -2712.425293
 4160/5000: episode: 4160, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -74.283, mean reward: -74.283 [-74.283, -74.283], mean action: 2.000 [2.000, 2.000],  loss: 3488279.000000, mae: 4174.030273, mean_q: -2723.843262
 4161/5000: episode: 4161, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -794.222, mean reward: -794.222 [-794.222, -794.222], mean action: 2.000 [2.000, 2.000],  loss: 2407969.250000, mae: 4108.406738, mean_q: -2712.091309
 4162/5000: episode: 4162, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4348.149, mean reward: -4348.149 [-4348.149, -4348.149], mean action: 2.000 [2.000, 2.000],  loss: 1913301.125000, mae: 4132.252441, mean_q: -2750.229492
 4163/5000: episode: 4163, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1128.083, mean reward: -1128.083 [-1128.083, -1128.083], mean action: 2.000 [2.000, 2.000],  loss: 2025057.625000, mae: 4099.664551, mean_q: -2727.783203
 4164/5000: episode: 4164, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -378.640, mean reward: -378.640 [-378.640, -378.640], mean action: 2.000 [2.000, 2.000],  loss: 2944261.000000, mae: 4211.778320, mean_q: -2718.605713
 4165/5000: episode: 4165, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4115.205, mean reward: -4115.205 [-4115.205, -4115.205], mean action: 2.000 [2.000, 2.000],  loss: 2370186.750000, mae: 4144.175781, mean_q: -2732.412109
 4166/5000: episode: 4166, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -1199.556, mean reward: -1199.556 [-1199.556, -1199.556], mean action: 2.000 [2.000, 2.000],  loss: 2028078.250000, mae: 4207.142578, mean_q: -2744.153320
 4167/5000: episode: 4167, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -67.992, mean reward: -67.992 [-67.992, -67.992], mean action: 2.000 [2.000, 2.000],  loss: 2544051.000000, mae: 4115.140625, mean_q: -2714.451172
 4168/5000: episode: 4168, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3880.037, mean reward: -3880.037 [-3880.037, -3880.037], mean action: 2.000 [2.000, 2.000],  loss: 3256799.000000, mae: 4084.971680, mean_q: -2709.604736
 4169/5000: episode: 4169, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5058.718, mean reward: -5058.718 [-5058.718, -5058.718], mean action: 2.000 [2.000, 2.000],  loss: 3157158.500000, mae: 4116.132812, mean_q: -2701.888184
 4170/5000: episode: 4170, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -1265.203, mean reward: -1265.203 [-1265.203, -1265.203], mean action: 2.000 [2.000, 2.000],  loss: 2161929.250000, mae: 4072.429443, mean_q: -2711.435791
 4171/5000: episode: 4171, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4473.373, mean reward: -4473.373 [-4473.373, -4473.373], mean action: 2.000 [2.000, 2.000],  loss: 2834660.000000, mae: 4082.583008, mean_q: -2691.812744
 4172/5000: episode: 4172, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -423.199, mean reward: -423.199 [-423.199, -423.199], mean action: 2.000 [2.000, 2.000],  loss: 1577365.625000, mae: 4038.541748, mean_q: -2706.943848
 4173/5000: episode: 4173, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -4570.206, mean reward: -4570.206 [-4570.206, -4570.206], mean action: 3.000 [3.000, 3.000],  loss: 2693247.250000, mae: 4111.955078, mean_q: -2703.183105
 4174/5000: episode: 4174, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -14728.515, mean reward: -14728.515 [-14728.515, -14728.515], mean action: 0.000 [0.000, 0.000],  loss: 2555884.250000, mae: 4153.389648, mean_q: -2696.093262
 4175/5000: episode: 4175, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -542.574, mean reward: -542.574 [-542.574, -542.574], mean action: 2.000 [2.000, 2.000],  loss: 1947965.250000, mae: 4042.059570, mean_q: -2662.523193
 4176/5000: episode: 4176, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5940.280, mean reward: -5940.280 [-5940.280, -5940.280], mean action: 2.000 [2.000, 2.000],  loss: 1255902.500000, mae: 3958.875488, mean_q: -2664.497070
 4177/5000: episode: 4177, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1495.661, mean reward: -1495.661 [-1495.661, -1495.661], mean action: 2.000 [2.000, 2.000],  loss: 2454696.250000, mae: 4086.626465, mean_q: -2649.950684
 4178/5000: episode: 4178, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5306.733, mean reward: -5306.733 [-5306.733, -5306.733], mean action: 2.000 [2.000, 2.000],  loss: 4815589.000000, mae: 4242.774414, mean_q: -2673.069092
 4179/5000: episode: 4179, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2237.736, mean reward: -2237.736 [-2237.736, -2237.736], mean action: 2.000 [2.000, 2.000],  loss: 2267319.750000, mae: 4066.211914, mean_q: -2658.897461
 4180/5000: episode: 4180, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6341.933, mean reward: -6341.933 [-6341.933, -6341.933], mean action: 2.000 [2.000, 2.000],  loss: 2406588.500000, mae: 3967.700684, mean_q: -2655.347168
 4181/5000: episode: 4181, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2334.332, mean reward: -2334.332 [-2334.332, -2334.332], mean action: 2.000 [2.000, 2.000],  loss: 2854222.000000, mae: 4134.011719, mean_q: -2647.204102
 4182/5000: episode: 4182, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1374.973, mean reward: -1374.973 [-1374.973, -1374.973], mean action: 2.000 [2.000, 2.000],  loss: 2300284.750000, mae: 3971.541016, mean_q: -2639.927246
 4183/5000: episode: 4183, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -329.609, mean reward: -329.609 [-329.609, -329.609], mean action: 2.000 [2.000, 2.000],  loss: 4256797.000000, mae: 4205.754883, mean_q: -2665.848877
 4184/5000: episode: 4184, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1475.332, mean reward: -1475.332 [-1475.332, -1475.332], mean action: 2.000 [2.000, 2.000],  loss: 2903939.000000, mae: 4040.350098, mean_q: -2640.798828
 4185/5000: episode: 4185, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2576.641, mean reward: -2576.641 [-2576.641, -2576.641], mean action: 2.000 [2.000, 2.000],  loss: 1310890.500000, mae: 4033.256836, mean_q: -2656.007568
 4186/5000: episode: 4186, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -89.509, mean reward: -89.509 [-89.509, -89.509], mean action: 2.000 [2.000, 2.000],  loss: 1649606.250000, mae: 3950.577637, mean_q: -2662.330811
 4187/5000: episode: 4187, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -620.935, mean reward: -620.935 [-620.935, -620.935], mean action: 3.000 [3.000, 3.000],  loss: 2747973.250000, mae: 4081.645996, mean_q: -2625.535645
 4188/5000: episode: 4188, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6139.032, mean reward: -6139.032 [-6139.032, -6139.032], mean action: 2.000 [2.000, 2.000],  loss: 1739363.000000, mae: 3992.138428, mean_q: -2629.939209
 4189/5000: episode: 4189, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -192.725, mean reward: -192.725 [-192.725, -192.725], mean action: 2.000 [2.000, 2.000],  loss: 3270508.000000, mae: 4139.028320, mean_q: -2628.180664
 4190/5000: episode: 4190, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1624.121, mean reward: -1624.121 [-1624.121, -1624.121], mean action: 2.000 [2.000, 2.000],  loss: 3267215.000000, mae: 4068.521484, mean_q: -2606.113281
 4191/5000: episode: 4191, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2441.319, mean reward: -2441.319 [-2441.319, -2441.319], mean action: 2.000 [2.000, 2.000],  loss: 2238994.750000, mae: 3972.688721, mean_q: -2610.842285
 4192/5000: episode: 4192, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2501.152, mean reward: -2501.152 [-2501.152, -2501.152], mean action: 2.000 [2.000, 2.000],  loss: 2977034.000000, mae: 4106.055664, mean_q: -2602.088379
 4193/5000: episode: 4193, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3035.253, mean reward: -3035.253 [-3035.253, -3035.253], mean action: 2.000 [2.000, 2.000],  loss: 1517922.500000, mae: 3876.686523, mean_q: -2595.495605
 4194/5000: episode: 4194, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2396.211, mean reward: -2396.211 [-2396.211, -2396.211], mean action: 2.000 [2.000, 2.000],  loss: 3885464.500000, mae: 4077.305664, mean_q: -2606.437012
 4195/5000: episode: 4195, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -712.499, mean reward: -712.499 [-712.499, -712.499], mean action: 2.000 [2.000, 2.000],  loss: 1931189.000000, mae: 3887.381836, mean_q: -2593.725342
 4196/5000: episode: 4196, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1485.217, mean reward: -1485.217 [-1485.217, -1485.217], mean action: 2.000 [2.000, 2.000],  loss: 2316870.000000, mae: 3976.705322, mean_q: -2599.883301
 4197/5000: episode: 4197, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -379.512, mean reward: -379.512 [-379.512, -379.512], mean action: 2.000 [2.000, 2.000],  loss: 3744496.500000, mae: 3969.258789, mean_q: -2601.336426
 4198/5000: episode: 4198, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -707.399, mean reward: -707.399 [-707.399, -707.399], mean action: 3.000 [3.000, 3.000],  loss: 2032374.375000, mae: 3936.211914, mean_q: -2591.842773
 4199/5000: episode: 4199, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6030.739, mean reward: -6030.739 [-6030.739, -6030.739], mean action: 2.000 [2.000, 2.000],  loss: 3161168.750000, mae: 3963.514404, mean_q: -2586.375977
 4200/5000: episode: 4200, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -14372.482, mean reward: -14372.482 [-14372.482, -14372.482], mean action: 0.000 [0.000, 0.000],  loss: 2173330.500000, mae: 4006.346680, mean_q: -2589.215332
 4201/5000: episode: 4201, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7419.181, mean reward: -7419.181 [-7419.181, -7419.181], mean action: 1.000 [1.000, 1.000],  loss: 3378188.000000, mae: 4024.575684, mean_q: -2579.322998
 4202/5000: episode: 4202, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4051.829, mean reward: -4051.829 [-4051.829, -4051.829], mean action: 2.000 [2.000, 2.000],  loss: 3931862.000000, mae: 4151.222168, mean_q: -2581.422852
 4203/5000: episode: 4203, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1687.693, mean reward: -1687.693 [-1687.693, -1687.693], mean action: 2.000 [2.000, 2.000],  loss: 2299630.750000, mae: 3980.472900, mean_q: -2571.406250
 4204/5000: episode: 4204, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4552.459, mean reward: -4552.459 [-4552.459, -4552.459], mean action: 2.000 [2.000, 2.000],  loss: 4341949.000000, mae: 4078.986816, mean_q: -2584.817383
 4205/5000: episode: 4205, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -241.511, mean reward: -241.511 [-241.511, -241.511], mean action: 2.000 [2.000, 2.000],  loss: 1435386.500000, mae: 3897.810059, mean_q: -2579.365967
 4206/5000: episode: 4206, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4486.467, mean reward: -4486.467 [-4486.467, -4486.467], mean action: 2.000 [2.000, 2.000],  loss: 1525683.375000, mae: 3949.443604, mean_q: -2606.029297
 4207/5000: episode: 4207, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -293.053, mean reward: -293.053 [-293.053, -293.053], mean action: 2.000 [2.000, 2.000],  loss: 3039113.000000, mae: 4111.571289, mean_q: -2596.844727
 4208/5000: episode: 4208, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -881.838, mean reward: -881.838 [-881.838, -881.838], mean action: 2.000 [2.000, 2.000],  loss: 1718113.250000, mae: 3905.359375, mean_q: -2590.578369
 4209/5000: episode: 4209, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1269.964, mean reward: -1269.964 [-1269.964, -1269.964], mean action: 2.000 [2.000, 2.000],  loss: 4874386.000000, mae: 4061.043945, mean_q: -2601.521484
 4210/5000: episode: 4210, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12918.800, mean reward: -12918.800 [-12918.800, -12918.800], mean action: 0.000 [0.000, 0.000],  loss: 1956372.750000, mae: 3961.895264, mean_q: -2580.008789
 4211/5000: episode: 4211, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4316.610, mean reward: -4316.610 [-4316.610, -4316.610], mean action: 2.000 [2.000, 2.000],  loss: 3253159.250000, mae: 4043.651367, mean_q: -2588.611816
 4212/5000: episode: 4212, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4831.343, mean reward: -4831.343 [-4831.343, -4831.343], mean action: 2.000 [2.000, 2.000],  loss: 2155020.000000, mae: 3968.285400, mean_q: -2573.046387
 4213/5000: episode: 4213, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4320.007, mean reward: -4320.007 [-4320.007, -4320.007], mean action: 2.000 [2.000, 2.000],  loss: 2825936.750000, mae: 3970.765869, mean_q: -2574.250244
 4214/5000: episode: 4214, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5797.969, mean reward: -5797.969 [-5797.969, -5797.969], mean action: 2.000 [2.000, 2.000],  loss: 1887103.375000, mae: 3892.829590, mean_q: -2579.114990
 4215/5000: episode: 4215, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5882.210, mean reward: -5882.210 [-5882.210, -5882.210], mean action: 2.000 [2.000, 2.000],  loss: 1999289.625000, mae: 3980.826904, mean_q: -2561.883301
 4216/5000: episode: 4216, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5078.684, mean reward: -5078.684 [-5078.684, -5078.684], mean action: 2.000 [2.000, 2.000],  loss: 3293894.000000, mae: 4050.202148, mean_q: -2561.852539
 4217/5000: episode: 4217, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5929.079, mean reward: -5929.079 [-5929.079, -5929.079], mean action: 3.000 [3.000, 3.000],  loss: 3912040.250000, mae: 4075.904785, mean_q: -2564.733398
 4218/5000: episode: 4218, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3291.479, mean reward: -3291.479 [-3291.479, -3291.479], mean action: 1.000 [1.000, 1.000],  loss: 2415505.500000, mae: 3936.475586, mean_q: -2564.398926
 4219/5000: episode: 4219, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1542.384, mean reward: -1542.384 [-1542.384, -1542.384], mean action: 2.000 [2.000, 2.000],  loss: 4128424.250000, mae: 3955.312500, mean_q: -2557.721436
 4220/5000: episode: 4220, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2335.659, mean reward: -2335.659 [-2335.659, -2335.659], mean action: 2.000 [2.000, 2.000],  loss: 3997980.250000, mae: 4046.027344, mean_q: -2558.232910
 4221/5000: episode: 4221, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1139.712, mean reward: -1139.712 [-1139.712, -1139.712], mean action: 2.000 [2.000, 2.000],  loss: 4193554.000000, mae: 4092.196777, mean_q: -2567.846924
 4222/5000: episode: 4222, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -323.649, mean reward: -323.649 [-323.649, -323.649], mean action: 2.000 [2.000, 2.000],  loss: 2334096.250000, mae: 3970.061279, mean_q: -2578.929932
 4223/5000: episode: 4223, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -2398.894, mean reward: -2398.894 [-2398.894, -2398.894], mean action: 2.000 [2.000, 2.000],  loss: 2082934.750000, mae: 3973.989258, mean_q: -2568.289795
 4224/5000: episode: 4224, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2618.506, mean reward: -2618.506 [-2618.506, -2618.506], mean action: 2.000 [2.000, 2.000],  loss: 2659814.500000, mae: 4024.649414, mean_q: -2570.408203
 4225/5000: episode: 4225, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2859.407, mean reward: -2859.407 [-2859.407, -2859.407], mean action: 2.000 [2.000, 2.000],  loss: 2982704.500000, mae: 4060.945312, mean_q: -2588.579590
 4226/5000: episode: 4226, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -988.215, mean reward: -988.215 [-988.215, -988.215], mean action: 2.000 [2.000, 2.000],  loss: 1508344.125000, mae: 3952.596680, mean_q: -2570.949707
 4227/5000: episode: 4227, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4082.763, mean reward: -4082.763 [-4082.763, -4082.763], mean action: 2.000 [2.000, 2.000],  loss: 3838584.750000, mae: 4182.410645, mean_q: -2584.491211
 4228/5000: episode: 4228, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -221.266, mean reward: -221.266 [-221.266, -221.266], mean action: 2.000 [2.000, 2.000],  loss: 1490471.500000, mae: 4027.230469, mean_q: -2573.658691
 4229/5000: episode: 4229, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1971.251, mean reward: -1971.251 [-1971.251, -1971.251], mean action: 2.000 [2.000, 2.000],  loss: 2658938.250000, mae: 4096.593750, mean_q: -2585.463867
 4230/5000: episode: 4230, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2175.810, mean reward: -2175.810 [-2175.810, -2175.810], mean action: 2.000 [2.000, 2.000],  loss: 3913974.000000, mae: 4197.916992, mean_q: -2594.936523
 4231/5000: episode: 4231, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2713.792, mean reward: -2713.792 [-2713.792, -2713.792], mean action: 2.000 [2.000, 2.000],  loss: 3156408.500000, mae: 4062.690430, mean_q: -2574.384766
 4232/5000: episode: 4232, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3097.023, mean reward: -3097.023 [-3097.023, -3097.023], mean action: 2.000 [2.000, 2.000],  loss: 2053442.625000, mae: 4104.299316, mean_q: -2590.910156
 4233/5000: episode: 4233, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1411.530, mean reward: -1411.530 [-1411.530, -1411.530], mean action: 2.000 [2.000, 2.000],  loss: 1616475.500000, mae: 3925.769531, mean_q: -2592.324463
 4234/5000: episode: 4234, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3883.290, mean reward: -3883.290 [-3883.290, -3883.290], mean action: 2.000 [2.000, 2.000],  loss: 3450665.250000, mae: 4173.347656, mean_q: -2611.798340
 4235/5000: episode: 4235, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4523.683, mean reward: -4523.683 [-4523.683, -4523.683], mean action: 2.000 [2.000, 2.000],  loss: 1955966.625000, mae: 4104.173828, mean_q: -2635.272705
 4236/5000: episode: 4236, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -859.913, mean reward: -859.913 [-859.913, -859.913], mean action: 2.000 [2.000, 2.000],  loss: 2112954.000000, mae: 4169.588379, mean_q: -2628.321777
 4237/5000: episode: 4237, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4467.406, mean reward: -4467.406 [-4467.406, -4467.406], mean action: 2.000 [2.000, 2.000],  loss: 4124600.750000, mae: 4211.550781, mean_q: -2641.216309
 4238/5000: episode: 4238, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1741.456, mean reward: -1741.456 [-1741.456, -1741.456], mean action: 2.000 [2.000, 2.000],  loss: 1579092.500000, mae: 4108.389160, mean_q: -2637.922363
 4239/5000: episode: 4239, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5029.267, mean reward: -5029.267 [-5029.267, -5029.267], mean action: 2.000 [2.000, 2.000],  loss: 3795985.000000, mae: 4191.483887, mean_q: -2655.279297
 4240/5000: episode: 4240, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3408.596, mean reward: -3408.596 [-3408.596, -3408.596], mean action: 2.000 [2.000, 2.000],  loss: 3902120.250000, mae: 4250.106934, mean_q: -2662.345947
 4241/5000: episode: 4241, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4370.077, mean reward: -4370.077 [-4370.077, -4370.077], mean action: 2.000 [2.000, 2.000],  loss: 1607947.750000, mae: 4046.043701, mean_q: -2680.564941
 4242/5000: episode: 4242, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3528.134, mean reward: -3528.134 [-3528.134, -3528.134], mean action: 2.000 [2.000, 2.000],  loss: 2696584.250000, mae: 4170.147461, mean_q: -2690.493164
 4243/5000: episode: 4243, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4098.223, mean reward: -4098.223 [-4098.223, -4098.223], mean action: 2.000 [2.000, 2.000],  loss: 2304860.000000, mae: 4152.438477, mean_q: -2669.789795
 4244/5000: episode: 4244, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2256.067, mean reward: -2256.067 [-2256.067, -2256.067], mean action: 2.000 [2.000, 2.000],  loss: 3182969.250000, mae: 4105.031738, mean_q: -2688.296875
 4245/5000: episode: 4245, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7000.277, mean reward: -7000.277 [-7000.277, -7000.277], mean action: 2.000 [2.000, 2.000],  loss: 4238638.500000, mae: 4250.189453, mean_q: -2687.538574
 4246/5000: episode: 4246, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -730.134, mean reward: -730.134 [-730.134, -730.134], mean action: 2.000 [2.000, 2.000],  loss: 2398092.750000, mae: 4117.132812, mean_q: -2687.750977
 4247/5000: episode: 4247, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7130.080, mean reward: -7130.080 [-7130.080, -7130.080], mean action: 2.000 [2.000, 2.000],  loss: 1909268.500000, mae: 4077.754150, mean_q: -2684.516602
 4248/5000: episode: 4248, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1066.434, mean reward: -1066.434 [-1066.434, -1066.434], mean action: 2.000 [2.000, 2.000],  loss: 4273064.000000, mae: 4210.840820, mean_q: -2679.697266
 4249/5000: episode: 4249, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2393.696, mean reward: -2393.696 [-2393.696, -2393.696], mean action: 2.000 [2.000, 2.000],  loss: 1542276.125000, mae: 3988.665039, mean_q: -2661.300781
 4250/5000: episode: 4250, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2474.084, mean reward: -2474.084 [-2474.084, -2474.084], mean action: 2.000 [2.000, 2.000],  loss: 2410390.000000, mae: 4133.935547, mean_q: -2665.165527
 4251/5000: episode: 4251, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2355.173, mean reward: -2355.173 [-2355.173, -2355.173], mean action: 2.000 [2.000, 2.000],  loss: 2300743.250000, mae: 4102.863281, mean_q: -2671.888184
 4252/5000: episode: 4252, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6839.451, mean reward: -6839.451 [-6839.451, -6839.451], mean action: 2.000 [2.000, 2.000],  loss: 2040901.625000, mae: 4117.408203, mean_q: -2671.716553
 4253/5000: episode: 4253, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1074.047, mean reward: -1074.047 [-1074.047, -1074.047], mean action: 2.000 [2.000, 2.000],  loss: 2672044.500000, mae: 3987.102539, mean_q: -2667.462891
 4254/5000: episode: 4254, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3497.013, mean reward: -3497.013 [-3497.013, -3497.013], mean action: 3.000 [3.000, 3.000],  loss: 4835701.000000, mae: 4225.195312, mean_q: -2654.341797
 4255/5000: episode: 4255, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3378.183, mean reward: -3378.183 [-3378.183, -3378.183], mean action: 2.000 [2.000, 2.000],  loss: 2521922.000000, mae: 4043.427246, mean_q: -2647.560059
 4256/5000: episode: 4256, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -231.055, mean reward: -231.055 [-231.055, -231.055], mean action: 2.000 [2.000, 2.000],  loss: 1438564.250000, mae: 4003.984863, mean_q: -2644.742432
 4257/5000: episode: 4257, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3416.674, mean reward: -3416.674 [-3416.674, -3416.674], mean action: 2.000 [2.000, 2.000],  loss: 1180714.750000, mae: 4006.841309, mean_q: -2639.460938
 4258/5000: episode: 4258, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4209.586, mean reward: -4209.586 [-4209.586, -4209.586], mean action: 0.000 [0.000, 0.000],  loss: 2095289.625000, mae: 4058.508789, mean_q: -2635.119873
 4259/5000: episode: 4259, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2088.910, mean reward: -2088.910 [-2088.910, -2088.910], mean action: 2.000 [2.000, 2.000],  loss: 3857354.750000, mae: 4184.712891, mean_q: -2630.244629
 4260/5000: episode: 4260, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2213.527, mean reward: -2213.527 [-2213.527, -2213.527], mean action: 2.000 [2.000, 2.000],  loss: 3872047.000000, mae: 4141.683594, mean_q: -2602.251465
 4261/5000: episode: 4261, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -551.959, mean reward: -551.959 [-551.959, -551.959], mean action: 2.000 [2.000, 2.000],  loss: 3158699.500000, mae: 4117.373047, mean_q: -2622.520752
 4262/5000: episode: 4262, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -10533.761, mean reward: -10533.761 [-10533.761, -10533.761], mean action: 2.000 [2.000, 2.000],  loss: 2613744.500000, mae: 3977.923828, mean_q: -2599.019287
 4263/5000: episode: 4263, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1364.920, mean reward: -1364.920 [-1364.920, -1364.920], mean action: 2.000 [2.000, 2.000],  loss: 1706883.125000, mae: 4017.656738, mean_q: -2584.005371
 4264/5000: episode: 4264, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4265.498, mean reward: -4265.498 [-4265.498, -4265.498], mean action: 2.000 [2.000, 2.000],  loss: 1416086.625000, mae: 3924.687012, mean_q: -2588.438477
 4265/5000: episode: 4265, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -518.469, mean reward: -518.469 [-518.469, -518.469], mean action: 2.000 [2.000, 2.000],  loss: 2872937.000000, mae: 4141.834961, mean_q: -2597.080078
 4266/5000: episode: 4266, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -404.154, mean reward: -404.154 [-404.154, -404.154], mean action: 2.000 [2.000, 2.000],  loss: 2011116.500000, mae: 4018.675049, mean_q: -2569.896240
 4267/5000: episode: 4267, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4537.333, mean reward: -4537.333 [-4537.333, -4537.333], mean action: 2.000 [2.000, 2.000],  loss: 2702386.500000, mae: 4068.098145, mean_q: -2563.198486
 4268/5000: episode: 4268, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2402.914, mean reward: -2402.914 [-2402.914, -2402.914], mean action: 2.000 [2.000, 2.000],  loss: 3393897.500000, mae: 4105.750977, mean_q: -2557.621582
 4269/5000: episode: 4269, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6290.454, mean reward: -6290.454 [-6290.454, -6290.454], mean action: 2.000 [2.000, 2.000],  loss: 2368200.750000, mae: 4125.523926, mean_q: -2558.458008
 4270/5000: episode: 4270, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -6.245, mean reward: -6.245 [-6.245, -6.245], mean action: 2.000 [2.000, 2.000],  loss: 2497639.000000, mae: 4072.352539, mean_q: -2572.425293
 4271/5000: episode: 4271, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7953.295, mean reward: -7953.295 [-7953.295, -7953.295], mean action: 0.000 [0.000, 0.000],  loss: 3112296.000000, mae: 4195.520508, mean_q: -2565.461182
 4272/5000: episode: 4272, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1455.695, mean reward: -1455.695 [-1455.695, -1455.695], mean action: 2.000 [2.000, 2.000],  loss: 4751972.500000, mae: 4180.434570, mean_q: -2576.641602
 4273/5000: episode: 4273, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -54.543, mean reward: -54.543 [-54.543, -54.543], mean action: 2.000 [2.000, 2.000],  loss: 1942011.750000, mae: 3938.570801, mean_q: -2564.790771
 4274/5000: episode: 4274, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3008.973, mean reward: -3008.973 [-3008.973, -3008.973], mean action: 2.000 [2.000, 2.000],  loss: 3167710.250000, mae: 4124.800781, mean_q: -2551.995361
 4275/5000: episode: 4275, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6028.503, mean reward: -6028.503 [-6028.503, -6028.503], mean action: 2.000 [2.000, 2.000],  loss: 2579437.500000, mae: 4076.641113, mean_q: -2554.954102
 4276/5000: episode: 4276, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1152.642, mean reward: -1152.642 [-1152.642, -1152.642], mean action: 2.000 [2.000, 2.000],  loss: 2179803.500000, mae: 4093.552734, mean_q: -2577.654297
 4277/5000: episode: 4277, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4242.155, mean reward: -4242.155 [-4242.155, -4242.155], mean action: 2.000 [2.000, 2.000],  loss: 2129740.000000, mae: 4110.418945, mean_q: -2578.921875
 4278/5000: episode: 4278, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6966.711, mean reward: -6966.711 [-6966.711, -6966.711], mean action: 1.000 [1.000, 1.000],  loss: 3938827.000000, mae: 4188.605469, mean_q: -2578.507568
 4279/5000: episode: 4279, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4501.335, mean reward: -4501.335 [-4501.335, -4501.335], mean action: 2.000 [2.000, 2.000],  loss: 1904802.625000, mae: 4041.671143, mean_q: -2591.963379
 4280/5000: episode: 4280, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -793.794, mean reward: -793.794 [-793.794, -793.794], mean action: 2.000 [2.000, 2.000],  loss: 2704802.000000, mae: 4146.266602, mean_q: -2608.701660
 4281/5000: episode: 4281, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1997.167, mean reward: -1997.167 [-1997.167, -1997.167], mean action: 2.000 [2.000, 2.000],  loss: 4119677.000000, mae: 4163.223633, mean_q: -2623.854980
 4282/5000: episode: 4282, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -83.571, mean reward: -83.571 [-83.571, -83.571], mean action: 2.000 [2.000, 2.000],  loss: 3203661.500000, mae: 4142.625977, mean_q: -2627.806641
 4283/5000: episode: 4283, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1326.427, mean reward: -1326.427 [-1326.427, -1326.427], mean action: 2.000 [2.000, 2.000],  loss: 2857550.750000, mae: 4120.495117, mean_q: -2629.455322
 4284/5000: episode: 4284, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3416.100, mean reward: -3416.100 [-3416.100, -3416.100], mean action: 2.000 [2.000, 2.000],  loss: 4299584.500000, mae: 4329.939453, mean_q: -2664.428223
 4285/5000: episode: 4285, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -1021.942, mean reward: -1021.942 [-1021.942, -1021.942], mean action: 2.000 [2.000, 2.000],  loss: 2465926.750000, mae: 4123.297852, mean_q: -2678.472168
 4286/5000: episode: 4286, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1841.596, mean reward: -1841.596 [-1841.596, -1841.596], mean action: 2.000 [2.000, 2.000],  loss: 2074489.000000, mae: 4093.072754, mean_q: -2677.225098
 4287/5000: episode: 4287, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6616.298, mean reward: -6616.298 [-6616.298, -6616.298], mean action: 2.000 [2.000, 2.000],  loss: 1405213.000000, mae: 3988.211670, mean_q: -2657.249512
 4288/5000: episode: 4288, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7718.320, mean reward: -7718.320 [-7718.320, -7718.320], mean action: 2.000 [2.000, 2.000],  loss: 2539978.500000, mae: 4103.107910, mean_q: -2676.502930
 4289/5000: episode: 4289, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3311.616, mean reward: -3311.616 [-3311.616, -3311.616], mean action: 2.000 [2.000, 2.000],  loss: 2664757.000000, mae: 4192.563965, mean_q: -2680.132324
 4290/5000: episode: 4290, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2667.535, mean reward: -2667.535 [-2667.535, -2667.535], mean action: 2.000 [2.000, 2.000],  loss: 2709867.000000, mae: 4043.769287, mean_q: -2672.304688
 4291/5000: episode: 4291, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -874.296, mean reward: -874.296 [-874.296, -874.296], mean action: 2.000 [2.000, 2.000],  loss: 2739203.000000, mae: 4142.357910, mean_q: -2690.205566
 4292/5000: episode: 4292, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3613.100, mean reward: -3613.100 [-3613.100, -3613.100], mean action: 2.000 [2.000, 2.000],  loss: 2589311.000000, mae: 4063.760986, mean_q: -2676.759033
 4293/5000: episode: 4293, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -853.821, mean reward: -853.821 [-853.821, -853.821], mean action: 2.000 [2.000, 2.000],  loss: 2435323.000000, mae: 4102.743164, mean_q: -2692.731934
 4294/5000: episode: 4294, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7796.795, mean reward: -7796.795 [-7796.795, -7796.795], mean action: 3.000 [3.000, 3.000],  loss: 2435536.250000, mae: 4085.055664, mean_q: -2699.268555
 4295/5000: episode: 4295, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -193.050, mean reward: -193.050 [-193.050, -193.050], mean action: 2.000 [2.000, 2.000],  loss: 2825099.500000, mae: 4105.097656, mean_q: -2696.144775
 4296/5000: episode: 4296, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -220.952, mean reward: -220.952 [-220.952, -220.952], mean action: 2.000 [2.000, 2.000],  loss: 2762588.500000, mae: 4108.613281, mean_q: -2690.895996
 4297/5000: episode: 4297, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1001.155, mean reward: -1001.155 [-1001.155, -1001.155], mean action: 2.000 [2.000, 2.000],  loss: 2771064.250000, mae: 4117.085938, mean_q: -2676.192627
 4298/5000: episode: 4298, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6734.043, mean reward: -6734.043 [-6734.043, -6734.043], mean action: 0.000 [0.000, 0.000],  loss: 2688794.750000, mae: 4162.078125, mean_q: -2689.880371
 4299/5000: episode: 4299, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1049.031, mean reward: -1049.031 [-1049.031, -1049.031], mean action: 2.000 [2.000, 2.000],  loss: 1780857.000000, mae: 4091.434570, mean_q: -2696.817383
 4300/5000: episode: 4300, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -400.014, mean reward: -400.014 [-400.014, -400.014], mean action: 2.000 [2.000, 2.000],  loss: 1840137.750000, mae: 4143.572266, mean_q: -2704.024902
 4301/5000: episode: 4301, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -98.852, mean reward: -98.852 [-98.852, -98.852], mean action: 2.000 [2.000, 2.000],  loss: 2014902.500000, mae: 4075.506104, mean_q: -2688.245361
 4302/5000: episode: 4302, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4016.665, mean reward: -4016.665 [-4016.665, -4016.665], mean action: 2.000 [2.000, 2.000],  loss: 2397825.500000, mae: 4200.475586, mean_q: -2712.572998
 4303/5000: episode: 4303, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -327.386, mean reward: -327.386 [-327.386, -327.386], mean action: 2.000 [2.000, 2.000],  loss: 2390723.000000, mae: 4083.777588, mean_q: -2717.802246
 4304/5000: episode: 4304, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2368.913, mean reward: -2368.913 [-2368.913, -2368.913], mean action: 2.000 [2.000, 2.000],  loss: 2770413.750000, mae: 4040.658691, mean_q: -2704.182129
 4305/5000: episode: 4305, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2336.051, mean reward: -2336.051 [-2336.051, -2336.051], mean action: 2.000 [2.000, 2.000],  loss: 2306260.750000, mae: 4190.436523, mean_q: -2727.918457
 4306/5000: episode: 4306, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5147.576, mean reward: -5147.576 [-5147.576, -5147.576], mean action: 2.000 [2.000, 2.000],  loss: 3664226.000000, mae: 4257.695312, mean_q: -2732.682617
 4307/5000: episode: 4307, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -504.208, mean reward: -504.208 [-504.208, -504.208], mean action: 2.000 [2.000, 2.000],  loss: 1795407.750000, mae: 4118.782715, mean_q: -2709.940918
 4308/5000: episode: 4308, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -266.609, mean reward: -266.609 [-266.609, -266.609], mean action: 2.000 [2.000, 2.000],  loss: 4109841.250000, mae: 4253.020020, mean_q: -2715.356689
 4309/5000: episode: 4309, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3795.591, mean reward: -3795.591 [-3795.591, -3795.591], mean action: 2.000 [2.000, 2.000],  loss: 2559279.750000, mae: 4114.574219, mean_q: -2710.700195
 4310/5000: episode: 4310, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2414.463, mean reward: -2414.463 [-2414.463, -2414.463], mean action: 2.000 [2.000, 2.000],  loss: 5419673.000000, mae: 4276.109375, mean_q: -2714.934814
 4311/5000: episode: 4311, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1816.509, mean reward: -1816.509 [-1816.509, -1816.509], mean action: 1.000 [1.000, 1.000],  loss: 2082082.000000, mae: 4023.624023, mean_q: -2702.163574
 4312/5000: episode: 4312, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -509.229, mean reward: -509.229 [-509.229, -509.229], mean action: 2.000 [2.000, 2.000],  loss: 3999744.000000, mae: 4221.200195, mean_q: -2689.975830
 4313/5000: episode: 4313, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -7853.570, mean reward: -7853.570 [-7853.570, -7853.570], mean action: 2.000 [2.000, 2.000],  loss: 1899268.500000, mae: 4097.333008, mean_q: -2685.497314
 4314/5000: episode: 4314, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -372.904, mean reward: -372.904 [-372.904, -372.904], mean action: 2.000 [2.000, 2.000],  loss: 2612567.000000, mae: 4147.577148, mean_q: -2695.259521
 4315/5000: episode: 4315, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -981.617, mean reward: -981.617 [-981.617, -981.617], mean action: 2.000 [2.000, 2.000],  loss: 2541722.500000, mae: 4070.316406, mean_q: -2689.825684
 4316/5000: episode: 4316, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5177.762, mean reward: -5177.762 [-5177.762, -5177.762], mean action: 2.000 [2.000, 2.000],  loss: 4330832.000000, mae: 4191.876953, mean_q: -2692.379883
 4317/5000: episode: 4317, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2493.086, mean reward: -2493.086 [-2493.086, -2493.086], mean action: 2.000 [2.000, 2.000],  loss: 2292843.500000, mae: 4091.085205, mean_q: -2681.303711
 4318/5000: episode: 4318, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6138.642, mean reward: -6138.642 [-6138.642, -6138.642], mean action: 2.000 [2.000, 2.000],  loss: 3698996.500000, mae: 4143.311035, mean_q: -2693.790039
 4319/5000: episode: 4319, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1460.119, mean reward: -1460.119 [-1460.119, -1460.119], mean action: 2.000 [2.000, 2.000],  loss: 3692435.000000, mae: 4184.992188, mean_q: -2687.302002
 4320/5000: episode: 4320, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -17.723, mean reward: -17.723 [-17.723, -17.723], mean action: 2.000 [2.000, 2.000],  loss: 2558735.000000, mae: 4032.059570, mean_q: -2676.824463
 4321/5000: episode: 4321, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -217.555, mean reward: -217.555 [-217.555, -217.555], mean action: 2.000 [2.000, 2.000],  loss: 1830483.000000, mae: 4005.964600, mean_q: -2681.525879
 4322/5000: episode: 4322, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2856.390, mean reward: -2856.390 [-2856.390, -2856.390], mean action: 2.000 [2.000, 2.000],  loss: 2944519.000000, mae: 4170.040039, mean_q: -2689.865723
 4323/5000: episode: 4323, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3081.669, mean reward: -3081.669 [-3081.669, -3081.669], mean action: 2.000 [2.000, 2.000],  loss: 3767768.000000, mae: 4123.891113, mean_q: -2653.866699
 4324/5000: episode: 4324, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2104.217, mean reward: -2104.217 [-2104.217, -2104.217], mean action: 2.000 [2.000, 2.000],  loss: 3118629.000000, mae: 4166.068359, mean_q: -2644.018555
 4325/5000: episode: 4325, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4541.224, mean reward: -4541.224 [-4541.224, -4541.224], mean action: 2.000 [2.000, 2.000],  loss: 1974241.750000, mae: 4070.092773, mean_q: -2657.472168
 4326/5000: episode: 4326, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -635.127, mean reward: -635.127 [-635.127, -635.127], mean action: 2.000 [2.000, 2.000],  loss: 2184579.500000, mae: 4137.347168, mean_q: -2664.592773
 4327/5000: episode: 4327, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8227.459, mean reward: -8227.459 [-8227.459, -8227.459], mean action: 0.000 [0.000, 0.000],  loss: 2828541.250000, mae: 4140.593750, mean_q: -2660.040771
 4328/5000: episode: 4328, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3211.959, mean reward: -3211.959 [-3211.959, -3211.959], mean action: 2.000 [2.000, 2.000],  loss: 4342436.000000, mae: 4145.486328, mean_q: -2631.903076
 4329/5000: episode: 4329, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8287.430, mean reward: -8287.430 [-8287.430, -8287.430], mean action: 2.000 [2.000, 2.000],  loss: 2980429.000000, mae: 4084.038086, mean_q: -2648.191406
 4330/5000: episode: 4330, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -204.594, mean reward: -204.594 [-204.594, -204.594], mean action: 2.000 [2.000, 2.000],  loss: 2709792.250000, mae: 4043.863281, mean_q: -2626.025879
 4331/5000: episode: 4331, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1406.371, mean reward: -1406.371 [-1406.371, -1406.371], mean action: 2.000 [2.000, 2.000],  loss: 1506964.875000, mae: 3974.317139, mean_q: -2629.432129
 4332/5000: episode: 4332, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6945.712, mean reward: -6945.712 [-6945.712, -6945.712], mean action: 2.000 [2.000, 2.000],  loss: 2775914.250000, mae: 4057.612061, mean_q: -2623.196289
 4333/5000: episode: 4333, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3851.717, mean reward: -3851.717 [-3851.717, -3851.717], mean action: 2.000 [2.000, 2.000],  loss: 4913777.500000, mae: 4185.651855, mean_q: -2644.578613
 4334/5000: episode: 4334, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -133.890, mean reward: -133.890 [-133.890, -133.890], mean action: 2.000 [2.000, 2.000],  loss: 1755390.750000, mae: 4008.127686, mean_q: -2621.828125
 4335/5000: episode: 4335, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4269.304, mean reward: -4269.304 [-4269.304, -4269.304], mean action: 2.000 [2.000, 2.000],  loss: 1722532.375000, mae: 3990.242188, mean_q: -2622.625000
 4336/5000: episode: 4336, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2883.221, mean reward: -2883.221 [-2883.221, -2883.221], mean action: 2.000 [2.000, 2.000],  loss: 1908369.625000, mae: 4024.800049, mean_q: -2634.452393
 4337/5000: episode: 4337, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4346.608, mean reward: -4346.608 [-4346.608, -4346.608], mean action: 2.000 [2.000, 2.000],  loss: 1339231.250000, mae: 3962.860596, mean_q: -2652.289551
 4338/5000: episode: 4338, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3815.889, mean reward: -3815.889 [-3815.889, -3815.889], mean action: 2.000 [2.000, 2.000],  loss: 3512161.000000, mae: 4166.554199, mean_q: -2660.372070
 4339/5000: episode: 4339, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6862.888, mean reward: -6862.888 [-6862.888, -6862.888], mean action: 2.000 [2.000, 2.000],  loss: 3079764.500000, mae: 4073.133301, mean_q: -2667.724854
 4340/5000: episode: 4340, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -8311.291, mean reward: -8311.291 [-8311.291, -8311.291], mean action: 2.000 [2.000, 2.000],  loss: 2898636.500000, mae: 4053.495117, mean_q: -2641.879883
 4341/5000: episode: 4341, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -135.821, mean reward: -135.821 [-135.821, -135.821], mean action: 2.000 [2.000, 2.000],  loss: 2082545.375000, mae: 4069.477539, mean_q: -2644.678711
 4342/5000: episode: 4342, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2511.803, mean reward: -2511.803 [-2511.803, -2511.803], mean action: 2.000 [2.000, 2.000],  loss: 2582841.000000, mae: 4065.666260, mean_q: -2658.193359
 4343/5000: episode: 4343, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1770.504, mean reward: -1770.504 [-1770.504, -1770.504], mean action: 2.000 [2.000, 2.000],  loss: 3111538.000000, mae: 4090.609863, mean_q: -2653.732910
 4344/5000: episode: 4344, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1177.831, mean reward: -1177.831 [-1177.831, -1177.831], mean action: 3.000 [3.000, 3.000],  loss: 2517773.250000, mae: 4080.774902, mean_q: -2657.347900
 4345/5000: episode: 4345, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1953.581, mean reward: -1953.581 [-1953.581, -1953.581], mean action: 2.000 [2.000, 2.000],  loss: 2950470.000000, mae: 4088.172852, mean_q: -2646.425781
 4346/5000: episode: 4346, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5330.228, mean reward: -5330.228 [-5330.228, -5330.228], mean action: 2.000 [2.000, 2.000],  loss: 2219533.500000, mae: 4029.970459, mean_q: -2653.625244
 4347/5000: episode: 4347, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2152.914, mean reward: -2152.914 [-2152.914, -2152.914], mean action: 2.000 [2.000, 2.000],  loss: 2484969.500000, mae: 4024.377686, mean_q: -2648.656738
 4348/5000: episode: 4348, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1709.352, mean reward: -1709.352 [-1709.352, -1709.352], mean action: 2.000 [2.000, 2.000],  loss: 3034127.750000, mae: 4196.868164, mean_q: -2643.934082
 4349/5000: episode: 4349, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2558.900, mean reward: -2558.900 [-2558.900, -2558.900], mean action: 2.000 [2.000, 2.000],  loss: 1789750.500000, mae: 3981.832520, mean_q: -2665.510254
 4350/5000: episode: 4350, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1513.153, mean reward: -1513.153 [-1513.153, -1513.153], mean action: 2.000 [2.000, 2.000],  loss: 2723148.000000, mae: 4108.948242, mean_q: -2641.517090
 4351/5000: episode: 4351, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5521.246, mean reward: -5521.246 [-5521.246, -5521.246], mean action: 2.000 [2.000, 2.000],  loss: 2123785.500000, mae: 4093.762695, mean_q: -2636.306641
 4352/5000: episode: 4352, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1390.963, mean reward: -1390.963 [-1390.963, -1390.963], mean action: 2.000 [2.000, 2.000],  loss: 2492746.500000, mae: 3994.453125, mean_q: -2648.050781
 4353/5000: episode: 4353, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3460.916, mean reward: -3460.916 [-3460.916, -3460.916], mean action: 2.000 [2.000, 2.000],  loss: 2298884.500000, mae: 3995.369629, mean_q: -2631.820557
 4354/5000: episode: 4354, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4121.787, mean reward: -4121.787 [-4121.787, -4121.787], mean action: 2.000 [2.000, 2.000],  loss: 1556552.500000, mae: 4005.990479, mean_q: -2642.261719
 4355/5000: episode: 4355, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3680.327, mean reward: -3680.327 [-3680.327, -3680.327], mean action: 2.000 [2.000, 2.000],  loss: 2565695.000000, mae: 4038.487305, mean_q: -2645.469238
 4356/5000: episode: 4356, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2628.882, mean reward: -2628.882 [-2628.882, -2628.882], mean action: 2.000 [2.000, 2.000],  loss: 2913340.000000, mae: 4122.816406, mean_q: -2663.443115
 4357/5000: episode: 4357, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -192.705, mean reward: -192.705 [-192.705, -192.705], mean action: 2.000 [2.000, 2.000],  loss: 4142896.750000, mae: 4240.497559, mean_q: -2664.180908
 4358/5000: episode: 4358, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1979.706, mean reward: -1979.706 [-1979.706, -1979.706], mean action: 2.000 [2.000, 2.000],  loss: 2668072.500000, mae: 4134.080078, mean_q: -2642.099609
 4359/5000: episode: 4359, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1202.978, mean reward: -1202.978 [-1202.978, -1202.978], mean action: 2.000 [2.000, 2.000],  loss: 2732746.000000, mae: 4045.181641, mean_q: -2649.488037
 4360/5000: episode: 4360, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5046.924, mean reward: -5046.924 [-5046.924, -5046.924], mean action: 2.000 [2.000, 2.000],  loss: 2547104.250000, mae: 4083.553223, mean_q: -2659.538574
 4361/5000: episode: 4361, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -652.950, mean reward: -652.950 [-652.950, -652.950], mean action: 2.000 [2.000, 2.000],  loss: 1850768.250000, mae: 4072.529297, mean_q: -2671.528809
 4362/5000: episode: 4362, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -15.582, mean reward: -15.582 [-15.582, -15.582], mean action: 2.000 [2.000, 2.000],  loss: 2046637.500000, mae: 4133.070312, mean_q: -2672.377686
 4363/5000: episode: 4363, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1231.341, mean reward: -1231.341 [-1231.341, -1231.341], mean action: 2.000 [2.000, 2.000],  loss: 3388068.500000, mae: 4157.951660, mean_q: -2664.182129
 4364/5000: episode: 4364, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1795.761, mean reward: -1795.761 [-1795.761, -1795.761], mean action: 2.000 [2.000, 2.000],  loss: 3265119.000000, mae: 4228.321289, mean_q: -2675.374512
 4365/5000: episode: 4365, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4526.495, mean reward: -4526.495 [-4526.495, -4526.495], mean action: 2.000 [2.000, 2.000],  loss: 1839654.750000, mae: 4070.212891, mean_q: -2692.821777
 4366/5000: episode: 4366, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1529.322, mean reward: -1529.322 [-1529.322, -1529.322], mean action: 2.000 [2.000, 2.000],  loss: 3353434.500000, mae: 4196.106445, mean_q: -2689.501953
 4367/5000: episode: 4367, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3264.542, mean reward: -3264.542 [-3264.542, -3264.542], mean action: 2.000 [2.000, 2.000],  loss: 1966022.375000, mae: 3957.808594, mean_q: -2690.422363
 4368/5000: episode: 4368, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2411.353, mean reward: -2411.353 [-2411.353, -2411.353], mean action: 2.000 [2.000, 2.000],  loss: 3590629.750000, mae: 4232.910645, mean_q: -2694.599854
 4369/5000: episode: 4369, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2289.483, mean reward: -2289.483 [-2289.483, -2289.483], mean action: 2.000 [2.000, 2.000],  loss: 3759871.000000, mae: 4151.351074, mean_q: -2690.239990
 4370/5000: episode: 4370, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -685.520, mean reward: -685.520 [-685.520, -685.520], mean action: 2.000 [2.000, 2.000],  loss: 4019849.000000, mae: 4316.993652, mean_q: -2696.031250
 4371/5000: episode: 4371, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6645.161, mean reward: -6645.161 [-6645.161, -6645.161], mean action: 2.000 [2.000, 2.000],  loss: 2653250.250000, mae: 4202.577148, mean_q: -2721.500000
 4372/5000: episode: 4372, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2222.289, mean reward: -2222.289 [-2222.289, -2222.289], mean action: 2.000 [2.000, 2.000],  loss: 2866613.250000, mae: 4161.339844, mean_q: -2721.451172
 4373/5000: episode: 4373, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4741.749, mean reward: -4741.749 [-4741.749, -4741.749], mean action: 2.000 [2.000, 2.000],  loss: 3140596.000000, mae: 4186.052734, mean_q: -2722.024658
 4374/5000: episode: 4374, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -318.233, mean reward: -318.233 [-318.233, -318.233], mean action: 2.000 [2.000, 2.000],  loss: 2398891.500000, mae: 4172.597168, mean_q: -2733.731689
 4375/5000: episode: 4375, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4173.664, mean reward: -4173.664 [-4173.664, -4173.664], mean action: 2.000 [2.000, 2.000],  loss: 4016419.000000, mae: 4234.165039, mean_q: -2743.620117
 4376/5000: episode: 4376, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3024.764, mean reward: -3024.764 [-3024.764, -3024.764], mean action: 2.000 [2.000, 2.000],  loss: 2369182.000000, mae: 4160.933594, mean_q: -2757.905762
 4377/5000: episode: 4377, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2840.419, mean reward: -2840.419 [-2840.419, -2840.419], mean action: 2.000 [2.000, 2.000],  loss: 2016366.625000, mae: 4194.888672, mean_q: -2740.224121
 4378/5000: episode: 4378, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1052.399, mean reward: -1052.399 [-1052.399, -1052.399], mean action: 2.000 [2.000, 2.000],  loss: 3485793.500000, mae: 4269.349609, mean_q: -2732.599609
 4379/5000: episode: 4379, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3028.325, mean reward: -3028.325 [-3028.325, -3028.325], mean action: 2.000 [2.000, 2.000],  loss: 4170608.000000, mae: 4315.000977, mean_q: -2751.053711
 4380/5000: episode: 4380, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3850.923, mean reward: -3850.923 [-3850.923, -3850.923], mean action: 2.000 [2.000, 2.000],  loss: 2012608.000000, mae: 4107.363281, mean_q: -2740.517090
 4381/5000: episode: 4381, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1029.865, mean reward: -1029.865 [-1029.865, -1029.865], mean action: 3.000 [3.000, 3.000],  loss: 2774512.250000, mae: 4183.311523, mean_q: -2728.282227
 4382/5000: episode: 4382, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1409.244, mean reward: -1409.244 [-1409.244, -1409.244], mean action: 2.000 [2.000, 2.000],  loss: 2141707.500000, mae: 4158.667969, mean_q: -2733.655518
 4383/5000: episode: 4383, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3122.075, mean reward: -3122.075 [-3122.075, -3122.075], mean action: 2.000 [2.000, 2.000],  loss: 3992634.500000, mae: 4279.895508, mean_q: -2713.009033
 4384/5000: episode: 4384, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -953.140, mean reward: -953.140 [-953.140, -953.140], mean action: 2.000 [2.000, 2.000],  loss: 2118323.500000, mae: 4114.682617, mean_q: -2716.003906
 4385/5000: episode: 4385, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2911.094, mean reward: -2911.094 [-2911.094, -2911.094], mean action: 2.000 [2.000, 2.000],  loss: 3305268.000000, mae: 4283.542480, mean_q: -2708.026611
 4386/5000: episode: 4386, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7704.445, mean reward: -7704.445 [-7704.445, -7704.445], mean action: 3.000 [3.000, 3.000],  loss: 2697790.500000, mae: 4097.749023, mean_q: -2718.248047
 4387/5000: episode: 4387, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1043.095, mean reward: -1043.095 [-1043.095, -1043.095], mean action: 2.000 [2.000, 2.000],  loss: 2312605.500000, mae: 4183.199219, mean_q: -2687.984863
 4388/5000: episode: 4388, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1877.994, mean reward: -1877.994 [-1877.994, -1877.994], mean action: 2.000 [2.000, 2.000],  loss: 2658695.750000, mae: 4170.528320, mean_q: -2698.880859
 4389/5000: episode: 4389, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -63.473, mean reward: -63.473 [-63.473, -63.473], mean action: 2.000 [2.000, 2.000],  loss: 5517276.000000, mae: 4347.201660, mean_q: -2708.723633
 4390/5000: episode: 4390, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2797.803, mean reward: -2797.803 [-2797.803, -2797.803], mean action: 2.000 [2.000, 2.000],  loss: 1917887.500000, mae: 4078.682129, mean_q: -2696.774902
 4391/5000: episode: 4391, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2512.533, mean reward: -2512.533 [-2512.533, -2512.533], mean action: 2.000 [2.000, 2.000],  loss: 3046296.500000, mae: 4108.289062, mean_q: -2696.281738
 4392/5000: episode: 4392, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5040.864, mean reward: -5040.864 [-5040.864, -5040.864], mean action: 2.000 [2.000, 2.000],  loss: 3003401.000000, mae: 4238.013672, mean_q: -2707.148926
 4393/5000: episode: 4393, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -839.978, mean reward: -839.978 [-839.978, -839.978], mean action: 2.000 [2.000, 2.000],  loss: 2983201.750000, mae: 4164.031738, mean_q: -2698.956543
 4394/5000: episode: 4394, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -590.377, mean reward: -590.377 [-590.377, -590.377], mean action: 2.000 [2.000, 2.000],  loss: 1506331.875000, mae: 4129.580078, mean_q: -2697.260254
 4395/5000: episode: 4395, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1755.250, mean reward: -1755.250 [-1755.250, -1755.250], mean action: 2.000 [2.000, 2.000],  loss: 2612861.500000, mae: 4140.630859, mean_q: -2711.855713
 4396/5000: episode: 4396, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1601.528, mean reward: -1601.528 [-1601.528, -1601.528], mean action: 2.000 [2.000, 2.000],  loss: 2278247.500000, mae: 4206.084961, mean_q: -2709.734619
 4397/5000: episode: 4397, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3661.363, mean reward: -3661.363 [-3661.363, -3661.363], mean action: 2.000 [2.000, 2.000],  loss: 2536629.250000, mae: 4125.738281, mean_q: -2705.552490
 4398/5000: episode: 4398, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4484.480, mean reward: -4484.480 [-4484.480, -4484.480], mean action: 2.000 [2.000, 2.000],  loss: 1728709.500000, mae: 4118.858398, mean_q: -2709.790283
 4399/5000: episode: 4399, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4675.342, mean reward: -4675.342 [-4675.342, -4675.342], mean action: 2.000 [2.000, 2.000],  loss: 2746220.250000, mae: 4232.533203, mean_q: -2680.423340
 4400/5000: episode: 4400, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1806.526, mean reward: -1806.526 [-1806.526, -1806.526], mean action: 2.000 [2.000, 2.000],  loss: 3380628.500000, mae: 4179.325195, mean_q: -2668.979736
 4401/5000: episode: 4401, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5946.657, mean reward: -5946.657 [-5946.657, -5946.657], mean action: 2.000 [2.000, 2.000],  loss: 1658709.625000, mae: 4137.255859, mean_q: -2675.096436
 4402/5000: episode: 4402, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2289.624, mean reward: -2289.624 [-2289.624, -2289.624], mean action: 2.000 [2.000, 2.000],  loss: 4236622.000000, mae: 4228.400391, mean_q: -2648.639160
 4403/5000: episode: 4403, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2135.093, mean reward: -2135.093 [-2135.093, -2135.093], mean action: 2.000 [2.000, 2.000],  loss: 3033469.500000, mae: 4192.172852, mean_q: -2657.268555
 4404/5000: episode: 4404, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -30.639, mean reward: -30.639 [-30.639, -30.639], mean action: 2.000 [2.000, 2.000],  loss: 3386315.500000, mae: 4251.399414, mean_q: -2667.229492
 4405/5000: episode: 4405, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2200.735, mean reward: -2200.735 [-2200.735, -2200.735], mean action: 2.000 [2.000, 2.000],  loss: 2181394.500000, mae: 4167.452148, mean_q: -2644.562744
 4406/5000: episode: 4406, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2587.682, mean reward: -2587.682 [-2587.682, -2587.682], mean action: 2.000 [2.000, 2.000],  loss: 3014275.500000, mae: 4261.374023, mean_q: -2647.744629
 4407/5000: episode: 4407, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -744.195, mean reward: -744.195 [-744.195, -744.195], mean action: 2.000 [2.000, 2.000],  loss: 2131443.000000, mae: 4156.942383, mean_q: -2658.900635
 4408/5000: episode: 4408, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -679.620, mean reward: -679.620 [-679.620, -679.620], mean action: 2.000 [2.000, 2.000],  loss: 1568943.000000, mae: 4096.220215, mean_q: -2663.156494
 4409/5000: episode: 4409, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1633.660, mean reward: -1633.660 [-1633.660, -1633.660], mean action: 0.000 [0.000, 0.000],  loss: 1608105.000000, mae: 4087.708008, mean_q: -2647.603027
 4410/5000: episode: 4410, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1117.113, mean reward: -1117.113 [-1117.113, -1117.113], mean action: 2.000 [2.000, 2.000],  loss: 3633782.000000, mae: 4314.702148, mean_q: -2673.562744
 4411/5000: episode: 4411, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2435.650, mean reward: -2435.650 [-2435.650, -2435.650], mean action: 2.000 [2.000, 2.000],  loss: 2457909.500000, mae: 4163.613281, mean_q: -2660.408691
 4412/5000: episode: 4412, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -678.477, mean reward: -678.477 [-678.477, -678.477], mean action: 2.000 [2.000, 2.000],  loss: 2008988.250000, mae: 4075.155518, mean_q: -2663.529785
 4413/5000: episode: 4413, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3022.475, mean reward: -3022.475 [-3022.475, -3022.475], mean action: 3.000 [3.000, 3.000],  loss: 1620804.250000, mae: 4105.218750, mean_q: -2667.534668
 4414/5000: episode: 4414, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1507.819, mean reward: -1507.819 [-1507.819, -1507.819], mean action: 2.000 [2.000, 2.000],  loss: 2119004.500000, mae: 4141.351562, mean_q: -2669.061035
 4415/5000: episode: 4415, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9331.232, mean reward: -9331.232 [-9331.232, -9331.232], mean action: 2.000 [2.000, 2.000],  loss: 2660410.250000, mae: 4234.406250, mean_q: -2703.013672
 4416/5000: episode: 4416, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1296.336, mean reward: -1296.336 [-1296.336, -1296.336], mean action: 2.000 [2.000, 2.000],  loss: 2919105.000000, mae: 4298.219727, mean_q: -2729.852539
 4417/5000: episode: 4417, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3306.598, mean reward: -3306.598 [-3306.598, -3306.598], mean action: 2.000 [2.000, 2.000],  loss: 3728192.000000, mae: 4388.179688, mean_q: -2731.942627
 4418/5000: episode: 4418, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7085.193, mean reward: -7085.193 [-7085.193, -7085.193], mean action: 2.000 [2.000, 2.000],  loss: 2099504.000000, mae: 4230.961426, mean_q: -2721.125977
 4419/5000: episode: 4419, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1419.133, mean reward: -1419.133 [-1419.133, -1419.133], mean action: 2.000 [2.000, 2.000],  loss: 3098960.500000, mae: 4265.352539, mean_q: -2744.483887
 4420/5000: episode: 4420, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -449.829, mean reward: -449.829 [-449.829, -449.829], mean action: 2.000 [2.000, 2.000],  loss: 4018299.500000, mae: 4333.985840, mean_q: -2723.630859
 4421/5000: episode: 4421, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2898.811, mean reward: -2898.811 [-2898.811, -2898.811], mean action: 2.000 [2.000, 2.000],  loss: 2942965.500000, mae: 4194.912109, mean_q: -2727.461914
 4422/5000: episode: 4422, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1352.528, mean reward: -1352.528 [-1352.528, -1352.528], mean action: 2.000 [2.000, 2.000],  loss: 3178383.500000, mae: 4299.339844, mean_q: -2717.139648
 4423/5000: episode: 4423, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1787.940, mean reward: -1787.940 [-1787.940, -1787.940], mean action: 2.000 [2.000, 2.000],  loss: 4852351.000000, mae: 4446.268066, mean_q: -2741.568604
 4424/5000: episode: 4424, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1485.267, mean reward: -1485.267 [-1485.267, -1485.267], mean action: 2.000 [2.000, 2.000],  loss: 3122091.750000, mae: 4344.286621, mean_q: -2721.439941
 4425/5000: episode: 4425, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2895.006, mean reward: -2895.006 [-2895.006, -2895.006], mean action: 2.000 [2.000, 2.000],  loss: 2452150.000000, mae: 4302.677734, mean_q: -2725.564941
 4426/5000: episode: 4426, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -530.473, mean reward: -530.473 [-530.473, -530.473], mean action: 2.000 [2.000, 2.000],  loss: 5839164.000000, mae: 4475.575195, mean_q: -2710.270264
 4427/5000: episode: 4427, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2956.292, mean reward: -2956.292 [-2956.292, -2956.292], mean action: 2.000 [2.000, 2.000],  loss: 3850613.750000, mae: 4397.550781, mean_q: -2707.674316
 4428/5000: episode: 4428, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -667.068, mean reward: -667.068 [-667.068, -667.068], mean action: 2.000 [2.000, 2.000],  loss: 1753125.250000, mae: 4138.874023, mean_q: -2685.898438
 4429/5000: episode: 4429, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5350.018, mean reward: -5350.018 [-5350.018, -5350.018], mean action: 2.000 [2.000, 2.000],  loss: 3152969.750000, mae: 4334.471680, mean_q: -2683.032715
 4430/5000: episode: 4430, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -511.611, mean reward: -511.611 [-511.611, -511.611], mean action: 2.000 [2.000, 2.000],  loss: 2127879.250000, mae: 4247.835938, mean_q: -2688.428223
 4431/5000: episode: 4431, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1162.981, mean reward: -1162.981 [-1162.981, -1162.981], mean action: 2.000 [2.000, 2.000],  loss: 1763132.000000, mae: 4173.593750, mean_q: -2643.984619
 4432/5000: episode: 4432, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -698.011, mean reward: -698.011 [-698.011, -698.011], mean action: 2.000 [2.000, 2.000],  loss: 4782737.500000, mae: 4390.460938, mean_q: -2654.783936
 4433/5000: episode: 4433, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1145.225, mean reward: -1145.225 [-1145.225, -1145.225], mean action: 2.000 [2.000, 2.000],  loss: 2785905.500000, mae: 4142.734863, mean_q: -2617.476318
 4434/5000: episode: 4434, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5876.096, mean reward: -5876.096 [-5876.096, -5876.096], mean action: 0.000 [0.000, 0.000],  loss: 1769289.500000, mae: 4145.186035, mean_q: -2618.285156
 4435/5000: episode: 4435, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1690.006, mean reward: -1690.006 [-1690.006, -1690.006], mean action: 2.000 [2.000, 2.000],  loss: 1987228.000000, mae: 4188.408203, mean_q: -2620.375000
 4436/5000: episode: 4436, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -56.365, mean reward: -56.365 [-56.365, -56.365], mean action: 2.000 [2.000, 2.000],  loss: 2741580.000000, mae: 4195.057129, mean_q: -2608.460693
 4437/5000: episode: 4437, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -699.837, mean reward: -699.837 [-699.837, -699.837], mean action: 2.000 [2.000, 2.000],  loss: 4974196.000000, mae: 4253.045898, mean_q: -2591.884521
 4438/5000: episode: 4438, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3343.267, mean reward: -3343.267 [-3343.267, -3343.267], mean action: 2.000 [2.000, 2.000],  loss: 2271991.500000, mae: 4178.773926, mean_q: -2603.234375
 4439/5000: episode: 4439, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1404.591, mean reward: -1404.591 [-1404.591, -1404.591], mean action: 2.000 [2.000, 2.000],  loss: 4030975.750000, mae: 4153.200195, mean_q: -2608.881836
 4440/5000: episode: 4440, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -8512.337, mean reward: -8512.337 [-8512.337, -8512.337], mean action: 2.000 [2.000, 2.000],  loss: 2531311.500000, mae: 4183.763672, mean_q: -2599.236816
 4441/5000: episode: 4441, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2757.977, mean reward: -2757.977 [-2757.977, -2757.977], mean action: 2.000 [2.000, 2.000],  loss: 2997729.000000, mae: 4226.176758, mean_q: -2603.423584
 4442/5000: episode: 4442, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4485.656, mean reward: -4485.656 [-4485.656, -4485.656], mean action: 2.000 [2.000, 2.000],  loss: 2213656.000000, mae: 4163.273438, mean_q: -2601.042480
 4443/5000: episode: 4443, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -2241.412, mean reward: -2241.412 [-2241.412, -2241.412], mean action: 2.000 [2.000, 2.000],  loss: 1951503.250000, mae: 4083.418213, mean_q: -2598.656738
 4444/5000: episode: 4444, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2941.069, mean reward: -2941.069 [-2941.069, -2941.069], mean action: 2.000 [2.000, 2.000],  loss: 3494723.500000, mae: 4283.991211, mean_q: -2631.441406
 4445/5000: episode: 4445, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2467.994, mean reward: -2467.994 [-2467.994, -2467.994], mean action: 2.000 [2.000, 2.000],  loss: 4428228.000000, mae: 4351.436035, mean_q: -2629.258545
 4446/5000: episode: 4446, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1869.488, mean reward: -1869.488 [-1869.488, -1869.488], mean action: 2.000 [2.000, 2.000],  loss: 2429938.250000, mae: 4235.530273, mean_q: -2625.200684
 4447/5000: episode: 4447, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7672.752, mean reward: -7672.752 [-7672.752, -7672.752], mean action: 2.000 [2.000, 2.000],  loss: 1977216.125000, mae: 4192.444824, mean_q: -2630.460205
 4448/5000: episode: 4448, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -105.994, mean reward: -105.994 [-105.994, -105.994], mean action: 2.000 [2.000, 2.000],  loss: 1515942.375000, mae: 4067.072021, mean_q: -2639.591797
 4449/5000: episode: 4449, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -374.849, mean reward: -374.849 [-374.849, -374.849], mean action: 2.000 [2.000, 2.000],  loss: 2125241.250000, mae: 4245.541016, mean_q: -2642.470703
 4450/5000: episode: 4450, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -419.606, mean reward: -419.606 [-419.606, -419.606], mean action: 2.000 [2.000, 2.000],  loss: 2457636.500000, mae: 4260.918945, mean_q: -2637.940430
 4451/5000: episode: 4451, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3266.484, mean reward: -3266.484 [-3266.484, -3266.484], mean action: 2.000 [2.000, 2.000],  loss: 3474552.500000, mae: 4310.741211, mean_q: -2637.216064
 4452/5000: episode: 4452, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -853.866, mean reward: -853.866 [-853.866, -853.866], mean action: 2.000 [2.000, 2.000],  loss: 2223336.250000, mae: 4198.391113, mean_q: -2646.898438
 4453/5000: episode: 4453, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2146.885, mean reward: -2146.885 [-2146.885, -2146.885], mean action: 2.000 [2.000, 2.000],  loss: 2847345.750000, mae: 4254.186523, mean_q: -2657.881836
 4454/5000: episode: 4454, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -892.319, mean reward: -892.319 [-892.319, -892.319], mean action: 2.000 [2.000, 2.000],  loss: 3801376.750000, mae: 4330.268555, mean_q: -2668.123047
 4455/5000: episode: 4455, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1370.463, mean reward: -1370.463 [-1370.463, -1370.463], mean action: 2.000 [2.000, 2.000],  loss: 1678201.000000, mae: 4195.100586, mean_q: -2644.041016
 4456/5000: episode: 4456, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7474.129, mean reward: -7474.129 [-7474.129, -7474.129], mean action: 2.000 [2.000, 2.000],  loss: 2223364.500000, mae: 4213.892090, mean_q: -2639.171875
 4457/5000: episode: 4457, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7674.449, mean reward: -7674.449 [-7674.449, -7674.449], mean action: 2.000 [2.000, 2.000],  loss: 1973903.250000, mae: 4210.956055, mean_q: -2666.338867
 4458/5000: episode: 4458, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -670.901, mean reward: -670.901 [-670.901, -670.901], mean action: 2.000 [2.000, 2.000],  loss: 2186318.500000, mae: 4296.488770, mean_q: -2646.463379
 4459/5000: episode: 4459, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -9140.606, mean reward: -9140.606 [-9140.606, -9140.606], mean action: 2.000 [2.000, 2.000],  loss: 1766914.125000, mae: 4170.780273, mean_q: -2646.923828
 4460/5000: episode: 4460, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -54.994, mean reward: -54.994 [-54.994, -54.994], mean action: 2.000 [2.000, 2.000],  loss: 1381880.250000, mae: 4142.375488, mean_q: -2632.204346
 4461/5000: episode: 4461, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4541.169, mean reward: -4541.169 [-4541.169, -4541.169], mean action: 2.000 [2.000, 2.000],  loss: 3992312.500000, mae: 4385.589355, mean_q: -2661.742188
 4462/5000: episode: 4462, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1936.983, mean reward: -1936.983 [-1936.983, -1936.983], mean action: 2.000 [2.000, 2.000],  loss: 2875406.000000, mae: 4176.865723, mean_q: -2640.891602
 4463/5000: episode: 4463, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1378.144, mean reward: -1378.144 [-1378.144, -1378.144], mean action: 2.000 [2.000, 2.000],  loss: 2176925.000000, mae: 4121.169922, mean_q: -2630.666748
 4464/5000: episode: 4464, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4948.412, mean reward: -4948.412 [-4948.412, -4948.412], mean action: 2.000 [2.000, 2.000],  loss: 3370237.000000, mae: 4190.221680, mean_q: -2628.580078
 4465/5000: episode: 4465, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2184.134, mean reward: -2184.134 [-2184.134, -2184.134], mean action: 2.000 [2.000, 2.000],  loss: 1914838.250000, mae: 4171.156250, mean_q: -2626.978516
 4466/5000: episode: 4466, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2489.063, mean reward: -2489.063 [-2489.063, -2489.063], mean action: 2.000 [2.000, 2.000],  loss: 1850834.875000, mae: 4117.770996, mean_q: -2628.349121
 4467/5000: episode: 4467, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8561.009, mean reward: -8561.009 [-8561.009, -8561.009], mean action: 2.000 [2.000, 2.000],  loss: 3454282.500000, mae: 4271.894043, mean_q: -2634.210693
 4468/5000: episode: 4468, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1621.649, mean reward: -1621.649 [-1621.649, -1621.649], mean action: 2.000 [2.000, 2.000],  loss: 1608648.500000, mae: 4150.747070, mean_q: -2620.842773
 4469/5000: episode: 4469, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3534.847, mean reward: -3534.847 [-3534.847, -3534.847], mean action: 2.000 [2.000, 2.000],  loss: 3183325.250000, mae: 4251.282715, mean_q: -2631.284424
 4470/5000: episode: 4470, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4932.008, mean reward: -4932.008 [-4932.008, -4932.008], mean action: 1.000 [1.000, 1.000],  loss: 1884786.750000, mae: 4078.406494, mean_q: -2633.745117
 4471/5000: episode: 4471, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1389.174, mean reward: -1389.174 [-1389.174, -1389.174], mean action: 2.000 [2.000, 2.000],  loss: 3423433.250000, mae: 4251.001953, mean_q: -2617.955566
 4472/5000: episode: 4472, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1236.392, mean reward: -1236.392 [-1236.392, -1236.392], mean action: 2.000 [2.000, 2.000],  loss: 2707583.000000, mae: 4233.748535, mean_q: -2622.325195
 4473/5000: episode: 4473, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3919.504, mean reward: -3919.504 [-3919.504, -3919.504], mean action: 2.000 [2.000, 2.000],  loss: 4337820.500000, mae: 4277.434570, mean_q: -2625.989258
 4474/5000: episode: 4474, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1034.834, mean reward: -1034.834 [-1034.834, -1034.834], mean action: 2.000 [2.000, 2.000],  loss: 3755205.000000, mae: 4194.408203, mean_q: -2610.047852
 4475/5000: episode: 4475, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2631.387, mean reward: -2631.387 [-2631.387, -2631.387], mean action: 2.000 [2.000, 2.000],  loss: 3623008.250000, mae: 4232.577148, mean_q: -2610.183105
 4476/5000: episode: 4476, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7790.083, mean reward: -7790.083 [-7790.083, -7790.083], mean action: 2.000 [2.000, 2.000],  loss: 1703979.250000, mae: 4179.625977, mean_q: -2618.264160
 4477/5000: episode: 4477, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5545.411, mean reward: -5545.411 [-5545.411, -5545.411], mean action: 2.000 [2.000, 2.000],  loss: 2319178.750000, mae: 4187.575195, mean_q: -2639.811035
 4478/5000: episode: 4478, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -8650.156, mean reward: -8650.156 [-8650.156, -8650.156], mean action: 0.000 [0.000, 0.000],  loss: 2337857.250000, mae: 4165.336914, mean_q: -2630.354736
 4479/5000: episode: 4479, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -8846.894, mean reward: -8846.894 [-8846.894, -8846.894], mean action: 2.000 [2.000, 2.000],  loss: 2993037.000000, mae: 4250.370605, mean_q: -2641.668457
 4480/5000: episode: 4480, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -80.315, mean reward: -80.315 [-80.315, -80.315], mean action: 2.000 [2.000, 2.000],  loss: 2571691.000000, mae: 4251.107422, mean_q: -2640.264648
 4481/5000: episode: 4481, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -968.636, mean reward: -968.636 [-968.636, -968.636], mean action: 2.000 [2.000, 2.000],  loss: 3402273.000000, mae: 4261.274414, mean_q: -2635.038574
 4482/5000: episode: 4482, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5022.069, mean reward: -5022.069 [-5022.069, -5022.069], mean action: 2.000 [2.000, 2.000],  loss: 4097796.250000, mae: 4201.680176, mean_q: -2628.630127
 4483/5000: episode: 4483, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2082.683, mean reward: -2082.683 [-2082.683, -2082.683], mean action: 2.000 [2.000, 2.000],  loss: 2365298.750000, mae: 4161.450684, mean_q: -2651.274414
 4484/5000: episode: 4484, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7420.387, mean reward: -7420.387 [-7420.387, -7420.387], mean action: 2.000 [2.000, 2.000],  loss: 2592042.500000, mae: 4151.250977, mean_q: -2663.275146
 4485/5000: episode: 4485, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1450.299, mean reward: -1450.299 [-1450.299, -1450.299], mean action: 2.000 [2.000, 2.000],  loss: 2596127.500000, mae: 4254.968750, mean_q: -2671.619141
 4486/5000: episode: 4486, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1622.833, mean reward: -1622.833 [-1622.833, -1622.833], mean action: 2.000 [2.000, 2.000],  loss: 2651051.500000, mae: 4198.394043, mean_q: -2649.611328
 4487/5000: episode: 4487, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3017.170, mean reward: -3017.170 [-3017.170, -3017.170], mean action: 2.000 [2.000, 2.000],  loss: 1923420.375000, mae: 4150.812988, mean_q: -2684.752686
 4488/5000: episode: 4488, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1271.906, mean reward: -1271.906 [-1271.906, -1271.906], mean action: 2.000 [2.000, 2.000],  loss: 2200677.500000, mae: 4209.301758, mean_q: -2681.541016
 4489/5000: episode: 4489, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2623.648, mean reward: -2623.648 [-2623.648, -2623.648], mean action: 2.000 [2.000, 2.000],  loss: 3034057.000000, mae: 4155.482422, mean_q: -2666.313232
 4490/5000: episode: 4490, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -3274.561, mean reward: -3274.561 [-3274.561, -3274.561], mean action: 2.000 [2.000, 2.000],  loss: 3363837.500000, mae: 4144.462891, mean_q: -2661.977783
 4491/5000: episode: 4491, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -25.208, mean reward: -25.208 [-25.208, -25.208], mean action: 2.000 [2.000, 2.000],  loss: 3453187.500000, mae: 4239.909180, mean_q: -2681.833252
 4492/5000: episode: 4492, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -9383.819, mean reward: -9383.819 [-9383.819, -9383.819], mean action: 2.000 [2.000, 2.000],  loss: 3532279.750000, mae: 4119.812012, mean_q: -2690.104248
 4493/5000: episode: 4493, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -432.160, mean reward: -432.160 [-432.160, -432.160], mean action: 2.000 [2.000, 2.000],  loss: 2221945.500000, mae: 4153.413086, mean_q: -2679.009521
 4494/5000: episode: 4494, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -3638.840, mean reward: -3638.840 [-3638.840, -3638.840], mean action: 2.000 [2.000, 2.000],  loss: 2336803.000000, mae: 4191.628418, mean_q: -2688.196045
 4495/5000: episode: 4495, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -171.438, mean reward: -171.438 [-171.438, -171.438], mean action: 2.000 [2.000, 2.000],  loss: 4345353.500000, mae: 4224.712402, mean_q: -2685.235352
 4496/5000: episode: 4496, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1077.920, mean reward: -1077.920 [-1077.920, -1077.920], mean action: 2.000 [2.000, 2.000],  loss: 3945563.500000, mae: 4268.318359, mean_q: -2664.355957
 4497/5000: episode: 4497, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2785.215, mean reward: -2785.215 [-2785.215, -2785.215], mean action: 2.000 [2.000, 2.000],  loss: 1663370.875000, mae: 4136.857910, mean_q: -2650.187744
 4498/5000: episode: 4498, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1812.892, mean reward: -1812.892 [-1812.892, -1812.892], mean action: 2.000 [2.000, 2.000],  loss: 2009454.375000, mae: 4223.319336, mean_q: -2677.459473
 4499/5000: episode: 4499, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3762.165, mean reward: -3762.165 [-3762.165, -3762.165], mean action: 2.000 [2.000, 2.000],  loss: 2571810.500000, mae: 4159.261719, mean_q: -2677.730469
 4500/5000: episode: 4500, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1365.197, mean reward: -1365.197 [-1365.197, -1365.197], mean action: 2.000 [2.000, 2.000],  loss: 2357049.250000, mae: 4210.001953, mean_q: -2687.125977
 4501/5000: episode: 4501, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -7197.445, mean reward: -7197.445 [-7197.445, -7197.445], mean action: 2.000 [2.000, 2.000],  loss: 2985643.000000, mae: 4203.478027, mean_q: -2677.979492
 4502/5000: episode: 4502, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1312.616, mean reward: -1312.616 [-1312.616, -1312.616], mean action: 2.000 [2.000, 2.000],  loss: 2881409.500000, mae: 4282.588379, mean_q: -2696.135254
 4503/5000: episode: 4503, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2096.358, mean reward: -2096.358 [-2096.358, -2096.358], mean action: 2.000 [2.000, 2.000],  loss: 3164760.250000, mae: 4316.802734, mean_q: -2690.642334
 4504/5000: episode: 4504, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2596.990, mean reward: -2596.990 [-2596.990, -2596.990], mean action: 2.000 [2.000, 2.000],  loss: 4091171.250000, mae: 4267.004395, mean_q: -2700.855957
 4505/5000: episode: 4505, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -1210.738, mean reward: -1210.738 [-1210.738, -1210.738], mean action: 2.000 [2.000, 2.000],  loss: 2879596.000000, mae: 4238.613770, mean_q: -2726.105469
 4506/5000: episode: 4506, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -452.387, mean reward: -452.387 [-452.387, -452.387], mean action: 2.000 [2.000, 2.000],  loss: 4311383.000000, mae: 4424.322754, mean_q: -2740.265625
 4507/5000: episode: 4507, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3155.862, mean reward: -3155.862 [-3155.862, -3155.862], mean action: 2.000 [2.000, 2.000],  loss: 1489989.500000, mae: 4183.457031, mean_q: -2727.787598
 4508/5000: episode: 4508, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2189.806, mean reward: -2189.806 [-2189.806, -2189.806], mean action: 2.000 [2.000, 2.000],  loss: 2983633.000000, mae: 4259.331055, mean_q: -2748.991699
 4509/5000: episode: 4509, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -29.782, mean reward: -29.782 [-29.782, -29.782], mean action: 2.000 [2.000, 2.000],  loss: 4192220.750000, mae: 4366.851074, mean_q: -2754.809082
 4510/5000: episode: 4510, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -84.255, mean reward: -84.255 [-84.255, -84.255], mean action: 2.000 [2.000, 2.000],  loss: 2550458.500000, mae: 4269.689453, mean_q: -2754.232422
 4511/5000: episode: 4511, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1260.807, mean reward: -1260.807 [-1260.807, -1260.807], mean action: 2.000 [2.000, 2.000],  loss: 3357594.500000, mae: 4245.390625, mean_q: -2737.993652
 4512/5000: episode: 4512, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -174.722, mean reward: -174.722 [-174.722, -174.722], mean action: 2.000 [2.000, 2.000],  loss: 3805819.500000, mae: 4236.438965, mean_q: -2764.699951
 4513/5000: episode: 4513, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2651.463, mean reward: -2651.463 [-2651.463, -2651.463], mean action: 2.000 [2.000, 2.000],  loss: 3749956.500000, mae: 4294.101562, mean_q: -2765.642334
 4514/5000: episode: 4514, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1099.309, mean reward: -1099.309 [-1099.309, -1099.309], mean action: 2.000 [2.000, 2.000],  loss: 2503154.000000, mae: 4268.439941, mean_q: -2749.832764
 4515/5000: episode: 4515, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -414.613, mean reward: -414.613 [-414.613, -414.613], mean action: 2.000 [2.000, 2.000],  loss: 2287656.500000, mae: 4187.382812, mean_q: -2727.948242
 4516/5000: episode: 4516, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1124.854, mean reward: -1124.854 [-1124.854, -1124.854], mean action: 2.000 [2.000, 2.000],  loss: 1601630.125000, mae: 4098.691406, mean_q: -2715.411865
 4517/5000: episode: 4517, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1535.174, mean reward: -1535.174 [-1535.174, -1535.174], mean action: 2.000 [2.000, 2.000],  loss: 1396295.875000, mae: 4083.748535, mean_q: -2694.294922
 4518/5000: episode: 4518, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -285.515, mean reward: -285.515 [-285.515, -285.515], mean action: 2.000 [2.000, 2.000],  loss: 2615587.500000, mae: 4108.479004, mean_q: -2709.789062
 4519/5000: episode: 4519, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -3415.025, mean reward: -3415.025 [-3415.025, -3415.025], mean action: 2.000 [2.000, 2.000],  loss: 2056290.250000, mae: 4096.535156, mean_q: -2689.305176
 4520/5000: episode: 4520, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1072.076, mean reward: -1072.076 [-1072.076, -1072.076], mean action: 2.000 [2.000, 2.000],  loss: 1890400.250000, mae: 4099.628418, mean_q: -2674.395996
 4521/5000: episode: 4521, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -901.071, mean reward: -901.071 [-901.071, -901.071], mean action: 2.000 [2.000, 2.000],  loss: 2845676.500000, mae: 4255.205566, mean_q: -2681.283936
 4522/5000: episode: 4522, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4340.647, mean reward: -4340.647 [-4340.647, -4340.647], mean action: 2.000 [2.000, 2.000],  loss: 2867924.750000, mae: 4239.001953, mean_q: -2651.172852
 4523/5000: episode: 4523, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1770.438, mean reward: -1770.438 [-1770.438, -1770.438], mean action: 2.000 [2.000, 2.000],  loss: 2410224.250000, mae: 4153.678711, mean_q: -2631.625488
 4524/5000: episode: 4524, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -6904.164, mean reward: -6904.164 [-6904.164, -6904.164], mean action: 3.000 [3.000, 3.000],  loss: 2686332.500000, mae: 4192.054688, mean_q: -2629.837402
 4525/5000: episode: 4525, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7549.034, mean reward: -7549.034 [-7549.034, -7549.034], mean action: 3.000 [3.000, 3.000],  loss: 2185884.750000, mae: 4205.107910, mean_q: -2619.603516
 4526/5000: episode: 4526, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -972.308, mean reward: -972.308 [-972.308, -972.308], mean action: 2.000 [2.000, 2.000],  loss: 2576443.250000, mae: 4108.269531, mean_q: -2595.515625
 4527/5000: episode: 4527, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -59.944, mean reward: -59.944 [-59.944, -59.944], mean action: 2.000 [2.000, 2.000],  loss: 2562800.250000, mae: 4140.855469, mean_q: -2579.631592
 4528/5000: episode: 4528, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3249.383, mean reward: -3249.383 [-3249.383, -3249.383], mean action: 2.000 [2.000, 2.000],  loss: 2659558.500000, mae: 4033.857666, mean_q: -2580.610352
 4529/5000: episode: 4529, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2905.293, mean reward: -2905.293 [-2905.293, -2905.293], mean action: 2.000 [2.000, 2.000],  loss: 3288908.750000, mae: 4142.932617, mean_q: -2605.986816
 4530/5000: episode: 4530, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1392.023, mean reward: -1392.023 [-1392.023, -1392.023], mean action: 2.000 [2.000, 2.000],  loss: 2927950.000000, mae: 4114.106934, mean_q: -2596.374756
 4531/5000: episode: 4531, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2917.636, mean reward: -2917.636 [-2917.636, -2917.636], mean action: 2.000 [2.000, 2.000],  loss: 3758533.500000, mae: 4191.392578, mean_q: -2589.484863
 4532/5000: episode: 4532, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2377.793, mean reward: -2377.793 [-2377.793, -2377.793], mean action: 2.000 [2.000, 2.000],  loss: 1693542.750000, mae: 4035.852051, mean_q: -2612.934814
 4533/5000: episode: 4533, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3407.624, mean reward: -3407.624 [-3407.624, -3407.624], mean action: 2.000 [2.000, 2.000],  loss: 2957632.000000, mae: 4127.893066, mean_q: -2616.662354
 4534/5000: episode: 4534, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1962.228, mean reward: -1962.228 [-1962.228, -1962.228], mean action: 2.000 [2.000, 2.000],  loss: 2852476.500000, mae: 4120.112305, mean_q: -2609.805664
 4535/5000: episode: 4535, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5395.174, mean reward: -5395.174 [-5395.174, -5395.174], mean action: 2.000 [2.000, 2.000],  loss: 2169568.000000, mae: 4044.725586, mean_q: -2635.835938
 4536/5000: episode: 4536, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3897.122, mean reward: -3897.122 [-3897.122, -3897.122], mean action: 2.000 [2.000, 2.000],  loss: 2002504.625000, mae: 4135.461914, mean_q: -2640.813965
 4537/5000: episode: 4537, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -894.519, mean reward: -894.519 [-894.519, -894.519], mean action: 2.000 [2.000, 2.000],  loss: 3518845.750000, mae: 4272.179688, mean_q: -2626.037109
 4538/5000: episode: 4538, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -662.744, mean reward: -662.744 [-662.744, -662.744], mean action: 2.000 [2.000, 2.000],  loss: 2712618.250000, mae: 4202.368652, mean_q: -2635.910156
 4539/5000: episode: 4539, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1388.098, mean reward: -1388.098 [-1388.098, -1388.098], mean action: 2.000 [2.000, 2.000],  loss: 3286727.500000, mae: 4308.754883, mean_q: -2663.181885
 4540/5000: episode: 4540, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1144.876, mean reward: -1144.876 [-1144.876, -1144.876], mean action: 2.000 [2.000, 2.000],  loss: 1907797.375000, mae: 4150.613281, mean_q: -2652.234863
 4541/5000: episode: 4541, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4722.590, mean reward: -4722.590 [-4722.590, -4722.590], mean action: 2.000 [2.000, 2.000],  loss: 2582623.000000, mae: 4200.533203, mean_q: -2651.984863
 4542/5000: episode: 4542, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -550.451, mean reward: -550.451 [-550.451, -550.451], mean action: 2.000 [2.000, 2.000],  loss: 2387851.500000, mae: 4173.452637, mean_q: -2660.273926
 4543/5000: episode: 4543, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -3774.141, mean reward: -3774.141 [-3774.141, -3774.141], mean action: 2.000 [2.000, 2.000],  loss: 2138433.250000, mae: 4178.813965, mean_q: -2651.762939
 4544/5000: episode: 4544, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3911.866, mean reward: -3911.866 [-3911.866, -3911.866], mean action: 2.000 [2.000, 2.000],  loss: 5746448.000000, mae: 4391.156250, mean_q: -2671.678223
 4545/5000: episode: 4545, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -164.817, mean reward: -164.817 [-164.817, -164.817], mean action: 2.000 [2.000, 2.000],  loss: 2301220.000000, mae: 4185.642578, mean_q: -2679.933594
 4546/5000: episode: 4546, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1174.701, mean reward: -1174.701 [-1174.701, -1174.701], mean action: 2.000 [2.000, 2.000],  loss: 3627027.750000, mae: 4366.756836, mean_q: -2674.874512
 4547/5000: episode: 4547, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -913.750, mean reward: -913.750 [-913.750, -913.750], mean action: 2.000 [2.000, 2.000],  loss: 2336189.500000, mae: 4236.482422, mean_q: -2686.306152
 4548/5000: episode: 4548, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -179.139, mean reward: -179.139 [-179.139, -179.139], mean action: 2.000 [2.000, 2.000],  loss: 2489719.750000, mae: 4256.300293, mean_q: -2680.634766
 4549/5000: episode: 4549, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2413.910, mean reward: -2413.910 [-2413.910, -2413.910], mean action: 2.000 [2.000, 2.000],  loss: 3026583.250000, mae: 4300.056152, mean_q: -2667.188965
 4550/5000: episode: 4550, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -631.828, mean reward: -631.828 [-631.828, -631.828], mean action: 2.000 [2.000, 2.000],  loss: 2300175.000000, mae: 4193.741211, mean_q: -2643.183594
 4551/5000: episode: 4551, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5274.148, mean reward: -5274.148 [-5274.148, -5274.148], mean action: 2.000 [2.000, 2.000],  loss: 2111252.000000, mae: 4187.862305, mean_q: -2656.175049
 4552/5000: episode: 4552, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1333.161, mean reward: -1333.161 [-1333.161, -1333.161], mean action: 2.000 [2.000, 2.000],  loss: 4174190.250000, mae: 4356.282227, mean_q: -2647.578613
 4553/5000: episode: 4553, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -486.433, mean reward: -486.433 [-486.433, -486.433], mean action: 2.000 [2.000, 2.000],  loss: 2490274.500000, mae: 4226.293457, mean_q: -2608.530273
 4554/5000: episode: 4554, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -186.848, mean reward: -186.848 [-186.848, -186.848], mean action: 2.000 [2.000, 2.000],  loss: 3971726.000000, mae: 4206.280273, mean_q: -2579.748535
 4555/5000: episode: 4555, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5322.752, mean reward: -5322.752 [-5322.752, -5322.752], mean action: 2.000 [2.000, 2.000],  loss: 2837561.000000, mae: 4168.435547, mean_q: -2598.111572
 4556/5000: episode: 4556, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -303.973, mean reward: -303.973 [-303.973, -303.973], mean action: 2.000 [2.000, 2.000],  loss: 1419032.000000, mae: 4112.746094, mean_q: -2565.188477
 4557/5000: episode: 4557, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1027.501, mean reward: -1027.501 [-1027.501, -1027.501], mean action: 2.000 [2.000, 2.000],  loss: 2949903.500000, mae: 4226.533691, mean_q: -2541.926514
 4558/5000: episode: 4558, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4160.540, mean reward: -4160.540 [-4160.540, -4160.540], mean action: 2.000 [2.000, 2.000],  loss: 2927159.500000, mae: 4196.425293, mean_q: -2532.635254
 4559/5000: episode: 4559, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2830.160, mean reward: -2830.160 [-2830.160, -2830.160], mean action: 2.000 [2.000, 2.000],  loss: 3282807.750000, mae: 4203.708984, mean_q: -2526.825684
 4560/5000: episode: 4560, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -832.830, mean reward: -832.830 [-832.830, -832.830], mean action: 2.000 [2.000, 2.000],  loss: 2118057.500000, mae: 4066.378906, mean_q: -2532.947266
 4561/5000: episode: 4561, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -233.147, mean reward: -233.147 [-233.147, -233.147], mean action: 2.000 [2.000, 2.000],  loss: 3902354.250000, mae: 4156.429199, mean_q: -2521.340576
 4562/5000: episode: 4562, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -661.609, mean reward: -661.609 [-661.609, -661.609], mean action: 2.000 [2.000, 2.000],  loss: 2452247.250000, mae: 3970.824951, mean_q: -2536.195312
 4563/5000: episode: 4563, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2994.772, mean reward: -2994.772 [-2994.772, -2994.772], mean action: 2.000 [2.000, 2.000],  loss: 2429591.250000, mae: 4049.547852, mean_q: -2534.243164
 4564/5000: episode: 4564, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3680.010, mean reward: -3680.010 [-3680.010, -3680.010], mean action: 2.000 [2.000, 2.000],  loss: 2619630.750000, mae: 4109.794922, mean_q: -2527.484863
 4565/5000: episode: 4565, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2408.967, mean reward: -2408.967 [-2408.967, -2408.967], mean action: 2.000 [2.000, 2.000],  loss: 2451994.750000, mae: 4136.635742, mean_q: -2539.433594
 4566/5000: episode: 4566, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2388.721, mean reward: -2388.721 [-2388.721, -2388.721], mean action: 2.000 [2.000, 2.000],  loss: 2228523.500000, mae: 3993.209961, mean_q: -2543.989014
 4567/5000: episode: 4567, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1484.031, mean reward: -1484.031 [-1484.031, -1484.031], mean action: 2.000 [2.000, 2.000],  loss: 2256190.000000, mae: 4066.930664, mean_q: -2535.296875
 4568/5000: episode: 4568, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -54.901, mean reward: -54.901 [-54.901, -54.901], mean action: 2.000 [2.000, 2.000],  loss: 1999718.375000, mae: 4042.788330, mean_q: -2538.114502
 4569/5000: episode: 4569, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3512.733, mean reward: -3512.733 [-3512.733, -3512.733], mean action: 2.000 [2.000, 2.000],  loss: 4248314.000000, mae: 4284.063477, mean_q: -2552.418457
 4570/5000: episode: 4570, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1219.503, mean reward: -1219.503 [-1219.503, -1219.503], mean action: 2.000 [2.000, 2.000],  loss: 4438482.000000, mae: 4277.428223, mean_q: -2559.707520
 4571/5000: episode: 4571, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4001.103, mean reward: -4001.103 [-4001.103, -4001.103], mean action: 2.000 [2.000, 2.000],  loss: 2138506.250000, mae: 4065.125977, mean_q: -2578.050049
 4572/5000: episode: 4572, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -2571.027, mean reward: -2571.027 [-2571.027, -2571.027], mean action: 2.000 [2.000, 2.000],  loss: 3449446.500000, mae: 4185.105469, mean_q: -2587.879883
 4573/5000: episode: 4573, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -818.529, mean reward: -818.529 [-818.529, -818.529], mean action: 2.000 [2.000, 2.000],  loss: 3406389.000000, mae: 4178.891113, mean_q: -2594.611328
 4574/5000: episode: 4574, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2021.929, mean reward: -2021.929 [-2021.929, -2021.929], mean action: 2.000 [2.000, 2.000],  loss: 2252133.500000, mae: 4136.083496, mean_q: -2608.311279
 4575/5000: episode: 4575, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8325.023, mean reward: -8325.023 [-8325.023, -8325.023], mean action: 2.000 [2.000, 2.000],  loss: 3432705.500000, mae: 4221.138672, mean_q: -2618.396973
 4576/5000: episode: 4576, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1257.299, mean reward: -1257.299 [-1257.299, -1257.299], mean action: 2.000 [2.000, 2.000],  loss: 2442282.000000, mae: 4088.836426, mean_q: -2607.225098
 4577/5000: episode: 4577, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -57.333, mean reward: -57.333 [-57.333, -57.333], mean action: 2.000 [2.000, 2.000],  loss: 2351386.000000, mae: 4117.709473, mean_q: -2621.699707
 4578/5000: episode: 4578, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3394.537, mean reward: -3394.537 [-3394.537, -3394.537], mean action: 2.000 [2.000, 2.000],  loss: 2726031.750000, mae: 4059.925537, mean_q: -2647.296387
 4579/5000: episode: 4579, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2787.036, mean reward: -2787.036 [-2787.036, -2787.036], mean action: 2.000 [2.000, 2.000],  loss: 1734773.750000, mae: 4112.806641, mean_q: -2650.800781
 4580/5000: episode: 4580, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3032.982, mean reward: -3032.982 [-3032.982, -3032.982], mean action: 2.000 [2.000, 2.000],  loss: 2210658.000000, mae: 4137.692871, mean_q: -2672.799316
 4581/5000: episode: 4581, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5207.745, mean reward: -5207.745 [-5207.745, -5207.745], mean action: 2.000 [2.000, 2.000],  loss: 2306244.750000, mae: 4078.607422, mean_q: -2640.831543
 4582/5000: episode: 4582, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3651.849, mean reward: -3651.849 [-3651.849, -3651.849], mean action: 2.000 [2.000, 2.000],  loss: 2384209.750000, mae: 4081.412354, mean_q: -2657.598389
 4583/5000: episode: 4583, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2994.721, mean reward: -2994.721 [-2994.721, -2994.721], mean action: 2.000 [2.000, 2.000],  loss: 2227612.250000, mae: 4083.843506, mean_q: -2643.241211
 4584/5000: episode: 4584, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1715.000, mean reward: -1715.000 [-1715.000, -1715.000], mean action: 2.000 [2.000, 2.000],  loss: 5175550.000000, mae: 4343.479980, mean_q: -2669.079102
 4585/5000: episode: 4585, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -997.105, mean reward: -997.105 [-997.105, -997.105], mean action: 2.000 [2.000, 2.000],  loss: 3478641.250000, mae: 4146.449707, mean_q: -2670.144043
 4586/5000: episode: 4586, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1013.210, mean reward: -1013.210 [-1013.210, -1013.210], mean action: 2.000 [2.000, 2.000],  loss: 1814929.750000, mae: 4076.484619, mean_q: -2682.868652
 4587/5000: episode: 4587, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2030.269, mean reward: -2030.269 [-2030.269, -2030.269], mean action: 2.000 [2.000, 2.000],  loss: 3299541.000000, mae: 4123.648438, mean_q: -2679.635254
 4588/5000: episode: 4588, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1031.198, mean reward: -1031.198 [-1031.198, -1031.198], mean action: 2.000 [2.000, 2.000],  loss: 1550195.375000, mae: 4067.692383, mean_q: -2700.579346
 4589/5000: episode: 4589, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4938.326, mean reward: -4938.326 [-4938.326, -4938.326], mean action: 2.000 [2.000, 2.000],  loss: 3288466.500000, mae: 4163.116211, mean_q: -2692.277344
 4590/5000: episode: 4590, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2740.718, mean reward: -2740.718 [-2740.718, -2740.718], mean action: 2.000 [2.000, 2.000],  loss: 3666289.000000, mae: 4189.585938, mean_q: -2668.433838
 4591/5000: episode: 4591, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2478.923, mean reward: -2478.923 [-2478.923, -2478.923], mean action: 2.000 [2.000, 2.000],  loss: 2803404.750000, mae: 4176.162109, mean_q: -2700.294434
 4592/5000: episode: 4592, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1471.036, mean reward: -1471.036 [-1471.036, -1471.036], mean action: 2.000 [2.000, 2.000],  loss: 2934622.000000, mae: 4145.830078, mean_q: -2685.904785
 4593/5000: episode: 4593, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1246.173, mean reward: -1246.173 [-1246.173, -1246.173], mean action: 2.000 [2.000, 2.000],  loss: 2277001.250000, mae: 4090.641846, mean_q: -2660.037842
 4594/5000: episode: 4594, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -768.963, mean reward: -768.963 [-768.963, -768.963], mean action: 2.000 [2.000, 2.000],  loss: 4717560.500000, mae: 4281.756836, mean_q: -2690.538574
 4595/5000: episode: 4595, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -11779.502, mean reward: -11779.502 [-11779.502, -11779.502], mean action: 2.000 [2.000, 2.000],  loss: 2715780.000000, mae: 4167.729980, mean_q: -2670.257324
 4596/5000: episode: 4596, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8308.253, mean reward: -8308.253 [-8308.253, -8308.253], mean action: 2.000 [2.000, 2.000],  loss: 2835159.000000, mae: 4138.733398, mean_q: -2690.704346
 4597/5000: episode: 4597, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2382.131, mean reward: -2382.131 [-2382.131, -2382.131], mean action: 3.000 [3.000, 3.000],  loss: 3044073.250000, mae: 4160.341797, mean_q: -2682.352051
 4598/5000: episode: 4598, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8345.917, mean reward: -8345.917 [-8345.917, -8345.917], mean action: 2.000 [2.000, 2.000],  loss: 2841329.000000, mae: 4197.862305, mean_q: -2667.595459
 4599/5000: episode: 4599, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2376.983, mean reward: -2376.983 [-2376.983, -2376.983], mean action: 2.000 [2.000, 2.000],  loss: 1635188.250000, mae: 4012.185547, mean_q: -2678.121338
 4600/5000: episode: 4600, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -667.597, mean reward: -667.597 [-667.597, -667.597], mean action: 2.000 [2.000, 2.000],  loss: 3053355.500000, mae: 4154.777832, mean_q: -2684.260254
 4601/5000: episode: 4601, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2995.327, mean reward: -2995.327 [-2995.327, -2995.327], mean action: 2.000 [2.000, 2.000],  loss: 1919229.125000, mae: 3964.964111, mean_q: -2671.451660
 4602/5000: episode: 4602, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -1032.806, mean reward: -1032.806 [-1032.806, -1032.806], mean action: 2.000 [2.000, 2.000],  loss: 3349768.250000, mae: 4128.783203, mean_q: -2683.516113
 4603/5000: episode: 4603, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -452.103, mean reward: -452.103 [-452.103, -452.103], mean action: 2.000 [2.000, 2.000],  loss: 2609158.500000, mae: 4123.625977, mean_q: -2657.909668
 4604/5000: episode: 4604, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -518.276, mean reward: -518.276 [-518.276, -518.276], mean action: 2.000 [2.000, 2.000],  loss: 4474418.000000, mae: 4263.600098, mean_q: -2677.950684
 4605/5000: episode: 4605, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1950.138, mean reward: -1950.138 [-1950.138, -1950.138], mean action: 2.000 [2.000, 2.000],  loss: 4808546.000000, mae: 4273.593262, mean_q: -2664.897949
 4606/5000: episode: 4606, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -95.212, mean reward: -95.212 [-95.212, -95.212], mean action: 2.000 [2.000, 2.000],  loss: 1791678.125000, mae: 4041.246094, mean_q: -2649.204346
 4607/5000: episode: 4607, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3908.947, mean reward: -3908.947 [-3908.947, -3908.947], mean action: 2.000 [2.000, 2.000],  loss: 2716521.000000, mae: 4112.599121, mean_q: -2657.186279
 4608/5000: episode: 4608, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -393.249, mean reward: -393.249 [-393.249, -393.249], mean action: 2.000 [2.000, 2.000],  loss: 2352978.500000, mae: 3984.458984, mean_q: -2675.195801
 4609/5000: episode: 4609, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2783.840, mean reward: -2783.840 [-2783.840, -2783.840], mean action: 2.000 [2.000, 2.000],  loss: 3795722.750000, mae: 4238.161133, mean_q: -2670.643799
 4610/5000: episode: 4610, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1548.638, mean reward: -1548.638 [-1548.638, -1548.638], mean action: 2.000 [2.000, 2.000],  loss: 3099881.750000, mae: 4089.082031, mean_q: -2675.843506
 4611/5000: episode: 4611, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -91.649, mean reward: -91.649 [-91.649, -91.649], mean action: 2.000 [2.000, 2.000],  loss: 1754495.250000, mae: 4031.856201, mean_q: -2670.829102
 4612/5000: episode: 4612, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1013.550, mean reward: -1013.550 [-1013.550, -1013.550], mean action: 2.000 [2.000, 2.000],  loss: 3340387.750000, mae: 4132.113770, mean_q: -2679.951172
 4613/5000: episode: 4613, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1630.572, mean reward: -1630.572 [-1630.572, -1630.572], mean action: 0.000 [0.000, 0.000],  loss: 2319627.500000, mae: 4056.805176, mean_q: -2651.460938
 4614/5000: episode: 4614, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -532.612, mean reward: -532.612 [-532.612, -532.612], mean action: 2.000 [2.000, 2.000],  loss: 3652568.500000, mae: 4167.003906, mean_q: -2657.105957
 4615/5000: episode: 4615, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -510.979, mean reward: -510.979 [-510.979, -510.979], mean action: 2.000 [2.000, 2.000],  loss: 3935454.500000, mae: 4186.585938, mean_q: -2669.384277
 4616/5000: episode: 4616, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -6796.156, mean reward: -6796.156 [-6796.156, -6796.156], mean action: 2.000 [2.000, 2.000],  loss: 2843664.500000, mae: 4065.311035, mean_q: -2653.295654
 4617/5000: episode: 4617, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1377.951, mean reward: -1377.951 [-1377.951, -1377.951], mean action: 2.000 [2.000, 2.000],  loss: 2100018.250000, mae: 4032.512207, mean_q: -2647.049316
 4618/5000: episode: 4618, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1478.276, mean reward: -1478.276 [-1478.276, -1478.276], mean action: 2.000 [2.000, 2.000],  loss: 3663727.750000, mae: 4178.765625, mean_q: -2659.914551
 4619/5000: episode: 4619, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -874.016, mean reward: -874.016 [-874.016, -874.016], mean action: 2.000 [2.000, 2.000],  loss: 3532203.500000, mae: 4136.329590, mean_q: -2655.416992
 4620/5000: episode: 4620, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -599.314, mean reward: -599.314 [-599.314, -599.314], mean action: 2.000 [2.000, 2.000],  loss: 3227891.000000, mae: 4221.140625, mean_q: -2683.646729
 4621/5000: episode: 4621, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1047.057, mean reward: -1047.057 [-1047.057, -1047.057], mean action: 2.000 [2.000, 2.000],  loss: 2752399.000000, mae: 4172.090332, mean_q: -2677.888916
 4622/5000: episode: 4622, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2652.970, mean reward: -2652.970 [-2652.970, -2652.970], mean action: 0.000 [0.000, 0.000],  loss: 1759745.750000, mae: 4079.793945, mean_q: -2683.905273
 4623/5000: episode: 4623, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1129.410, mean reward: -1129.410 [-1129.410, -1129.410], mean action: 2.000 [2.000, 2.000],  loss: 2752497.500000, mae: 4093.021973, mean_q: -2671.178223
 4624/5000: episode: 4624, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5721.054, mean reward: -5721.054 [-5721.054, -5721.054], mean action: 2.000 [2.000, 2.000],  loss: 3308682.000000, mae: 4209.779297, mean_q: -2684.821289
 4625/5000: episode: 4625, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1220.082, mean reward: -1220.082 [-1220.082, -1220.082], mean action: 2.000 [2.000, 2.000],  loss: 4505580.000000, mae: 4274.319336, mean_q: -2690.737793
 4626/5000: episode: 4626, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -989.480, mean reward: -989.480 [-989.480, -989.480], mean action: 3.000 [3.000, 3.000],  loss: 3558651.500000, mae: 4268.597656, mean_q: -2670.262207
 4627/5000: episode: 4627, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1011.861, mean reward: -1011.861 [-1011.861, -1011.861], mean action: 2.000 [2.000, 2.000],  loss: 3349403.500000, mae: 4183.038086, mean_q: -2695.543945
 4628/5000: episode: 4628, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1099.462, mean reward: -1099.462 [-1099.462, -1099.462], mean action: 2.000 [2.000, 2.000],  loss: 2502778.000000, mae: 4230.333984, mean_q: -2693.466309
 4629/5000: episode: 4629, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4301.511, mean reward: -4301.511 [-4301.511, -4301.511], mean action: 2.000 [2.000, 2.000],  loss: 3247033.250000, mae: 4274.742188, mean_q: -2692.853027
 4630/5000: episode: 4630, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2544.894, mean reward: -2544.894 [-2544.894, -2544.894], mean action: 2.000 [2.000, 2.000],  loss: 2980207.250000, mae: 4221.531250, mean_q: -2702.705566
 4631/5000: episode: 4631, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1559.593, mean reward: -1559.593 [-1559.593, -1559.593], mean action: 2.000 [2.000, 2.000],  loss: 1838530.375000, mae: 4186.476562, mean_q: -2710.960938
 4632/5000: episode: 4632, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -874.551, mean reward: -874.551 [-874.551, -874.551], mean action: 2.000 [2.000, 2.000],  loss: 3029778.500000, mae: 4216.591309, mean_q: -2716.560547
 4633/5000: episode: 4633, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2777.260, mean reward: -2777.260 [-2777.260, -2777.260], mean action: 2.000 [2.000, 2.000],  loss: 1299053.000000, mae: 4115.958984, mean_q: -2714.142822
 4634/5000: episode: 4634, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2429.653, mean reward: -2429.653 [-2429.653, -2429.653], mean action: 2.000 [2.000, 2.000],  loss: 3101379.500000, mae: 4276.655762, mean_q: -2719.711426
 4635/5000: episode: 4635, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1656.141, mean reward: -1656.141 [-1656.141, -1656.141], mean action: 2.000 [2.000, 2.000],  loss: 4057758.000000, mae: 4336.357910, mean_q: -2728.412842
 4636/5000: episode: 4636, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -464.596, mean reward: -464.596 [-464.596, -464.596], mean action: 2.000 [2.000, 2.000],  loss: 3146787.500000, mae: 4306.249023, mean_q: -2718.528809
 4637/5000: episode: 4637, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4103.630, mean reward: -4103.630 [-4103.630, -4103.630], mean action: 2.000 [2.000, 2.000],  loss: 3073685.500000, mae: 4316.514648, mean_q: -2727.615479
 4638/5000: episode: 4638, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2809.999, mean reward: -2809.999 [-2809.999, -2809.999], mean action: 3.000 [3.000, 3.000],  loss: 2985050.500000, mae: 4297.334961, mean_q: -2736.145264
 4639/5000: episode: 4639, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -636.352, mean reward: -636.352 [-636.352, -636.352], mean action: 2.000 [2.000, 2.000],  loss: 2426417.250000, mae: 4294.414551, mean_q: -2728.098877
 4640/5000: episode: 4640, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5837.863, mean reward: -5837.863 [-5837.863, -5837.863], mean action: 2.000 [2.000, 2.000],  loss: 1816604.875000, mae: 4136.810547, mean_q: -2707.588379
 4641/5000: episode: 4641, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -906.763, mean reward: -906.763 [-906.763, -906.763], mean action: 2.000 [2.000, 2.000],  loss: 4734820.000000, mae: 4281.408203, mean_q: -2721.423828
 4642/5000: episode: 4642, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3170.556, mean reward: -3170.556 [-3170.556, -3170.556], mean action: 2.000 [2.000, 2.000],  loss: 2657570.500000, mae: 4242.687988, mean_q: -2720.788818
 4643/5000: episode: 4643, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2259.166, mean reward: -2259.166 [-2259.166, -2259.166], mean action: 2.000 [2.000, 2.000],  loss: 1555888.500000, mae: 4192.030762, mean_q: -2716.818848
 4644/5000: episode: 4644, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3391.721, mean reward: -3391.721 [-3391.721, -3391.721], mean action: 2.000 [2.000, 2.000],  loss: 1691296.250000, mae: 4093.620850, mean_q: -2678.509277
 4645/5000: episode: 4645, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2422.772, mean reward: -2422.772 [-2422.772, -2422.772], mean action: 2.000 [2.000, 2.000],  loss: 3502299.250000, mae: 4246.175293, mean_q: -2686.698486
 4646/5000: episode: 4646, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -4119.457, mean reward: -4119.457 [-4119.457, -4119.457], mean action: 2.000 [2.000, 2.000],  loss: 2024743.375000, mae: 4074.627441, mean_q: -2693.751465
 4647/5000: episode: 4647, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1798.332, mean reward: -1798.332 [-1798.332, -1798.332], mean action: 2.000 [2.000, 2.000],  loss: 1550291.500000, mae: 4151.143066, mean_q: -2665.071777
 4648/5000: episode: 4648, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -1850.593, mean reward: -1850.593 [-1850.593, -1850.593], mean action: 2.000 [2.000, 2.000],  loss: 3056799.000000, mae: 4301.488770, mean_q: -2672.089844
 4649/5000: episode: 4649, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2198.091, mean reward: -2198.091 [-2198.091, -2198.091], mean action: 2.000 [2.000, 2.000],  loss: 2920640.750000, mae: 4248.793945, mean_q: -2666.392090
 4650/5000: episode: 4650, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -8098.491, mean reward: -8098.491 [-8098.491, -8098.491], mean action: 2.000 [2.000, 2.000],  loss: 3396467.750000, mae: 4325.138672, mean_q: -2672.062256
 4651/5000: episode: 4651, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5012.867, mean reward: -5012.867 [-5012.867, -5012.867], mean action: 2.000 [2.000, 2.000],  loss: 2428226.500000, mae: 4087.219238, mean_q: -2670.695557
 4652/5000: episode: 4652, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1954.959, mean reward: -1954.959 [-1954.959, -1954.959], mean action: 2.000 [2.000, 2.000],  loss: 1875389.000000, mae: 4168.795898, mean_q: -2674.874512
 4653/5000: episode: 4653, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3356.777, mean reward: -3356.777 [-3356.777, -3356.777], mean action: 2.000 [2.000, 2.000],  loss: 3041780.750000, mae: 4307.093750, mean_q: -2682.345215
 4654/5000: episode: 4654, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1963.867, mean reward: -1963.867 [-1963.867, -1963.867], mean action: 2.000 [2.000, 2.000],  loss: 1543145.750000, mae: 4118.791016, mean_q: -2671.637451
 4655/5000: episode: 4655, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4947.508, mean reward: -4947.508 [-4947.508, -4947.508], mean action: 2.000 [2.000, 2.000],  loss: 2213768.250000, mae: 4152.822266, mean_q: -2685.509277
 4656/5000: episode: 4656, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2625.835, mean reward: -2625.835 [-2625.835, -2625.835], mean action: 2.000 [2.000, 2.000],  loss: 2213105.250000, mae: 4159.025391, mean_q: -2679.641846
 4657/5000: episode: 4657, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -754.710, mean reward: -754.710 [-754.710, -754.710], mean action: 2.000 [2.000, 2.000],  loss: 1782097.000000, mae: 4120.311523, mean_q: -2658.943359
 4658/5000: episode: 4658, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1112.513, mean reward: -1112.513 [-1112.513, -1112.513], mean action: 2.000 [2.000, 2.000],  loss: 2902597.500000, mae: 4288.369141, mean_q: -2701.907959
 4659/5000: episode: 4659, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -956.753, mean reward: -956.753 [-956.753, -956.753], mean action: 2.000 [2.000, 2.000],  loss: 1990352.500000, mae: 4156.683105, mean_q: -2679.525879
 4660/5000: episode: 4660, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3712.432, mean reward: -3712.432 [-3712.432, -3712.432], mean action: 2.000 [2.000, 2.000],  loss: 1450389.750000, mae: 4128.341797, mean_q: -2675.355469
 4661/5000: episode: 4661, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1521.870, mean reward: -1521.870 [-1521.870, -1521.870], mean action: 2.000 [2.000, 2.000],  loss: 3896736.000000, mae: 4259.458008, mean_q: -2682.712891
 4662/5000: episode: 4662, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -191.543, mean reward: -191.543 [-191.543, -191.543], mean action: 2.000 [2.000, 2.000],  loss: 2044469.375000, mae: 4181.204102, mean_q: -2668.311035
 4663/5000: episode: 4663, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2406.072, mean reward: -2406.072 [-2406.072, -2406.072], mean action: 2.000 [2.000, 2.000],  loss: 1889942.375000, mae: 4138.093750, mean_q: -2688.418701
 4664/5000: episode: 4664, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1544.864, mean reward: -1544.864 [-1544.864, -1544.864], mean action: 2.000 [2.000, 2.000],  loss: 2490283.500000, mae: 4107.141602, mean_q: -2681.192383
 4665/5000: episode: 4665, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2798.144, mean reward: -2798.144 [-2798.144, -2798.144], mean action: 2.000 [2.000, 2.000],  loss: 2381776.000000, mae: 4183.391113, mean_q: -2686.351318
 4666/5000: episode: 4666, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -172.279, mean reward: -172.279 [-172.279, -172.279], mean action: 2.000 [2.000, 2.000],  loss: 2303777.000000, mae: 4172.077148, mean_q: -2696.292480
 4667/5000: episode: 4667, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -326.554, mean reward: -326.554 [-326.554, -326.554], mean action: 2.000 [2.000, 2.000],  loss: 3225661.750000, mae: 4334.737305, mean_q: -2691.116455
 4668/5000: episode: 4668, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -107.419, mean reward: -107.419 [-107.419, -107.419], mean action: 2.000 [2.000, 2.000],  loss: 2880607.000000, mae: 4242.620117, mean_q: -2702.557373
 4669/5000: episode: 4669, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -65.966, mean reward: -65.966 [-65.966, -65.966], mean action: 2.000 [2.000, 2.000],  loss: 1537801.500000, mae: 4199.287109, mean_q: -2706.269043
 4670/5000: episode: 4670, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1555.944, mean reward: -1555.944 [-1555.944, -1555.944], mean action: 2.000 [2.000, 2.000],  loss: 2843505.500000, mae: 4179.579102, mean_q: -2699.153809
 4671/5000: episode: 4671, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2044.053, mean reward: -2044.053 [-2044.053, -2044.053], mean action: 2.000 [2.000, 2.000],  loss: 3269589.750000, mae: 4341.929688, mean_q: -2717.774414
 4672/5000: episode: 4672, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2511.829, mean reward: -2511.829 [-2511.829, -2511.829], mean action: 2.000 [2.000, 2.000],  loss: 2457324.500000, mae: 4262.036133, mean_q: -2706.296387
 4673/5000: episode: 4673, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2834.272, mean reward: -2834.272 [-2834.272, -2834.272], mean action: 2.000 [2.000, 2.000],  loss: 2130732.500000, mae: 4219.238281, mean_q: -2681.200684
 4674/5000: episode: 4674, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6832.229, mean reward: -6832.229 [-6832.229, -6832.229], mean action: 0.000 [0.000, 0.000],  loss: 4679366.000000, mae: 4371.069336, mean_q: -2674.185547
 4675/5000: episode: 4675, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2766.284, mean reward: -2766.284 [-2766.284, -2766.284], mean action: 2.000 [2.000, 2.000],  loss: 2036451.125000, mae: 4227.807617, mean_q: -2675.395020
 4676/5000: episode: 4676, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3447.504, mean reward: -3447.504 [-3447.504, -3447.504], mean action: 3.000 [3.000, 3.000],  loss: 3476460.000000, mae: 4218.357422, mean_q: -2668.205566
 4677/5000: episode: 4677, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4397.526, mean reward: -4397.526 [-4397.526, -4397.526], mean action: 2.000 [2.000, 2.000],  loss: 2242152.750000, mae: 4235.145020, mean_q: -2663.038574
 4678/5000: episode: 4678, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2863.840, mean reward: -2863.840 [-2863.840, -2863.840], mean action: 2.000 [2.000, 2.000],  loss: 2783345.250000, mae: 4195.118164, mean_q: -2687.126465
 4679/5000: episode: 4679, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -99.400, mean reward: -99.400 [-99.400, -99.400], mean action: 2.000 [2.000, 2.000],  loss: 2735986.250000, mae: 4202.062500, mean_q: -2675.895996
 4680/5000: episode: 4680, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -333.846, mean reward: -333.846 [-333.846, -333.846], mean action: 2.000 [2.000, 2.000],  loss: 3499358.750000, mae: 4294.560547, mean_q: -2688.779297
 4681/5000: episode: 4681, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1885.339, mean reward: -1885.339 [-1885.339, -1885.339], mean action: 2.000 [2.000, 2.000],  loss: 3097889.500000, mae: 4352.991211, mean_q: -2690.701416
 4682/5000: episode: 4682, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -845.567, mean reward: -845.567 [-845.567, -845.567], mean action: 2.000 [2.000, 2.000],  loss: 3207068.500000, mae: 4259.862305, mean_q: -2702.206543
 4683/5000: episode: 4683, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -788.660, mean reward: -788.660 [-788.660, -788.660], mean action: 2.000 [2.000, 2.000],  loss: 3438230.750000, mae: 4306.630859, mean_q: -2696.007324
 4684/5000: episode: 4684, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -357.661, mean reward: -357.661 [-357.661, -357.661], mean action: 2.000 [2.000, 2.000],  loss: 2549501.750000, mae: 4181.618164, mean_q: -2707.449707
 4685/5000: episode: 4685, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4896.476, mean reward: -4896.476 [-4896.476, -4896.476], mean action: 2.000 [2.000, 2.000],  loss: 3825282.500000, mae: 4226.153320, mean_q: -2711.458984
 4686/5000: episode: 4686, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7418.625, mean reward: -7418.625 [-7418.625, -7418.625], mean action: 2.000 [2.000, 2.000],  loss: 2041538.125000, mae: 4175.208496, mean_q: -2704.405762
 4687/5000: episode: 4687, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1496.141, mean reward: -1496.141 [-1496.141, -1496.141], mean action: 2.000 [2.000, 2.000],  loss: 1684171.750000, mae: 4244.594238, mean_q: -2744.343994
 4688/5000: episode: 4688, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1342.515, mean reward: -1342.515 [-1342.515, -1342.515], mean action: 2.000 [2.000, 2.000],  loss: 1457750.000000, mae: 4241.164062, mean_q: -2740.385254
 4689/5000: episode: 4689, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1827.634, mean reward: -1827.634 [-1827.634, -1827.634], mean action: 2.000 [2.000, 2.000],  loss: 1720688.750000, mae: 4211.207031, mean_q: -2691.406250
 4690/5000: episode: 4690, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1757.957, mean reward: -1757.957 [-1757.957, -1757.957], mean action: 2.000 [2.000, 2.000],  loss: 1612794.375000, mae: 4168.167969, mean_q: -2724.485596
 4691/5000: episode: 4691, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4810.251, mean reward: -4810.251 [-4810.251, -4810.251], mean action: 2.000 [2.000, 2.000],  loss: 2510636.500000, mae: 4211.954102, mean_q: -2717.952637
 4692/5000: episode: 4692, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2988.349, mean reward: -2988.349 [-2988.349, -2988.349], mean action: 2.000 [2.000, 2.000],  loss: 2373852.500000, mae: 4246.952148, mean_q: -2717.890869
 4693/5000: episode: 4693, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3772.081, mean reward: -3772.081 [-3772.081, -3772.081], mean action: 2.000 [2.000, 2.000],  loss: 2455582.000000, mae: 4248.247070, mean_q: -2706.312744
 4694/5000: episode: 4694, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1231.875, mean reward: -1231.875 [-1231.875, -1231.875], mean action: 2.000 [2.000, 2.000],  loss: 4422591.000000, mae: 4295.636230, mean_q: -2689.545654
 4695/5000: episode: 4695, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3340.022, mean reward: -3340.022 [-3340.022, -3340.022], mean action: 2.000 [2.000, 2.000],  loss: 3885070.000000, mae: 4345.739258, mean_q: -2703.301270
 4696/5000: episode: 4696, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3035.996, mean reward: -3035.996 [-3035.996, -3035.996], mean action: 2.000 [2.000, 2.000],  loss: 1656062.750000, mae: 4206.804688, mean_q: -2674.419922
 4697/5000: episode: 4697, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1153.805, mean reward: -1153.805 [-1153.805, -1153.805], mean action: 2.000 [2.000, 2.000],  loss: 1873693.250000, mae: 4077.623535, mean_q: -2668.167480
 4698/5000: episode: 4698, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1053.136, mean reward: -1053.136 [-1053.136, -1053.136], mean action: 2.000 [2.000, 2.000],  loss: 2486638.250000, mae: 4143.897461, mean_q: -2642.348633
 4699/5000: episode: 4699, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -611.813, mean reward: -611.813 [-611.813, -611.813], mean action: 2.000 [2.000, 2.000],  loss: 2244937.250000, mae: 4097.878418, mean_q: -2656.317383
 4700/5000: episode: 4700, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4897.360, mean reward: -4897.360 [-4897.360, -4897.360], mean action: 2.000 [2.000, 2.000],  loss: 1975738.250000, mae: 4149.630371, mean_q: -2625.101074
 4701/5000: episode: 4701, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2123.680, mean reward: -2123.680 [-2123.680, -2123.680], mean action: 2.000 [2.000, 2.000],  loss: 2733568.500000, mae: 4217.130859, mean_q: -2639.431152
 4702/5000: episode: 4702, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7678.561, mean reward: -7678.561 [-7678.561, -7678.561], mean action: 0.000 [0.000, 0.000],  loss: 4943398.000000, mae: 4254.120117, mean_q: -2639.053223
 4703/5000: episode: 4703, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -2369.637, mean reward: -2369.637 [-2369.637, -2369.637], mean action: 2.000 [2.000, 2.000],  loss: 2099333.000000, mae: 4045.483398, mean_q: -2620.536621
 4704/5000: episode: 4704, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1021.082, mean reward: -1021.082 [-1021.082, -1021.082], mean action: 1.000 [1.000, 1.000],  loss: 2518356.500000, mae: 4166.862305, mean_q: -2621.376465
 4705/5000: episode: 4705, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2621.457, mean reward: -2621.457 [-2621.457, -2621.457], mean action: 2.000 [2.000, 2.000],  loss: 2269411.000000, mae: 4166.186523, mean_q: -2616.533936
 4706/5000: episode: 4706, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1294.120, mean reward: -1294.120 [-1294.120, -1294.120], mean action: 2.000 [2.000, 2.000],  loss: 3386081.500000, mae: 4192.211426, mean_q: -2606.389160
 4707/5000: episode: 4707, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1126.329, mean reward: -1126.329 [-1126.329, -1126.329], mean action: 2.000 [2.000, 2.000],  loss: 2824609.500000, mae: 4188.513672, mean_q: -2610.493408
 4708/5000: episode: 4708, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2019.954, mean reward: -2019.954 [-2019.954, -2019.954], mean action: 2.000 [2.000, 2.000],  loss: 2944209.000000, mae: 4239.112305, mean_q: -2612.097168
 4709/5000: episode: 4709, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3154.334, mean reward: -3154.334 [-3154.334, -3154.334], mean action: 2.000 [2.000, 2.000],  loss: 3578146.500000, mae: 4191.464844, mean_q: -2615.832520
 4710/5000: episode: 4710, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5018.095, mean reward: -5018.095 [-5018.095, -5018.095], mean action: 2.000 [2.000, 2.000],  loss: 2284884.250000, mae: 4112.371094, mean_q: -2611.016113
 4711/5000: episode: 4711, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3118.859, mean reward: -3118.859 [-3118.859, -3118.859], mean action: 2.000 [2.000, 2.000],  loss: 2070002.000000, mae: 4051.922363, mean_q: -2608.059570
 4712/5000: episode: 4712, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -56.739, mean reward: -56.739 [-56.739, -56.739], mean action: 2.000 [2.000, 2.000],  loss: 3813091.750000, mae: 4186.139648, mean_q: -2594.437012
 4713/5000: episode: 4713, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -273.834, mean reward: -273.834 [-273.834, -273.834], mean action: 2.000 [2.000, 2.000],  loss: 2662270.000000, mae: 4197.187500, mean_q: -2610.387451
 4714/5000: episode: 4714, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -640.797, mean reward: -640.797 [-640.797, -640.797], mean action: 2.000 [2.000, 2.000],  loss: 2272397.000000, mae: 4134.812500, mean_q: -2598.131348
 4715/5000: episode: 4715, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2424.227, mean reward: -2424.227 [-2424.227, -2424.227], mean action: 2.000 [2.000, 2.000],  loss: 2804425.000000, mae: 4087.788574, mean_q: -2602.048340
 4716/5000: episode: 4716, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7775.279, mean reward: -7775.279 [-7775.279, -7775.279], mean action: 0.000 [0.000, 0.000],  loss: 2879075.500000, mae: 4196.844727, mean_q: -2614.586426
 4717/5000: episode: 4717, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1977.511, mean reward: -1977.511 [-1977.511, -1977.511], mean action: 2.000 [2.000, 2.000],  loss: 1747649.625000, mae: 4058.518066, mean_q: -2624.021729
 4718/5000: episode: 4718, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2847.992, mean reward: -2847.992 [-2847.992, -2847.992], mean action: 2.000 [2.000, 2.000],  loss: 3064702.000000, mae: 4194.907227, mean_q: -2634.115723
 4719/5000: episode: 4719, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -987.376, mean reward: -987.376 [-987.376, -987.376], mean action: 2.000 [2.000, 2.000],  loss: 1482381.250000, mae: 4088.704102, mean_q: -2618.797363
 4720/5000: episode: 4720, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -515.116, mean reward: -515.116 [-515.116, -515.116], mean action: 2.000 [2.000, 2.000],  loss: 1931166.125000, mae: 4110.732422, mean_q: -2617.094971
 4721/5000: episode: 4721, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2278.230, mean reward: -2278.230 [-2278.230, -2278.230], mean action: 2.000 [2.000, 2.000],  loss: 2550576.250000, mae: 4117.305176, mean_q: -2633.081055
 4722/5000: episode: 4722, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -188.046, mean reward: -188.046 [-188.046, -188.046], mean action: 2.000 [2.000, 2.000],  loss: 1545885.000000, mae: 4119.912598, mean_q: -2613.995361
 4723/5000: episode: 4723, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1610.892, mean reward: -1610.892 [-1610.892, -1610.892], mean action: 2.000 [2.000, 2.000],  loss: 2996393.000000, mae: 4162.458984, mean_q: -2620.429199
 4724/5000: episode: 4724, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1178.981, mean reward: -1178.981 [-1178.981, -1178.981], mean action: 2.000 [2.000, 2.000],  loss: 2215418.250000, mae: 4113.868652, mean_q: -2604.961914
 4725/5000: episode: 4725, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -44.989, mean reward: -44.989 [-44.989, -44.989], mean action: 2.000 [2.000, 2.000],  loss: 2459847.500000, mae: 4178.653320, mean_q: -2580.620117
 4726/5000: episode: 4726, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1909.414, mean reward: -1909.414 [-1909.414, -1909.414], mean action: 2.000 [2.000, 2.000],  loss: 1583060.250000, mae: 4053.836426, mean_q: -2588.786133
 4727/5000: episode: 4727, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4644.672, mean reward: -4644.672 [-4644.672, -4644.672], mean action: 2.000 [2.000, 2.000],  loss: 3198194.500000, mae: 4184.970215, mean_q: -2575.389160
 4728/5000: episode: 4728, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2015.716, mean reward: -2015.716 [-2015.716, -2015.716], mean action: 1.000 [1.000, 1.000],  loss: 2622057.500000, mae: 4189.842773, mean_q: -2554.807861
 4729/5000: episode: 4729, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2565.184, mean reward: -2565.184 [-2565.184, -2565.184], mean action: 2.000 [2.000, 2.000],  loss: 2526831.000000, mae: 4078.767090, mean_q: -2560.236816
 4730/5000: episode: 4730, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -118.373, mean reward: -118.373 [-118.373, -118.373], mean action: 2.000 [2.000, 2.000],  loss: 1745020.000000, mae: 3994.852051, mean_q: -2531.267578
 4731/5000: episode: 4731, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5588.825, mean reward: -5588.825 [-5588.825, -5588.825], mean action: 2.000 [2.000, 2.000],  loss: 2479579.000000, mae: 4095.944824, mean_q: -2501.552002
 4732/5000: episode: 4732, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2687.302, mean reward: -2687.302 [-2687.302, -2687.302], mean action: 2.000 [2.000, 2.000],  loss: 2668736.750000, mae: 4055.569336, mean_q: -2528.923828
 4733/5000: episode: 4733, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -110.512, mean reward: -110.512 [-110.512, -110.512], mean action: 2.000 [2.000, 2.000],  loss: 2051314.750000, mae: 4032.768311, mean_q: -2507.838135
 4734/5000: episode: 4734, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -661.336, mean reward: -661.336 [-661.336, -661.336], mean action: 2.000 [2.000, 2.000],  loss: 2274758.500000, mae: 4043.132568, mean_q: -2508.658203
 4735/5000: episode: 4735, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9168.130, mean reward: -9168.130 [-9168.130, -9168.130], mean action: 2.000 [2.000, 2.000],  loss: 2782081.250000, mae: 4081.735840, mean_q: -2524.132080
 4736/5000: episode: 4736, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1432.823, mean reward: -1432.823 [-1432.823, -1432.823], mean action: 2.000 [2.000, 2.000],  loss: 1262595.750000, mae: 4011.679932, mean_q: -2523.378662
 4737/5000: episode: 4737, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5360.031, mean reward: -5360.031 [-5360.031, -5360.031], mean action: 2.000 [2.000, 2.000],  loss: 3108801.000000, mae: 4124.505371, mean_q: -2532.209961
 4738/5000: episode: 4738, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1345.891, mean reward: -1345.891 [-1345.891, -1345.891], mean action: 2.000 [2.000, 2.000],  loss: 1905974.250000, mae: 4088.710205, mean_q: -2520.644043
 4739/5000: episode: 4739, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1139.414, mean reward: -1139.414 [-1139.414, -1139.414], mean action: 2.000 [2.000, 2.000],  loss: 3544020.250000, mae: 4106.130859, mean_q: -2525.079834
 4740/5000: episode: 4740, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1833.844, mean reward: -1833.844 [-1833.844, -1833.844], mean action: 2.000 [2.000, 2.000],  loss: 2413543.750000, mae: 4118.633789, mean_q: -2537.656982
 4741/5000: episode: 4741, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -492.242, mean reward: -492.242 [-492.242, -492.242], mean action: 2.000 [2.000, 2.000],  loss: 4766252.500000, mae: 4168.441406, mean_q: -2539.431152
 4742/5000: episode: 4742, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3620.033, mean reward: -3620.033 [-3620.033, -3620.033], mean action: 2.000 [2.000, 2.000],  loss: 3496361.000000, mae: 4145.684082, mean_q: -2529.138184
 4743/5000: episode: 4743, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1664.977, mean reward: -1664.977 [-1664.977, -1664.977], mean action: 2.000 [2.000, 2.000],  loss: 1863051.750000, mae: 4070.295410, mean_q: -2568.893555
 4744/5000: episode: 4744, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4957.156, mean reward: -4957.156 [-4957.156, -4957.156], mean action: 2.000 [2.000, 2.000],  loss: 3050079.750000, mae: 4067.463379, mean_q: -2556.951172
 4745/5000: episode: 4745, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -3474.007, mean reward: -3474.007 [-3474.007, -3474.007], mean action: 1.000 [1.000, 1.000],  loss: 2102200.750000, mae: 4062.489746, mean_q: -2596.608887
 4746/5000: episode: 4746, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -504.555, mean reward: -504.555 [-504.555, -504.555], mean action: 2.000 [2.000, 2.000],  loss: 3077788.750000, mae: 4112.948730, mean_q: -2581.378174
 4747/5000: episode: 4747, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -7847.838, mean reward: -7847.838 [-7847.838, -7847.838], mean action: 2.000 [2.000, 2.000],  loss: 2220154.250000, mae: 4063.905273, mean_q: -2600.734375
 4748/5000: episode: 4748, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2026.319, mean reward: -2026.319 [-2026.319, -2026.319], mean action: 2.000 [2.000, 2.000],  loss: 2290153.250000, mae: 4049.886230, mean_q: -2609.388184
 4749/5000: episode: 4749, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -219.929, mean reward: -219.929 [-219.929, -219.929], mean action: 2.000 [2.000, 2.000],  loss: 1778184.625000, mae: 4088.551758, mean_q: -2623.038086
 4750/5000: episode: 4750, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5939.390, mean reward: -5939.390 [-5939.390, -5939.390], mean action: 1.000 [1.000, 1.000],  loss: 1669903.875000, mae: 4015.203613, mean_q: -2601.465820
 4751/5000: episode: 4751, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3299.634, mean reward: -3299.634 [-3299.634, -3299.634], mean action: 2.000 [2.000, 2.000],  loss: 2957564.000000, mae: 4137.697266, mean_q: -2632.120605
 4752/5000: episode: 4752, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -552.923, mean reward: -552.923 [-552.923, -552.923], mean action: 2.000 [2.000, 2.000],  loss: 3449957.000000, mae: 4166.615234, mean_q: -2649.486572
 4753/5000: episode: 4753, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3117.655, mean reward: -3117.655 [-3117.655, -3117.655], mean action: 2.000 [2.000, 2.000],  loss: 2122010.000000, mae: 4110.185059, mean_q: -2643.855713
 4754/5000: episode: 4754, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2653.319, mean reward: -2653.319 [-2653.319, -2653.319], mean action: 2.000 [2.000, 2.000],  loss: 3035245.750000, mae: 4097.998047, mean_q: -2620.005859
 4755/5000: episode: 4755, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2734.568, mean reward: -2734.568 [-2734.568, -2734.568], mean action: 2.000 [2.000, 2.000],  loss: 2829361.000000, mae: 4095.116211, mean_q: -2636.887695
 4756/5000: episode: 4756, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2853.215, mean reward: -2853.215 [-2853.215, -2853.215], mean action: 2.000 [2.000, 2.000],  loss: 2322771.500000, mae: 4115.919922, mean_q: -2651.319336
 4757/5000: episode: 4757, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -59.270, mean reward: -59.270 [-59.270, -59.270], mean action: 2.000 [2.000, 2.000],  loss: 4168319.000000, mae: 4261.866699, mean_q: -2651.843018
 4758/5000: episode: 4758, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1411.224, mean reward: -1411.224 [-1411.224, -1411.224], mean action: 2.000 [2.000, 2.000],  loss: 2751304.000000, mae: 4095.279297, mean_q: -2648.133301
 4759/5000: episode: 4759, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1789.793, mean reward: -1789.793 [-1789.793, -1789.793], mean action: 2.000 [2.000, 2.000],  loss: 2270574.500000, mae: 4066.085693, mean_q: -2653.792725
 4760/5000: episode: 4760, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -171.454, mean reward: -171.454 [-171.454, -171.454], mean action: 2.000 [2.000, 2.000],  loss: 1958393.500000, mae: 3986.125732, mean_q: -2622.877441
 4761/5000: episode: 4761, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3869.691, mean reward: -3869.691 [-3869.691, -3869.691], mean action: 2.000 [2.000, 2.000],  loss: 2586540.500000, mae: 4123.767578, mean_q: -2668.531738
 4762/5000: episode: 4762, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2553.607, mean reward: -2553.607 [-2553.607, -2553.607], mean action: 2.000 [2.000, 2.000],  loss: 2642630.750000, mae: 4180.162598, mean_q: -2650.446777
 4763/5000: episode: 4763, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3227.594, mean reward: -3227.594 [-3227.594, -3227.594], mean action: 2.000 [2.000, 2.000],  loss: 3660783.500000, mae: 4184.770508, mean_q: -2684.101562
 4764/5000: episode: 4764, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2334.695, mean reward: -2334.695 [-2334.695, -2334.695], mean action: 2.000 [2.000, 2.000],  loss: 2735567.500000, mae: 4102.504883, mean_q: -2669.501465
 4765/5000: episode: 4765, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3616.731, mean reward: -3616.731 [-3616.731, -3616.731], mean action: 2.000 [2.000, 2.000],  loss: 3448802.000000, mae: 4253.740234, mean_q: -2683.359375
 4766/5000: episode: 4766, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5339.268, mean reward: -5339.268 [-5339.268, -5339.268], mean action: 2.000 [2.000, 2.000],  loss: 3658575.500000, mae: 4261.524902, mean_q: -2676.520020
 4767/5000: episode: 4767, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1137.438, mean reward: -1137.438 [-1137.438, -1137.438], mean action: 2.000 [2.000, 2.000],  loss: 1877562.875000, mae: 4117.390625, mean_q: -2677.196289
 4768/5000: episode: 4768, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4233.153, mean reward: -4233.153 [-4233.153, -4233.153], mean action: 2.000 [2.000, 2.000],  loss: 2147712.000000, mae: 4074.988770, mean_q: -2662.478027
 4769/5000: episode: 4769, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -1876.289, mean reward: -1876.289 [-1876.289, -1876.289], mean action: 2.000 [2.000, 2.000],  loss: 2236180.000000, mae: 4204.898438, mean_q: -2707.477539
 4770/5000: episode: 4770, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1921.318, mean reward: -1921.318 [-1921.318, -1921.318], mean action: 2.000 [2.000, 2.000],  loss: 4458542.000000, mae: 4305.132812, mean_q: -2692.170654
 4771/5000: episode: 4771, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1022.880, mean reward: -1022.880 [-1022.880, -1022.880], mean action: 2.000 [2.000, 2.000],  loss: 2472752.500000, mae: 4184.532227, mean_q: -2681.436035
 4772/5000: episode: 4772, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2969.674, mean reward: -2969.674 [-2969.674, -2969.674], mean action: 2.000 [2.000, 2.000],  loss: 3354279.750000, mae: 4243.833008, mean_q: -2680.818848
 4773/5000: episode: 4773, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6779.845, mean reward: -6779.845 [-6779.845, -6779.845], mean action: 2.000 [2.000, 2.000],  loss: 1719408.750000, mae: 4096.218262, mean_q: -2677.487305
 4774/5000: episode: 4774, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3800.052, mean reward: -3800.052 [-3800.052, -3800.052], mean action: 2.000 [2.000, 2.000],  loss: 2922704.250000, mae: 4178.853516, mean_q: -2694.111328
 4775/5000: episode: 4775, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4280.940, mean reward: -4280.940 [-4280.940, -4280.940], mean action: 2.000 [2.000, 2.000],  loss: 2397629.250000, mae: 4226.762207, mean_q: -2690.874023
 4776/5000: episode: 4776, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -100.034, mean reward: -100.034 [-100.034, -100.034], mean action: 2.000 [2.000, 2.000],  loss: 3802731.250000, mae: 4230.559570, mean_q: -2684.334961
 4777/5000: episode: 4777, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -11091.245, mean reward: -11091.245 [-11091.245, -11091.245], mean action: 0.000 [0.000, 0.000],  loss: 4675219.000000, mae: 4336.237305, mean_q: -2714.466797
 4778/5000: episode: 4778, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -445.307, mean reward: -445.307 [-445.307, -445.307], mean action: 2.000 [2.000, 2.000],  loss: 2118572.750000, mae: 4154.879883, mean_q: -2707.771240
 4779/5000: episode: 4779, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5208.437, mean reward: -5208.437 [-5208.437, -5208.437], mean action: 2.000 [2.000, 2.000],  loss: 3144400.000000, mae: 4215.800781, mean_q: -2698.827148
 4780/5000: episode: 4780, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2147.494, mean reward: -2147.494 [-2147.494, -2147.494], mean action: 2.000 [2.000, 2.000],  loss: 3198138.500000, mae: 4252.797852, mean_q: -2722.062744
 4781/5000: episode: 4781, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -578.725, mean reward: -578.725 [-578.725, -578.725], mean action: 2.000 [2.000, 2.000],  loss: 4230477.500000, mae: 4242.071289, mean_q: -2723.450928
 4782/5000: episode: 4782, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2950.059, mean reward: -2950.059 [-2950.059, -2950.059], mean action: 2.000 [2.000, 2.000],  loss: 2694735.750000, mae: 4210.164551, mean_q: -2759.591064
 4783/5000: episode: 4783, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1632.713, mean reward: -1632.713 [-1632.713, -1632.713], mean action: 2.000 [2.000, 2.000],  loss: 1888006.000000, mae: 4183.400391, mean_q: -2738.968018
 4784/5000: episode: 4784, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4005.047, mean reward: -4005.047 [-4005.047, -4005.047], mean action: 2.000 [2.000, 2.000],  loss: 4079626.000000, mae: 4375.708496, mean_q: -2748.550781
 4785/5000: episode: 4785, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -839.637, mean reward: -839.637 [-839.637, -839.637], mean action: 2.000 [2.000, 2.000],  loss: 2630801.000000, mae: 4312.245117, mean_q: -2760.385254
 4786/5000: episode: 4786, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1952.307, mean reward: -1952.307 [-1952.307, -1952.307], mean action: 2.000 [2.000, 2.000],  loss: 3604956.500000, mae: 4352.112305, mean_q: -2772.393311
 4787/5000: episode: 4787, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5262.698, mean reward: -5262.698 [-5262.698, -5262.698], mean action: 2.000 [2.000, 2.000],  loss: 2494313.500000, mae: 4302.785645, mean_q: -2775.791748
 4788/5000: episode: 4788, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -695.224, mean reward: -695.224 [-695.224, -695.224], mean action: 2.000 [2.000, 2.000],  loss: 2637863.000000, mae: 4308.823730, mean_q: -2774.802490
 4789/5000: episode: 4789, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1186.811, mean reward: -1186.811 [-1186.811, -1186.811], mean action: 2.000 [2.000, 2.000],  loss: 2071844.250000, mae: 4314.808594, mean_q: -2774.510254
 4790/5000: episode: 4790, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3835.325, mean reward: -3835.325 [-3835.325, -3835.325], mean action: 0.000 [0.000, 0.000],  loss: 2966282.500000, mae: 4390.592773, mean_q: -2781.191406
 4791/5000: episode: 4791, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5045.975, mean reward: -5045.975 [-5045.975, -5045.975], mean action: 2.000 [2.000, 2.000],  loss: 3354657.500000, mae: 4317.163574, mean_q: -2758.590332
 4792/5000: episode: 4792, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5510.012, mean reward: -5510.012 [-5510.012, -5510.012], mean action: 2.000 [2.000, 2.000],  loss: 2437184.000000, mae: 4277.509766, mean_q: -2786.706787
 4793/5000: episode: 4793, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4662.545, mean reward: -4662.545 [-4662.545, -4662.545], mean action: 2.000 [2.000, 2.000],  loss: 1729290.625000, mae: 4206.491211, mean_q: -2758.139648
 4794/5000: episode: 4794, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -398.194, mean reward: -398.194 [-398.194, -398.194], mean action: 2.000 [2.000, 2.000],  loss: 2651842.000000, mae: 4277.550293, mean_q: -2770.887695
 4795/5000: episode: 4795, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -27.233, mean reward: -27.233 [-27.233, -27.233], mean action: 2.000 [2.000, 2.000],  loss: 2986876.750000, mae: 4299.473633, mean_q: -2775.928467
 4796/5000: episode: 4796, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5621.913, mean reward: -5621.913 [-5621.913, -5621.913], mean action: 2.000 [2.000, 2.000],  loss: 3095410.500000, mae: 4399.391602, mean_q: -2763.037354
 4797/5000: episode: 4797, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -817.233, mean reward: -817.233 [-817.233, -817.233], mean action: 2.000 [2.000, 2.000],  loss: 2948359.000000, mae: 4329.071289, mean_q: -2753.449219
 4798/5000: episode: 4798, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1844.938, mean reward: -1844.938 [-1844.938, -1844.938], mean action: 2.000 [2.000, 2.000],  loss: 2414494.000000, mae: 4346.699707, mean_q: -2757.234619
 4799/5000: episode: 4799, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2192.631, mean reward: -2192.631 [-2192.631, -2192.631], mean action: 2.000 [2.000, 2.000],  loss: 2949257.750000, mae: 4341.934570, mean_q: -2747.345215
 4800/5000: episode: 4800, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3423.474, mean reward: -3423.474 [-3423.474, -3423.474], mean action: 2.000 [2.000, 2.000],  loss: 2929970.500000, mae: 4346.244141, mean_q: -2743.950684
 4801/5000: episode: 4801, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -12189.247, mean reward: -12189.247 [-12189.247, -12189.247], mean action: 0.000 [0.000, 0.000],  loss: 1715631.750000, mae: 4199.392578, mean_q: -2714.762207
 4802/5000: episode: 4802, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10429.367, mean reward: -10429.367 [-10429.367, -10429.367], mean action: 2.000 [2.000, 2.000],  loss: 3189241.000000, mae: 4287.178711, mean_q: -2702.228027
 4803/5000: episode: 4803, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3978.603, mean reward: -3978.603 [-3978.603, -3978.603], mean action: 2.000 [2.000, 2.000],  loss: 3953894.000000, mae: 4452.211914, mean_q: -2729.689941
 4804/5000: episode: 4804, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6543.967, mean reward: -6543.967 [-6543.967, -6543.967], mean action: 2.000 [2.000, 2.000],  loss: 1599350.000000, mae: 4234.536133, mean_q: -2704.083984
 4805/5000: episode: 4805, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1244.322, mean reward: -1244.322 [-1244.322, -1244.322], mean action: 2.000 [2.000, 2.000],  loss: 2289550.500000, mae: 4278.616211, mean_q: -2699.714355
 4806/5000: episode: 4806, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4384.653, mean reward: -4384.653 [-4384.653, -4384.653], mean action: 2.000 [2.000, 2.000],  loss: 1757916.500000, mae: 4240.309570, mean_q: -2691.586914
 4807/5000: episode: 4807, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -838.138, mean reward: -838.138 [-838.138, -838.138], mean action: 2.000 [2.000, 2.000],  loss: 1776143.625000, mae: 4192.963867, mean_q: -2667.962891
 4808/5000: episode: 4808, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -660.511, mean reward: -660.511 [-660.511, -660.511], mean action: 2.000 [2.000, 2.000],  loss: 3100556.750000, mae: 4273.354492, mean_q: -2671.045898
 4809/5000: episode: 4809, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1108.011, mean reward: -1108.011 [-1108.011, -1108.011], mean action: 2.000 [2.000, 2.000],  loss: 2355903.500000, mae: 4233.136719, mean_q: -2654.290527
 4810/5000: episode: 4810, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2998.746, mean reward: -2998.746 [-2998.746, -2998.746], mean action: 2.000 [2.000, 2.000],  loss: 1364454.750000, mae: 4073.881836, mean_q: -2612.843262
 4811/5000: episode: 4811, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2688.625, mean reward: -2688.625 [-2688.625, -2688.625], mean action: 2.000 [2.000, 2.000],  loss: 2237993.500000, mae: 4245.127441, mean_q: -2643.254883
 4812/5000: episode: 4812, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3219.078, mean reward: -3219.078 [-3219.078, -3219.078], mean action: 2.000 [2.000, 2.000],  loss: 1738105.625000, mae: 4098.188477, mean_q: -2632.154785
 4813/5000: episode: 4813, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5041.619, mean reward: -5041.619 [-5041.619, -5041.619], mean action: 0.000 [0.000, 0.000],  loss: 3592963.000000, mae: 4307.821289, mean_q: -2623.225342
 4814/5000: episode: 4814, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -90.779, mean reward: -90.779 [-90.779, -90.779], mean action: 2.000 [2.000, 2.000],  loss: 2201832.500000, mae: 4165.568848, mean_q: -2601.772217
 4815/5000: episode: 4815, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4114.611, mean reward: -4114.611 [-4114.611, -4114.611], mean action: 2.000 [2.000, 2.000],  loss: 2039299.250000, mae: 4128.792969, mean_q: -2587.235596
 4816/5000: episode: 4816, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3544.881, mean reward: -3544.881 [-3544.881, -3544.881], mean action: 2.000 [2.000, 2.000],  loss: 2167907.500000, mae: 4137.117676, mean_q: -2584.675781
 4817/5000: episode: 4817, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1137.098, mean reward: -1137.098 [-1137.098, -1137.098], mean action: 3.000 [3.000, 3.000],  loss: 3375796.750000, mae: 4210.845215, mean_q: -2586.572754
 4818/5000: episode: 4818, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2583.514, mean reward: -2583.514 [-2583.514, -2583.514], mean action: 2.000 [2.000, 2.000],  loss: 3186885.250000, mae: 4261.278320, mean_q: -2600.271484
 4819/5000: episode: 4819, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2850.301, mean reward: -2850.301 [-2850.301, -2850.301], mean action: 2.000 [2.000, 2.000],  loss: 2384866.250000, mae: 3924.039062, mean_q: -2591.303711
 4820/5000: episode: 4820, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -870.735, mean reward: -870.735 [-870.735, -870.735], mean action: 2.000 [2.000, 2.000],  loss: 3073554.500000, mae: 4155.743652, mean_q: -2612.782715
 4821/5000: episode: 4821, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1361.293, mean reward: -1361.293 [-1361.293, -1361.293], mean action: 2.000 [2.000, 2.000],  loss: 2273805.000000, mae: 4185.697266, mean_q: -2618.550293
 4822/5000: episode: 4822, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4236.272, mean reward: -4236.272 [-4236.272, -4236.272], mean action: 2.000 [2.000, 2.000],  loss: 2844734.500000, mae: 4219.139160, mean_q: -2606.639648
 4823/5000: episode: 4823, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5081.731, mean reward: -5081.731 [-5081.731, -5081.731], mean action: 2.000 [2.000, 2.000],  loss: 3567573.500000, mae: 4284.562988, mean_q: -2620.597900
 4824/5000: episode: 4824, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3554.376, mean reward: -3554.376 [-3554.376, -3554.376], mean action: 2.000 [2.000, 2.000],  loss: 2668167.500000, mae: 4095.920898, mean_q: -2605.966309
 4825/5000: episode: 4825, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1547.958, mean reward: -1547.958 [-1547.958, -1547.958], mean action: 2.000 [2.000, 2.000],  loss: 4781329.000000, mae: 4295.188477, mean_q: -2643.718750
 4826/5000: episode: 4826, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3480.651, mean reward: -3480.651 [-3480.651, -3480.651], mean action: 2.000 [2.000, 2.000],  loss: 3954717.500000, mae: 4316.167480, mean_q: -2652.322021
 4827/5000: episode: 4827, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4532.999, mean reward: -4532.999 [-4532.999, -4532.999], mean action: 2.000 [2.000, 2.000],  loss: 1747095.125000, mae: 4124.937500, mean_q: -2659.095459
 4828/5000: episode: 4828, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1253.582, mean reward: -1253.582 [-1253.582, -1253.582], mean action: 2.000 [2.000, 2.000],  loss: 2657826.500000, mae: 4137.031738, mean_q: -2645.555176
 4829/5000: episode: 4829, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5600.391, mean reward: -5600.391 [-5600.391, -5600.391], mean action: 2.000 [2.000, 2.000],  loss: 3490240.500000, mae: 4161.255859, mean_q: -2661.920410
 4830/5000: episode: 4830, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -902.482, mean reward: -902.482 [-902.482, -902.482], mean action: 2.000 [2.000, 2.000],  loss: 2399181.500000, mae: 4155.203125, mean_q: -2695.439697
 4831/5000: episode: 4831, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5338.011, mean reward: -5338.011 [-5338.011, -5338.011], mean action: 2.000 [2.000, 2.000],  loss: 1493193.750000, mae: 4016.658203, mean_q: -2643.403809
 4832/5000: episode: 4832, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4780.298, mean reward: -4780.298 [-4780.298, -4780.298], mean action: 2.000 [2.000, 2.000],  loss: 2403174.500000, mae: 4098.937500, mean_q: -2635.341797
 4833/5000: episode: 4833, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4336.016, mean reward: -4336.016 [-4336.016, -4336.016], mean action: 2.000 [2.000, 2.000],  loss: 3744412.500000, mae: 4189.343750, mean_q: -2652.699951
 4834/5000: episode: 4834, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3104.883, mean reward: -3104.883 [-3104.883, -3104.883], mean action: 2.000 [2.000, 2.000],  loss: 4130865.500000, mae: 4285.431641, mean_q: -2636.197021
 4835/5000: episode: 4835, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1455.953, mean reward: -1455.953 [-1455.953, -1455.953], mean action: 2.000 [2.000, 2.000],  loss: 2585059.250000, mae: 4120.042969, mean_q: -2620.223633
 4836/5000: episode: 4836, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1229.756, mean reward: -1229.756 [-1229.756, -1229.756], mean action: 2.000 [2.000, 2.000],  loss: 2477475.500000, mae: 4067.325928, mean_q: -2629.108887
 4837/5000: episode: 4837, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -811.950, mean reward: -811.950 [-811.950, -811.950], mean action: 2.000 [2.000, 2.000],  loss: 2768538.000000, mae: 4156.457520, mean_q: -2617.112793
 4838/5000: episode: 4838, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -673.648, mean reward: -673.648 [-673.648, -673.648], mean action: 2.000 [2.000, 2.000],  loss: 2423741.000000, mae: 4112.698242, mean_q: -2593.086670
 4839/5000: episode: 4839, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5202.070, mean reward: -5202.070 [-5202.070, -5202.070], mean action: 2.000 [2.000, 2.000],  loss: 2368037.000000, mae: 4122.344727, mean_q: -2586.968750
 4840/5000: episode: 4840, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1930.358, mean reward: -1930.358 [-1930.358, -1930.358], mean action: 2.000 [2.000, 2.000],  loss: 2012266.000000, mae: 4065.685303, mean_q: -2563.735352
 4841/5000: episode: 4841, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1554.786, mean reward: -1554.786 [-1554.786, -1554.786], mean action: 2.000 [2.000, 2.000],  loss: 2849827.750000, mae: 4158.197266, mean_q: -2592.197266
 4842/5000: episode: 4842, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3485.720, mean reward: -3485.720 [-3485.720, -3485.720], mean action: 2.000 [2.000, 2.000],  loss: 4635160.000000, mae: 4144.851562, mean_q: -2584.850830
 4843/5000: episode: 4843, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -1999.937, mean reward: -1999.937 [-1999.937, -1999.937], mean action: 2.000 [2.000, 2.000],  loss: 2648963.500000, mae: 4106.615234, mean_q: -2555.324219
 4844/5000: episode: 4844, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2172.613, mean reward: -2172.613 [-2172.613, -2172.613], mean action: 2.000 [2.000, 2.000],  loss: 3279016.000000, mae: 4177.504883, mean_q: -2547.031982
 4845/5000: episode: 4845, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2381.887, mean reward: -2381.887 [-2381.887, -2381.887], mean action: 2.000 [2.000, 2.000],  loss: 3677685.500000, mae: 4140.076172, mean_q: -2563.881592
 4846/5000: episode: 4846, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2316.157, mean reward: -2316.157 [-2316.157, -2316.157], mean action: 2.000 [2.000, 2.000],  loss: 4511642.000000, mae: 4211.518555, mean_q: -2563.078613
 4847/5000: episode: 4847, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -963.144, mean reward: -963.144 [-963.144, -963.144], mean action: 2.000 [2.000, 2.000],  loss: 1992839.750000, mae: 4020.331543, mean_q: -2554.893066
 4848/5000: episode: 4848, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -752.637, mean reward: -752.637 [-752.637, -752.637], mean action: 2.000 [2.000, 2.000],  loss: 2041266.000000, mae: 4046.010010, mean_q: -2543.481934
 4849/5000: episode: 4849, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9354.611, mean reward: -9354.611 [-9354.611, -9354.611], mean action: 2.000 [2.000, 2.000],  loss: 2723866.250000, mae: 4023.015869, mean_q: -2542.523438
 4850/5000: episode: 4850, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -128.236, mean reward: -128.236 [-128.236, -128.236], mean action: 2.000 [2.000, 2.000],  loss: 2138962.500000, mae: 4034.873047, mean_q: -2558.087891
 4851/5000: episode: 4851, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5156.245, mean reward: -5156.245 [-5156.245, -5156.245], mean action: 2.000 [2.000, 2.000],  loss: 3203448.500000, mae: 4108.257812, mean_q: -2572.962402
 4852/5000: episode: 4852, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2535.829, mean reward: -2535.829 [-2535.829, -2535.829], mean action: 2.000 [2.000, 2.000],  loss: 1735954.500000, mae: 4062.619141, mean_q: -2574.767578
 4853/5000: episode: 4853, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3296.991, mean reward: -3296.991 [-3296.991, -3296.991], mean action: 2.000 [2.000, 2.000],  loss: 2543103.250000, mae: 4011.830566, mean_q: -2574.020508
 4854/5000: episode: 4854, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1410.977, mean reward: -1410.977 [-1410.977, -1410.977], mean action: 2.000 [2.000, 2.000],  loss: 3620263.250000, mae: 4143.584961, mean_q: -2594.276367
 4855/5000: episode: 4855, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5740.567, mean reward: -5740.567 [-5740.567, -5740.567], mean action: 2.000 [2.000, 2.000],  loss: 3846254.000000, mae: 4122.419922, mean_q: -2579.104004
 4856/5000: episode: 4856, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -402.070, mean reward: -402.070 [-402.070, -402.070], mean action: 2.000 [2.000, 2.000],  loss: 1246958.000000, mae: 3997.462402, mean_q: -2602.127441
 4857/5000: episode: 4857, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4084.036, mean reward: -4084.036 [-4084.036, -4084.036], mean action: 2.000 [2.000, 2.000],  loss: 3247827.000000, mae: 4168.398438, mean_q: -2606.816895
 4858/5000: episode: 4858, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -68.802, mean reward: -68.802 [-68.802, -68.802], mean action: 2.000 [2.000, 2.000],  loss: 2461169.000000, mae: 3994.787598, mean_q: -2592.783203
 4859/5000: episode: 4859, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9164.286, mean reward: -9164.286 [-9164.286, -9164.286], mean action: 2.000 [2.000, 2.000],  loss: 2006277.125000, mae: 4056.808105, mean_q: -2603.061523
 4860/5000: episode: 4860, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3103.061, mean reward: -3103.061 [-3103.061, -3103.061], mean action: 2.000 [2.000, 2.000],  loss: 4094840.500000, mae: 4168.766602, mean_q: -2625.633301
 4861/5000: episode: 4861, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -566.788, mean reward: -566.788 [-566.788, -566.788], mean action: 2.000 [2.000, 2.000],  loss: 2413859.000000, mae: 4039.822510, mean_q: -2608.377441
 4862/5000: episode: 4862, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5.606, mean reward: -5.606 [-5.606, -5.606], mean action: 2.000 [2.000, 2.000],  loss: 2761138.000000, mae: 4082.569580, mean_q: -2637.572754
 4863/5000: episode: 4863, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3359.263, mean reward: -3359.263 [-3359.263, -3359.263], mean action: 2.000 [2.000, 2.000],  loss: 2594791.250000, mae: 4121.811523, mean_q: -2627.490723
 4864/5000: episode: 4864, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1237.475, mean reward: -1237.475 [-1237.475, -1237.475], mean action: 2.000 [2.000, 2.000],  loss: 2028233.750000, mae: 4025.755859, mean_q: -2639.190674
 4865/5000: episode: 4865, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7734.769, mean reward: -7734.769 [-7734.769, -7734.769], mean action: 2.000 [2.000, 2.000],  loss: 2591446.000000, mae: 4102.889648, mean_q: -2648.042480
 4866/5000: episode: 4866, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1022.017, mean reward: -1022.017 [-1022.017, -1022.017], mean action: 2.000 [2.000, 2.000],  loss: 2768587.500000, mae: 3967.708496, mean_q: -2624.319824
 4867/5000: episode: 4867, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3831.202, mean reward: -3831.202 [-3831.202, -3831.202], mean action: 2.000 [2.000, 2.000],  loss: 2586416.000000, mae: 4083.492432, mean_q: -2669.479492
 4868/5000: episode: 4868, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1052.791, mean reward: -1052.791 [-1052.791, -1052.791], mean action: 2.000 [2.000, 2.000],  loss: 2301520.000000, mae: 4040.181641, mean_q: -2647.310303
 4869/5000: episode: 4869, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1617.096, mean reward: -1617.096 [-1617.096, -1617.096], mean action: 2.000 [2.000, 2.000],  loss: 1778920.000000, mae: 3996.955566, mean_q: -2654.573486
 4870/5000: episode: 4870, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2335.433, mean reward: -2335.433 [-2335.433, -2335.433], mean action: 2.000 [2.000, 2.000],  loss: 3331790.000000, mae: 4239.593750, mean_q: -2661.485840
 4871/5000: episode: 4871, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2778.238, mean reward: -2778.238 [-2778.238, -2778.238], mean action: 2.000 [2.000, 2.000],  loss: 3887604.500000, mae: 4196.208008, mean_q: -2645.645020
 4872/5000: episode: 4872, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2786.929, mean reward: -2786.929 [-2786.929, -2786.929], mean action: 2.000 [2.000, 2.000],  loss: 2467742.500000, mae: 4122.566895, mean_q: -2659.328125
 4873/5000: episode: 4873, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -1348.427, mean reward: -1348.427 [-1348.427, -1348.427], mean action: 2.000 [2.000, 2.000],  loss: 3100603.500000, mae: 4119.609375, mean_q: -2663.916748
 4874/5000: episode: 4874, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1304.718, mean reward: -1304.718 [-1304.718, -1304.718], mean action: 2.000 [2.000, 2.000],  loss: 4071124.500000, mae: 4165.542969, mean_q: -2658.436035
 4875/5000: episode: 4875, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1790.106, mean reward: -1790.106 [-1790.106, -1790.106], mean action: 2.000 [2.000, 2.000],  loss: 3453390.500000, mae: 4196.193359, mean_q: -2678.796875
 4876/5000: episode: 4876, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3687.488, mean reward: -3687.488 [-3687.488, -3687.488], mean action: 2.000 [2.000, 2.000],  loss: 1305665.500000, mae: 4053.418213, mean_q: -2693.294189
 4877/5000: episode: 4877, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -399.915, mean reward: -399.915 [-399.915, -399.915], mean action: 2.000 [2.000, 2.000],  loss: 2425636.000000, mae: 4125.561523, mean_q: -2705.039062
 4878/5000: episode: 4878, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -787.497, mean reward: -787.497 [-787.497, -787.497], mean action: 2.000 [2.000, 2.000],  loss: 3989423.500000, mae: 4278.264648, mean_q: -2722.984863
 4879/5000: episode: 4879, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -155.392, mean reward: -155.392 [-155.392, -155.392], mean action: 2.000 [2.000, 2.000],  loss: 2034424.625000, mae: 4164.854004, mean_q: -2705.175293
 4880/5000: episode: 4880, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1107.751, mean reward: -1107.751 [-1107.751, -1107.751], mean action: 2.000 [2.000, 2.000],  loss: 3408348.250000, mae: 4289.118164, mean_q: -2720.317871
 4881/5000: episode: 4881, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3055.657, mean reward: -3055.657 [-3055.657, -3055.657], mean action: 2.000 [2.000, 2.000],  loss: 3055312.750000, mae: 4270.616211, mean_q: -2698.694824
 4882/5000: episode: 4882, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3518.410, mean reward: -3518.410 [-3518.410, -3518.410], mean action: 2.000 [2.000, 2.000],  loss: 2636839.500000, mae: 4224.641602, mean_q: -2709.686279
 4883/5000: episode: 4883, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1838.876, mean reward: -1838.876 [-1838.876, -1838.876], mean action: 2.000 [2.000, 2.000],  loss: 1698284.375000, mae: 4018.861816, mean_q: -2693.180176
 4884/5000: episode: 4884, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -310.713, mean reward: -310.713 [-310.713, -310.713], mean action: 2.000 [2.000, 2.000],  loss: 2873893.500000, mae: 4246.916016, mean_q: -2706.064453
 4885/5000: episode: 4885, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -6285.067, mean reward: -6285.067 [-6285.067, -6285.067], mean action: 2.000 [2.000, 2.000],  loss: 1966521.000000, mae: 4174.296387, mean_q: -2712.818604
 4886/5000: episode: 4886, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -881.989, mean reward: -881.989 [-881.989, -881.989], mean action: 2.000 [2.000, 2.000],  loss: 2806229.500000, mae: 4230.007324, mean_q: -2710.071533
 4887/5000: episode: 4887, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1375.207, mean reward: -1375.207 [-1375.207, -1375.207], mean action: 2.000 [2.000, 2.000],  loss: 2172842.000000, mae: 4233.084961, mean_q: -2726.177246
 4888/5000: episode: 4888, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -6967.965, mean reward: -6967.965 [-6967.965, -6967.965], mean action: 1.000 [1.000, 1.000],  loss: 4390071.500000, mae: 4356.022461, mean_q: -2727.801758
 4889/5000: episode: 4889, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1011.812, mean reward: -1011.812 [-1011.812, -1011.812], mean action: 2.000 [2.000, 2.000],  loss: 1416350.250000, mae: 4138.828125, mean_q: -2701.840332
 4890/5000: episode: 4890, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2115.396, mean reward: -2115.396 [-2115.396, -2115.396], mean action: 2.000 [2.000, 2.000],  loss: 2658042.000000, mae: 4263.301758, mean_q: -2714.897949
 4891/5000: episode: 4891, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6705.594, mean reward: -6705.594 [-6705.594, -6705.594], mean action: 3.000 [3.000, 3.000],  loss: 2551120.500000, mae: 4201.553711, mean_q: -2675.941406
 4892/5000: episode: 4892, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4736.173, mean reward: -4736.173 [-4736.173, -4736.173], mean action: 1.000 [1.000, 1.000],  loss: 2889154.500000, mae: 4160.917969, mean_q: -2667.375244
 4893/5000: episode: 4893, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3010.090, mean reward: -3010.090 [-3010.090, -3010.090], mean action: 2.000 [2.000, 2.000],  loss: 2421110.250000, mae: 4167.554688, mean_q: -2657.345703
 4894/5000: episode: 4894, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1836.993, mean reward: -1836.993 [-1836.993, -1836.993], mean action: 2.000 [2.000, 2.000],  loss: 3219500.500000, mae: 4303.385742, mean_q: -2670.134277
 4895/5000: episode: 4895, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -873.022, mean reward: -873.022 [-873.022, -873.022], mean action: 1.000 [1.000, 1.000],  loss: 1917100.000000, mae: 4150.492676, mean_q: -2639.994141
 4896/5000: episode: 4896, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4803.589, mean reward: -4803.589 [-4803.589, -4803.589], mean action: 2.000 [2.000, 2.000],  loss: 3553521.000000, mae: 4230.646973, mean_q: -2622.440186
 4897/5000: episode: 4897, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -961.722, mean reward: -961.722 [-961.722, -961.722], mean action: 2.000 [2.000, 2.000],  loss: 1656641.375000, mae: 4037.298584, mean_q: -2601.976807
 4898/5000: episode: 4898, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5744.591, mean reward: -5744.591 [-5744.591, -5744.591], mean action: 2.000 [2.000, 2.000],  loss: 1693437.750000, mae: 4163.050781, mean_q: -2599.225586
 4899/5000: episode: 4899, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9.392, mean reward: -9.392 [-9.392, -9.392], mean action: 2.000 [2.000, 2.000],  loss: 2100204.500000, mae: 4086.536865, mean_q: -2614.699707
 4900/5000: episode: 4900, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -6829.273, mean reward: -6829.273 [-6829.273, -6829.273], mean action: 2.000 [2.000, 2.000],  loss: 1869256.250000, mae: 4183.464355, mean_q: -2609.783203
 4901/5000: episode: 4901, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2219.633, mean reward: -2219.633 [-2219.633, -2219.633], mean action: 2.000 [2.000, 2.000],  loss: 1479162.000000, mae: 3994.550293, mean_q: -2591.808350
 4902/5000: episode: 4902, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4942.472, mean reward: -4942.472 [-4942.472, -4942.472], mean action: 2.000 [2.000, 2.000],  loss: 2621035.250000, mae: 4096.312012, mean_q: -2564.663574
 4903/5000: episode: 4903, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -325.383, mean reward: -325.383 [-325.383, -325.383], mean action: 2.000 [2.000, 2.000],  loss: 3059777.750000, mae: 4142.345703, mean_q: -2588.488281
 4904/5000: episode: 4904, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -1021.633, mean reward: -1021.633 [-1021.633, -1021.633], mean action: 2.000 [2.000, 2.000],  loss: 2568535.000000, mae: 4200.697266, mean_q: -2598.099609
 4905/5000: episode: 4905, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -286.058, mean reward: -286.058 [-286.058, -286.058], mean action: 2.000 [2.000, 2.000],  loss: 2116061.000000, mae: 4063.418945, mean_q: -2585.523682
 4906/5000: episode: 4906, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2072.242, mean reward: -2072.242 [-2072.242, -2072.242], mean action: 2.000 [2.000, 2.000],  loss: 2222904.750000, mae: 4075.954590, mean_q: -2569.262695
 4907/5000: episode: 4907, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1622.505, mean reward: -1622.505 [-1622.505, -1622.505], mean action: 2.000 [2.000, 2.000],  loss: 1964587.250000, mae: 4007.636719, mean_q: -2577.770508
 4908/5000: episode: 4908, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1613.820, mean reward: -1613.820 [-1613.820, -1613.820], mean action: 2.000 [2.000, 2.000],  loss: 1697693.750000, mae: 4041.476318, mean_q: -2572.268066
 4909/5000: episode: 4909, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1930.446, mean reward: -1930.446 [-1930.446, -1930.446], mean action: 2.000 [2.000, 2.000],  loss: 1611594.000000, mae: 3989.992676, mean_q: -2565.588867
 4910/5000: episode: 4910, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2262.055, mean reward: -2262.055 [-2262.055, -2262.055], mean action: 2.000 [2.000, 2.000],  loss: 3058891.500000, mae: 4135.151367, mean_q: -2586.834961
 4911/5000: episode: 4911, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2949.269, mean reward: -2949.269 [-2949.269, -2949.269], mean action: 2.000 [2.000, 2.000],  loss: 2831333.000000, mae: 4117.555664, mean_q: -2574.078613
 4912/5000: episode: 4912, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1819.037, mean reward: -1819.037 [-1819.037, -1819.037], mean action: 2.000 [2.000, 2.000],  loss: 1945789.625000, mae: 4079.654541, mean_q: -2580.712402
 4913/5000: episode: 4913, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5520.606, mean reward: -5520.606 [-5520.606, -5520.606], mean action: 2.000 [2.000, 2.000],  loss: 1915694.375000, mae: 4116.962891, mean_q: -2592.306641
 4914/5000: episode: 4914, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5252.701, mean reward: -5252.701 [-5252.701, -5252.701], mean action: 2.000 [2.000, 2.000],  loss: 3890019.500000, mae: 4311.094238, mean_q: -2628.165527
 4915/5000: episode: 4915, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3479.325, mean reward: -3479.325 [-3479.325, -3479.325], mean action: 2.000 [2.000, 2.000],  loss: 4535131.500000, mae: 4186.808594, mean_q: -2614.771729
 4916/5000: episode: 4916, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2408.820, mean reward: -2408.820 [-2408.820, -2408.820], mean action: 2.000 [2.000, 2.000],  loss: 3033200.750000, mae: 4201.369629, mean_q: -2634.860352
 4917/5000: episode: 4917, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2261.522, mean reward: -2261.522 [-2261.522, -2261.522], mean action: 2.000 [2.000, 2.000],  loss: 2494505.000000, mae: 4129.111328, mean_q: -2631.745117
 4918/5000: episode: 4918, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -1074.647, mean reward: -1074.647 [-1074.647, -1074.647], mean action: 2.000 [2.000, 2.000],  loss: 3904682.000000, mae: 4223.887695, mean_q: -2649.937744
 4919/5000: episode: 4919, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1026.336, mean reward: -1026.336 [-1026.336, -1026.336], mean action: 2.000 [2.000, 2.000],  loss: 2326529.000000, mae: 4190.666016, mean_q: -2655.063232
 4920/5000: episode: 4920, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4911.728, mean reward: -4911.728 [-4911.728, -4911.728], mean action: 2.000 [2.000, 2.000],  loss: 2642577.500000, mae: 4175.554688, mean_q: -2637.429199
 4921/5000: episode: 4921, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1738.575, mean reward: -1738.575 [-1738.575, -1738.575], mean action: 2.000 [2.000, 2.000],  loss: 2657321.750000, mae: 4172.603027, mean_q: -2659.749268
 4922/5000: episode: 4922, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4008.696, mean reward: -4008.696 [-4008.696, -4008.696], mean action: 2.000 [2.000, 2.000],  loss: 1467047.750000, mae: 4081.330322, mean_q: -2653.564697
 4923/5000: episode: 4923, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2050.940, mean reward: -2050.940 [-2050.940, -2050.940], mean action: 2.000 [2.000, 2.000],  loss: 3627406.500000, mae: 4186.931641, mean_q: -2672.775391
 4924/5000: episode: 4924, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -6163.908, mean reward: -6163.908 [-6163.908, -6163.908], mean action: 2.000 [2.000, 2.000],  loss: 3361552.500000, mae: 4208.523926, mean_q: -2674.874023
 4925/5000: episode: 4925, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -1266.500, mean reward: -1266.500 [-1266.500, -1266.500], mean action: 2.000 [2.000, 2.000],  loss: 2152729.000000, mae: 4151.258301, mean_q: -2690.888184
 4926/5000: episode: 4926, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2443.881, mean reward: -2443.881 [-2443.881, -2443.881], mean action: 2.000 [2.000, 2.000],  loss: 2308751.500000, mae: 4129.578125, mean_q: -2676.603516
 4927/5000: episode: 4927, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -829.055, mean reward: -829.055 [-829.055, -829.055], mean action: 2.000 [2.000, 2.000],  loss: 2039457.375000, mae: 4116.999023, mean_q: -2657.050049
 4928/5000: episode: 4928, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5445.070, mean reward: -5445.070 [-5445.070, -5445.070], mean action: 2.000 [2.000, 2.000],  loss: 2488239.250000, mae: 4164.625000, mean_q: -2654.077637
 4929/5000: episode: 4929, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3953.159, mean reward: -3953.159 [-3953.159, -3953.159], mean action: 2.000 [2.000, 2.000],  loss: 2819834.000000, mae: 4211.682617, mean_q: -2685.831543
 4930/5000: episode: 4930, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1489.844, mean reward: -1489.844 [-1489.844, -1489.844], mean action: 2.000 [2.000, 2.000],  loss: 2214671.000000, mae: 4183.096680, mean_q: -2703.411621
 4931/5000: episode: 4931, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6001.545, mean reward: -6001.545 [-6001.545, -6001.545], mean action: 2.000 [2.000, 2.000],  loss: 2093223.250000, mae: 4207.966797, mean_q: -2704.433105
 4932/5000: episode: 4932, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -544.189, mean reward: -544.189 [-544.189, -544.189], mean action: 2.000 [2.000, 2.000],  loss: 2196850.750000, mae: 4255.586914, mean_q: -2722.920166
 4933/5000: episode: 4933, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2258.192, mean reward: -2258.192 [-2258.192, -2258.192], mean action: 2.000 [2.000, 2.000],  loss: 3400119.000000, mae: 4290.458984, mean_q: -2711.463867
 4934/5000: episode: 4934, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2530.532, mean reward: -2530.532 [-2530.532, -2530.532], mean action: 2.000 [2.000, 2.000],  loss: 1587161.750000, mae: 4174.468750, mean_q: -2697.266602
 4935/5000: episode: 4935, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1010.651, mean reward: -1010.651 [-1010.651, -1010.651], mean action: 2.000 [2.000, 2.000],  loss: 1726929.250000, mae: 4133.030762, mean_q: -2686.559082
 4936/5000: episode: 4936, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -935.856, mean reward: -935.856 [-935.856, -935.856], mean action: 2.000 [2.000, 2.000],  loss: 3192541.000000, mae: 4102.002441, mean_q: -2692.460938
 4937/5000: episode: 4937, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -140.210, mean reward: -140.210 [-140.210, -140.210], mean action: 2.000 [2.000, 2.000],  loss: 1875085.000000, mae: 4093.427246, mean_q: -2675.663086
 4938/5000: episode: 4938, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4002.440, mean reward: -4002.440 [-4002.440, -4002.440], mean action: 2.000 [2.000, 2.000],  loss: 1968597.875000, mae: 4225.842285, mean_q: -2698.574219
 4939/5000: episode: 4939, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7169.451, mean reward: -7169.451 [-7169.451, -7169.451], mean action: 2.000 [2.000, 2.000],  loss: 2460673.750000, mae: 4015.339111, mean_q: -2643.782715
 4940/5000: episode: 4940, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2020.677, mean reward: -2020.677 [-2020.677, -2020.677], mean action: 2.000 [2.000, 2.000],  loss: 3722821.000000, mae: 4138.680664, mean_q: -2645.875488
 4941/5000: episode: 4941, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4849.885, mean reward: -4849.885 [-4849.885, -4849.885], mean action: 2.000 [2.000, 2.000],  loss: 1867244.250000, mae: 4090.196289, mean_q: -2628.526855
 4942/5000: episode: 4942, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1486.280, mean reward: -1486.280 [-1486.280, -1486.280], mean action: 2.000 [2.000, 2.000],  loss: 3885724.250000, mae: 4238.073242, mean_q: -2655.911377
 4943/5000: episode: 4943, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -47.522, mean reward: -47.522 [-47.522, -47.522], mean action: 2.000 [2.000, 2.000],  loss: 2388869.500000, mae: 4251.042480, mean_q: -2635.218994
 4944/5000: episode: 4944, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -395.851, mean reward: -395.851 [-395.851, -395.851], mean action: 2.000 [2.000, 2.000],  loss: 1972549.250000, mae: 4046.619141, mean_q: -2615.662842
 4945/5000: episode: 4945, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -371.767, mean reward: -371.767 [-371.767, -371.767], mean action: 2.000 [2.000, 2.000],  loss: 1957131.500000, mae: 4124.894043, mean_q: -2635.878418
 4946/5000: episode: 4946, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1801.134, mean reward: -1801.134 [-1801.134, -1801.134], mean action: 2.000 [2.000, 2.000],  loss: 3164339.500000, mae: 4202.673340, mean_q: -2642.187500
 4947/5000: episode: 4947, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3848.726, mean reward: -3848.726 [-3848.726, -3848.726], mean action: 2.000 [2.000, 2.000],  loss: 2288271.000000, mae: 4175.282227, mean_q: -2620.714844
 4948/5000: episode: 4948, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8369.282, mean reward: -8369.282 [-8369.282, -8369.282], mean action: 0.000 [0.000, 0.000],  loss: 1418107.750000, mae: 4090.476562, mean_q: -2615.788086
 4949/5000: episode: 4949, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3810.663, mean reward: -3810.663 [-3810.663, -3810.663], mean action: 3.000 [3.000, 3.000],  loss: 3161684.000000, mae: 4246.514648, mean_q: -2625.672607
 4950/5000: episode: 4950, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6014.245, mean reward: -6014.245 [-6014.245, -6014.245], mean action: 2.000 [2.000, 2.000],  loss: 1490415.500000, mae: 4167.561035, mean_q: -2640.506836
 4951/5000: episode: 4951, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6276.609, mean reward: -6276.609 [-6276.609, -6276.609], mean action: 2.000 [2.000, 2.000],  loss: 2525376.500000, mae: 4219.780273, mean_q: -2631.169678
 4952/5000: episode: 4952, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5880.414, mean reward: -5880.414 [-5880.414, -5880.414], mean action: 2.000 [2.000, 2.000],  loss: 2588950.250000, mae: 4163.595215, mean_q: -2653.673340
 4953/5000: episode: 4953, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -349.617, mean reward: -349.617 [-349.617, -349.617], mean action: 2.000 [2.000, 2.000],  loss: 4771210.000000, mae: 4398.181641, mean_q: -2646.486328
 4954/5000: episode: 4954, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -966.364, mean reward: -966.364 [-966.364, -966.364], mean action: 2.000 [2.000, 2.000],  loss: 4492319.000000, mae: 4398.650391, mean_q: -2649.293945
 4955/5000: episode: 4955, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -628.465, mean reward: -628.465 [-628.465, -628.465], mean action: 2.000 [2.000, 2.000],  loss: 2497586.000000, mae: 4212.231445, mean_q: -2641.833008
 4956/5000: episode: 4956, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -993.700, mean reward: -993.700 [-993.700, -993.700], mean action: 2.000 [2.000, 2.000],  loss: 2416509.250000, mae: 4136.000488, mean_q: -2667.043213
 4957/5000: episode: 4957, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6140.119, mean reward: -6140.119 [-6140.119, -6140.119], mean action: 2.000 [2.000, 2.000],  loss: 5359501.500000, mae: 4339.553711, mean_q: -2668.488770
 4958/5000: episode: 4958, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -66.875, mean reward: -66.875 [-66.875, -66.875], mean action: 2.000 [2.000, 2.000],  loss: 2484810.500000, mae: 4198.623047, mean_q: -2675.090820
 4959/5000: episode: 4959, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -532.099, mean reward: -532.099 [-532.099, -532.099], mean action: 2.000 [2.000, 2.000],  loss: 2062800.000000, mae: 4146.601562, mean_q: -2674.654541
 4960/5000: episode: 4960, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3062.943, mean reward: -3062.943 [-3062.943, -3062.943], mean action: 2.000 [2.000, 2.000],  loss: 1686473.000000, mae: 4155.792969, mean_q: -2696.792236
 4961/5000: episode: 4961, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3581.997, mean reward: -3581.997 [-3581.997, -3581.997], mean action: 2.000 [2.000, 2.000],  loss: 2631443.250000, mae: 4224.909180, mean_q: -2691.636475
 4962/5000: episode: 4962, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1166.179, mean reward: -1166.179 [-1166.179, -1166.179], mean action: 2.000 [2.000, 2.000],  loss: 3280371.500000, mae: 4263.466797, mean_q: -2683.523926
 4963/5000: episode: 4963, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1217.723, mean reward: -1217.723 [-1217.723, -1217.723], mean action: 2.000 [2.000, 2.000],  loss: 1504284.500000, mae: 4111.484863, mean_q: -2678.372559
 4964/5000: episode: 4964, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -818.210, mean reward: -818.210 [-818.210, -818.210], mean action: 2.000 [2.000, 2.000],  loss: 3822200.750000, mae: 4256.687988, mean_q: -2696.494385
 4965/5000: episode: 4965, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -155.186, mean reward: -155.186 [-155.186, -155.186], mean action: 2.000 [2.000, 2.000],  loss: 3724497.750000, mae: 4263.890625, mean_q: -2697.363770
 4966/5000: episode: 4966, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -337.237, mean reward: -337.237 [-337.237, -337.237], mean action: 2.000 [2.000, 2.000],  loss: 2232150.000000, mae: 4209.050781, mean_q: -2703.017822
 4967/5000: episode: 4967, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1039.294, mean reward: -1039.294 [-1039.294, -1039.294], mean action: 2.000 [2.000, 2.000],  loss: 2555675.250000, mae: 4223.785156, mean_q: -2671.132812
 4968/5000: episode: 4968, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1250.565, mean reward: -1250.565 [-1250.565, -1250.565], mean action: 2.000 [2.000, 2.000],  loss: 4104202.250000, mae: 4322.896484, mean_q: -2702.123047
 4969/5000: episode: 4969, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2411.426, mean reward: -2411.426 [-2411.426, -2411.426], mean action: 2.000 [2.000, 2.000],  loss: 2239466.500000, mae: 4241.051758, mean_q: -2680.970459
 4970/5000: episode: 4970, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -317.022, mean reward: -317.022 [-317.022, -317.022], mean action: 2.000 [2.000, 2.000],  loss: 2086973.500000, mae: 4221.398438, mean_q: -2710.327637
 4971/5000: episode: 4971, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3736.341, mean reward: -3736.341 [-3736.341, -3736.341], mean action: 2.000 [2.000, 2.000],  loss: 1569269.375000, mae: 4136.163086, mean_q: -2667.078125
 4972/5000: episode: 4972, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1761.762, mean reward: -1761.762 [-1761.762, -1761.762], mean action: 2.000 [2.000, 2.000],  loss: 4154804.500000, mae: 4280.576660, mean_q: -2688.941406
 4973/5000: episode: 4973, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -36.631, mean reward: -36.631 [-36.631, -36.631], mean action: 2.000 [2.000, 2.000],  loss: 3232177.500000, mae: 4298.021484, mean_q: -2706.592285
 4974/5000: episode: 4974, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1695.200, mean reward: -1695.200 [-1695.200, -1695.200], mean action: 2.000 [2.000, 2.000],  loss: 3384836.000000, mae: 4262.977539, mean_q: -2705.434814
 4975/5000: episode: 4975, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1808.272, mean reward: -1808.272 [-1808.272, -1808.272], mean action: 1.000 [1.000, 1.000],  loss: 2545100.500000, mae: 4132.951172, mean_q: -2710.989746
 4976/5000: episode: 4976, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1722.782, mean reward: -1722.782 [-1722.782, -1722.782], mean action: 2.000 [2.000, 2.000],  loss: 2626104.250000, mae: 4219.852051, mean_q: -2741.073730
 4977/5000: episode: 4977, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3961.166, mean reward: -3961.166 [-3961.166, -3961.166], mean action: 2.000 [2.000, 2.000],  loss: 2856325.500000, mae: 4305.035156, mean_q: -2721.540527
 4978/5000: episode: 4978, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5.555, mean reward: -5.555 [-5.555, -5.555], mean action: 2.000 [2.000, 2.000],  loss: 2197488.000000, mae: 4245.394531, mean_q: -2704.915527
 4979/5000: episode: 4979, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3452.314, mean reward: -3452.314 [-3452.314, -3452.314], mean action: 2.000 [2.000, 2.000],  loss: 3622044.250000, mae: 4233.273438, mean_q: -2712.187988
 4980/5000: episode: 4980, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2611.702, mean reward: -2611.702 [-2611.702, -2611.702], mean action: 2.000 [2.000, 2.000],  loss: 1701142.250000, mae: 4137.591797, mean_q: -2693.711182
 4981/5000: episode: 4981, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7263.802, mean reward: -7263.802 [-7263.802, -7263.802], mean action: 2.000 [2.000, 2.000],  loss: 3976965.500000, mae: 4335.049805, mean_q: -2715.221680
 4982/5000: episode: 4982, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -10.995, mean reward: -10.995 [-10.995, -10.995], mean action: 2.000 [2.000, 2.000],  loss: 3131591.750000, mae: 4176.129883, mean_q: -2679.518799
 4983/5000: episode: 4983, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6884.860, mean reward: -6884.860 [-6884.860, -6884.860], mean action: 2.000 [2.000, 2.000],  loss: 2824858.500000, mae: 4187.815430, mean_q: -2691.193359
 4984/5000: episode: 4984, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -765.137, mean reward: -765.137 [-765.137, -765.137], mean action: 2.000 [2.000, 2.000],  loss: 2660981.750000, mae: 4166.990234, mean_q: -2680.860352
 4985/5000: episode: 4985, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -57.697, mean reward: -57.697 [-57.697, -57.697], mean action: 2.000 [2.000, 2.000],  loss: 2794380.000000, mae: 4257.431641, mean_q: -2715.968262
 4986/5000: episode: 4986, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3970.829, mean reward: -3970.829 [-3970.829, -3970.829], mean action: 2.000 [2.000, 2.000],  loss: 2465583.500000, mae: 4172.803711, mean_q: -2685.712402
 4987/5000: episode: 4987, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -404.028, mean reward: -404.028 [-404.028, -404.028], mean action: 2.000 [2.000, 2.000],  loss: 1718642.000000, mae: 4158.544434, mean_q: -2685.704346
 4988/5000: episode: 4988, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5856.720, mean reward: -5856.720 [-5856.720, -5856.720], mean action: 2.000 [2.000, 2.000],  loss: 3516314.500000, mae: 4126.908691, mean_q: -2673.216797
 4989/5000: episode: 4989, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2451.042, mean reward: -2451.042 [-2451.042, -2451.042], mean action: 2.000 [2.000, 2.000],  loss: 8447652.000000, mae: 4491.400391, mean_q: -2680.631348
 4990/5000: episode: 4990, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4190.974, mean reward: -4190.974 [-4190.974, -4190.974], mean action: 2.000 [2.000, 2.000],  loss: 3075714.250000, mae: 4177.397461, mean_q: -2659.347168
 4991/5000: episode: 4991, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3236.746, mean reward: -3236.746 [-3236.746, -3236.746], mean action: 2.000 [2.000, 2.000],  loss: 2296893.500000, mae: 4205.262695, mean_q: -2661.637695
 4992/5000: episode: 4992, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -2481.226, mean reward: -2481.226 [-2481.226, -2481.226], mean action: 2.000 [2.000, 2.000],  loss: 3041070.000000, mae: 4233.171387, mean_q: -2663.108154
 4993/5000: episode: 4993, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3230.953, mean reward: -3230.953 [-3230.953, -3230.953], mean action: 2.000 [2.000, 2.000],  loss: 1947850.500000, mae: 4147.105469, mean_q: -2647.056152
 4994/5000: episode: 4994, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7160.683, mean reward: -7160.683 [-7160.683, -7160.683], mean action: 2.000 [2.000, 2.000],  loss: 3194920.000000, mae: 4243.795898, mean_q: -2660.022949
 4995/5000: episode: 4995, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2463.790, mean reward: -2463.790 [-2463.790, -2463.790], mean action: 2.000 [2.000, 2.000],  loss: 2065509.375000, mae: 4135.868652, mean_q: -2665.558594
 4996/5000: episode: 4996, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8848.782, mean reward: -8848.782 [-8848.782, -8848.782], mean action: 2.000 [2.000, 2.000],  loss: 2934346.500000, mae: 4284.652344, mean_q: -2653.158203
 4997/5000: episode: 4997, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1061.371, mean reward: -1061.371 [-1061.371, -1061.371], mean action: 2.000 [2.000, 2.000],  loss: 2404565.500000, mae: 4188.496582, mean_q: -2666.243652
 4998/5000: episode: 4998, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2367.920, mean reward: -2367.920 [-2367.920, -2367.920], mean action: 2.000 [2.000, 2.000],  loss: 3748850.500000, mae: 4251.885742, mean_q: -2670.833984
 4999/5000: episode: 4999, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4718.700, mean reward: -4718.700 [-4718.700, -4718.700], mean action: 1.000 [1.000, 1.000],  loss: 2583287.500000, mae: 4113.946777, mean_q: -2672.065674
 5000/5000: episode: 5000, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4188.577, mean reward: -4188.577 [-4188.577, -4188.577], mean action: 2.000 [2.000, 2.000],  loss: 2563297.500000, mae: 4174.039062, mean_q: -2675.703857
done, took 257.857 seconds
