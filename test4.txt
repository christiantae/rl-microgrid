Training for 5000 steps ...
    1/5000: episode: 1, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -4914.910, mean reward: -4914.910 [-4914.910, -4914.910], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
    2/5000: episode: 2, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8776.185, mean reward: -8776.185 [-8776.185, -8776.185], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
    3/5000: episode: 3, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -17587.830, mean reward: -17587.830 [-17587.830, -17587.830], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
    4/5000: episode: 4, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -880.198, mean reward: -880.198 [-880.198, -880.198], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
    5/5000: episode: 5, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4267.396, mean reward: -4267.396 [-4267.396, -4267.396], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
    6/5000: episode: 6, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -11764.947, mean reward: -11764.947 [-11764.947, -11764.947], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
    7/5000: episode: 7, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9950.581, mean reward: -9950.581 [-9950.581, -9950.581], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
    8/5000: episode: 8, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9759.625, mean reward: -9759.625 [-9759.625, -9759.625], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
    9/5000: episode: 9, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5550.439, mean reward: -5550.439 [-5550.439, -5550.439], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   10/5000: episode: 10, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7874.699, mean reward: -7874.699 [-7874.699, -7874.699], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   11/5000: episode: 11, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8178.725, mean reward: -8178.725 [-8178.725, -8178.725], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   12/5000: episode: 12, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1131.990, mean reward: -1131.990 [-1131.990, -1131.990], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   13/5000: episode: 13, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -6883.286, mean reward: -6883.286 [-6883.286, -6883.286], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   14/5000: episode: 14, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -11313.666, mean reward: -11313.666 [-11313.666, -11313.666], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   15/5000: episode: 15, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3886.647, mean reward: -3886.647 [-3886.647, -3886.647], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   16/5000: episode: 16, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -14026.009, mean reward: -14026.009 [-14026.009, -14026.009], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   17/5000: episode: 17, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -9529.117, mean reward: -9529.117 [-9529.117, -9529.117], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   18/5000: episode: 18, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -2252.138, mean reward: -2252.138 [-2252.138, -2252.138], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   19/5000: episode: 19, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1079.310, mean reward: -1079.310 [-1079.310, -1079.310], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   20/5000: episode: 20, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -11972.514, mean reward: -11972.514 [-11972.514, -11972.514], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   21/5000: episode: 21, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -10557.088, mean reward: -10557.088 [-10557.088, -10557.088], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   22/5000: episode: 22, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -9487.819, mean reward: -9487.819 [-9487.819, -9487.819], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   23/5000: episode: 23, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -12101.241, mean reward: -12101.241 [-12101.241, -12101.241], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   24/5000: episode: 24, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -3475.427, mean reward: -3475.427 [-3475.427, -3475.427], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   25/5000: episode: 25, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9309.300, mean reward: -9309.300 [-9309.300, -9309.300], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   26/5000: episode: 26, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -9110.357, mean reward: -9110.357 [-9110.357, -9110.357], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   27/5000: episode: 27, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4510.482, mean reward: -4510.482 [-4510.482, -4510.482], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   28/5000: episode: 28, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4117.555, mean reward: -4117.555 [-4117.555, -4117.555], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   29/5000: episode: 29, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -2529.974, mean reward: -2529.974 [-2529.974, -2529.974], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   30/5000: episode: 30, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3600.619, mean reward: -3600.619 [-3600.619, -3600.619], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   31/5000: episode: 31, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4747.726, mean reward: -4747.726 [-4747.726, -4747.726], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   32/5000: episode: 32, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8723.545, mean reward: -8723.545 [-8723.545, -8723.545], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   33/5000: episode: 33, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7913.067, mean reward: -7913.067 [-7913.067, -7913.067], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   34/5000: episode: 34, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -8843.595, mean reward: -8843.595 [-8843.595, -8843.595], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   35/5000: episode: 35, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3506.557, mean reward: -3506.557 [-3506.557, -3506.557], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   36/5000: episode: 36, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4257.234, mean reward: -4257.234 [-4257.234, -4257.234], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   37/5000: episode: 37, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4921.743, mean reward: -4921.743 [-4921.743, -4921.743], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   38/5000: episode: 38, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2239.718, mean reward: -2239.718 [-2239.718, -2239.718], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   39/5000: episode: 39, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5321.991, mean reward: -5321.991 [-5321.991, -5321.991], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   40/5000: episode: 40, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5681.125, mean reward: -5681.125 [-5681.125, -5681.125], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   41/5000: episode: 41, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6057.752, mean reward: -6057.752 [-6057.752, -6057.752], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   42/5000: episode: 42, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -6696.869, mean reward: -6696.869 [-6696.869, -6696.869], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   43/5000: episode: 43, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2420.047, mean reward: -2420.047 [-2420.047, -2420.047], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   44/5000: episode: 44, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -16507.960, mean reward: -16507.960 [-16507.960, -16507.960], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   45/5000: episode: 45, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -9526.645, mean reward: -9526.645 [-9526.645, -9526.645], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   46/5000: episode: 46, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -9939.893, mean reward: -9939.893 [-9939.893, -9939.893], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   47/5000: episode: 47, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4460.772, mean reward: -4460.772 [-4460.772, -4460.772], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
   48/5000: episode: 48, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2265.118, mean reward: -2265.118 [-2265.118, -2265.118], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   49/5000: episode: 49, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2527.734, mean reward: -2527.734 [-2527.734, -2527.734], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   50/5000: episode: 50, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -9993.421, mean reward: -9993.421 [-9993.421, -9993.421], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   51/5000: episode: 51, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3756.142, mean reward: -3756.142 [-3756.142, -3756.142], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   52/5000: episode: 52, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7773.277, mean reward: -7773.277 [-7773.277, -7773.277], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   53/5000: episode: 53, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8704.483, mean reward: -8704.483 [-8704.483, -8704.483], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   54/5000: episode: 54, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -13308.159, mean reward: -13308.159 [-13308.159, -13308.159], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   55/5000: episode: 55, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3915.890, mean reward: -3915.890 [-3915.890, -3915.890], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   56/5000: episode: 56, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -10807.871, mean reward: -10807.871 [-10807.871, -10807.871], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   57/5000: episode: 57, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6869.398, mean reward: -6869.398 [-6869.398, -6869.398], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
   58/5000: episode: 58, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4515.400, mean reward: -4515.400 [-4515.400, -4515.400], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   59/5000: episode: 59, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6166.304, mean reward: -6166.304 [-6166.304, -6166.304], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   60/5000: episode: 60, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2518.719, mean reward: -2518.719 [-2518.719, -2518.719], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   61/5000: episode: 61, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1370.187, mean reward: -1370.187 [-1370.187, -1370.187], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   62/5000: episode: 62, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9888.375, mean reward: -9888.375 [-9888.375, -9888.375], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   63/5000: episode: 63, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -8914.820, mean reward: -8914.820 [-8914.820, -8914.820], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   64/5000: episode: 64, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -11964.912, mean reward: -11964.912 [-11964.912, -11964.912], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   65/5000: episode: 65, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -10191.527, mean reward: -10191.527 [-10191.527, -10191.527], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   66/5000: episode: 66, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9353.343, mean reward: -9353.343 [-9353.343, -9353.343], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   67/5000: episode: 67, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10396.767, mean reward: -10396.767 [-10396.767, -10396.767], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   68/5000: episode: 68, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5427.712, mean reward: -5427.712 [-5427.712, -5427.712], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   69/5000: episode: 69, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -12743.177, mean reward: -12743.177 [-12743.177, -12743.177], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   70/5000: episode: 70, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3680.784, mean reward: -3680.784 [-3680.784, -3680.784], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   71/5000: episode: 71, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4522.154, mean reward: -4522.154 [-4522.154, -4522.154], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   72/5000: episode: 72, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5424.027, mean reward: -5424.027 [-5424.027, -5424.027], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   73/5000: episode: 73, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -10263.999, mean reward: -10263.999 [-10263.999, -10263.999], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   74/5000: episode: 74, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4845.148, mean reward: -4845.148 [-4845.148, -4845.148], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   75/5000: episode: 75, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6831.372, mean reward: -6831.372 [-6831.372, -6831.372], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   76/5000: episode: 76, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10903.084, mean reward: -10903.084 [-10903.084, -10903.084], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   77/5000: episode: 77, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5982.753, mean reward: -5982.753 [-5982.753, -5982.753], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   78/5000: episode: 78, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -6616.550, mean reward: -6616.550 [-6616.550, -6616.550], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   79/5000: episode: 79, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -12263.147, mean reward: -12263.147 [-12263.147, -12263.147], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   80/5000: episode: 80, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1844.286, mean reward: -1844.286 [-1844.286, -1844.286], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   81/5000: episode: 81, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -10899.914, mean reward: -10899.914 [-10899.914, -10899.914], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   82/5000: episode: 82, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8133.542, mean reward: -8133.542 [-8133.542, -8133.542], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   83/5000: episode: 83, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -11814.101, mean reward: -11814.101 [-11814.101, -11814.101], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   84/5000: episode: 84, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6506.828, mean reward: -6506.828 [-6506.828, -6506.828], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   85/5000: episode: 85, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2464.266, mean reward: -2464.266 [-2464.266, -2464.266], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   86/5000: episode: 86, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4740.425, mean reward: -4740.425 [-4740.425, -4740.425], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   87/5000: episode: 87, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -7810.774, mean reward: -7810.774 [-7810.774, -7810.774], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   88/5000: episode: 88, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3329.532, mean reward: -3329.532 [-3329.532, -3329.532], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   89/5000: episode: 89, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6326.143, mean reward: -6326.143 [-6326.143, -6326.143], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   90/5000: episode: 90, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -12790.180, mean reward: -12790.180 [-12790.180, -12790.180], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   91/5000: episode: 91, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3827.669, mean reward: -3827.669 [-3827.669, -3827.669], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   92/5000: episode: 92, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5058.218, mean reward: -5058.218 [-5058.218, -5058.218], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   93/5000: episode: 93, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -13932.252, mean reward: -13932.252 [-13932.252, -13932.252], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   94/5000: episode: 94, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5481.834, mean reward: -5481.834 [-5481.834, -5481.834], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   95/5000: episode: 95, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7044.050, mean reward: -7044.050 [-7044.050, -7044.050], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   96/5000: episode: 96, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -12865.393, mean reward: -12865.393 [-12865.393, -12865.393], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   97/5000: episode: 97, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9251.482, mean reward: -9251.482 [-9251.482, -9251.482], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   98/5000: episode: 98, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -10598.154, mean reward: -10598.154 [-10598.154, -10598.154], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
   99/5000: episode: 99, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11774.517, mean reward: -11774.517 [-11774.517, -11774.517], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  100/5000: episode: 100, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7286.828, mean reward: -7286.828 [-7286.828, -7286.828], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  101/5000: episode: 101, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3414.677, mean reward: -3414.677 [-3414.677, -3414.677], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  102/5000: episode: 102, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11704.388, mean reward: -11704.388 [-11704.388, -11704.388], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  103/5000: episode: 103, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8473.101, mean reward: -8473.101 [-8473.101, -8473.101], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  104/5000: episode: 104, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3548.925, mean reward: -3548.925 [-3548.925, -3548.925], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  105/5000: episode: 105, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -12907.124, mean reward: -12907.124 [-12907.124, -12907.124], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  106/5000: episode: 106, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10841.014, mean reward: -10841.014 [-10841.014, -10841.014], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  107/5000: episode: 107, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -13195.275, mean reward: -13195.275 [-13195.275, -13195.275], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  108/5000: episode: 108, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -2079.975, mean reward: -2079.975 [-2079.975, -2079.975], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  109/5000: episode: 109, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9260.525, mean reward: -9260.525 [-9260.525, -9260.525], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  110/5000: episode: 110, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -9592.806, mean reward: -9592.806 [-9592.806, -9592.806], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  111/5000: episode: 111, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7369.019, mean reward: -7369.019 [-7369.019, -7369.019], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  112/5000: episode: 112, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5188.260, mean reward: -5188.260 [-5188.260, -5188.260], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  113/5000: episode: 113, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -9688.001, mean reward: -9688.001 [-9688.001, -9688.001], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  114/5000: episode: 114, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -8021.050, mean reward: -8021.050 [-8021.050, -8021.050], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  115/5000: episode: 115, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4342.933, mean reward: -4342.933 [-4342.933, -4342.933], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  116/5000: episode: 116, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5709.122, mean reward: -5709.122 [-5709.122, -5709.122], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  117/5000: episode: 117, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6342.180, mean reward: -6342.180 [-6342.180, -6342.180], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  118/5000: episode: 118, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2652.330, mean reward: -2652.330 [-2652.330, -2652.330], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  119/5000: episode: 119, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5015.026, mean reward: -5015.026 [-5015.026, -5015.026], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  120/5000: episode: 120, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5657.920, mean reward: -5657.920 [-5657.920, -5657.920], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  121/5000: episode: 121, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3000.549, mean reward: -3000.549 [-3000.549, -3000.549], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  122/5000: episode: 122, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5618.373, mean reward: -5618.373 [-5618.373, -5618.373], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  123/5000: episode: 123, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -6568.273, mean reward: -6568.273 [-6568.273, -6568.273], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  124/5000: episode: 124, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -962.038, mean reward: -962.038 [-962.038, -962.038], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  125/5000: episode: 125, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -13589.433, mean reward: -13589.433 [-13589.433, -13589.433], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  126/5000: episode: 126, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4709.910, mean reward: -4709.910 [-4709.910, -4709.910], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  127/5000: episode: 127, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6052.926, mean reward: -6052.926 [-6052.926, -6052.926], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  128/5000: episode: 128, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6348.682, mean reward: -6348.682 [-6348.682, -6348.682], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  129/5000: episode: 129, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -10112.218, mean reward: -10112.218 [-10112.218, -10112.218], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  130/5000: episode: 130, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10904.505, mean reward: -10904.505 [-10904.505, -10904.505], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  131/5000: episode: 131, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -8377.128, mean reward: -8377.128 [-8377.128, -8377.128], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  132/5000: episode: 132, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1749.537, mean reward: -1749.537 [-1749.537, -1749.537], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  133/5000: episode: 133, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7992.903, mean reward: -7992.903 [-7992.903, -7992.903], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  134/5000: episode: 134, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7582.700, mean reward: -7582.700 [-7582.700, -7582.700], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  135/5000: episode: 135, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -13314.394, mean reward: -13314.394 [-13314.394, -13314.394], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  136/5000: episode: 136, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7880.002, mean reward: -7880.002 [-7880.002, -7880.002], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  137/5000: episode: 137, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -6075.725, mean reward: -6075.725 [-6075.725, -6075.725], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  138/5000: episode: 138, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -7860.249, mean reward: -7860.249 [-7860.249, -7860.249], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  139/5000: episode: 139, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3648.639, mean reward: -3648.639 [-3648.639, -3648.639], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  140/5000: episode: 140, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5601.249, mean reward: -5601.249 [-5601.249, -5601.249], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  141/5000: episode: 141, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1895.451, mean reward: -1895.451 [-1895.451, -1895.451], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  142/5000: episode: 142, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3662.634, mean reward: -3662.634 [-3662.634, -3662.634], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  143/5000: episode: 143, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6820.895, mean reward: -6820.895 [-6820.895, -6820.895], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  144/5000: episode: 144, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3983.750, mean reward: -3983.750 [-3983.750, -3983.750], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  145/5000: episode: 145, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8065.683, mean reward: -8065.683 [-8065.683, -8065.683], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  146/5000: episode: 146, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7290.706, mean reward: -7290.706 [-7290.706, -7290.706], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  147/5000: episode: 147, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4888.971, mean reward: -4888.971 [-4888.971, -4888.971], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  148/5000: episode: 148, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9238.348, mean reward: -9238.348 [-9238.348, -9238.348], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  149/5000: episode: 149, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8268.724, mean reward: -8268.724 [-8268.724, -8268.724], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  150/5000: episode: 150, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -12348.502, mean reward: -12348.502 [-12348.502, -12348.502], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  151/5000: episode: 151, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6789.959, mean reward: -6789.959 [-6789.959, -6789.959], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  152/5000: episode: 152, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5879.795, mean reward: -5879.795 [-5879.795, -5879.795], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  153/5000: episode: 153, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2429.692, mean reward: -2429.692 [-2429.692, -2429.692], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  154/5000: episode: 154, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -9606.305, mean reward: -9606.305 [-9606.305, -9606.305], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  155/5000: episode: 155, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7914.781, mean reward: -7914.781 [-7914.781, -7914.781], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  156/5000: episode: 156, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -6644.922, mean reward: -6644.922 [-6644.922, -6644.922], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  157/5000: episode: 157, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4488.732, mean reward: -4488.732 [-4488.732, -4488.732], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  158/5000: episode: 158, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2664.988, mean reward: -2664.988 [-2664.988, -2664.988], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  159/5000: episode: 159, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6736.243, mean reward: -6736.243 [-6736.243, -6736.243], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  160/5000: episode: 160, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -13432.151, mean reward: -13432.151 [-13432.151, -13432.151], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  161/5000: episode: 161, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4317.759, mean reward: -4317.759 [-4317.759, -4317.759], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  162/5000: episode: 162, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5366.360, mean reward: -5366.360 [-5366.360, -5366.360], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  163/5000: episode: 163, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -12143.785, mean reward: -12143.785 [-12143.785, -12143.785], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  164/5000: episode: 164, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -11360.128, mean reward: -11360.128 [-11360.128, -11360.128], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  165/5000: episode: 165, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7290.137, mean reward: -7290.137 [-7290.137, -7290.137], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  166/5000: episode: 166, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7182.979, mean reward: -7182.979 [-7182.979, -7182.979], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  167/5000: episode: 167, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2099.032, mean reward: -2099.032 [-2099.032, -2099.032], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  168/5000: episode: 168, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7335.769, mean reward: -7335.769 [-7335.769, -7335.769], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  169/5000: episode: 169, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3209.156, mean reward: -3209.156 [-3209.156, -3209.156], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  170/5000: episode: 170, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7073.358, mean reward: -7073.358 [-7073.358, -7073.358], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  171/5000: episode: 171, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -2523.465, mean reward: -2523.465 [-2523.465, -2523.465], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  172/5000: episode: 172, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6552.247, mean reward: -6552.247 [-6552.247, -6552.247], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  173/5000: episode: 173, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -9826.310, mean reward: -9826.310 [-9826.310, -9826.310], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  174/5000: episode: 174, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -3047.106, mean reward: -3047.106 [-3047.106, -3047.106], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  175/5000: episode: 175, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -6460.148, mean reward: -6460.148 [-6460.148, -6460.148], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  176/5000: episode: 176, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8950.499, mean reward: -8950.499 [-8950.499, -8950.499], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  177/5000: episode: 177, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -13314.185, mean reward: -13314.185 [-13314.185, -13314.185], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  178/5000: episode: 178, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9284.317, mean reward: -9284.317 [-9284.317, -9284.317], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  179/5000: episode: 179, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -4189.203, mean reward: -4189.203 [-4189.203, -4189.203], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  180/5000: episode: 180, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -7287.716, mean reward: -7287.716 [-7287.716, -7287.716], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  181/5000: episode: 181, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -6403.460, mean reward: -6403.460 [-6403.460, -6403.460], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  182/5000: episode: 182, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5623.512, mean reward: -5623.512 [-5623.512, -5623.512], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  183/5000: episode: 183, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6315.944, mean reward: -6315.944 [-6315.944, -6315.944], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  184/5000: episode: 184, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4720.864, mean reward: -4720.864 [-4720.864, -4720.864], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  185/5000: episode: 185, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -13136.729, mean reward: -13136.729 [-13136.729, -13136.729], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  186/5000: episode: 186, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6878.052, mean reward: -6878.052 [-6878.052, -6878.052], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  187/5000: episode: 187, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -4794.110, mean reward: -4794.110 [-4794.110, -4794.110], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  188/5000: episode: 188, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5611.272, mean reward: -5611.272 [-5611.272, -5611.272], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  189/5000: episode: 189, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6915.885, mean reward: -6915.885 [-6915.885, -6915.885], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  190/5000: episode: 190, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -7893.880, mean reward: -7893.880 [-7893.880, -7893.880], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  191/5000: episode: 191, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -9250.675, mean reward: -9250.675 [-9250.675, -9250.675], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  192/5000: episode: 192, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -10820.053, mean reward: -10820.053 [-10820.053, -10820.053], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  193/5000: episode: 193, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5835.914, mean reward: -5835.914 [-5835.914, -5835.914], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  194/5000: episode: 194, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3522.471, mean reward: -3522.471 [-3522.471, -3522.471], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  195/5000: episode: 195, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8032.173, mean reward: -8032.173 [-8032.173, -8032.173], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  196/5000: episode: 196, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5504.495, mean reward: -5504.495 [-5504.495, -5504.495], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  197/5000: episode: 197, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5531.379, mean reward: -5531.379 [-5531.379, -5531.379], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  198/5000: episode: 198, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -8125.440, mean reward: -8125.440 [-8125.440, -8125.440], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  199/5000: episode: 199, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4154.298, mean reward: -4154.298 [-4154.298, -4154.298], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  200/5000: episode: 200, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -6054.939, mean reward: -6054.939 [-6054.939, -6054.939], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  201/5000: episode: 201, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -11246.541, mean reward: -11246.541 [-11246.541, -11246.541], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  202/5000: episode: 202, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3762.943, mean reward: -3762.943 [-3762.943, -3762.943], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  203/5000: episode: 203, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9000.844, mean reward: -9000.844 [-9000.844, -9000.844], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  204/5000: episode: 204, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6680.397, mean reward: -6680.397 [-6680.397, -6680.397], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  205/5000: episode: 205, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -1043.710, mean reward: -1043.710 [-1043.710, -1043.710], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  206/5000: episode: 206, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8209.595, mean reward: -8209.595 [-8209.595, -8209.595], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  207/5000: episode: 207, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -10041.955, mean reward: -10041.955 [-10041.955, -10041.955], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  208/5000: episode: 208, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4265.192, mean reward: -4265.192 [-4265.192, -4265.192], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  209/5000: episode: 209, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -6625.952, mean reward: -6625.952 [-6625.952, -6625.952], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  210/5000: episode: 210, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -11461.189, mean reward: -11461.189 [-11461.189, -11461.189], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  211/5000: episode: 211, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -10455.467, mean reward: -10455.467 [-10455.467, -10455.467], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  212/5000: episode: 212, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3273.723, mean reward: -3273.723 [-3273.723, -3273.723], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  213/5000: episode: 213, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4785.013, mean reward: -4785.013 [-4785.013, -4785.013], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  214/5000: episode: 214, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -8542.554, mean reward: -8542.554 [-8542.554, -8542.554], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  215/5000: episode: 215, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10573.687, mean reward: -10573.687 [-10573.687, -10573.687], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  216/5000: episode: 216, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -17267.628, mean reward: -17267.628 [-17267.628, -17267.628], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  217/5000: episode: 217, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3610.189, mean reward: -3610.189 [-3610.189, -3610.189], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  218/5000: episode: 218, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7140.026, mean reward: -7140.026 [-7140.026, -7140.026], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  219/5000: episode: 219, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7376.001, mean reward: -7376.001 [-7376.001, -7376.001], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  220/5000: episode: 220, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5040.310, mean reward: -5040.310 [-5040.310, -5040.310], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  221/5000: episode: 221, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -13796.495, mean reward: -13796.495 [-13796.495, -13796.495], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  222/5000: episode: 222, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2762.901, mean reward: -2762.901 [-2762.901, -2762.901], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  223/5000: episode: 223, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5914.586, mean reward: -5914.586 [-5914.586, -5914.586], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  224/5000: episode: 224, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -2046.830, mean reward: -2046.830 [-2046.830, -2046.830], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  225/5000: episode: 225, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -11831.047, mean reward: -11831.047 [-11831.047, -11831.047], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  226/5000: episode: 226, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3478.853, mean reward: -3478.853 [-3478.853, -3478.853], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  227/5000: episode: 227, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -12890.398, mean reward: -12890.398 [-12890.398, -12890.398], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  228/5000: episode: 228, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3665.044, mean reward: -3665.044 [-3665.044, -3665.044], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  229/5000: episode: 229, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4438.475, mean reward: -4438.475 [-4438.475, -4438.475], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  230/5000: episode: 230, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5809.224, mean reward: -5809.224 [-5809.224, -5809.224], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  231/5000: episode: 231, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -15711.015, mean reward: -15711.015 [-15711.015, -15711.015], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  232/5000: episode: 232, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6655.034, mean reward: -6655.034 [-6655.034, -6655.034], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  233/5000: episode: 233, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10576.407, mean reward: -10576.407 [-10576.407, -10576.407], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  234/5000: episode: 234, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2563.013, mean reward: -2563.013 [-2563.013, -2563.013], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  235/5000: episode: 235, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4515.325, mean reward: -4515.325 [-4515.325, -4515.325], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  236/5000: episode: 236, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2934.572, mean reward: -2934.572 [-2934.572, -2934.572], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  237/5000: episode: 237, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6503.716, mean reward: -6503.716 [-6503.716, -6503.716], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  238/5000: episode: 238, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -3308.784, mean reward: -3308.784 [-3308.784, -3308.784], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  239/5000: episode: 239, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -8973.050, mean reward: -8973.050 [-8973.050, -8973.050], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  240/5000: episode: 240, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7558.656, mean reward: -7558.656 [-7558.656, -7558.656], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  241/5000: episode: 241, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3944.073, mean reward: -3944.073 [-3944.073, -3944.073], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  242/5000: episode: 242, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3058.286, mean reward: -3058.286 [-3058.286, -3058.286], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  243/5000: episode: 243, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3434.945, mean reward: -3434.945 [-3434.945, -3434.945], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  244/5000: episode: 244, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -12890.145, mean reward: -12890.145 [-12890.145, -12890.145], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  245/5000: episode: 245, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6777.222, mean reward: -6777.222 [-6777.222, -6777.222], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  246/5000: episode: 246, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2365.303, mean reward: -2365.303 [-2365.303, -2365.303], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  247/5000: episode: 247, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -9053.899, mean reward: -9053.899 [-9053.899, -9053.899], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  248/5000: episode: 248, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3174.541, mean reward: -3174.541 [-3174.541, -3174.541], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  249/5000: episode: 249, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6725.971, mean reward: -6725.971 [-6725.971, -6725.971], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  250/5000: episode: 250, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9352.565, mean reward: -9352.565 [-9352.565, -9352.565], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  251/5000: episode: 251, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2880.388, mean reward: -2880.388 [-2880.388, -2880.388], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  252/5000: episode: 252, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -15373.466, mean reward: -15373.466 [-15373.466, -15373.466], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  253/5000: episode: 253, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6741.983, mean reward: -6741.983 [-6741.983, -6741.983], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  254/5000: episode: 254, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7931.967, mean reward: -7931.967 [-7931.967, -7931.967], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  255/5000: episode: 255, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -6178.475, mean reward: -6178.475 [-6178.475, -6178.475], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  256/5000: episode: 256, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -11670.443, mean reward: -11670.443 [-11670.443, -11670.443], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  257/5000: episode: 257, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6975.094, mean reward: -6975.094 [-6975.094, -6975.094], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  258/5000: episode: 258, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2899.238, mean reward: -2899.238 [-2899.238, -2899.238], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  259/5000: episode: 259, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5188.423, mean reward: -5188.423 [-5188.423, -5188.423], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  260/5000: episode: 260, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -15547.914, mean reward: -15547.914 [-15547.914, -15547.914], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  261/5000: episode: 261, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3391.825, mean reward: -3391.825 [-3391.825, -3391.825], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  262/5000: episode: 262, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -12861.132, mean reward: -12861.132 [-12861.132, -12861.132], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  263/5000: episode: 263, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7365.953, mean reward: -7365.953 [-7365.953, -7365.953], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  264/5000: episode: 264, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3266.590, mean reward: -3266.590 [-3266.590, -3266.590], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  265/5000: episode: 265, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6101.713, mean reward: -6101.713 [-6101.713, -6101.713], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  266/5000: episode: 266, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7776.691, mean reward: -7776.691 [-7776.691, -7776.691], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  267/5000: episode: 267, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1890.870, mean reward: -1890.870 [-1890.870, -1890.870], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  268/5000: episode: 268, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -13684.538, mean reward: -13684.538 [-13684.538, -13684.538], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  269/5000: episode: 269, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -8592.047, mean reward: -8592.047 [-8592.047, -8592.047], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  270/5000: episode: 270, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -16.039, mean reward: -16.039 [-16.039, -16.039], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  271/5000: episode: 271, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5879.964, mean reward: -5879.964 [-5879.964, -5879.964], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  272/5000: episode: 272, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7815.334, mean reward: -7815.334 [-7815.334, -7815.334], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  273/5000: episode: 273, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6624.829, mean reward: -6624.829 [-6624.829, -6624.829], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  274/5000: episode: 274, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -12889.786, mean reward: -12889.786 [-12889.786, -12889.786], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  275/5000: episode: 275, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -6196.183, mean reward: -6196.183 [-6196.183, -6196.183], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  276/5000: episode: 276, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3532.734, mean reward: -3532.734 [-3532.734, -3532.734], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  277/5000: episode: 277, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3953.355, mean reward: -3953.355 [-3953.355, -3953.355], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  278/5000: episode: 278, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -6773.846, mean reward: -6773.846 [-6773.846, -6773.846], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  279/5000: episode: 279, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6533.651, mean reward: -6533.651 [-6533.651, -6533.651], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  280/5000: episode: 280, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6273.551, mean reward: -6273.551 [-6273.551, -6273.551], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  281/5000: episode: 281, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8137.556, mean reward: -8137.556 [-8137.556, -8137.556], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  282/5000: episode: 282, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3053.499, mean reward: -3053.499 [-3053.499, -3053.499], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  283/5000: episode: 283, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3408.902, mean reward: -3408.902 [-3408.902, -3408.902], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  284/5000: episode: 284, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -9385.677, mean reward: -9385.677 [-9385.677, -9385.677], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  285/5000: episode: 285, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5348.832, mean reward: -5348.832 [-5348.832, -5348.832], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  286/5000: episode: 286, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10323.380, mean reward: -10323.380 [-10323.380, -10323.380], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  287/5000: episode: 287, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5378.320, mean reward: -5378.320 [-5378.320, -5378.320], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  288/5000: episode: 288, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8386.862, mean reward: -8386.862 [-8386.862, -8386.862], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  289/5000: episode: 289, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6078.304, mean reward: -6078.304 [-6078.304, -6078.304], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  290/5000: episode: 290, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6580.624, mean reward: -6580.624 [-6580.624, -6580.624], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  291/5000: episode: 291, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6251.170, mean reward: -6251.170 [-6251.170, -6251.170], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  292/5000: episode: 292, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -13502.699, mean reward: -13502.699 [-13502.699, -13502.699], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  293/5000: episode: 293, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5138.389, mean reward: -5138.389 [-5138.389, -5138.389], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  294/5000: episode: 294, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -15324.163, mean reward: -15324.163 [-15324.163, -15324.163], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  295/5000: episode: 295, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3034.838, mean reward: -3034.838 [-3034.838, -3034.838], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  296/5000: episode: 296, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2948.706, mean reward: -2948.706 [-2948.706, -2948.706], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  297/5000: episode: 297, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3837.711, mean reward: -3837.711 [-3837.711, -3837.711], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  298/5000: episode: 298, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -10823.186, mean reward: -10823.186 [-10823.186, -10823.186], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  299/5000: episode: 299, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2152.216, mean reward: -2152.216 [-2152.216, -2152.216], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  300/5000: episode: 300, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -9487.089, mean reward: -9487.089 [-9487.089, -9487.089], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  301/5000: episode: 301, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -7033.554, mean reward: -7033.554 [-7033.554, -7033.554], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  302/5000: episode: 302, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1427.048, mean reward: -1427.048 [-1427.048, -1427.048], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  303/5000: episode: 303, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6041.495, mean reward: -6041.495 [-6041.495, -6041.495], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  304/5000: episode: 304, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3478.624, mean reward: -3478.624 [-3478.624, -3478.624], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  305/5000: episode: 305, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -11462.855, mean reward: -11462.855 [-11462.855, -11462.855], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  306/5000: episode: 306, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2386.504, mean reward: -2386.504 [-2386.504, -2386.504], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  307/5000: episode: 307, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -9942.822, mean reward: -9942.822 [-9942.822, -9942.822], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  308/5000: episode: 308, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9505.854, mean reward: -9505.854 [-9505.854, -9505.854], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  309/5000: episode: 309, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7448.718, mean reward: -7448.718 [-7448.718, -7448.718], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  310/5000: episode: 310, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2565.580, mean reward: -2565.580 [-2565.580, -2565.580], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  311/5000: episode: 311, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6533.706, mean reward: -6533.706 [-6533.706, -6533.706], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  312/5000: episode: 312, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5056.630, mean reward: -5056.630 [-5056.630, -5056.630], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  313/5000: episode: 313, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6684.314, mean reward: -6684.314 [-6684.314, -6684.314], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  314/5000: episode: 314, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7354.123, mean reward: -7354.123 [-7354.123, -7354.123], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  315/5000: episode: 315, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6184.092, mean reward: -6184.092 [-6184.092, -6184.092], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  316/5000: episode: 316, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7582.222, mean reward: -7582.222 [-7582.222, -7582.222], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  317/5000: episode: 317, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -11521.878, mean reward: -11521.878 [-11521.878, -11521.878], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  318/5000: episode: 318, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -9687.663, mean reward: -9687.663 [-9687.663, -9687.663], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  319/5000: episode: 319, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2704.183, mean reward: -2704.183 [-2704.183, -2704.183], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  320/5000: episode: 320, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -9677.953, mean reward: -9677.953 [-9677.953, -9677.953], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  321/5000: episode: 321, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2604.341, mean reward: -2604.341 [-2604.341, -2604.341], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  322/5000: episode: 322, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -8499.687, mean reward: -8499.687 [-8499.687, -8499.687], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  323/5000: episode: 323, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -696.012, mean reward: -696.012 [-696.012, -696.012], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  324/5000: episode: 324, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5853.998, mean reward: -5853.998 [-5853.998, -5853.998], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  325/5000: episode: 325, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -8499.446, mean reward: -8499.446 [-8499.446, -8499.446], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  326/5000: episode: 326, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2272.410, mean reward: -2272.410 [-2272.410, -2272.410], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  327/5000: episode: 327, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5150.616, mean reward: -5150.616 [-5150.616, -5150.616], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  328/5000: episode: 328, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -8229.211, mean reward: -8229.211 [-8229.211, -8229.211], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  329/5000: episode: 329, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -1860.470, mean reward: -1860.470 [-1860.470, -1860.470], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  330/5000: episode: 330, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -11953.901, mean reward: -11953.901 [-11953.901, -11953.901], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  331/5000: episode: 331, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -612.618, mean reward: -612.618 [-612.618, -612.618], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  332/5000: episode: 332, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6035.823, mean reward: -6035.823 [-6035.823, -6035.823], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  333/5000: episode: 333, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10311.180, mean reward: -10311.180 [-10311.180, -10311.180], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  334/5000: episode: 334, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9750.816, mean reward: -9750.816 [-9750.816, -9750.816], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  335/5000: episode: 335, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -10175.008, mean reward: -10175.008 [-10175.008, -10175.008], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  336/5000: episode: 336, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -10063.723, mean reward: -10063.723 [-10063.723, -10063.723], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  337/5000: episode: 337, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2349.354, mean reward: -2349.354 [-2349.354, -2349.354], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  338/5000: episode: 338, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6891.093, mean reward: -6891.093 [-6891.093, -6891.093], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  339/5000: episode: 339, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5771.836, mean reward: -5771.836 [-5771.836, -5771.836], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  340/5000: episode: 340, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5486.171, mean reward: -5486.171 [-5486.171, -5486.171], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  341/5000: episode: 341, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4430.915, mean reward: -4430.915 [-4430.915, -4430.915], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  342/5000: episode: 342, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5080.275, mean reward: -5080.275 [-5080.275, -5080.275], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  343/5000: episode: 343, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6170.372, mean reward: -6170.372 [-6170.372, -6170.372], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  344/5000: episode: 344, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10249.006, mean reward: -10249.006 [-10249.006, -10249.006], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  345/5000: episode: 345, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3668.178, mean reward: -3668.178 [-3668.178, -3668.178], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  346/5000: episode: 346, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -13822.909, mean reward: -13822.909 [-13822.909, -13822.909], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  347/5000: episode: 347, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -11943.838, mean reward: -11943.838 [-11943.838, -11943.838], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  348/5000: episode: 348, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -11800.502, mean reward: -11800.502 [-11800.502, -11800.502], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  349/5000: episode: 349, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7258.375, mean reward: -7258.375 [-7258.375, -7258.375], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  350/5000: episode: 350, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10772.366, mean reward: -10772.366 [-10772.366, -10772.366], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  351/5000: episode: 351, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3895.923, mean reward: -3895.923 [-3895.923, -3895.923], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  352/5000: episode: 352, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.434, mean reward: -5000.434 [-5000.434, -5000.434], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  353/5000: episode: 353, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5166.882, mean reward: -5166.882 [-5166.882, -5166.882], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  354/5000: episode: 354, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8033.536, mean reward: -8033.536 [-8033.536, -8033.536], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  355/5000: episode: 355, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -13628.019, mean reward: -13628.019 [-13628.019, -13628.019], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  356/5000: episode: 356, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4508.466, mean reward: -4508.466 [-4508.466, -4508.466], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  357/5000: episode: 357, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6959.936, mean reward: -6959.936 [-6959.936, -6959.936], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  358/5000: episode: 358, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2797.992, mean reward: -2797.992 [-2797.992, -2797.992], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  359/5000: episode: 359, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4610.858, mean reward: -4610.858 [-4610.858, -4610.858], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  360/5000: episode: 360, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1324.669, mean reward: -1324.669 [-1324.669, -1324.669], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  361/5000: episode: 361, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1241.580, mean reward: -1241.580 [-1241.580, -1241.580], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  362/5000: episode: 362, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6107.120, mean reward: -6107.120 [-6107.120, -6107.120], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  363/5000: episode: 363, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2402.847, mean reward: -2402.847 [-2402.847, -2402.847], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  364/5000: episode: 364, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -4219.286, mean reward: -4219.286 [-4219.286, -4219.286], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  365/5000: episode: 365, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3635.151, mean reward: -3635.151 [-3635.151, -3635.151], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  366/5000: episode: 366, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -12169.189, mean reward: -12169.189 [-12169.189, -12169.189], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  367/5000: episode: 367, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7173.236, mean reward: -7173.236 [-7173.236, -7173.236], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  368/5000: episode: 368, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -12779.748, mean reward: -12779.748 [-12779.748, -12779.748], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  369/5000: episode: 369, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5988.040, mean reward: -5988.040 [-5988.040, -5988.040], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  370/5000: episode: 370, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -502.123, mean reward: -502.123 [-502.123, -502.123], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  371/5000: episode: 371, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3543.721, mean reward: -3543.721 [-3543.721, -3543.721], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  372/5000: episode: 372, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8170.439, mean reward: -8170.439 [-8170.439, -8170.439], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  373/5000: episode: 373, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -529.675, mean reward: -529.675 [-529.675, -529.675], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  374/5000: episode: 374, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3341.252, mean reward: -3341.252 [-3341.252, -3341.252], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  375/5000: episode: 375, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6335.339, mean reward: -6335.339 [-6335.339, -6335.339], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  376/5000: episode: 376, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6876.107, mean reward: -6876.107 [-6876.107, -6876.107], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  377/5000: episode: 377, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -6561.893, mean reward: -6561.893 [-6561.893, -6561.893], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  378/5000: episode: 378, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3885.244, mean reward: -3885.244 [-3885.244, -3885.244], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  379/5000: episode: 379, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7172.303, mean reward: -7172.303 [-7172.303, -7172.303], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  380/5000: episode: 380, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6861.100, mean reward: -6861.100 [-6861.100, -6861.100], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  381/5000: episode: 381, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5557.560, mean reward: -5557.560 [-5557.560, -5557.560], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  382/5000: episode: 382, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -7419.038, mean reward: -7419.038 [-7419.038, -7419.038], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  383/5000: episode: 383, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6795.355, mean reward: -6795.355 [-6795.355, -6795.355], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  384/5000: episode: 384, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3853.992, mean reward: -3853.992 [-3853.992, -3853.992], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  385/5000: episode: 385, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -13575.452, mean reward: -13575.452 [-13575.452, -13575.452], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  386/5000: episode: 386, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10518.818, mean reward: -10518.818 [-10518.818, -10518.818], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  387/5000: episode: 387, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10783.807, mean reward: -10783.807 [-10783.807, -10783.807], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  388/5000: episode: 388, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10126.974, mean reward: -10126.974 [-10126.974, -10126.974], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  389/5000: episode: 389, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3394.677, mean reward: -3394.677 [-3394.677, -3394.677], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  390/5000: episode: 390, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -10457.805, mean reward: -10457.805 [-10457.805, -10457.805], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  391/5000: episode: 391, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -6813.671, mean reward: -6813.671 [-6813.671, -6813.671], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  392/5000: episode: 392, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -2928.550, mean reward: -2928.550 [-2928.550, -2928.550], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  393/5000: episode: 393, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6640.172, mean reward: -6640.172 [-6640.172, -6640.172], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  394/5000: episode: 394, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7552.396, mean reward: -7552.396 [-7552.396, -7552.396], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  395/5000: episode: 395, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2590.092, mean reward: -2590.092 [-2590.092, -2590.092], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  396/5000: episode: 396, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4378.142, mean reward: -4378.142 [-4378.142, -4378.142], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  397/5000: episode: 397, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -12854.179, mean reward: -12854.179 [-12854.179, -12854.179], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  398/5000: episode: 398, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -8063.158, mean reward: -8063.158 [-8063.158, -8063.158], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  399/5000: episode: 399, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4815.623, mean reward: -4815.623 [-4815.623, -4815.623], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  400/5000: episode: 400, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -7658.931, mean reward: -7658.931 [-7658.931, -7658.931], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  401/5000: episode: 401, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4345.958, mean reward: -4345.958 [-4345.958, -4345.958], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  402/5000: episode: 402, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3692.195, mean reward: -3692.195 [-3692.195, -3692.195], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  403/5000: episode: 403, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -8745.108, mean reward: -8745.108 [-8745.108, -8745.108], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  404/5000: episode: 404, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -13815.809, mean reward: -13815.809 [-13815.809, -13815.809], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  405/5000: episode: 405, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -13702.458, mean reward: -13702.458 [-13702.458, -13702.458], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  406/5000: episode: 406, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5566.058, mean reward: -5566.058 [-5566.058, -5566.058], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  407/5000: episode: 407, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10458.408, mean reward: -10458.408 [-10458.408, -10458.408], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  408/5000: episode: 408, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -976.649, mean reward: -976.649 [-976.649, -976.649], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  409/5000: episode: 409, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -13607.344, mean reward: -13607.344 [-13607.344, -13607.344], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  410/5000: episode: 410, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9351.659, mean reward: -9351.659 [-9351.659, -9351.659], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  411/5000: episode: 411, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5040.078, mean reward: -5040.078 [-5040.078, -5040.078], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  412/5000: episode: 412, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5431.373, mean reward: -5431.373 [-5431.373, -5431.373], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  413/5000: episode: 413, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4824.988, mean reward: -4824.988 [-4824.988, -4824.988], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  414/5000: episode: 414, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3229.994, mean reward: -3229.994 [-3229.994, -3229.994], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  415/5000: episode: 415, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -8212.384, mean reward: -8212.384 [-8212.384, -8212.384], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  416/5000: episode: 416, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -18344.824, mean reward: -18344.824 [-18344.824, -18344.824], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  417/5000: episode: 417, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -9908.059, mean reward: -9908.059 [-9908.059, -9908.059], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  418/5000: episode: 418, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6229.303, mean reward: -6229.303 [-6229.303, -6229.303], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  419/5000: episode: 419, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10722.520, mean reward: -10722.520 [-10722.520, -10722.520], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  420/5000: episode: 420, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9153.504, mean reward: -9153.504 [-9153.504, -9153.504], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  421/5000: episode: 421, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5297.454, mean reward: -5297.454 [-5297.454, -5297.454], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  422/5000: episode: 422, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -7353.831, mean reward: -7353.831 [-7353.831, -7353.831], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  423/5000: episode: 423, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11284.490, mean reward: -11284.490 [-11284.490, -11284.490], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  424/5000: episode: 424, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2305.943, mean reward: -2305.943 [-2305.943, -2305.943], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  425/5000: episode: 425, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3823.813, mean reward: -3823.813 [-3823.813, -3823.813], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  426/5000: episode: 426, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3535.635, mean reward: -3535.635 [-3535.635, -3535.635], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  427/5000: episode: 427, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -11495.194, mean reward: -11495.194 [-11495.194, -11495.194], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  428/5000: episode: 428, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1622.212, mean reward: -1622.212 [-1622.212, -1622.212], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  429/5000: episode: 429, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8081.745, mean reward: -8081.745 [-8081.745, -8081.745], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  430/5000: episode: 430, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5060.440, mean reward: -5060.440 [-5060.440, -5060.440], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  431/5000: episode: 431, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7114.637, mean reward: -7114.637 [-7114.637, -7114.637], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  432/5000: episode: 432, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8879.875, mean reward: -8879.875 [-8879.875, -8879.875], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  433/5000: episode: 433, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1837.388, mean reward: -1837.388 [-1837.388, -1837.388], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  434/5000: episode: 434, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8923.407, mean reward: -8923.407 [-8923.407, -8923.407], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  435/5000: episode: 435, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -11670.314, mean reward: -11670.314 [-11670.314, -11670.314], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  436/5000: episode: 436, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1933.549, mean reward: -1933.549 [-1933.549, -1933.549], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  437/5000: episode: 437, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -16525.224, mean reward: -16525.224 [-16525.224, -16525.224], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  438/5000: episode: 438, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -10556.811, mean reward: -10556.811 [-10556.811, -10556.811], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  439/5000: episode: 439, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -12076.103, mean reward: -12076.103 [-12076.103, -12076.103], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  440/5000: episode: 440, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5408.875, mean reward: -5408.875 [-5408.875, -5408.875], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  441/5000: episode: 441, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5360.715, mean reward: -5360.715 [-5360.715, -5360.715], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  442/5000: episode: 442, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -10033.445, mean reward: -10033.445 [-10033.445, -10033.445], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  443/5000: episode: 443, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4310.480, mean reward: -4310.480 [-4310.480, -4310.480], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  444/5000: episode: 444, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8383.689, mean reward: -8383.689 [-8383.689, -8383.689], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  445/5000: episode: 445, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3001.055, mean reward: -3001.055 [-3001.055, -3001.055], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  446/5000: episode: 446, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6301.344, mean reward: -6301.344 [-6301.344, -6301.344], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  447/5000: episode: 447, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3440.696, mean reward: -3440.696 [-3440.696, -3440.696], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  448/5000: episode: 448, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8798.226, mean reward: -8798.226 [-8798.226, -8798.226], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  449/5000: episode: 449, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4743.405, mean reward: -4743.405 [-4743.405, -4743.405], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  450/5000: episode: 450, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -10550.506, mean reward: -10550.506 [-10550.506, -10550.506], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  451/5000: episode: 451, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4025.880, mean reward: -4025.880 [-4025.880, -4025.880], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  452/5000: episode: 452, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5946.118, mean reward: -5946.118 [-5946.118, -5946.118], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  453/5000: episode: 453, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6229.919, mean reward: -6229.919 [-6229.919, -6229.919], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  454/5000: episode: 454, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3618.310, mean reward: -3618.310 [-3618.310, -3618.310], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  455/5000: episode: 455, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1792.829, mean reward: -1792.829 [-1792.829, -1792.829], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  456/5000: episode: 456, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8660.655, mean reward: -8660.655 [-8660.655, -8660.655], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  457/5000: episode: 457, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -12394.426, mean reward: -12394.426 [-12394.426, -12394.426], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  458/5000: episode: 458, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -9595.944, mean reward: -9595.944 [-9595.944, -9595.944], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  459/5000: episode: 459, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1668.068, mean reward: -1668.068 [-1668.068, -1668.068], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  460/5000: episode: 460, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5389.357, mean reward: -5389.357 [-5389.357, -5389.357], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  461/5000: episode: 461, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5111.043, mean reward: -5111.043 [-5111.043, -5111.043], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  462/5000: episode: 462, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -8111.964, mean reward: -8111.964 [-8111.964, -8111.964], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  463/5000: episode: 463, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -6735.110, mean reward: -6735.110 [-6735.110, -6735.110], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  464/5000: episode: 464, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9790.173, mean reward: -9790.173 [-9790.173, -9790.173], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  465/5000: episode: 465, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1992.029, mean reward: -1992.029 [-1992.029, -1992.029], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  466/5000: episode: 466, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5262.230, mean reward: -5262.230 [-5262.230, -5262.230], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  467/5000: episode: 467, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7056.228, mean reward: -7056.228 [-7056.228, -7056.228], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  468/5000: episode: 468, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5446.371, mean reward: -5446.371 [-5446.371, -5446.371], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  469/5000: episode: 469, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4024.617, mean reward: -4024.617 [-4024.617, -4024.617], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  470/5000: episode: 470, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4997.211, mean reward: -4997.211 [-4997.211, -4997.211], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  471/5000: episode: 471, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -8485.335, mean reward: -8485.335 [-8485.335, -8485.335], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  472/5000: episode: 472, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3660.862, mean reward: -3660.862 [-3660.862, -3660.862], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  473/5000: episode: 473, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3099.629, mean reward: -3099.629 [-3099.629, -3099.629], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  474/5000: episode: 474, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6097.412, mean reward: -6097.412 [-6097.412, -6097.412], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  475/5000: episode: 475, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3826.580, mean reward: -3826.580 [-3826.580, -3826.580], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  476/5000: episode: 476, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5980.951, mean reward: -5980.951 [-5980.951, -5980.951], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  477/5000: episode: 477, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8963.151, mean reward: -8963.151 [-8963.151, -8963.151], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  478/5000: episode: 478, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -7013.445, mean reward: -7013.445 [-7013.445, -7013.445], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  479/5000: episode: 479, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -13229.268, mean reward: -13229.268 [-13229.268, -13229.268], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  480/5000: episode: 480, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10941.460, mean reward: -10941.460 [-10941.460, -10941.460], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  481/5000: episode: 481, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4081.101, mean reward: -4081.101 [-4081.101, -4081.101], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  482/5000: episode: 482, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2996.511, mean reward: -2996.511 [-2996.511, -2996.511], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  483/5000: episode: 483, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -4705.720, mean reward: -4705.720 [-4705.720, -4705.720], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  484/5000: episode: 484, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4069.574, mean reward: -4069.574 [-4069.574, -4069.574], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  485/5000: episode: 485, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -7900.269, mean reward: -7900.269 [-7900.269, -7900.269], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  486/5000: episode: 486, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4524.417, mean reward: -4524.417 [-4524.417, -4524.417], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  487/5000: episode: 487, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4365.110, mean reward: -4365.110 [-4365.110, -4365.110], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  488/5000: episode: 488, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -14566.319, mean reward: -14566.319 [-14566.319, -14566.319], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  489/5000: episode: 489, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2621.120, mean reward: -2621.120 [-2621.120, -2621.120], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  490/5000: episode: 490, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7773.904, mean reward: -7773.904 [-7773.904, -7773.904], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  491/5000: episode: 491, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -10564.957, mean reward: -10564.957 [-10564.957, -10564.957], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  492/5000: episode: 492, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5824.927, mean reward: -5824.927 [-5824.927, -5824.927], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  493/5000: episode: 493, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8900.933, mean reward: -8900.933 [-8900.933, -8900.933], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  494/5000: episode: 494, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7493.383, mean reward: -7493.383 [-7493.383, -7493.383], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  495/5000: episode: 495, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5882.409, mean reward: -5882.409 [-5882.409, -5882.409], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  496/5000: episode: 496, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -6414.812, mean reward: -6414.812 [-6414.812, -6414.812], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  497/5000: episode: 497, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3292.264, mean reward: -3292.264 [-3292.264, -3292.264], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  498/5000: episode: 498, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -11197.471, mean reward: -11197.471 [-11197.471, -11197.471], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  499/5000: episode: 499, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2403.206, mean reward: -2403.206 [-2403.206, -2403.206], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  500/5000: episode: 500, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10536.595, mean reward: -10536.595 [-10536.595, -10536.595], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  501/5000: episode: 501, duration: 0.396s, episode steps:   1, steps per second:   3, episode reward: -10697.470, mean reward: -10697.470 [-10697.470, -10697.470], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  502/5000: episode: 502, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -12551.213, mean reward: -12551.213 [-12551.213, -12551.213], mean action: 0.000 [0.000, 0.000],  loss: 37692012.000000, mae: 1935.735840, mean_q: 0.351365
  503/5000: episode: 503, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2449.621, mean reward: -2449.621 [-2449.621, -2449.621], mean action: 0.000 [0.000, 0.000],  loss: 41569532.000000, mae: 2088.705078, mean_q: 0.326731
  504/5000: episode: 504, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5218.713, mean reward: -5218.713 [-5218.713, -5218.713], mean action: 0.000 [0.000, 0.000],  loss: 32824606.000000, mae: 1838.465576, mean_q: 0.293260
  505/5000: episode: 505, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6838.280, mean reward: -6838.280 [-6838.280, -6838.280], mean action: 0.000 [0.000, 0.000],  loss: 37257992.000000, mae: 1910.331787, mean_q: 0.269462
  506/5000: episode: 506, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -13249.542, mean reward: -13249.542 [-13249.542, -13249.542], mean action: 0.000 [0.000, 0.000],  loss: 40720600.000000, mae: 1964.591187, mean_q: 0.239342
  507/5000: episode: 507, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3860.723, mean reward: -3860.723 [-3860.723, -3860.723], mean action: 2.000 [2.000, 2.000],  loss: 29176782.000000, mae: 1778.016968, mean_q: 0.217403
  508/5000: episode: 508, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2239.342, mean reward: -2239.342 [-2239.342, -2239.342], mean action: 2.000 [2.000, 2.000],  loss: 34352424.000000, mae: 1885.397583, mean_q: 0.205362
  509/5000: episode: 509, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5165.920, mean reward: -5165.920 [-5165.920, -5165.920], mean action: 2.000 [2.000, 2.000],  loss: 41538888.000000, mae: 2024.640747, mean_q: 0.209371
  510/5000: episode: 510, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9009.617, mean reward: -9009.617 [-9009.617, -9009.617], mean action: 1.000 [1.000, 1.000],  loss: 20366960.000000, mae: 1407.081055, mean_q: 0.204635
  511/5000: episode: 511, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5.646, mean reward: -5.646 [-5.646, -5.646], mean action: 2.000 [2.000, 2.000],  loss: 32859940.000000, mae: 1832.590332, mean_q: 0.205204
  512/5000: episode: 512, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -4560.627, mean reward: -4560.627 [-4560.627, -4560.627], mean action: 2.000 [2.000, 2.000],  loss: 31547314.000000, mae: 1788.163940, mean_q: 0.206709
  513/5000: episode: 513, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9425.284, mean reward: -9425.284 [-9425.284, -9425.284], mean action: 2.000 [2.000, 2.000],  loss: 29955148.000000, mae: 1733.183960, mean_q: 0.197161
  514/5000: episode: 514, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6798.526, mean reward: -6798.526 [-6798.526, -6798.526], mean action: 2.000 [2.000, 2.000],  loss: 25054438.000000, mae: 1587.077881, mean_q: 0.201124
  515/5000: episode: 515, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3304.467, mean reward: -3304.467 [-3304.467, -3304.467], mean action: 2.000 [2.000, 2.000],  loss: 32701592.000000, mae: 1871.075195, mean_q: 0.198591
  516/5000: episode: 516, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5519.613, mean reward: -5519.613 [-5519.613, -5519.613], mean action: 2.000 [2.000, 2.000],  loss: 30362928.000000, mae: 1674.718018, mean_q: 0.199497
  517/5000: episode: 517, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4742.793, mean reward: -4742.793 [-4742.793, -4742.793], mean action: 2.000 [2.000, 2.000],  loss: 19919028.000000, mae: 1396.041382, mean_q: 0.202512
  518/5000: episode: 518, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -237.673, mean reward: -237.673 [-237.673, -237.673], mean action: 2.000 [2.000, 2.000],  loss: 28362046.000000, mae: 1690.453247, mean_q: 0.190245
  519/5000: episode: 519, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3560.342, mean reward: -3560.342 [-3560.342, -3560.342], mean action: 2.000 [2.000, 2.000],  loss: 28578200.000000, mae: 1660.842651, mean_q: 0.199976
  520/5000: episode: 520, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -928.174, mean reward: -928.174 [-928.174, -928.174], mean action: 2.000 [2.000, 2.000],  loss: 27861596.000000, mae: 1662.186157, mean_q: 0.189442
  521/5000: episode: 521, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4987.881, mean reward: -4987.881 [-4987.881, -4987.881], mean action: 1.000 [1.000, 1.000],  loss: 35279144.000000, mae: 1898.416138, mean_q: 0.186328
  522/5000: episode: 522, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -420.370, mean reward: -420.370 [-420.370, -420.370], mean action: 2.000 [2.000, 2.000],  loss: 28315862.000000, mae: 1670.672852, mean_q: 0.184312
  523/5000: episode: 523, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -203.586, mean reward: -203.586 [-203.586, -203.586], mean action: 2.000 [2.000, 2.000],  loss: 24165880.000000, mae: 1538.230225, mean_q: 0.184953
  524/5000: episode: 524, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1921.302, mean reward: -1921.302 [-1921.302, -1921.302], mean action: 2.000 [2.000, 2.000],  loss: 30205984.000000, mae: 1735.043945, mean_q: 0.191272
  525/5000: episode: 525, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -9634.014, mean reward: -9634.014 [-9634.014, -9634.014], mean action: 1.000 [1.000, 1.000],  loss: 28536744.000000, mae: 1662.109497, mean_q: 0.183495
  526/5000: episode: 526, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -556.390, mean reward: -556.390 [-556.390, -556.390], mean action: 2.000 [2.000, 2.000],  loss: 35959176.000000, mae: 1940.407104, mean_q: 0.178858
  527/5000: episode: 527, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4249.889, mean reward: -4249.889 [-4249.889, -4249.889], mean action: 1.000 [1.000, 1.000],  loss: 28672190.000000, mae: 1602.103271, mean_q: 0.181358
  528/5000: episode: 528, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3396.508, mean reward: -3396.508 [-3396.508, -3396.508], mean action: 2.000 [2.000, 2.000],  loss: 30725316.000000, mae: 1718.012939, mean_q: 0.179689
  529/5000: episode: 529, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2524.876, mean reward: -2524.876 [-2524.876, -2524.876], mean action: 2.000 [2.000, 2.000],  loss: 25786312.000000, mae: 1595.175537, mean_q: 0.170598
  530/5000: episode: 530, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1686.314, mean reward: -1686.314 [-1686.314, -1686.314], mean action: 2.000 [2.000, 2.000],  loss: 24933904.000000, mae: 1531.686523, mean_q: 0.175413
  531/5000: episode: 531, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -546.197, mean reward: -546.197 [-546.197, -546.197], mean action: 3.000 [3.000, 3.000],  loss: 40818664.000000, mae: 2041.323975, mean_q: 0.171569
  532/5000: episode: 532, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -1907.758, mean reward: -1907.758 [-1907.758, -1907.758], mean action: 2.000 [2.000, 2.000],  loss: 28841500.000000, mae: 1704.563599, mean_q: 0.172064
  533/5000: episode: 533, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1329.137, mean reward: -1329.137 [-1329.137, -1329.137], mean action: 1.000 [1.000, 1.000],  loss: 32669488.000000, mae: 1763.219604, mean_q: 0.172212
  534/5000: episode: 534, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4512.455, mean reward: -4512.455 [-4512.455, -4512.455], mean action: 1.000 [1.000, 1.000],  loss: 36233648.000000, mae: 1902.209106, mean_q: 0.168094
  535/5000: episode: 535, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3425.220, mean reward: -3425.220 [-3425.220, -3425.220], mean action: 2.000 [2.000, 2.000],  loss: 19028672.000000, mae: 1352.366577, mean_q: 0.165199
  536/5000: episode: 536, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1894.449, mean reward: -1894.449 [-1894.449, -1894.449], mean action: 1.000 [1.000, 1.000],  loss: 28873504.000000, mae: 1620.871704, mean_q: 0.167390
  537/5000: episode: 537, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3755.619, mean reward: -3755.619 [-3755.619, -3755.619], mean action: 1.000 [1.000, 1.000],  loss: 33437844.000000, mae: 1800.151611, mean_q: 0.177287
  538/5000: episode: 538, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5594.746, mean reward: -5594.746 [-5594.746, -5594.746], mean action: 1.000 [1.000, 1.000],  loss: 33702776.000000, mae: 1858.830811, mean_q: 0.176261
  539/5000: episode: 539, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -388.529, mean reward: -388.529 [-388.529, -388.529], mean action: 1.000 [1.000, 1.000],  loss: 28264220.000000, mae: 1663.070068, mean_q: 0.169990
  540/5000: episode: 540, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2177.268, mean reward: -2177.268 [-2177.268, -2177.268], mean action: 1.000 [1.000, 1.000],  loss: 26240864.000000, mae: 1543.588013, mean_q: 0.169255
  541/5000: episode: 541, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4883.309, mean reward: -4883.309 [-4883.309, -4883.309], mean action: 1.000 [1.000, 1.000],  loss: 36106884.000000, mae: 1880.313477, mean_q: 0.172791
  542/5000: episode: 542, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3561.277, mean reward: -3561.277 [-3561.277, -3561.277], mean action: 1.000 [1.000, 1.000],  loss: 27380188.000000, mae: 1703.901123, mean_q: 0.167505
  543/5000: episode: 543, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4858.996, mean reward: -4858.996 [-4858.996, -4858.996], mean action: 1.000 [1.000, 1.000],  loss: 33665748.000000, mae: 1858.738159, mean_q: 0.174739
  544/5000: episode: 544, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2782.742, mean reward: -2782.742 [-2782.742, -2782.742], mean action: 1.000 [1.000, 1.000],  loss: 31898740.000000, mae: 1821.079346, mean_q: 0.180058
  545/5000: episode: 545, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -3022.617, mean reward: -3022.617 [-3022.617, -3022.617], mean action: 1.000 [1.000, 1.000],  loss: 26895848.000000, mae: 1663.438965, mean_q: 0.173791
  546/5000: episode: 546, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3703.760, mean reward: -3703.760 [-3703.760, -3703.760], mean action: 1.000 [1.000, 1.000],  loss: 24236830.000000, mae: 1448.684814, mean_q: 0.174123
  547/5000: episode: 547, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1203.354, mean reward: -1203.354 [-1203.354, -1203.354], mean action: 1.000 [1.000, 1.000],  loss: 36107460.000000, mae: 1874.057739, mean_q: 0.188508
  548/5000: episode: 548, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1746.447, mean reward: -1746.447 [-1746.447, -1746.447], mean action: 1.000 [1.000, 1.000],  loss: 27204524.000000, mae: 1605.898682, mean_q: 0.180160
  549/5000: episode: 549, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1586.382, mean reward: -1586.382 [-1586.382, -1586.382], mean action: 1.000 [1.000, 1.000],  loss: 27436778.000000, mae: 1563.378418, mean_q: 0.188018
  550/5000: episode: 550, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4856.679, mean reward: -4856.679 [-4856.679, -4856.679], mean action: 1.000 [1.000, 1.000],  loss: 30377886.000000, mae: 1691.683594, mean_q: 0.183538
  551/5000: episode: 551, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2185.220, mean reward: -2185.220 [-2185.220, -2185.220], mean action: 1.000 [1.000, 1.000],  loss: 21338234.000000, mae: 1487.052979, mean_q: 0.186574
  552/5000: episode: 552, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -434.584, mean reward: -434.584 [-434.584, -434.584], mean action: 1.000 [1.000, 1.000],  loss: 34127292.000000, mae: 1811.835449, mean_q: 0.189150
  553/5000: episode: 553, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8563.831, mean reward: -8563.831 [-8563.831, -8563.831], mean action: 1.000 [1.000, 1.000],  loss: 23470124.000000, mae: 1527.660767, mean_q: 0.183408
  554/5000: episode: 554, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3225.353, mean reward: -3225.353 [-3225.353, -3225.353], mean action: 1.000 [1.000, 1.000],  loss: 32447904.000000, mae: 1784.671143, mean_q: 0.173278
  555/5000: episode: 555, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10559.092, mean reward: -10559.092 [-10559.092, -10559.092], mean action: 1.000 [1.000, 1.000],  loss: 24699374.000000, mae: 1485.626709, mean_q: 0.180314
  556/5000: episode: 556, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5919.428, mean reward: -5919.428 [-5919.428, -5919.428], mean action: 2.000 [2.000, 2.000],  loss: 27530746.000000, mae: 1649.372070, mean_q: 0.184426
  557/5000: episode: 557, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -6386.731, mean reward: -6386.731 [-6386.731, -6386.731], mean action: 1.000 [1.000, 1.000],  loss: 28635120.000000, mae: 1683.578735, mean_q: 0.174051
  558/5000: episode: 558, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3585.947, mean reward: -3585.947 [-3585.947, -3585.947], mean action: 1.000 [1.000, 1.000],  loss: 27454712.000000, mae: 1657.234741, mean_q: 0.175481
  559/5000: episode: 559, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4847.898, mean reward: -4847.898 [-4847.898, -4847.898], mean action: 1.000 [1.000, 1.000],  loss: 39591936.000000, mae: 1950.433350, mean_q: 0.182360
  560/5000: episode: 560, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6314.908, mean reward: -6314.908 [-6314.908, -6314.908], mean action: 1.000 [1.000, 1.000],  loss: 22648036.000000, mae: 1492.890381, mean_q: 0.181963
  561/5000: episode: 561, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -11877.926, mean reward: -11877.926 [-11877.926, -11877.926], mean action: 1.000 [1.000, 1.000],  loss: 22543760.000000, mae: 1515.371582, mean_q: 0.169816
  562/5000: episode: 562, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3224.077, mean reward: -3224.077 [-3224.077, -3224.077], mean action: 1.000 [1.000, 1.000],  loss: 34652032.000000, mae: 1905.246460, mean_q: 0.170617
  563/5000: episode: 563, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3254.703, mean reward: -3254.703 [-3254.703, -3254.703], mean action: 1.000 [1.000, 1.000],  loss: 29253782.000000, mae: 1725.673340, mean_q: 0.164794
  564/5000: episode: 564, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1121.349, mean reward: -1121.349 [-1121.349, -1121.349], mean action: 1.000 [1.000, 1.000],  loss: 29181632.000000, mae: 1744.895996, mean_q: 0.154664
  565/5000: episode: 565, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2915.868, mean reward: -2915.868 [-2915.868, -2915.868], mean action: 3.000 [3.000, 3.000],  loss: 32760716.000000, mae: 1734.462524, mean_q: 0.155626
  566/5000: episode: 566, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2076.225, mean reward: -2076.225 [-2076.225, -2076.225], mean action: 1.000 [1.000, 1.000],  loss: 27012342.000000, mae: 1667.921387, mean_q: 0.157447
  567/5000: episode: 567, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -4212.537, mean reward: -4212.537 [-4212.537, -4212.537], mean action: 1.000 [1.000, 1.000],  loss: 30084800.000000, mae: 1694.819824, mean_q: 0.155288
  568/5000: episode: 568, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3648.832, mean reward: -3648.832 [-3648.832, -3648.832], mean action: 1.000 [1.000, 1.000],  loss: 26461864.000000, mae: 1636.920776, mean_q: 0.147738
  569/5000: episode: 569, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6320.554, mean reward: -6320.554 [-6320.554, -6320.554], mean action: 1.000 [1.000, 1.000],  loss: 30596828.000000, mae: 1713.288330, mean_q: 0.136345
  570/5000: episode: 570, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2229.881, mean reward: -2229.881 [-2229.881, -2229.881], mean action: 1.000 [1.000, 1.000],  loss: 25908694.000000, mae: 1541.668213, mean_q: 0.144705
  571/5000: episode: 571, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3101.466, mean reward: -3101.466 [-3101.466, -3101.466], mean action: 1.000 [1.000, 1.000],  loss: 26298038.000000, mae: 1613.116577, mean_q: 0.138905
  572/5000: episode: 572, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2685.886, mean reward: -2685.886 [-2685.886, -2685.886], mean action: 1.000 [1.000, 1.000],  loss: 21997764.000000, mae: 1473.241577, mean_q: 0.134645
  573/5000: episode: 573, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5165.491, mean reward: -5165.491 [-5165.491, -5165.491], mean action: 1.000 [1.000, 1.000],  loss: 39252464.000000, mae: 1981.260620, mean_q: 0.140287
  574/5000: episode: 574, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -640.379, mean reward: -640.379 [-640.379, -640.379], mean action: 1.000 [1.000, 1.000],  loss: 24394026.000000, mae: 1477.911987, mean_q: 0.127227
  575/5000: episode: 575, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2387.532, mean reward: -2387.532 [-2387.532, -2387.532], mean action: 1.000 [1.000, 1.000],  loss: 26261388.000000, mae: 1570.556641, mean_q: 0.124994
  576/5000: episode: 576, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6916.914, mean reward: -6916.914 [-6916.914, -6916.914], mean action: 1.000 [1.000, 1.000],  loss: 26592076.000000, mae: 1528.421509, mean_q: 0.128148
  577/5000: episode: 577, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4803.364, mean reward: -4803.364 [-4803.364, -4803.364], mean action: 1.000 [1.000, 1.000],  loss: 19644572.000000, mae: 1401.797363, mean_q: 0.132230
  578/5000: episode: 578, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6001.706, mean reward: -6001.706 [-6001.706, -6001.706], mean action: 1.000 [1.000, 1.000],  loss: 24376588.000000, mae: 1498.568970, mean_q: 0.130386
  579/5000: episode: 579, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4476.309, mean reward: -4476.309 [-4476.309, -4476.309], mean action: 1.000 [1.000, 1.000],  loss: 24697024.000000, mae: 1511.967407, mean_q: 0.122273
  580/5000: episode: 580, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1977.038, mean reward: -1977.038 [-1977.038, -1977.038], mean action: 1.000 [1.000, 1.000],  loss: 31211316.000000, mae: 1678.607666, mean_q: 0.117010
  581/5000: episode: 581, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7466.581, mean reward: -7466.581 [-7466.581, -7466.581], mean action: 1.000 [1.000, 1.000],  loss: 23714176.000000, mae: 1577.100952, mean_q: 0.117413
  582/5000: episode: 582, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -94.671, mean reward: -94.671 [-94.671, -94.671], mean action: 1.000 [1.000, 1.000],  loss: 34391800.000000, mae: 1791.601562, mean_q: 0.113085
  583/5000: episode: 583, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -502.753, mean reward: -502.753 [-502.753, -502.753], mean action: 1.000 [1.000, 1.000],  loss: 21908898.000000, mae: 1423.249512, mean_q: 0.111746
  584/5000: episode: 584, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9318.864, mean reward: -9318.864 [-9318.864, -9318.864], mean action: 1.000 [1.000, 1.000],  loss: 24401664.000000, mae: 1433.432129, mean_q: 0.110078
  585/5000: episode: 585, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6347.067, mean reward: -6347.067 [-6347.067, -6347.067], mean action: 1.000 [1.000, 1.000],  loss: 24437390.000000, mae: 1557.021240, mean_q: 0.104779
  586/5000: episode: 586, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4555.242, mean reward: -4555.242 [-4555.242, -4555.242], mean action: 1.000 [1.000, 1.000],  loss: 27452660.000000, mae: 1632.752441, mean_q: 0.101460
  587/5000: episode: 587, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7333.846, mean reward: -7333.846 [-7333.846, -7333.846], mean action: 1.000 [1.000, 1.000],  loss: 31127512.000000, mae: 1817.582642, mean_q: 0.098538
  588/5000: episode: 588, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9408.358, mean reward: -9408.358 [-9408.358, -9408.358], mean action: 1.000 [1.000, 1.000],  loss: 30507152.000000, mae: 1761.276123, mean_q: 0.100609
  589/5000: episode: 589, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -743.603, mean reward: -743.603 [-743.603, -743.603], mean action: 1.000 [1.000, 1.000],  loss: 20864836.000000, mae: 1451.394531, mean_q: 0.082215
  590/5000: episode: 590, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -838.817, mean reward: -838.817 [-838.817, -838.817], mean action: 1.000 [1.000, 1.000],  loss: 28160166.000000, mae: 1685.769043, mean_q: 0.098256
  591/5000: episode: 591, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7225.898, mean reward: -7225.898 [-7225.898, -7225.898], mean action: 3.000 [3.000, 3.000],  loss: 30421538.000000, mae: 1752.783203, mean_q: 0.094527
  592/5000: episode: 592, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5946.413, mean reward: -5946.413 [-5946.413, -5946.413], mean action: 1.000 [1.000, 1.000],  loss: 24552508.000000, mae: 1540.152832, mean_q: 0.086851
  593/5000: episode: 593, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5478.542, mean reward: -5478.542 [-5478.542, -5478.542], mean action: 1.000 [1.000, 1.000],  loss: 27575716.000000, mae: 1565.245361, mean_q: 0.076232
  594/5000: episode: 594, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -7852.213, mean reward: -7852.213 [-7852.213, -7852.213], mean action: 1.000 [1.000, 1.000],  loss: 28692510.000000, mae: 1630.702393, mean_q: 0.083542
  595/5000: episode: 595, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3254.687, mean reward: -3254.687 [-3254.687, -3254.687], mean action: 1.000 [1.000, 1.000],  loss: 24747524.000000, mae: 1439.103027, mean_q: 0.075598
  596/5000: episode: 596, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2882.606, mean reward: -2882.606 [-2882.606, -2882.606], mean action: 1.000 [1.000, 1.000],  loss: 30555436.000000, mae: 1794.618408, mean_q: 0.072793
  597/5000: episode: 597, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -7258.487, mean reward: -7258.487 [-7258.487, -7258.487], mean action: 1.000 [1.000, 1.000],  loss: 29359898.000000, mae: 1770.646729, mean_q: 0.072997
  598/5000: episode: 598, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -638.268, mean reward: -638.268 [-638.268, -638.268], mean action: 1.000 [1.000, 1.000],  loss: 21977444.000000, mae: 1491.835327, mean_q: 0.064919
  599/5000: episode: 599, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2413.902, mean reward: -2413.902 [-2413.902, -2413.902], mean action: 1.000 [1.000, 1.000],  loss: 28460680.000000, mae: 1719.109619, mean_q: 0.061163
  600/5000: episode: 600, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4703.193, mean reward: -4703.193 [-4703.193, -4703.193], mean action: 1.000 [1.000, 1.000],  loss: 33172716.000000, mae: 1829.482666, mean_q: 0.059792
  601/5000: episode: 601, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -13089.851, mean reward: -13089.851 [-13089.851, -13089.851], mean action: 1.000 [1.000, 1.000],  loss: 36568516.000000, mae: 1836.307373, mean_q: 0.052671
  602/5000: episode: 602, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1865.047, mean reward: -1865.047 [-1865.047, -1865.047], mean action: 1.000 [1.000, 1.000],  loss: 30242912.000000, mae: 1768.774780, mean_q: 0.044194
  603/5000: episode: 603, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1145.461, mean reward: -1145.461 [-1145.461, -1145.461], mean action: 1.000 [1.000, 1.000],  loss: 29795416.000000, mae: 1678.012451, mean_q: 0.049031
  604/5000: episode: 604, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -9045.842, mean reward: -9045.842 [-9045.842, -9045.842], mean action: 1.000 [1.000, 1.000],  loss: 34012272.000000, mae: 1866.528076, mean_q: 0.049671
  605/5000: episode: 605, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2757.121, mean reward: -2757.121 [-2757.121, -2757.121], mean action: 1.000 [1.000, 1.000],  loss: 31543566.000000, mae: 1733.508301, mean_q: 0.036251
  606/5000: episode: 606, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9286.467, mean reward: -9286.467 [-9286.467, -9286.467], mean action: 1.000 [1.000, 1.000],  loss: 20629218.000000, mae: 1438.804932, mean_q: 0.039739
  607/5000: episode: 607, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2078.779, mean reward: -2078.779 [-2078.779, -2078.779], mean action: 1.000 [1.000, 1.000],  loss: 27721932.000000, mae: 1590.075317, mean_q: 0.033232
  608/5000: episode: 608, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1863.735, mean reward: -1863.735 [-1863.735, -1863.735], mean action: 1.000 [1.000, 1.000],  loss: 19282000.000000, mae: 1353.444092, mean_q: 0.030317
  609/5000: episode: 609, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1137.662, mean reward: -1137.662 [-1137.662, -1137.662], mean action: 1.000 [1.000, 1.000],  loss: 23049420.000000, mae: 1523.874634, mean_q: 0.029744
  610/5000: episode: 610, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5320.886, mean reward: -5320.886 [-5320.886, -5320.886], mean action: 1.000 [1.000, 1.000],  loss: 23923184.000000, mae: 1427.062256, mean_q: 0.029220
  611/5000: episode: 611, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9492.197, mean reward: -9492.197 [-9492.197, -9492.197], mean action: 1.000 [1.000, 1.000],  loss: 30002578.000000, mae: 1693.769775, mean_q: 0.013070
  612/5000: episode: 612, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3142.882, mean reward: -3142.882 [-3142.882, -3142.882], mean action: 1.000 [1.000, 1.000],  loss: 25635696.000000, mae: 1603.101807, mean_q: 0.018904
  613/5000: episode: 613, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2177.536, mean reward: -2177.536 [-2177.536, -2177.536], mean action: 1.000 [1.000, 1.000],  loss: 16829336.000000, mae: 1331.118408, mean_q: -0.000124
  614/5000: episode: 614, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5555.593, mean reward: -5555.593 [-5555.593, -5555.593], mean action: 1.000 [1.000, 1.000],  loss: 26591436.000000, mae: 1645.766113, mean_q: -0.007441
  615/5000: episode: 615, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3854.130, mean reward: -3854.130 [-3854.130, -3854.130], mean action: 1.000 [1.000, 1.000],  loss: 27931324.000000, mae: 1573.276611, mean_q: -0.006255
  616/5000: episode: 616, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6619.715, mean reward: -6619.715 [-6619.715, -6619.715], mean action: 3.000 [3.000, 3.000],  loss: 36193200.000000, mae: 1970.229126, mean_q: -0.017147
  617/5000: episode: 617, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2178.164, mean reward: -2178.164 [-2178.164, -2178.164], mean action: 1.000 [1.000, 1.000],  loss: 21595580.000000, mae: 1396.848633, mean_q: -0.022722
  618/5000: episode: 618, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3187.983, mean reward: -3187.983 [-3187.983, -3187.983], mean action: 1.000 [1.000, 1.000],  loss: 30034308.000000, mae: 1653.487549, mean_q: -0.016089
  619/5000: episode: 619, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2254.590, mean reward: -2254.590 [-2254.590, -2254.590], mean action: 1.000 [1.000, 1.000],  loss: 28512484.000000, mae: 1669.750488, mean_q: -0.028892
  620/5000: episode: 620, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5503.618, mean reward: -5503.618 [-5503.618, -5503.618], mean action: 1.000 [1.000, 1.000],  loss: 26308172.000000, mae: 1686.624023, mean_q: -0.037033
  621/5000: episode: 621, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1640.092, mean reward: -1640.092 [-1640.092, -1640.092], mean action: 1.000 [1.000, 1.000],  loss: 30070838.000000, mae: 1619.017578, mean_q: -0.032481
  622/5000: episode: 622, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3528.514, mean reward: -3528.514 [-3528.514, -3528.514], mean action: 1.000 [1.000, 1.000],  loss: 30142856.000000, mae: 1701.176514, mean_q: -0.045335
  623/5000: episode: 623, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5151.735, mean reward: -5151.735 [-5151.735, -5151.735], mean action: 1.000 [1.000, 1.000],  loss: 36145044.000000, mae: 1919.104126, mean_q: -0.045415
  624/5000: episode: 624, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1589.621, mean reward: -1589.621 [-1589.621, -1589.621], mean action: 1.000 [1.000, 1.000],  loss: 27595436.000000, mae: 1672.456787, mean_q: -0.053589
  625/5000: episode: 625, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -4773.029, mean reward: -4773.029 [-4773.029, -4773.029], mean action: 1.000 [1.000, 1.000],  loss: 27368316.000000, mae: 1641.804443, mean_q: -0.055042
  626/5000: episode: 626, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -4919.630, mean reward: -4919.630 [-4919.630, -4919.630], mean action: 1.000 [1.000, 1.000],  loss: 30078330.000000, mae: 1760.274658, mean_q: -0.060096
  627/5000: episode: 627, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2506.175, mean reward: -2506.175 [-2506.175, -2506.175], mean action: 1.000 [1.000, 1.000],  loss: 30738644.000000, mae: 1732.147461, mean_q: -0.062041
  628/5000: episode: 628, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2559.331, mean reward: -2559.331 [-2559.331, -2559.331], mean action: 1.000 [1.000, 1.000],  loss: 17621238.000000, mae: 1309.396240, mean_q: -0.067985
  629/5000: episode: 629, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1591.148, mean reward: -1591.148 [-1591.148, -1591.148], mean action: 1.000 [1.000, 1.000],  loss: 34213688.000000, mae: 1819.145264, mean_q: -0.064793
  630/5000: episode: 630, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3389.063, mean reward: -3389.063 [-3389.063, -3389.063], mean action: 1.000 [1.000, 1.000],  loss: 20754920.000000, mae: 1443.791138, mean_q: -0.078913
  631/5000: episode: 631, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -4010.754, mean reward: -4010.754 [-4010.754, -4010.754], mean action: 1.000 [1.000, 1.000],  loss: 24125692.000000, mae: 1447.068359, mean_q: -0.082478
  632/5000: episode: 632, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8259.934, mean reward: -8259.934 [-8259.934, -8259.934], mean action: 1.000 [1.000, 1.000],  loss: 25952352.000000, mae: 1597.214966, mean_q: -0.088731
  633/5000: episode: 633, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -449.769, mean reward: -449.769 [-449.769, -449.769], mean action: 3.000 [3.000, 3.000],  loss: 30088500.000000, mae: 1752.052246, mean_q: -0.090634
  634/5000: episode: 634, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1605.077, mean reward: -1605.077 [-1605.077, -1605.077], mean action: 1.000 [1.000, 1.000],  loss: 28405974.000000, mae: 1714.907471, mean_q: -0.104905
  635/5000: episode: 635, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4535.823, mean reward: -4535.823 [-4535.823, -4535.823], mean action: 1.000 [1.000, 1.000],  loss: 24146274.000000, mae: 1504.518433, mean_q: -0.110470
  636/5000: episode: 636, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1592.660, mean reward: -1592.660 [-1592.660, -1592.660], mean action: 2.000 [2.000, 2.000],  loss: 25500836.000000, mae: 1522.720459, mean_q: -0.105572
  637/5000: episode: 637, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2722.165, mean reward: -2722.165 [-2722.165, -2722.165], mean action: 1.000 [1.000, 1.000],  loss: 26647464.000000, mae: 1513.982910, mean_q: -0.122962
  638/5000: episode: 638, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -7806.734, mean reward: -7806.734 [-7806.734, -7806.734], mean action: 1.000 [1.000, 1.000],  loss: 22856238.000000, mae: 1478.080444, mean_q: -0.121352
  639/5000: episode: 639, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6522.760, mean reward: -6522.760 [-6522.760, -6522.760], mean action: 1.000 [1.000, 1.000],  loss: 24338608.000000, mae: 1552.889893, mean_q: -0.126529
  640/5000: episode: 640, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3791.859, mean reward: -3791.859 [-3791.859, -3791.859], mean action: 1.000 [1.000, 1.000],  loss: 19950596.000000, mae: 1358.137451, mean_q: -0.141345
  641/5000: episode: 641, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3050.083, mean reward: -3050.083 [-3050.083, -3050.083], mean action: 1.000 [1.000, 1.000],  loss: 31013124.000000, mae: 1757.497314, mean_q: -0.135421
  642/5000: episode: 642, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -7723.586, mean reward: -7723.586 [-7723.586, -7723.586], mean action: 1.000 [1.000, 1.000],  loss: 27122998.000000, mae: 1485.935791, mean_q: -0.148446
  643/5000: episode: 643, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1750.960, mean reward: -1750.960 [-1750.960, -1750.960], mean action: 1.000 [1.000, 1.000],  loss: 28893788.000000, mae: 1680.911987, mean_q: -0.155137
  644/5000: episode: 644, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3473.609, mean reward: -3473.609 [-3473.609, -3473.609], mean action: 1.000 [1.000, 1.000],  loss: 28581416.000000, mae: 1570.015137, mean_q: -0.163842
  645/5000: episode: 645, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5028.565, mean reward: -5028.565 [-5028.565, -5028.565], mean action: 1.000 [1.000, 1.000],  loss: 22577732.000000, mae: 1446.814087, mean_q: -0.167856
  646/5000: episode: 646, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2484.929, mean reward: -2484.929 [-2484.929, -2484.929], mean action: 1.000 [1.000, 1.000],  loss: 27307776.000000, mae: 1560.555176, mean_q: -0.179249
  647/5000: episode: 647, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9445.510, mean reward: -9445.510 [-9445.510, -9445.510], mean action: 1.000 [1.000, 1.000],  loss: 32119896.000000, mae: 1719.412842, mean_q: -0.188242
  648/5000: episode: 648, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2123.284, mean reward: -2123.284 [-2123.284, -2123.284], mean action: 1.000 [1.000, 1.000],  loss: 30422712.000000, mae: 1757.447754, mean_q: -0.201681
  649/5000: episode: 649, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7359.718, mean reward: -7359.718 [-7359.718, -7359.718], mean action: 1.000 [1.000, 1.000],  loss: 29687936.000000, mae: 1670.401123, mean_q: -0.201099
  650/5000: episode: 650, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3686.680, mean reward: -3686.680 [-3686.680, -3686.680], mean action: 1.000 [1.000, 1.000],  loss: 24384136.000000, mae: 1520.718384, mean_q: -0.206939
  651/5000: episode: 651, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4511.220, mean reward: -4511.220 [-4511.220, -4511.220], mean action: 1.000 [1.000, 1.000],  loss: 24136812.000000, mae: 1452.134766, mean_q: -0.218460
  652/5000: episode: 652, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8230.645, mean reward: -8230.645 [-8230.645, -8230.645], mean action: 1.000 [1.000, 1.000],  loss: 22471316.000000, mae: 1404.131470, mean_q: -0.231199
  653/5000: episode: 653, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6249.690, mean reward: -6249.690 [-6249.690, -6249.690], mean action: 1.000 [1.000, 1.000],  loss: 27557462.000000, mae: 1627.292725, mean_q: -0.229127
  654/5000: episode: 654, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -620.208, mean reward: -620.208 [-620.208, -620.208], mean action: 1.000 [1.000, 1.000],  loss: 25888514.000000, mae: 1561.319092, mean_q: -0.238141
  655/5000: episode: 655, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1637.362, mean reward: -1637.362 [-1637.362, -1637.362], mean action: 1.000 [1.000, 1.000],  loss: 25193358.000000, mae: 1573.538574, mean_q: -0.253613
  656/5000: episode: 656, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -474.540, mean reward: -474.540 [-474.540, -474.540], mean action: 1.000 [1.000, 1.000],  loss: 22041248.000000, mae: 1493.247070, mean_q: -0.269729
  657/5000: episode: 657, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2982.893, mean reward: -2982.893 [-2982.893, -2982.893], mean action: 1.000 [1.000, 1.000],  loss: 36130464.000000, mae: 1799.894775, mean_q: -0.260188
  658/5000: episode: 658, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3705.377, mean reward: -3705.377 [-3705.377, -3705.377], mean action: 1.000 [1.000, 1.000],  loss: 24145096.000000, mae: 1525.259033, mean_q: -0.277588
  659/5000: episode: 659, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5323.871, mean reward: -5323.871 [-5323.871, -5323.871], mean action: 1.000 [1.000, 1.000],  loss: 15340936.000000, mae: 1194.808105, mean_q: -0.286555
  660/5000: episode: 660, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -3957.988, mean reward: -3957.988 [-3957.988, -3957.988], mean action: 1.000 [1.000, 1.000],  loss: 28127376.000000, mae: 1675.430664, mean_q: -0.294402
  661/5000: episode: 661, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5959.876, mean reward: -5959.876 [-5959.876, -5959.876], mean action: 1.000 [1.000, 1.000],  loss: 22146110.000000, mae: 1406.827393, mean_q: -0.309092
  662/5000: episode: 662, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4620.946, mean reward: -4620.946 [-4620.946, -4620.946], mean action: 1.000 [1.000, 1.000],  loss: 20581236.000000, mae: 1461.336670, mean_q: -0.319640
  663/5000: episode: 663, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1879.972, mean reward: -1879.972 [-1879.972, -1879.972], mean action: 1.000 [1.000, 1.000],  loss: 24616728.000000, mae: 1408.189087, mean_q: -0.319743
  664/5000: episode: 664, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10871.251, mean reward: -10871.251 [-10871.251, -10871.251], mean action: 1.000 [1.000, 1.000],  loss: 26794506.000000, mae: 1608.999756, mean_q: -0.318740
  665/5000: episode: 665, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8485.047, mean reward: -8485.047 [-8485.047, -8485.047], mean action: 1.000 [1.000, 1.000],  loss: 22854948.000000, mae: 1489.723267, mean_q: -0.338909
  666/5000: episode: 666, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5946.862, mean reward: -5946.862 [-5946.862, -5946.862], mean action: 1.000 [1.000, 1.000],  loss: 23207860.000000, mae: 1495.321289, mean_q: -0.343758
  667/5000: episode: 667, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1894.504, mean reward: -1894.504 [-1894.504, -1894.504], mean action: 1.000 [1.000, 1.000],  loss: 21838598.000000, mae: 1483.162109, mean_q: -0.357087
  668/5000: episode: 668, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9948.573, mean reward: -9948.573 [-9948.573, -9948.573], mean action: 1.000 [1.000, 1.000],  loss: 26452596.000000, mae: 1483.763184, mean_q: -0.363375
  669/5000: episode: 669, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4318.786, mean reward: -4318.786 [-4318.786, -4318.786], mean action: 1.000 [1.000, 1.000],  loss: 21356768.000000, mae: 1359.937500, mean_q: -0.379445
  670/5000: episode: 670, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2996.555, mean reward: -2996.555 [-2996.555, -2996.555], mean action: 1.000 [1.000, 1.000],  loss: 26695404.000000, mae: 1584.052490, mean_q: -0.378251
  671/5000: episode: 671, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7378.152, mean reward: -7378.152 [-7378.152, -7378.152], mean action: 1.000 [1.000, 1.000],  loss: 29075528.000000, mae: 1710.248779, mean_q: -0.390931
  672/5000: episode: 672, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -156.443, mean reward: -156.443 [-156.443, -156.443], mean action: 2.000 [2.000, 2.000],  loss: 32940174.000000, mae: 1773.969604, mean_q: -0.401135
  673/5000: episode: 673, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4087.691, mean reward: -4087.691 [-4087.691, -4087.691], mean action: 1.000 [1.000, 1.000],  loss: 25215432.000000, mae: 1489.070679, mean_q: -0.411195
  674/5000: episode: 674, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3662.805, mean reward: -3662.805 [-3662.805, -3662.805], mean action: 1.000 [1.000, 1.000],  loss: 23444930.000000, mae: 1542.393311, mean_q: -0.431646
  675/5000: episode: 675, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1763.768, mean reward: -1763.768 [-1763.768, -1763.768], mean action: 3.000 [3.000, 3.000],  loss: 23607472.000000, mae: 1568.959961, mean_q: -0.433202
  676/5000: episode: 676, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5338.871, mean reward: -5338.871 [-5338.871, -5338.871], mean action: 1.000 [1.000, 1.000],  loss: 26414500.000000, mae: 1615.103027, mean_q: -0.450212
  677/5000: episode: 677, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1620.748, mean reward: -1620.748 [-1620.748, -1620.748], mean action: 1.000 [1.000, 1.000],  loss: 18632030.000000, mae: 1329.587646, mean_q: -0.470707
  678/5000: episode: 678, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8252.332, mean reward: -8252.332 [-8252.332, -8252.332], mean action: 1.000 [1.000, 1.000],  loss: 24604862.000000, mae: 1554.656982, mean_q: -0.469380
  679/5000: episode: 679, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1723.377, mean reward: -1723.377 [-1723.377, -1723.377], mean action: 1.000 [1.000, 1.000],  loss: 25488992.000000, mae: 1520.752686, mean_q: -0.482842
  680/5000: episode: 680, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1085.825, mean reward: -1085.825 [-1085.825, -1085.825], mean action: 1.000 [1.000, 1.000],  loss: 24695172.000000, mae: 1547.766602, mean_q: -0.485655
  681/5000: episode: 681, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3152.432, mean reward: -3152.432 [-3152.432, -3152.432], mean action: 1.000 [1.000, 1.000],  loss: 25213120.000000, mae: 1543.458252, mean_q: -0.503497
  682/5000: episode: 682, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -235.044, mean reward: -235.044 [-235.044, -235.044], mean action: 1.000 [1.000, 1.000],  loss: 33562848.000000, mae: 1773.863892, mean_q: -0.507427
  683/5000: episode: 683, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5052.681, mean reward: -5052.681 [-5052.681, -5052.681], mean action: 1.000 [1.000, 1.000],  loss: 23155690.000000, mae: 1476.032715, mean_q: -0.532186
  684/5000: episode: 684, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7519.213, mean reward: -7519.213 [-7519.213, -7519.213], mean action: 1.000 [1.000, 1.000],  loss: 31075798.000000, mae: 1647.645508, mean_q: -0.532173
  685/5000: episode: 685, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4810.753, mean reward: -4810.753 [-4810.753, -4810.753], mean action: 1.000 [1.000, 1.000],  loss: 25381606.000000, mae: 1566.426514, mean_q: -0.548639
  686/5000: episode: 686, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -773.702, mean reward: -773.702 [-773.702, -773.702], mean action: 1.000 [1.000, 1.000],  loss: 30138814.000000, mae: 1774.155396, mean_q: -0.547705
  687/5000: episode: 687, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5371.767, mean reward: -5371.767 [-5371.767, -5371.767], mean action: 1.000 [1.000, 1.000],  loss: 18042504.000000, mae: 1331.203369, mean_q: -0.570703
  688/5000: episode: 688, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5747.362, mean reward: -5747.362 [-5747.362, -5747.362], mean action: 1.000 [1.000, 1.000],  loss: 31921132.000000, mae: 1763.312988, mean_q: -0.572017
  689/5000: episode: 689, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4682.807, mean reward: -4682.807 [-4682.807, -4682.807], mean action: 1.000 [1.000, 1.000],  loss: 25411570.000000, mae: 1612.395508, mean_q: -0.590582
  690/5000: episode: 690, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3386.243, mean reward: -3386.243 [-3386.243, -3386.243], mean action: 1.000 [1.000, 1.000],  loss: 24293650.000000, mae: 1524.628540, mean_q: -0.596337
  691/5000: episode: 691, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2548.646, mean reward: -2548.646 [-2548.646, -2548.646], mean action: 1.000 [1.000, 1.000],  loss: 24712160.000000, mae: 1593.794678, mean_q: -0.604625
  692/5000: episode: 692, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7366.096, mean reward: -7366.096 [-7366.096, -7366.096], mean action: 1.000 [1.000, 1.000],  loss: 27078832.000000, mae: 1503.063843, mean_q: -0.622274
  693/5000: episode: 693, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2409.021, mean reward: -2409.021 [-2409.021, -2409.021], mean action: 2.000 [2.000, 2.000],  loss: 22513336.000000, mae: 1478.916504, mean_q: -0.625974
  694/5000: episode: 694, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2616.453, mean reward: -2616.453 [-2616.453, -2616.453], mean action: 1.000 [1.000, 1.000],  loss: 30067132.000000, mae: 1750.231323, mean_q: -0.632731
  695/5000: episode: 695, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3764.445, mean reward: -3764.445 [-3764.445, -3764.445], mean action: 1.000 [1.000, 1.000],  loss: 36517920.000000, mae: 1835.416260, mean_q: -0.648150
  696/5000: episode: 696, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8379.346, mean reward: -8379.346 [-8379.346, -8379.346], mean action: 1.000 [1.000, 1.000],  loss: 29651276.000000, mae: 1736.525879, mean_q: -0.654037
  697/5000: episode: 697, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4762.150, mean reward: -4762.150 [-4762.150, -4762.150], mean action: 1.000 [1.000, 1.000],  loss: 22508604.000000, mae: 1483.766357, mean_q: -0.679387
  698/5000: episode: 698, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -8063.940, mean reward: -8063.940 [-8063.940, -8063.940], mean action: 1.000 [1.000, 1.000],  loss: 19651384.000000, mae: 1395.326660, mean_q: -0.690388
  699/5000: episode: 699, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -799.243, mean reward: -799.243 [-799.243, -799.243], mean action: 1.000 [1.000, 1.000],  loss: 22208114.000000, mae: 1508.971680, mean_q: -0.690150
  700/5000: episode: 700, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -494.921, mean reward: -494.921 [-494.921, -494.921], mean action: 1.000 [1.000, 1.000],  loss: 24098862.000000, mae: 1508.405640, mean_q: -0.704462
  701/5000: episode: 701, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2283.595, mean reward: -2283.595 [-2283.595, -2283.595], mean action: 2.000 [2.000, 2.000],  loss: 23662660.000000, mae: 1502.420166, mean_q: -0.721985
  702/5000: episode: 702, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5859.956, mean reward: -5859.956 [-5859.956, -5859.956], mean action: 1.000 [1.000, 1.000],  loss: 22926940.000000, mae: 1475.058594, mean_q: -0.736876
  703/5000: episode: 703, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -3304.489, mean reward: -3304.489 [-3304.489, -3304.489], mean action: 1.000 [1.000, 1.000],  loss: 22600466.000000, mae: 1491.177124, mean_q: -0.743587
  704/5000: episode: 704, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6356.585, mean reward: -6356.585 [-6356.585, -6356.585], mean action: 1.000 [1.000, 1.000],  loss: 22292320.000000, mae: 1434.333252, mean_q: -0.765598
  705/5000: episode: 705, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2349.791, mean reward: -2349.791 [-2349.791, -2349.791], mean action: 1.000 [1.000, 1.000],  loss: 23136802.000000, mae: 1480.185791, mean_q: -0.776779
  706/5000: episode: 706, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2757.452, mean reward: -2757.452 [-2757.452, -2757.452], mean action: 1.000 [1.000, 1.000],  loss: 30045322.000000, mae: 1659.504883, mean_q: -0.788035
  707/5000: episode: 707, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -7813.379, mean reward: -7813.379 [-7813.379, -7813.379], mean action: 1.000 [1.000, 1.000],  loss: 19083104.000000, mae: 1340.091919, mean_q: -0.805977
  708/5000: episode: 708, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5498.081, mean reward: -5498.081 [-5498.081, -5498.081], mean action: 1.000 [1.000, 1.000],  loss: 19797688.000000, mae: 1340.812622, mean_q: -0.818086
  709/5000: episode: 709, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3732.828, mean reward: -3732.828 [-3732.828, -3732.828], mean action: 1.000 [1.000, 1.000],  loss: 31952094.000000, mae: 1732.175293, mean_q: -0.819770
  710/5000: episode: 710, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3521.517, mean reward: -3521.517 [-3521.517, -3521.517], mean action: 1.000 [1.000, 1.000],  loss: 23166708.000000, mae: 1538.502197, mean_q: -0.829705
  711/5000: episode: 711, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4829.550, mean reward: -4829.550 [-4829.550, -4829.550], mean action: 1.000 [1.000, 1.000],  loss: 23857376.000000, mae: 1439.523315, mean_q: -0.841110
  712/5000: episode: 712, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1483.730, mean reward: -1483.730 [-1483.730, -1483.730], mean action: 1.000 [1.000, 1.000],  loss: 27942480.000000, mae: 1651.411743, mean_q: -0.852030
  713/5000: episode: 713, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4072.665, mean reward: -4072.665 [-4072.665, -4072.665], mean action: 1.000 [1.000, 1.000],  loss: 21389850.000000, mae: 1481.935059, mean_q: -0.870984
  714/5000: episode: 714, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -731.703, mean reward: -731.703 [-731.703, -731.703], mean action: 3.000 [3.000, 3.000],  loss: 26844716.000000, mae: 1573.652832, mean_q: -0.884788
  715/5000: episode: 715, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4966.120, mean reward: -4966.120 [-4966.120, -4966.120], mean action: 1.000 [1.000, 1.000],  loss: 30462690.000000, mae: 1728.415771, mean_q: -0.898449
  716/5000: episode: 716, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -396.418, mean reward: -396.418 [-396.418, -396.418], mean action: 1.000 [1.000, 1.000],  loss: 24744050.000000, mae: 1458.778442, mean_q: -0.920542
  717/5000: episode: 717, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3130.034, mean reward: -3130.034 [-3130.034, -3130.034], mean action: 1.000 [1.000, 1.000],  loss: 18594194.000000, mae: 1266.380249, mean_q: -0.937386
  718/5000: episode: 718, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3723.128, mean reward: -3723.128 [-3723.128, -3723.128], mean action: 1.000 [1.000, 1.000],  loss: 33821440.000000, mae: 1845.196533, mean_q: -0.936391
  719/5000: episode: 719, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3748.941, mean reward: -3748.941 [-3748.941, -3748.941], mean action: 1.000 [1.000, 1.000],  loss: 19557084.000000, mae: 1276.763306, mean_q: -0.960812
  720/5000: episode: 720, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1560.022, mean reward: -1560.022 [-1560.022, -1560.022], mean action: 2.000 [2.000, 2.000],  loss: 22806104.000000, mae: 1403.736084, mean_q: -0.976173
  721/5000: episode: 721, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -3930.646, mean reward: -3930.646 [-3930.646, -3930.646], mean action: 1.000 [1.000, 1.000],  loss: 29849806.000000, mae: 1644.579834, mean_q: -0.983963
  722/5000: episode: 722, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2926.141, mean reward: -2926.141 [-2926.141, -2926.141], mean action: 1.000 [1.000, 1.000],  loss: 31847064.000000, mae: 1717.210693, mean_q: -0.998128
  723/5000: episode: 723, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5920.078, mean reward: -5920.078 [-5920.078, -5920.078], mean action: 1.000 [1.000, 1.000],  loss: 18468298.000000, mae: 1267.305786, mean_q: -1.017533
  724/5000: episode: 724, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8570.245, mean reward: -8570.245 [-8570.245, -8570.245], mean action: 1.000 [1.000, 1.000],  loss: 31105812.000000, mae: 1673.118896, mean_q: -1.021894
  725/5000: episode: 725, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -6499.928, mean reward: -6499.928 [-6499.928, -6499.928], mean action: 1.000 [1.000, 1.000],  loss: 28532240.000000, mae: 1650.555176, mean_q: -1.043856
  726/5000: episode: 726, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2071.321, mean reward: -2071.321 [-2071.321, -2071.321], mean action: 1.000 [1.000, 1.000],  loss: 24460208.000000, mae: 1508.371582, mean_q: -1.062133
  727/5000: episode: 727, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6700.832, mean reward: -6700.832 [-6700.832, -6700.832], mean action: 1.000 [1.000, 1.000],  loss: 21533826.000000, mae: 1462.382812, mean_q: -1.073591
  728/5000: episode: 728, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8329.055, mean reward: -8329.055 [-8329.055, -8329.055], mean action: 1.000 [1.000, 1.000],  loss: 20362794.000000, mae: 1397.518799, mean_q: -1.085185
  729/5000: episode: 729, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -413.928, mean reward: -413.928 [-413.928, -413.928], mean action: 1.000 [1.000, 1.000],  loss: 19196716.000000, mae: 1349.024414, mean_q: -1.101445
  730/5000: episode: 730, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9878.971, mean reward: -9878.971 [-9878.971, -9878.971], mean action: 1.000 [1.000, 1.000],  loss: 25209916.000000, mae: 1519.317871, mean_q: -1.114029
  731/5000: episode: 731, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8328.901, mean reward: -8328.901 [-8328.901, -8328.901], mean action: 1.000 [1.000, 1.000],  loss: 30175588.000000, mae: 1689.903076, mean_q: -1.128122
  732/5000: episode: 732, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4897.376, mean reward: -4897.376 [-4897.376, -4897.376], mean action: 1.000 [1.000, 1.000],  loss: 35643912.000000, mae: 1855.433838, mean_q: -1.137059
  733/5000: episode: 733, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6037.744, mean reward: -6037.744 [-6037.744, -6037.744], mean action: 1.000 [1.000, 1.000],  loss: 18124372.000000, mae: 1313.993286, mean_q: -1.158849
  734/5000: episode: 734, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9656.719, mean reward: -9656.719 [-9656.719, -9656.719], mean action: 1.000 [1.000, 1.000],  loss: 15158372.000000, mae: 1132.277222, mean_q: -1.183995
  735/5000: episode: 735, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2718.541, mean reward: -2718.541 [-2718.541, -2718.541], mean action: 1.000 [1.000, 1.000],  loss: 30578650.000000, mae: 1669.320190, mean_q: -1.183790
  736/5000: episode: 736, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5833.040, mean reward: -5833.040 [-5833.040, -5833.040], mean action: 1.000 [1.000, 1.000],  loss: 22173270.000000, mae: 1405.036499, mean_q: -1.206747
  737/5000: episode: 737, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5513.699, mean reward: -5513.699 [-5513.699, -5513.699], mean action: 1.000 [1.000, 1.000],  loss: 28700452.000000, mae: 1703.315308, mean_q: -1.216247
  738/5000: episode: 738, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4763.792, mean reward: -4763.792 [-4763.792, -4763.792], mean action: 1.000 [1.000, 1.000],  loss: 22906960.000000, mae: 1514.894043, mean_q: -1.236131
  739/5000: episode: 739, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2111.096, mean reward: -2111.096 [-2111.096, -2111.096], mean action: 1.000 [1.000, 1.000],  loss: 27221588.000000, mae: 1618.984131, mean_q: -1.249669
  740/5000: episode: 740, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -4253.100, mean reward: -4253.100 [-4253.100, -4253.100], mean action: 1.000 [1.000, 1.000],  loss: 22681614.000000, mae: 1472.943726, mean_q: -1.271031
  741/5000: episode: 741, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3459.136, mean reward: -3459.136 [-3459.136, -3459.136], mean action: 0.000 [0.000, 0.000],  loss: 20670188.000000, mae: 1385.674805, mean_q: -1.285039
  742/5000: episode: 742, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1673.401, mean reward: -1673.401 [-1673.401, -1673.401], mean action: 1.000 [1.000, 1.000],  loss: 26335588.000000, mae: 1577.223877, mean_q: -1.300171
  743/5000: episode: 743, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3683.403, mean reward: -3683.403 [-3683.403, -3683.403], mean action: 1.000 [1.000, 1.000],  loss: 24884980.000000, mae: 1492.813599, mean_q: -1.317461
  744/5000: episode: 744, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5256.679, mean reward: -5256.679 [-5256.679, -5256.679], mean action: 1.000 [1.000, 1.000],  loss: 16045605.000000, mae: 1155.567139, mean_q: -1.327088
  745/5000: episode: 745, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5370.850, mean reward: -5370.850 [-5370.850, -5370.850], mean action: 3.000 [3.000, 3.000],  loss: 21893248.000000, mae: 1490.280029, mean_q: -1.348929
  746/5000: episode: 746, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6270.021, mean reward: -6270.021 [-6270.021, -6270.021], mean action: 1.000 [1.000, 1.000],  loss: 31986896.000000, mae: 1762.271973, mean_q: -1.352954
  747/5000: episode: 747, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3239.007, mean reward: -3239.007 [-3239.007, -3239.007], mean action: 1.000 [1.000, 1.000],  loss: 27444288.000000, mae: 1627.050171, mean_q: -1.375298
  748/5000: episode: 748, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6218.362, mean reward: -6218.362 [-6218.362, -6218.362], mean action: 1.000 [1.000, 1.000],  loss: 34834292.000000, mae: 1888.940063, mean_q: -1.388574
  749/5000: episode: 749, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2198.379, mean reward: -2198.379 [-2198.379, -2198.379], mean action: 1.000 [1.000, 1.000],  loss: 15328532.000000, mae: 1197.067627, mean_q: -1.416872
  750/5000: episode: 750, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5185.691, mean reward: -5185.691 [-5185.691, -5185.691], mean action: 2.000 [2.000, 2.000],  loss: 22425382.000000, mae: 1502.640137, mean_q: -1.419987
  751/5000: episode: 751, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -771.004, mean reward: -771.004 [-771.004, -771.004], mean action: 1.000 [1.000, 1.000],  loss: 33860748.000000, mae: 1774.436401, mean_q: -1.432607
  752/5000: episode: 752, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6187.339, mean reward: -6187.339 [-6187.339, -6187.339], mean action: 1.000 [1.000, 1.000],  loss: 24769496.000000, mae: 1437.448975, mean_q: -1.458308
  753/5000: episode: 753, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4087.284, mean reward: -4087.284 [-4087.284, -4087.284], mean action: 1.000 [1.000, 1.000],  loss: 19171516.000000, mae: 1394.549438, mean_q: -1.486961
  754/5000: episode: 754, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9190.542, mean reward: -9190.542 [-9190.542, -9190.542], mean action: 1.000 [1.000, 1.000],  loss: 22090926.000000, mae: 1427.240112, mean_q: -1.483110
  755/5000: episode: 755, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5491.427, mean reward: -5491.427 [-5491.427, -5491.427], mean action: 1.000 [1.000, 1.000],  loss: 26723066.000000, mae: 1605.969482, mean_q: -1.504417
  756/5000: episode: 756, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2944.627, mean reward: -2944.627 [-2944.627, -2944.627], mean action: 1.000 [1.000, 1.000],  loss: 24739952.000000, mae: 1534.866333, mean_q: -1.531516
  757/5000: episode: 757, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10425.238, mean reward: -10425.238 [-10425.238, -10425.238], mean action: 0.000 [0.000, 0.000],  loss: 25630492.000000, mae: 1587.416992, mean_q: -1.531660
  758/5000: episode: 758, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -858.379, mean reward: -858.379 [-858.379, -858.379], mean action: 1.000 [1.000, 1.000],  loss: 22132870.000000, mae: 1440.963379, mean_q: -1.548021
  759/5000: episode: 759, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2625.495, mean reward: -2625.495 [-2625.495, -2625.495], mean action: 1.000 [1.000, 1.000],  loss: 16205613.000000, mae: 1212.622559, mean_q: -1.573629
  760/5000: episode: 760, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6337.027, mean reward: -6337.027 [-6337.027, -6337.027], mean action: 1.000 [1.000, 1.000],  loss: 24201276.000000, mae: 1514.017822, mean_q: -1.583028
  761/5000: episode: 761, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7608.229, mean reward: -7608.229 [-7608.229, -7608.229], mean action: 1.000 [1.000, 1.000],  loss: 23280068.000000, mae: 1470.651611, mean_q: -1.595967
  762/5000: episode: 762, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1918.508, mean reward: -1918.508 [-1918.508, -1918.508], mean action: 1.000 [1.000, 1.000],  loss: 19655636.000000, mae: 1369.860596, mean_q: -1.614451
  763/5000: episode: 763, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1193.285, mean reward: -1193.285 [-1193.285, -1193.285], mean action: 2.000 [2.000, 2.000],  loss: 17707492.000000, mae: 1237.616455, mean_q: -1.640677
  764/5000: episode: 764, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4346.804, mean reward: -4346.804 [-4346.804, -4346.804], mean action: 1.000 [1.000, 1.000],  loss: 26217748.000000, mae: 1571.246094, mean_q: -1.636688
  765/5000: episode: 765, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1432.399, mean reward: -1432.399 [-1432.399, -1432.399], mean action: 1.000 [1.000, 1.000],  loss: 21322658.000000, mae: 1414.052246, mean_q: -1.666186
  766/5000: episode: 766, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6567.015, mean reward: -6567.015 [-6567.015, -6567.015], mean action: 1.000 [1.000, 1.000],  loss: 23297728.000000, mae: 1490.413574, mean_q: -1.677318
  767/5000: episode: 767, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4970.792, mean reward: -4970.792 [-4970.792, -4970.792], mean action: 0.000 [0.000, 0.000],  loss: 20676332.000000, mae: 1466.726562, mean_q: -1.694684
  768/5000: episode: 768, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -267.877, mean reward: -267.877 [-267.877, -267.877], mean action: 1.000 [1.000, 1.000],  loss: 21762596.000000, mae: 1391.054688, mean_q: -1.709569
  769/5000: episode: 769, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -850.267, mean reward: -850.267 [-850.267, -850.267], mean action: 1.000 [1.000, 1.000],  loss: 20715638.000000, mae: 1381.337646, mean_q: -1.736624
  770/5000: episode: 770, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8213.814, mean reward: -8213.814 [-8213.814, -8213.814], mean action: 1.000 [1.000, 1.000],  loss: 20777892.000000, mae: 1374.679443, mean_q: -1.748576
  771/5000: episode: 771, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3261.416, mean reward: -3261.416 [-3261.416, -3261.416], mean action: 1.000 [1.000, 1.000],  loss: 20655032.000000, mae: 1425.103760, mean_q: -1.762799
  772/5000: episode: 772, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3236.870, mean reward: -3236.870 [-3236.870, -3236.870], mean action: 1.000 [1.000, 1.000],  loss: 23865618.000000, mae: 1537.911865, mean_q: -1.779958
  773/5000: episode: 773, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -4719.356, mean reward: -4719.356 [-4719.356, -4719.356], mean action: 1.000 [1.000, 1.000],  loss: 21626242.000000, mae: 1413.218384, mean_q: -1.807913
  774/5000: episode: 774, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5299.109, mean reward: -5299.109 [-5299.109, -5299.109], mean action: 1.000 [1.000, 1.000],  loss: 19141292.000000, mae: 1409.138062, mean_q: -1.823673
  775/5000: episode: 775, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2380.060, mean reward: -2380.060 [-2380.060, -2380.060], mean action: 1.000 [1.000, 1.000],  loss: 25286596.000000, mae: 1471.955322, mean_q: -1.839163
  776/5000: episode: 776, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1961.021, mean reward: -1961.021 [-1961.021, -1961.021], mean action: 1.000 [1.000, 1.000],  loss: 25108532.000000, mae: 1552.542725, mean_q: -1.851793
  777/5000: episode: 777, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8189.308, mean reward: -8189.308 [-8189.308, -8189.308], mean action: 1.000 [1.000, 1.000],  loss: 21582332.000000, mae: 1416.286865, mean_q: -1.871195
  778/5000: episode: 778, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2471.652, mean reward: -2471.652 [-2471.652, -2471.652], mean action: 2.000 [2.000, 2.000],  loss: 24580652.000000, mae: 1515.036133, mean_q: -1.888179
  779/5000: episode: 779, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6931.829, mean reward: -6931.829 [-6931.829, -6931.829], mean action: 1.000 [1.000, 1.000],  loss: 28299744.000000, mae: 1662.298096, mean_q: -1.901799
  780/5000: episode: 780, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4088.332, mean reward: -4088.332 [-4088.332, -4088.332], mean action: 1.000 [1.000, 1.000],  loss: 20402634.000000, mae: 1392.912231, mean_q: -1.916717
  781/5000: episode: 781, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6479.397, mean reward: -6479.397 [-6479.397, -6479.397], mean action: 2.000 [2.000, 2.000],  loss: 25614276.000000, mae: 1530.785889, mean_q: -1.932748
  782/5000: episode: 782, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2973.757, mean reward: -2973.757 [-2973.757, -2973.757], mean action: 1.000 [1.000, 1.000],  loss: 15773462.000000, mae: 1232.909058, mean_q: -1.957044
  783/5000: episode: 783, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2874.541, mean reward: -2874.541 [-2874.541, -2874.541], mean action: 2.000 [2.000, 2.000],  loss: 24639304.000000, mae: 1475.058228, mean_q: -1.967184
  784/5000: episode: 784, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1955.232, mean reward: -1955.232 [-1955.232, -1955.232], mean action: 2.000 [2.000, 2.000],  loss: 19932058.000000, mae: 1404.792358, mean_q: -1.975524
  785/5000: episode: 785, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8768.094, mean reward: -8768.094 [-8768.094, -8768.094], mean action: 1.000 [1.000, 1.000],  loss: 30982566.000000, mae: 1768.589600, mean_q: -1.986507
  786/5000: episode: 786, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12271.668, mean reward: -12271.668 [-12271.668, -12271.668], mean action: 0.000 [0.000, 0.000],  loss: 27736280.000000, mae: 1649.912109, mean_q: -2.000453
  787/5000: episode: 787, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2396.799, mean reward: -2396.799 [-2396.799, -2396.799], mean action: 2.000 [2.000, 2.000],  loss: 23351782.000000, mae: 1455.937500, mean_q: -2.020841
  788/5000: episode: 788, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -108.294, mean reward: -108.294 [-108.294, -108.294], mean action: 2.000 [2.000, 2.000],  loss: 26246580.000000, mae: 1531.514771, mean_q: -2.031000
  789/5000: episode: 789, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1805.505, mean reward: -1805.505 [-1805.505, -1805.505], mean action: 2.000 [2.000, 2.000],  loss: 20840012.000000, mae: 1314.276855, mean_q: -2.047304
  790/5000: episode: 790, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6181.050, mean reward: -6181.050 [-6181.050, -6181.050], mean action: 2.000 [2.000, 2.000],  loss: 24580854.000000, mae: 1506.889038, mean_q: -2.052227
  791/5000: episode: 791, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -15710.788, mean reward: -15710.788 [-15710.788, -15710.788], mean action: 0.000 [0.000, 0.000],  loss: 22296160.000000, mae: 1441.958740, mean_q: -2.073382
  792/5000: episode: 792, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1405.475, mean reward: -1405.475 [-1405.475, -1405.475], mean action: 2.000 [2.000, 2.000],  loss: 25516328.000000, mae: 1577.550415, mean_q: -2.079942
  793/5000: episode: 793, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5299.223, mean reward: -5299.223 [-5299.223, -5299.223], mean action: 2.000 [2.000, 2.000],  loss: 20946928.000000, mae: 1260.413574, mean_q: -2.111467
  794/5000: episode: 794, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -893.591, mean reward: -893.591 [-893.591, -893.591], mean action: 2.000 [2.000, 2.000],  loss: 16538986.000000, mae: 1249.274658, mean_q: -2.118402
  795/5000: episode: 795, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5486.143, mean reward: -5486.143 [-5486.143, -5486.143], mean action: 2.000 [2.000, 2.000],  loss: 29866048.000000, mae: 1596.801758, mean_q: -2.111296
  796/5000: episode: 796, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1549.962, mean reward: -1549.962 [-1549.962, -1549.962], mean action: 2.000 [2.000, 2.000],  loss: 22526560.000000, mae: 1466.808105, mean_q: -2.136921
  797/5000: episode: 797, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -39.241, mean reward: -39.241 [-39.241, -39.241], mean action: 2.000 [2.000, 2.000],  loss: 17511846.000000, mae: 1239.595215, mean_q: -2.162580
  798/5000: episode: 798, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6561.099, mean reward: -6561.099 [-6561.099, -6561.099], mean action: 2.000 [2.000, 2.000],  loss: 28101252.000000, mae: 1563.526489, mean_q: -2.174591
  799/5000: episode: 799, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -609.470, mean reward: -609.470 [-609.470, -609.470], mean action: 2.000 [2.000, 2.000],  loss: 25873836.000000, mae: 1567.719849, mean_q: -2.190773
  800/5000: episode: 800, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -514.485, mean reward: -514.485 [-514.485, -514.485], mean action: 2.000 [2.000, 2.000],  loss: 16752468.000000, mae: 1202.602417, mean_q: -2.213519
  801/5000: episode: 801, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1056.632, mean reward: -1056.632 [-1056.632, -1056.632], mean action: 2.000 [2.000, 2.000],  loss: 22090204.000000, mae: 1427.891357, mean_q: -2.202471
  802/5000: episode: 802, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6593.849, mean reward: -6593.849 [-6593.849, -6593.849], mean action: 2.000 [2.000, 2.000],  loss: 21762058.000000, mae: 1471.955078, mean_q: -2.226481
  803/5000: episode: 803, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -495.901, mean reward: -495.901 [-495.901, -495.901], mean action: 2.000 [2.000, 2.000],  loss: 25555628.000000, mae: 1533.088745, mean_q: -2.244401
  804/5000: episode: 804, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5998.616, mean reward: -5998.616 [-5998.616, -5998.616], mean action: 2.000 [2.000, 2.000],  loss: 19496336.000000, mae: 1329.821411, mean_q: -2.257155
  805/5000: episode: 805, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6669.678, mean reward: -6669.678 [-6669.678, -6669.678], mean action: 2.000 [2.000, 2.000],  loss: 21015712.000000, mae: 1470.327026, mean_q: -2.271384
  806/5000: episode: 806, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -329.581, mean reward: -329.581 [-329.581, -329.581], mean action: 2.000 [2.000, 2.000],  loss: 23593148.000000, mae: 1507.133301, mean_q: -2.287073
  807/5000: episode: 807, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -310.369, mean reward: -310.369 [-310.369, -310.369], mean action: 2.000 [2.000, 2.000],  loss: 17751674.000000, mae: 1311.705322, mean_q: -2.315229
  808/5000: episode: 808, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1887.007, mean reward: -1887.007 [-1887.007, -1887.007], mean action: 3.000 [3.000, 3.000],  loss: 25656380.000000, mae: 1598.881348, mean_q: -2.331584
  809/5000: episode: 809, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1811.257, mean reward: -1811.257 [-1811.257, -1811.257], mean action: 3.000 [3.000, 3.000],  loss: 28464116.000000, mae: 1592.427979, mean_q: -2.335822
  810/5000: episode: 810, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2881.099, mean reward: -2881.099 [-2881.099, -2881.099], mean action: 2.000 [2.000, 2.000],  loss: 25117268.000000, mae: 1551.300537, mean_q: -2.367047
  811/5000: episode: 811, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -331.451, mean reward: -331.451 [-331.451, -331.451], mean action: 2.000 [2.000, 2.000],  loss: 27705684.000000, mae: 1667.404663, mean_q: -2.364609
  812/5000: episode: 812, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -680.056, mean reward: -680.056 [-680.056, -680.056], mean action: 2.000 [2.000, 2.000],  loss: 25766822.000000, mae: 1581.368896, mean_q: -2.384191
  813/5000: episode: 813, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1191.510, mean reward: -1191.510 [-1191.510, -1191.510], mean action: 1.000 [1.000, 1.000],  loss: 25023612.000000, mae: 1538.777832, mean_q: -2.406590
  814/5000: episode: 814, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2978.296, mean reward: -2978.296 [-2978.296, -2978.296], mean action: 2.000 [2.000, 2.000],  loss: 18474738.000000, mae: 1236.892578, mean_q: -2.427833
  815/5000: episode: 815, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -483.525, mean reward: -483.525 [-483.525, -483.525], mean action: 2.000 [2.000, 2.000],  loss: 14851258.000000, mae: 1169.831787, mean_q: -2.444706
  816/5000: episode: 816, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3275.021, mean reward: -3275.021 [-3275.021, -3275.021], mean action: 2.000 [2.000, 2.000],  loss: 28265216.000000, mae: 1602.676025, mean_q: -2.447426
  817/5000: episode: 817, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -398.534, mean reward: -398.534 [-398.534, -398.534], mean action: 2.000 [2.000, 2.000],  loss: 22123950.000000, mae: 1416.129395, mean_q: -2.453550
  818/5000: episode: 818, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1799.801, mean reward: -1799.801 [-1799.801, -1799.801], mean action: 2.000 [2.000, 2.000],  loss: 26067340.000000, mae: 1577.135132, mean_q: -2.470934
  819/5000: episode: 819, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3115.155, mean reward: -3115.155 [-3115.155, -3115.155], mean action: 2.000 [2.000, 2.000],  loss: 21391294.000000, mae: 1376.786377, mean_q: -2.491503
  820/5000: episode: 820, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2756.410, mean reward: -2756.410 [-2756.410, -2756.410], mean action: 2.000 [2.000, 2.000],  loss: 21301130.000000, mae: 1439.643188, mean_q: -2.518678
  821/5000: episode: 821, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9789.952, mean reward: -9789.952 [-9789.952, -9789.952], mean action: 2.000 [2.000, 2.000],  loss: 27335880.000000, mae: 1708.374146, mean_q: -2.505097
  822/5000: episode: 822, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1314.342, mean reward: -1314.342 [-1314.342, -1314.342], mean action: 2.000 [2.000, 2.000],  loss: 16226112.000000, mae: 1196.228760, mean_q: -2.528162
  823/5000: episode: 823, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5061.550, mean reward: -5061.550 [-5061.550, -5061.550], mean action: 2.000 [2.000, 2.000],  loss: 23532870.000000, mae: 1479.954102, mean_q: -2.541190
  824/5000: episode: 824, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1707.303, mean reward: -1707.303 [-1707.303, -1707.303], mean action: 2.000 [2.000, 2.000],  loss: 24078276.000000, mae: 1402.599121, mean_q: -2.558145
  825/5000: episode: 825, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2392.765, mean reward: -2392.765 [-2392.765, -2392.765], mean action: 2.000 [2.000, 2.000],  loss: 23748804.000000, mae: 1451.122314, mean_q: -2.573395
  826/5000: episode: 826, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1647.390, mean reward: -1647.390 [-1647.390, -1647.390], mean action: 2.000 [2.000, 2.000],  loss: 27172856.000000, mae: 1606.378906, mean_q: -2.591792
  827/5000: episode: 827, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -754.506, mean reward: -754.506 [-754.506, -754.506], mean action: 2.000 [2.000, 2.000],  loss: 24134372.000000, mae: 1547.802734, mean_q: -2.597035
  828/5000: episode: 828, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1801.820, mean reward: -1801.820 [-1801.820, -1801.820], mean action: 2.000 [2.000, 2.000],  loss: 22307336.000000, mae: 1344.862549, mean_q: -2.612828
  829/5000: episode: 829, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -178.161, mean reward: -178.161 [-178.161, -178.161], mean action: 2.000 [2.000, 2.000],  loss: 15758394.000000, mae: 1287.240234, mean_q: -2.648091
  830/5000: episode: 830, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1632.355, mean reward: -1632.355 [-1632.355, -1632.355], mean action: 2.000 [2.000, 2.000],  loss: 24455548.000000, mae: 1522.200195, mean_q: -2.631848
  831/5000: episode: 831, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3607.010, mean reward: -3607.010 [-3607.010, -3607.010], mean action: 2.000 [2.000, 2.000],  loss: 22359928.000000, mae: 1446.578369, mean_q: -2.665175
  832/5000: episode: 832, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -282.903, mean reward: -282.903 [-282.903, -282.903], mean action: 2.000 [2.000, 2.000],  loss: 29712458.000000, mae: 1691.715454, mean_q: -2.665927
  833/5000: episode: 833, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -706.104, mean reward: -706.104 [-706.104, -706.104], mean action: 2.000 [2.000, 2.000],  loss: 20309482.000000, mae: 1363.053223, mean_q: -2.700951
  834/5000: episode: 834, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2559.389, mean reward: -2559.389 [-2559.389, -2559.389], mean action: 2.000 [2.000, 2.000],  loss: 21766910.000000, mae: 1417.135986, mean_q: -2.709858
  835/5000: episode: 835, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3288.906, mean reward: -3288.906 [-3288.906, -3288.906], mean action: 2.000 [2.000, 2.000],  loss: 16424974.000000, mae: 1229.234619, mean_q: -2.735645
  836/5000: episode: 836, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2049.994, mean reward: -2049.994 [-2049.994, -2049.994], mean action: 2.000 [2.000, 2.000],  loss: 22704008.000000, mae: 1482.766846, mean_q: -2.745794
  837/5000: episode: 837, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6962.855, mean reward: -6962.855 [-6962.855, -6962.855], mean action: 1.000 [1.000, 1.000],  loss: 21244134.000000, mae: 1487.567139, mean_q: -2.777500
  838/5000: episode: 838, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1483.944, mean reward: -1483.944 [-1483.944, -1483.944], mean action: 2.000 [2.000, 2.000],  loss: 16179502.000000, mae: 1149.106445, mean_q: -2.809797
  839/5000: episode: 839, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1341.776, mean reward: -1341.776 [-1341.776, -1341.776], mean action: 1.000 [1.000, 1.000],  loss: 22088780.000000, mae: 1475.003052, mean_q: -2.822909
  840/5000: episode: 840, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1593.197, mean reward: -1593.197 [-1593.197, -1593.197], mean action: 2.000 [2.000, 2.000],  loss: 27158960.000000, mae: 1556.747314, mean_q: -2.831639
  841/5000: episode: 841, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -105.797, mean reward: -105.797 [-105.797, -105.797], mean action: 2.000 [2.000, 2.000],  loss: 21469350.000000, mae: 1408.849854, mean_q: -2.876227
  842/5000: episode: 842, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3129.559, mean reward: -3129.559 [-3129.559, -3129.559], mean action: 2.000 [2.000, 2.000],  loss: 25365072.000000, mae: 1519.745483, mean_q: -2.888279
  843/5000: episode: 843, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5392.873, mean reward: -5392.873 [-5392.873, -5392.873], mean action: 2.000 [2.000, 2.000],  loss: 21330008.000000, mae: 1445.664551, mean_q: -2.909678
  844/5000: episode: 844, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3014.547, mean reward: -3014.547 [-3014.547, -3014.547], mean action: 2.000 [2.000, 2.000],  loss: 17671396.000000, mae: 1232.910156, mean_q: -2.926965
  845/5000: episode: 845, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6708.283, mean reward: -6708.283 [-6708.283, -6708.283], mean action: 2.000 [2.000, 2.000],  loss: 20958224.000000, mae: 1432.388916, mean_q: -2.938095
  846/5000: episode: 846, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5072.814, mean reward: -5072.814 [-5072.814, -5072.814], mean action: 2.000 [2.000, 2.000],  loss: 25455226.000000, mae: 1586.627930, mean_q: -2.959985
  847/5000: episode: 847, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -379.508, mean reward: -379.508 [-379.508, -379.508], mean action: 2.000 [2.000, 2.000],  loss: 29287040.000000, mae: 1580.650269, mean_q: -2.982322
  848/5000: episode: 848, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2947.191, mean reward: -2947.191 [-2947.191, -2947.191], mean action: 2.000 [2.000, 2.000],  loss: 28726914.000000, mae: 1559.429688, mean_q: -3.018070
  849/5000: episode: 849, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4505.896, mean reward: -4505.896 [-4505.896, -4505.896], mean action: 2.000 [2.000, 2.000],  loss: 26670868.000000, mae: 1611.834961, mean_q: -3.034129
  850/5000: episode: 850, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2119.395, mean reward: -2119.395 [-2119.395, -2119.395], mean action: 2.000 [2.000, 2.000],  loss: 17080038.000000, mae: 1168.292236, mean_q: -3.058316
  851/5000: episode: 851, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5490.276, mean reward: -5490.276 [-5490.276, -5490.276], mean action: 3.000 [3.000, 3.000],  loss: 25746588.000000, mae: 1591.499146, mean_q: -3.073426
  852/5000: episode: 852, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3827.584, mean reward: -3827.584 [-3827.584, -3827.584], mean action: 2.000 [2.000, 2.000],  loss: 19598496.000000, mae: 1400.137939, mean_q: -3.098368
  853/5000: episode: 853, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7631.889, mean reward: -7631.889 [-7631.889, -7631.889], mean action: 0.000 [0.000, 0.000],  loss: 21490672.000000, mae: 1320.431641, mean_q: -3.115188
  854/5000: episode: 854, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1452.702, mean reward: -1452.702 [-1452.702, -1452.702], mean action: 2.000 [2.000, 2.000],  loss: 28776232.000000, mae: 1636.974609, mean_q: -3.127192
  855/5000: episode: 855, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -754.913, mean reward: -754.913 [-754.913, -754.913], mean action: 2.000 [2.000, 2.000],  loss: 18967162.000000, mae: 1278.469116, mean_q: -3.140489
  856/5000: episode: 856, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4767.740, mean reward: -4767.740 [-4767.740, -4767.740], mean action: 2.000 [2.000, 2.000],  loss: 23727320.000000, mae: 1509.561768, mean_q: -3.165957
  857/5000: episode: 857, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2131.503, mean reward: -2131.503 [-2131.503, -2131.503], mean action: 2.000 [2.000, 2.000],  loss: 11231388.000000, mae: 968.869751, mean_q: -3.194487
  858/5000: episode: 858, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1957.703, mean reward: -1957.703 [-1957.703, -1957.703], mean action: 3.000 [3.000, 3.000],  loss: 18102300.000000, mae: 1293.652588, mean_q: -3.195638
  859/5000: episode: 859, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -997.801, mean reward: -997.801 [-997.801, -997.801], mean action: 2.000 [2.000, 2.000],  loss: 16919444.000000, mae: 1265.111572, mean_q: -3.231138
  860/5000: episode: 860, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2425.926, mean reward: -2425.926 [-2425.926, -2425.926], mean action: 3.000 [3.000, 3.000],  loss: 21014892.000000, mae: 1345.423096, mean_q: -3.241614
  861/5000: episode: 861, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -7180.790, mean reward: -7180.790 [-7180.790, -7180.790], mean action: 3.000 [3.000, 3.000],  loss: 25093338.000000, mae: 1581.883423, mean_q: -3.247818
  862/5000: episode: 862, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2529.902, mean reward: -2529.902 [-2529.902, -2529.902], mean action: 3.000 [3.000, 3.000],  loss: 15046444.000000, mae: 1208.841675, mean_q: -3.285005
  863/5000: episode: 863, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -267.754, mean reward: -267.754 [-267.754, -267.754], mean action: 2.000 [2.000, 2.000],  loss: 25000168.000000, mae: 1505.473999, mean_q: -3.309598
  864/5000: episode: 864, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -4823.384, mean reward: -4823.384 [-4823.384, -4823.384], mean action: 3.000 [3.000, 3.000],  loss: 25364514.000000, mae: 1446.236084, mean_q: -3.319674
  865/5000: episode: 865, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -121.301, mean reward: -121.301 [-121.301, -121.301], mean action: 2.000 [2.000, 2.000],  loss: 20501644.000000, mae: 1404.711182, mean_q: -3.340775
  866/5000: episode: 866, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5773.061, mean reward: -5773.061 [-5773.061, -5773.061], mean action: 3.000 [3.000, 3.000],  loss: 23029046.000000, mae: 1412.692383, mean_q: -3.362643
  867/5000: episode: 867, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1708.345, mean reward: -1708.345 [-1708.345, -1708.345], mean action: 2.000 [2.000, 2.000],  loss: 19087704.000000, mae: 1324.995117, mean_q: -3.386727
  868/5000: episode: 868, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1532.134, mean reward: -1532.134 [-1532.134, -1532.134], mean action: 2.000 [2.000, 2.000],  loss: 21662914.000000, mae: 1396.877197, mean_q: -3.389659
  869/5000: episode: 869, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1840.755, mean reward: -1840.755 [-1840.755, -1840.755], mean action: 3.000 [3.000, 3.000],  loss: 22883580.000000, mae: 1446.213135, mean_q: -3.419350
  870/5000: episode: 870, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -666.740, mean reward: -666.740 [-666.740, -666.740], mean action: 3.000 [3.000, 3.000],  loss: 23775556.000000, mae: 1468.976074, mean_q: -3.423116
  871/5000: episode: 871, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -9200.154, mean reward: -9200.154 [-9200.154, -9200.154], mean action: 3.000 [3.000, 3.000],  loss: 28231384.000000, mae: 1634.633545, mean_q: -3.440320
  872/5000: episode: 872, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -936.741, mean reward: -936.741 [-936.741, -936.741], mean action: 3.000 [3.000, 3.000],  loss: 27707206.000000, mae: 1630.649658, mean_q: -3.450580
  873/5000: episode: 873, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -6871.095, mean reward: -6871.095 [-6871.095, -6871.095], mean action: 3.000 [3.000, 3.000],  loss: 16725657.000000, mae: 1211.687378, mean_q: -3.501381
  874/5000: episode: 874, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1388.716, mean reward: -1388.716 [-1388.716, -1388.716], mean action: 3.000 [3.000, 3.000],  loss: 19819134.000000, mae: 1398.927246, mean_q: -3.511850
  875/5000: episode: 875, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2891.762, mean reward: -2891.762 [-2891.762, -2891.762], mean action: 3.000 [3.000, 3.000],  loss: 23250500.000000, mae: 1456.401001, mean_q: -3.518428
  876/5000: episode: 876, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3526.470, mean reward: -3526.470 [-3526.470, -3526.470], mean action: 3.000 [3.000, 3.000],  loss: 22472190.000000, mae: 1414.061768, mean_q: -3.530979
  877/5000: episode: 877, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1506.228, mean reward: -1506.228 [-1506.228, -1506.228], mean action: 3.000 [3.000, 3.000],  loss: 20557770.000000, mae: 1414.714111, mean_q: -3.555734
  878/5000: episode: 878, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4305.772, mean reward: -4305.772 [-4305.772, -4305.772], mean action: 1.000 [1.000, 1.000],  loss: 22289294.000000, mae: 1372.118408, mean_q: -3.573343
  879/5000: episode: 879, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1781.545, mean reward: -1781.545 [-1781.545, -1781.545], mean action: 3.000 [3.000, 3.000],  loss: 20307012.000000, mae: 1368.793213, mean_q: -3.587598
  880/5000: episode: 880, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4251.074, mean reward: -4251.074 [-4251.074, -4251.074], mean action: 3.000 [3.000, 3.000],  loss: 17096456.000000, mae: 1315.537720, mean_q: -3.616245
  881/5000: episode: 881, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -797.086, mean reward: -797.086 [-797.086, -797.086], mean action: 3.000 [3.000, 3.000],  loss: 27923148.000000, mae: 1652.516846, mean_q: -3.616187
  882/5000: episode: 882, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1330.442, mean reward: -1330.442 [-1330.442, -1330.442], mean action: 3.000 [3.000, 3.000],  loss: 18511888.000000, mae: 1294.220825, mean_q: -3.672030
  883/5000: episode: 883, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -990.585, mean reward: -990.585 [-990.585, -990.585], mean action: 3.000 [3.000, 3.000],  loss: 17563216.000000, mae: 1231.955811, mean_q: -3.677387
  884/5000: episode: 884, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6700.383, mean reward: -6700.383 [-6700.383, -6700.383], mean action: 3.000 [3.000, 3.000],  loss: 14439660.000000, mae: 1201.943237, mean_q: -3.701969
  885/5000: episode: 885, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3511.275, mean reward: -3511.275 [-3511.275, -3511.275], mean action: 3.000 [3.000, 3.000],  loss: 27505888.000000, mae: 1608.625244, mean_q: -3.701373
  886/5000: episode: 886, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3240.247, mean reward: -3240.247 [-3240.247, -3240.247], mean action: 3.000 [3.000, 3.000],  loss: 27193504.000000, mae: 1523.866943, mean_q: -3.730821
  887/5000: episode: 887, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1137.567, mean reward: -1137.567 [-1137.567, -1137.567], mean action: 3.000 [3.000, 3.000],  loss: 23571202.000000, mae: 1474.895386, mean_q: -3.760237
  888/5000: episode: 888, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3686.498, mean reward: -3686.498 [-3686.498, -3686.498], mean action: 1.000 [1.000, 1.000],  loss: 27203024.000000, mae: 1514.547607, mean_q: -3.771897
  889/5000: episode: 889, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -10888.756, mean reward: -10888.756 [-10888.756, -10888.756], mean action: 3.000 [3.000, 3.000],  loss: 19530204.000000, mae: 1297.979858, mean_q: -3.817344
  890/5000: episode: 890, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1484.814, mean reward: -1484.814 [-1484.814, -1484.814], mean action: 3.000 [3.000, 3.000],  loss: 26437292.000000, mae: 1616.004761, mean_q: -3.823184
  891/5000: episode: 891, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7883.148, mean reward: -7883.148 [-7883.148, -7883.148], mean action: 3.000 [3.000, 3.000],  loss: 20464768.000000, mae: 1324.935913, mean_q: -3.855031
  892/5000: episode: 892, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1349.201, mean reward: -1349.201 [-1349.201, -1349.201], mean action: 3.000 [3.000, 3.000],  loss: 16819940.000000, mae: 1189.626465, mean_q: -3.882195
  893/5000: episode: 893, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -88.243, mean reward: -88.243 [-88.243, -88.243], mean action: 3.000 [3.000, 3.000],  loss: 25677848.000000, mae: 1482.392334, mean_q: -3.894197
  894/5000: episode: 894, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3634.237, mean reward: -3634.237 [-3634.237, -3634.237], mean action: 3.000 [3.000, 3.000],  loss: 17769388.000000, mae: 1191.039185, mean_q: -3.932194
  895/5000: episode: 895, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5905.040, mean reward: -5905.040 [-5905.040, -5905.040], mean action: 3.000 [3.000, 3.000],  loss: 26983316.000000, mae: 1538.206299, mean_q: -3.930159
  896/5000: episode: 896, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2272.264, mean reward: -2272.264 [-2272.264, -2272.264], mean action: 3.000 [3.000, 3.000],  loss: 22257004.000000, mae: 1381.092529, mean_q: -3.963263
  897/5000: episode: 897, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10599.505, mean reward: -10599.505 [-10599.505, -10599.505], mean action: 1.000 [1.000, 1.000],  loss: 22494684.000000, mae: 1445.652954, mean_q: -3.979527
  898/5000: episode: 898, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4143.576, mean reward: -4143.576 [-4143.576, -4143.576], mean action: 3.000 [3.000, 3.000],  loss: 27856336.000000, mae: 1666.435303, mean_q: -3.997532
  899/5000: episode: 899, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -16.975, mean reward: -16.975 [-16.975, -16.975], mean action: 3.000 [3.000, 3.000],  loss: 19531406.000000, mae: 1240.469727, mean_q: -4.040465
  900/5000: episode: 900, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1950.591, mean reward: -1950.591 [-1950.591, -1950.591], mean action: 1.000 [1.000, 1.000],  loss: 25204152.000000, mae: 1507.819458, mean_q: -4.063075
  901/5000: episode: 901, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4947.046, mean reward: -4947.046 [-4947.046, -4947.046], mean action: 3.000 [3.000, 3.000],  loss: 15512113.000000, mae: 1210.758301, mean_q: -4.091729
  902/5000: episode: 902, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -38.238, mean reward: -38.238 [-38.238, -38.238], mean action: 2.000 [2.000, 2.000],  loss: 25288110.000000, mae: 1543.582031, mean_q: -4.106731
  903/5000: episode: 903, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9030.665, mean reward: -9030.665 [-9030.665, -9030.665], mean action: 1.000 [1.000, 1.000],  loss: 19666348.000000, mae: 1283.006348, mean_q: -4.138138
  904/5000: episode: 904, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -622.208, mean reward: -622.208 [-622.208, -622.208], mean action: 2.000 [2.000, 2.000],  loss: 22864478.000000, mae: 1412.060059, mean_q: -4.171766
  905/5000: episode: 905, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1459.701, mean reward: -1459.701 [-1459.701, -1459.701], mean action: 2.000 [2.000, 2.000],  loss: 24174294.000000, mae: 1499.802734, mean_q: -4.175644
  906/5000: episode: 906, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -776.187, mean reward: -776.187 [-776.187, -776.187], mean action: 2.000 [2.000, 2.000],  loss: 27771320.000000, mae: 1547.808594, mean_q: -4.203488
  907/5000: episode: 907, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -814.267, mean reward: -814.267 [-814.267, -814.267], mean action: 2.000 [2.000, 2.000],  loss: 24424634.000000, mae: 1510.727173, mean_q: -4.233181
  908/5000: episode: 908, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1884.500, mean reward: -1884.500 [-1884.500, -1884.500], mean action: 2.000 [2.000, 2.000],  loss: 12673190.000000, mae: 1055.645264, mean_q: -4.262509
  909/5000: episode: 909, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9215.564, mean reward: -9215.564 [-9215.564, -9215.564], mean action: 2.000 [2.000, 2.000],  loss: 17062702.000000, mae: 1173.625854, mean_q: -4.291080
  910/5000: episode: 910, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -808.653, mean reward: -808.653 [-808.653, -808.653], mean action: 2.000 [2.000, 2.000],  loss: 14717228.000000, mae: 1159.843994, mean_q: -4.321720
  911/5000: episode: 911, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2680.051, mean reward: -2680.051 [-2680.051, -2680.051], mean action: 2.000 [2.000, 2.000],  loss: 18491914.000000, mae: 1372.413818, mean_q: -4.326271
  912/5000: episode: 912, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2941.065, mean reward: -2941.065 [-2941.065, -2941.065], mean action: 2.000 [2.000, 2.000],  loss: 18522128.000000, mae: 1260.901245, mean_q: -4.344208
  913/5000: episode: 913, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1928.974, mean reward: -1928.974 [-1928.974, -1928.974], mean action: 0.000 [0.000, 0.000],  loss: 18094864.000000, mae: 1327.113403, mean_q: -4.388372
  914/5000: episode: 914, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1715.767, mean reward: -1715.767 [-1715.767, -1715.767], mean action: 3.000 [3.000, 3.000],  loss: 17467364.000000, mae: 1191.296387, mean_q: -4.404285
  915/5000: episode: 915, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -4355.145, mean reward: -4355.145 [-4355.145, -4355.145], mean action: 2.000 [2.000, 2.000],  loss: 13322587.000000, mae: 1117.712036, mean_q: -4.431752
  916/5000: episode: 916, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1205.694, mean reward: -1205.694 [-1205.694, -1205.694], mean action: 2.000 [2.000, 2.000],  loss: 23170550.000000, mae: 1340.076294, mean_q: -4.444695
  917/5000: episode: 917, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3466.578, mean reward: -3466.578 [-3466.578, -3466.578], mean action: 1.000 [1.000, 1.000],  loss: 20016024.000000, mae: 1344.366211, mean_q: -4.456254
  918/5000: episode: 918, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -789.142, mean reward: -789.142 [-789.142, -789.142], mean action: 2.000 [2.000, 2.000],  loss: 23460000.000000, mae: 1458.307617, mean_q: -4.497500
  919/5000: episode: 919, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2747.153, mean reward: -2747.153 [-2747.153, -2747.153], mean action: 2.000 [2.000, 2.000],  loss: 20227088.000000, mae: 1322.512329, mean_q: -4.522470
  920/5000: episode: 920, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1950.880, mean reward: -1950.880 [-1950.880, -1950.880], mean action: 2.000 [2.000, 2.000],  loss: 25763016.000000, mae: 1568.414307, mean_q: -4.533226
  921/5000: episode: 921, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2043.833, mean reward: -2043.833 [-2043.833, -2043.833], mean action: 2.000 [2.000, 2.000],  loss: 27625938.000000, mae: 1664.289551, mean_q: -4.547347
  922/5000: episode: 922, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -8574.165, mean reward: -8574.165 [-8574.165, -8574.165], mean action: 0.000 [0.000, 0.000],  loss: 14899146.000000, mae: 1230.673584, mean_q: -4.605347
  923/5000: episode: 923, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2551.301, mean reward: -2551.301 [-2551.301, -2551.301], mean action: 2.000 [2.000, 2.000],  loss: 18168180.000000, mae: 1298.457153, mean_q: -4.621229
  924/5000: episode: 924, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -132.130, mean reward: -132.130 [-132.130, -132.130], mean action: 2.000 [2.000, 2.000],  loss: 20607932.000000, mae: 1401.418091, mean_q: -4.665845
  925/5000: episode: 925, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3232.481, mean reward: -3232.481 [-3232.481, -3232.481], mean action: 1.000 [1.000, 1.000],  loss: 28082196.000000, mae: 1516.153442, mean_q: -4.648303
  926/5000: episode: 926, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3100.615, mean reward: -3100.615 [-3100.615, -3100.615], mean action: 1.000 [1.000, 1.000],  loss: 24003172.000000, mae: 1397.447998, mean_q: -4.691339
  927/5000: episode: 927, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2112.868, mean reward: -2112.868 [-2112.868, -2112.868], mean action: 1.000 [1.000, 1.000],  loss: 18190220.000000, mae: 1306.560547, mean_q: -4.710242
  928/5000: episode: 928, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1840.871, mean reward: -1840.871 [-1840.871, -1840.871], mean action: 2.000 [2.000, 2.000],  loss: 25694124.000000, mae: 1471.416504, mean_q: -4.732842
  929/5000: episode: 929, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1912.330, mean reward: -1912.330 [-1912.330, -1912.330], mean action: 2.000 [2.000, 2.000],  loss: 29631144.000000, mae: 1657.535889, mean_q: -4.756900
  930/5000: episode: 930, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -500.249, mean reward: -500.249 [-500.249, -500.249], mean action: 1.000 [1.000, 1.000],  loss: 20986106.000000, mae: 1375.261597, mean_q: -4.795902
  931/5000: episode: 931, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10026.363, mean reward: -10026.363 [-10026.363, -10026.363], mean action: 1.000 [1.000, 1.000],  loss: 14693511.000000, mae: 1128.241577, mean_q: -4.826303
  932/5000: episode: 932, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5474.716, mean reward: -5474.716 [-5474.716, -5474.716], mean action: 1.000 [1.000, 1.000],  loss: 19838700.000000, mae: 1348.797852, mean_q: -4.841913
  933/5000: episode: 933, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8840.432, mean reward: -8840.432 [-8840.432, -8840.432], mean action: 1.000 [1.000, 1.000],  loss: 22354038.000000, mae: 1432.725830, mean_q: -4.844729
  934/5000: episode: 934, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -614.031, mean reward: -614.031 [-614.031, -614.031], mean action: 1.000 [1.000, 1.000],  loss: 24169056.000000, mae: 1467.775513, mean_q: -4.874036
  935/5000: episode: 935, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3036.773, mean reward: -3036.773 [-3036.773, -3036.773], mean action: 1.000 [1.000, 1.000],  loss: 21184840.000000, mae: 1348.902832, mean_q: -4.910821
  936/5000: episode: 936, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7151.806, mean reward: -7151.806 [-7151.806, -7151.806], mean action: 1.000 [1.000, 1.000],  loss: 24462176.000000, mae: 1509.392578, mean_q: -4.889377
  937/5000: episode: 937, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2456.142, mean reward: -2456.142 [-2456.142, -2456.142], mean action: 1.000 [1.000, 1.000],  loss: 26584516.000000, mae: 1568.094482, mean_q: -4.928859
  938/5000: episode: 938, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4828.213, mean reward: -4828.213 [-4828.213, -4828.213], mean action: 1.000 [1.000, 1.000],  loss: 18290976.000000, mae: 1274.842163, mean_q: -4.968861
  939/5000: episode: 939, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5360.108, mean reward: -5360.108 [-5360.108, -5360.108], mean action: 1.000 [1.000, 1.000],  loss: 21963152.000000, mae: 1389.313477, mean_q: -4.997188
  940/5000: episode: 940, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5985.325, mean reward: -5985.325 [-5985.325, -5985.325], mean action: 1.000 [1.000, 1.000],  loss: 26626794.000000, mae: 1539.739868, mean_q: -4.999217
  941/5000: episode: 941, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -11175.781, mean reward: -11175.781 [-11175.781, -11175.781], mean action: 1.000 [1.000, 1.000],  loss: 24659656.000000, mae: 1574.121460, mean_q: -5.040479
  942/5000: episode: 942, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2207.067, mean reward: -2207.067 [-2207.067, -2207.067], mean action: 1.000 [1.000, 1.000],  loss: 19091320.000000, mae: 1349.317871, mean_q: -5.068061
  943/5000: episode: 943, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -905.499, mean reward: -905.499 [-905.499, -905.499], mean action: 1.000 [1.000, 1.000],  loss: 23811552.000000, mae: 1520.336182, mean_q: -5.087813
  944/5000: episode: 944, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2459.091, mean reward: -2459.091 [-2459.091, -2459.091], mean action: 1.000 [1.000, 1.000],  loss: 23252052.000000, mae: 1478.949951, mean_q: -5.114519
  945/5000: episode: 945, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -8435.871, mean reward: -8435.871 [-8435.871, -8435.871], mean action: 1.000 [1.000, 1.000],  loss: 21531392.000000, mae: 1366.898438, mean_q: -5.142803
  946/5000: episode: 946, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3477.971, mean reward: -3477.971 [-3477.971, -3477.971], mean action: 1.000 [1.000, 1.000],  loss: 27837088.000000, mae: 1538.735962, mean_q: -5.148103
  947/5000: episode: 947, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5611.304, mean reward: -5611.304 [-5611.304, -5611.304], mean action: 1.000 [1.000, 1.000],  loss: 28312432.000000, mae: 1666.616943, mean_q: -5.169098
  948/5000: episode: 948, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6575.549, mean reward: -6575.549 [-6575.549, -6575.549], mean action: 1.000 [1.000, 1.000],  loss: 23612756.000000, mae: 1499.028564, mean_q: -5.190520
  949/5000: episode: 949, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6118.427, mean reward: -6118.427 [-6118.427, -6118.427], mean action: 1.000 [1.000, 1.000],  loss: 26527490.000000, mae: 1615.293945, mean_q: -5.221057
  950/5000: episode: 950, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -3792.083, mean reward: -3792.083 [-3792.083, -3792.083], mean action: 1.000 [1.000, 1.000],  loss: 20341536.000000, mae: 1421.313232, mean_q: -5.267245
  951/5000: episode: 951, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3161.188, mean reward: -3161.188 [-3161.188, -3161.188], mean action: 1.000 [1.000, 1.000],  loss: 21956548.000000, mae: 1389.019653, mean_q: -5.304605
  952/5000: episode: 952, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6867.227, mean reward: -6867.227 [-6867.227, -6867.227], mean action: 1.000 [1.000, 1.000],  loss: 17573978.000000, mae: 1269.320557, mean_q: -5.310561
  953/5000: episode: 953, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -278.763, mean reward: -278.763 [-278.763, -278.763], mean action: 1.000 [1.000, 1.000],  loss: 17645460.000000, mae: 1227.055054, mean_q: -5.345528
  954/5000: episode: 954, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -453.666, mean reward: -453.666 [-453.666, -453.666], mean action: 1.000 [1.000, 1.000],  loss: 13695486.000000, mae: 989.666321, mean_q: -5.376716
  955/5000: episode: 955, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9674.748, mean reward: -9674.748 [-9674.748, -9674.748], mean action: 1.000 [1.000, 1.000],  loss: 25141096.000000, mae: 1512.371216, mean_q: -5.380166
  956/5000: episode: 956, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6146.299, mean reward: -6146.299 [-6146.299, -6146.299], mean action: 1.000 [1.000, 1.000],  loss: 14641382.000000, mae: 1066.296631, mean_q: -5.431464
  957/5000: episode: 957, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9834.158, mean reward: -9834.158 [-9834.158, -9834.158], mean action: 1.000 [1.000, 1.000],  loss: 18195332.000000, mae: 1312.294678, mean_q: -5.420291
  958/5000: episode: 958, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6725.243, mean reward: -6725.243 [-6725.243, -6725.243], mean action: 1.000 [1.000, 1.000],  loss: 18764456.000000, mae: 1233.842285, mean_q: -5.450507
  959/5000: episode: 959, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6809.398, mean reward: -6809.398 [-6809.398, -6809.398], mean action: 1.000 [1.000, 1.000],  loss: 15022999.000000, mae: 1164.202393, mean_q: -5.509762
  960/5000: episode: 960, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1697.290, mean reward: -1697.290 [-1697.290, -1697.290], mean action: 1.000 [1.000, 1.000],  loss: 27801842.000000, mae: 1555.150146, mean_q: -5.497268
  961/5000: episode: 961, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4959.114, mean reward: -4959.114 [-4959.114, -4959.114], mean action: 1.000 [1.000, 1.000],  loss: 15957928.000000, mae: 1192.598633, mean_q: -5.540862
  962/5000: episode: 962, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3031.257, mean reward: -3031.257 [-3031.257, -3031.257], mean action: 1.000 [1.000, 1.000],  loss: 19909722.000000, mae: 1422.322510, mean_q: -5.534511
  963/5000: episode: 963, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1375.319, mean reward: -1375.319 [-1375.319, -1375.319], mean action: 1.000 [1.000, 1.000],  loss: 17077506.000000, mae: 1262.077637, mean_q: -5.608047
  964/5000: episode: 964, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7090.861, mean reward: -7090.861 [-7090.861, -7090.861], mean action: 1.000 [1.000, 1.000],  loss: 19524102.000000, mae: 1345.776855, mean_q: -5.624843
  965/5000: episode: 965, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4770.865, mean reward: -4770.865 [-4770.865, -4770.865], mean action: 3.000 [3.000, 3.000],  loss: 28087984.000000, mae: 1579.305298, mean_q: -5.640854
  966/5000: episode: 966, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9082.160, mean reward: -9082.160 [-9082.160, -9082.160], mean action: 1.000 [1.000, 1.000],  loss: 19066974.000000, mae: 1315.658447, mean_q: -5.680526
  967/5000: episode: 967, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -2062.089, mean reward: -2062.089 [-2062.089, -2062.089], mean action: 1.000 [1.000, 1.000],  loss: 16251347.000000, mae: 1166.397583, mean_q: -5.703799
  968/5000: episode: 968, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10438.194, mean reward: -10438.194 [-10438.194, -10438.194], mean action: 1.000 [1.000, 1.000],  loss: 17685880.000000, mae: 1153.892456, mean_q: -5.735787
  969/5000: episode: 969, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9598.831, mean reward: -9598.831 [-9598.831, -9598.831], mean action: 1.000 [1.000, 1.000],  loss: 18739530.000000, mae: 1312.404053, mean_q: -5.761060
  970/5000: episode: 970, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7074.330, mean reward: -7074.330 [-7074.330, -7074.330], mean action: 1.000 [1.000, 1.000],  loss: 21801840.000000, mae: 1387.962646, mean_q: -5.793815
  971/5000: episode: 971, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -911.350, mean reward: -911.350 [-911.350, -911.350], mean action: 2.000 [2.000, 2.000],  loss: 21296252.000000, mae: 1408.697388, mean_q: -5.802047
  972/5000: episode: 972, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6875.007, mean reward: -6875.007 [-6875.007, -6875.007], mean action: 1.000 [1.000, 1.000],  loss: 30020736.000000, mae: 1659.989380, mean_q: -5.806454
  973/5000: episode: 973, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2942.783, mean reward: -2942.783 [-2942.783, -2942.783], mean action: 1.000 [1.000, 1.000],  loss: 22509324.000000, mae: 1297.742188, mean_q: -5.854033
  974/5000: episode: 974, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3465.948, mean reward: -3465.948 [-3465.948, -3465.948], mean action: 1.000 [1.000, 1.000],  loss: 27084882.000000, mae: 1591.493896, mean_q: -5.849203
  975/5000: episode: 975, duration: 0.065s, episode steps:   1, steps per second:  16, episode reward: -1311.179, mean reward: -1311.179 [-1311.179, -1311.179], mean action: 1.000 [1.000, 1.000],  loss: 21673364.000000, mae: 1314.624756, mean_q: -5.884747
  976/5000: episode: 976, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -924.198, mean reward: -924.198 [-924.198, -924.198], mean action: 1.000 [1.000, 1.000],  loss: 22940596.000000, mae: 1407.114990, mean_q: -5.921618
  977/5000: episode: 977, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -934.354, mean reward: -934.354 [-934.354, -934.354], mean action: 1.000 [1.000, 1.000],  loss: 20382040.000000, mae: 1415.787354, mean_q: -5.951193
  978/5000: episode: 978, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1754.557, mean reward: -1754.557 [-1754.557, -1754.557], mean action: 1.000 [1.000, 1.000],  loss: 21860376.000000, mae: 1337.135010, mean_q: -5.989475
  979/5000: episode: 979, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3667.866, mean reward: -3667.866 [-3667.866, -3667.866], mean action: 1.000 [1.000, 1.000],  loss: 23231312.000000, mae: 1484.824707, mean_q: -6.006278
  980/5000: episode: 980, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7166.309, mean reward: -7166.309 [-7166.309, -7166.309], mean action: 1.000 [1.000, 1.000],  loss: 29319964.000000, mae: 1595.463623, mean_q: -6.020000
  981/5000: episode: 981, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4481.472, mean reward: -4481.472 [-4481.472, -4481.472], mean action: 1.000 [1.000, 1.000],  loss: 21301328.000000, mae: 1392.182617, mean_q: -6.051408
  982/5000: episode: 982, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3174.151, mean reward: -3174.151 [-3174.151, -3174.151], mean action: 1.000 [1.000, 1.000],  loss: 19992144.000000, mae: 1320.726929, mean_q: -6.099661
  983/5000: episode: 983, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2538.765, mean reward: -2538.765 [-2538.765, -2538.765], mean action: 1.000 [1.000, 1.000],  loss: 30945036.000000, mae: 1662.132935, mean_q: -6.104537
  984/5000: episode: 984, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -392.918, mean reward: -392.918 [-392.918, -392.918], mean action: 1.000 [1.000, 1.000],  loss: 23562392.000000, mae: 1434.610596, mean_q: -6.129757
  985/5000: episode: 985, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9002.781, mean reward: -9002.781 [-9002.781, -9002.781], mean action: 1.000 [1.000, 1.000],  loss: 19863744.000000, mae: 1331.307861, mean_q: -6.183646
  986/5000: episode: 986, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3513.233, mean reward: -3513.233 [-3513.233, -3513.233], mean action: 1.000 [1.000, 1.000],  loss: 16069136.000000, mae: 1247.719482, mean_q: -6.201836
  987/5000: episode: 987, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6786.091, mean reward: -6786.091 [-6786.091, -6786.091], mean action: 0.000 [0.000, 0.000],  loss: 20659724.000000, mae: 1343.790039, mean_q: -6.226291
  988/5000: episode: 988, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4030.502, mean reward: -4030.502 [-4030.502, -4030.502], mean action: 1.000 [1.000, 1.000],  loss: 21070198.000000, mae: 1367.601807, mean_q: -6.250796
  989/5000: episode: 989, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5010.701, mean reward: -5010.701 [-5010.701, -5010.701], mean action: 1.000 [1.000, 1.000],  loss: 17254768.000000, mae: 1235.787109, mean_q: -6.289721
  990/5000: episode: 990, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2395.283, mean reward: -2395.283 [-2395.283, -2395.283], mean action: 1.000 [1.000, 1.000],  loss: 15945891.000000, mae: 1224.429199, mean_q: -6.320740
  991/5000: episode: 991, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1658.501, mean reward: -1658.501 [-1658.501, -1658.501], mean action: 1.000 [1.000, 1.000],  loss: 14620811.000000, mae: 1161.591553, mean_q: -6.339800
  992/5000: episode: 992, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -802.979, mean reward: -802.979 [-802.979, -802.979], mean action: 1.000 [1.000, 1.000],  loss: 20483088.000000, mae: 1278.373291, mean_q: -6.349630
  993/5000: episode: 993, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2127.144, mean reward: -2127.144 [-2127.144, -2127.144], mean action: 1.000 [1.000, 1.000],  loss: 22818116.000000, mae: 1531.948486, mean_q: -6.369595
  994/5000: episode: 994, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.394, mean reward: -5000.394 [-5000.394, -5000.394], mean action: 1.000 [1.000, 1.000],  loss: 30589734.000000, mae: 1698.592773, mean_q: -6.381210
  995/5000: episode: 995, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -9236.421, mean reward: -9236.421 [-9236.421, -9236.421], mean action: 1.000 [1.000, 1.000],  loss: 21926384.000000, mae: 1343.541260, mean_q: -6.428411
  996/5000: episode: 996, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2639.540, mean reward: -2639.540 [-2639.540, -2639.540], mean action: 1.000 [1.000, 1.000],  loss: 21354410.000000, mae: 1369.861450, mean_q: -6.452748
  997/5000: episode: 997, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5979.198, mean reward: -5979.198 [-5979.198, -5979.198], mean action: 1.000 [1.000, 1.000],  loss: 25522770.000000, mae: 1502.272705, mean_q: -6.476056
  998/5000: episode: 998, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1119.091, mean reward: -1119.091 [-1119.091, -1119.091], mean action: 2.000 [2.000, 2.000],  loss: 19828352.000000, mae: 1256.390015, mean_q: -6.547507
  999/5000: episode: 999, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4928.217, mean reward: -4928.217 [-4928.217, -4928.217], mean action: 1.000 [1.000, 1.000],  loss: 22821352.000000, mae: 1445.595825, mean_q: -6.533059
 1000/5000: episode: 1000, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -7630.974, mean reward: -7630.974 [-7630.974, -7630.974], mean action: 1.000 [1.000, 1.000],  loss: 12425449.000000, mae: 1088.702637, mean_q: -6.580485
 1001/5000: episode: 1001, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5454.810, mean reward: -5454.810 [-5454.810, -5454.810], mean action: 1.000 [1.000, 1.000],  loss: 28555004.000000, mae: 1596.697998, mean_q: -6.590359
 1002/5000: episode: 1002, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1384.385, mean reward: -1384.385 [-1384.385, -1384.385], mean action: 1.000 [1.000, 1.000],  loss: 15869186.000000, mae: 1163.023560, mean_q: -6.637377
 1003/5000: episode: 1003, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2445.684, mean reward: -2445.684 [-2445.684, -2445.684], mean action: 1.000 [1.000, 1.000],  loss: 17848756.000000, mae: 1239.317871, mean_q: -6.669474
 1004/5000: episode: 1004, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4866.300, mean reward: -4866.300 [-4866.300, -4866.300], mean action: 1.000 [1.000, 1.000],  loss: 13772296.000000, mae: 1065.844116, mean_q: -6.709019
 1005/5000: episode: 1005, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1322.761, mean reward: -1322.761 [-1322.761, -1322.761], mean action: 1.000 [1.000, 1.000],  loss: 17245932.000000, mae: 1261.092773, mean_q: -6.723742
 1006/5000: episode: 1006, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1202.471, mean reward: -1202.471 [-1202.471, -1202.471], mean action: 1.000 [1.000, 1.000],  loss: 18742872.000000, mae: 1318.327148, mean_q: -6.735416
 1007/5000: episode: 1007, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4184.127, mean reward: -4184.127 [-4184.127, -4184.127], mean action: 1.000 [1.000, 1.000],  loss: 20843496.000000, mae: 1373.512085, mean_q: -6.748605
 1008/5000: episode: 1008, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1810.557, mean reward: -1810.557 [-1810.557, -1810.557], mean action: 1.000 [1.000, 1.000],  loss: 15601778.000000, mae: 1159.979004, mean_q: -6.814425
 1009/5000: episode: 1009, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7928.737, mean reward: -7928.737 [-7928.737, -7928.737], mean action: 1.000 [1.000, 1.000],  loss: 24869034.000000, mae: 1488.414917, mean_q: -6.830663
 1010/5000: episode: 1010, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2885.170, mean reward: -2885.170 [-2885.170, -2885.170], mean action: 1.000 [1.000, 1.000],  loss: 19353954.000000, mae: 1318.921021, mean_q: -6.863754
 1011/5000: episode: 1011, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1085.630, mean reward: -1085.630 [-1085.630, -1085.630], mean action: 2.000 [2.000, 2.000],  loss: 15410646.000000, mae: 1251.783569, mean_q: -6.910996
 1012/5000: episode: 1012, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6717.870, mean reward: -6717.870 [-6717.870, -6717.870], mean action: 1.000 [1.000, 1.000],  loss: 26974346.000000, mae: 1597.822510, mean_q: -6.892883
 1013/5000: episode: 1013, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5332.020, mean reward: -5332.020 [-5332.020, -5332.020], mean action: 1.000 [1.000, 1.000],  loss: 21395842.000000, mae: 1315.415771, mean_q: -6.920179
 1014/5000: episode: 1014, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2935.206, mean reward: -2935.206 [-2935.206, -2935.206], mean action: 1.000 [1.000, 1.000],  loss: 27627294.000000, mae: 1516.164307, mean_q: -6.948935
 1015/5000: episode: 1015, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -602.130, mean reward: -602.130 [-602.130, -602.130], mean action: 2.000 [2.000, 2.000],  loss: 18102874.000000, mae: 1290.068848, mean_q: -7.007401
 1016/5000: episode: 1016, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6176.554, mean reward: -6176.554 [-6176.554, -6176.554], mean action: 1.000 [1.000, 1.000],  loss: 20783002.000000, mae: 1340.725464, mean_q: -7.027084
 1017/5000: episode: 1017, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1394.664, mean reward: -1394.664 [-1394.664, -1394.664], mean action: 2.000 [2.000, 2.000],  loss: 23281360.000000, mae: 1484.403809, mean_q: -7.011772
 1018/5000: episode: 1018, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2554.519, mean reward: -2554.519 [-2554.519, -2554.519], mean action: 1.000 [1.000, 1.000],  loss: 17574030.000000, mae: 1269.931885, mean_q: -7.111826
 1019/5000: episode: 1019, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3939.109, mean reward: -3939.109 [-3939.109, -3939.109], mean action: 1.000 [1.000, 1.000],  loss: 14848098.000000, mae: 1119.382080, mean_q: -7.134351
 1020/5000: episode: 1020, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -202.444, mean reward: -202.444 [-202.444, -202.444], mean action: 2.000 [2.000, 2.000],  loss: 17794280.000000, mae: 1208.748291, mean_q: -7.124832
 1021/5000: episode: 1021, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1220.906, mean reward: -1220.906 [-1220.906, -1220.906], mean action: 2.000 [2.000, 2.000],  loss: 24098378.000000, mae: 1459.778809, mean_q: -7.176776
 1022/5000: episode: 1022, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -6088.033, mean reward: -6088.033 [-6088.033, -6088.033], mean action: 2.000 [2.000, 2.000],  loss: 23348912.000000, mae: 1479.168091, mean_q: -7.197474
 1023/5000: episode: 1023, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1574.465, mean reward: -1574.465 [-1574.465, -1574.465], mean action: 2.000 [2.000, 2.000],  loss: 24558278.000000, mae: 1560.239990, mean_q: -7.194836
 1024/5000: episode: 1024, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3403.085, mean reward: -3403.085 [-3403.085, -3403.085], mean action: 2.000 [2.000, 2.000],  loss: 21694038.000000, mae: 1395.725220, mean_q: -7.231845
 1025/5000: episode: 1025, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4845.528, mean reward: -4845.528 [-4845.528, -4845.528], mean action: 2.000 [2.000, 2.000],  loss: 18896330.000000, mae: 1357.003540, mean_q: -7.288178
 1026/5000: episode: 1026, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3599.559, mean reward: -3599.559 [-3599.559, -3599.559], mean action: 2.000 [2.000, 2.000],  loss: 18353340.000000, mae: 1314.760986, mean_q: -7.310891
 1027/5000: episode: 1027, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1130.462, mean reward: -1130.462 [-1130.462, -1130.462], mean action: 2.000 [2.000, 2.000],  loss: 19916622.000000, mae: 1384.979492, mean_q: -7.345296
 1028/5000: episode: 1028, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2665.584, mean reward: -2665.584 [-2665.584, -2665.584], mean action: 2.000 [2.000, 2.000],  loss: 14558099.000000, mae: 1180.289551, mean_q: -7.364147
 1029/5000: episode: 1029, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -274.340, mean reward: -274.340 [-274.340, -274.340], mean action: 2.000 [2.000, 2.000],  loss: 20534438.000000, mae: 1320.552734, mean_q: -7.388344
 1030/5000: episode: 1030, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -652.948, mean reward: -652.948 [-652.948, -652.948], mean action: 2.000 [2.000, 2.000],  loss: 18712084.000000, mae: 1270.616333, mean_q: -7.425077
 1031/5000: episode: 1031, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9100.616, mean reward: -9100.616 [-9100.616, -9100.616], mean action: 1.000 [1.000, 1.000],  loss: 11362506.000000, mae: 1081.982178, mean_q: -7.443007
 1032/5000: episode: 1032, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1314.086, mean reward: -1314.086 [-1314.086, -1314.086], mean action: 2.000 [2.000, 2.000],  loss: 21425034.000000, mae: 1412.757812, mean_q: -7.449768
 1033/5000: episode: 1033, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4607.699, mean reward: -4607.699 [-4607.699, -4607.699], mean action: 2.000 [2.000, 2.000],  loss: 20226178.000000, mae: 1405.977051, mean_q: -7.529258
 1034/5000: episode: 1034, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -247.819, mean reward: -247.819 [-247.819, -247.819], mean action: 2.000 [2.000, 2.000],  loss: 23024880.000000, mae: 1492.906982, mean_q: -7.559146
 1035/5000: episode: 1035, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2999.495, mean reward: -2999.495 [-2999.495, -2999.495], mean action: 2.000 [2.000, 2.000],  loss: 23597684.000000, mae: 1521.369141, mean_q: -7.593982
 1036/5000: episode: 1036, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1971.527, mean reward: -1971.527 [-1971.527, -1971.527], mean action: 2.000 [2.000, 2.000],  loss: 22660924.000000, mae: 1436.115967, mean_q: -7.616816
 1037/5000: episode: 1037, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -218.367, mean reward: -218.367 [-218.367, -218.367], mean action: 2.000 [2.000, 2.000],  loss: 15993395.000000, mae: 1225.686646, mean_q: -7.646149
 1038/5000: episode: 1038, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3479.929, mean reward: -3479.929 [-3479.929, -3479.929], mean action: 2.000 [2.000, 2.000],  loss: 37390620.000000, mae: 1908.608398, mean_q: -7.615610
 1039/5000: episode: 1039, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1923.802, mean reward: -1923.802 [-1923.802, -1923.802], mean action: 2.000 [2.000, 2.000],  loss: 24495772.000000, mae: 1490.054077, mean_q: -7.687996
 1040/5000: episode: 1040, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -6714.909, mean reward: -6714.909 [-6714.909, -6714.909], mean action: 2.000 [2.000, 2.000],  loss: 26996220.000000, mae: 1597.584229, mean_q: -7.734688
 1041/5000: episode: 1041, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2730.134, mean reward: -2730.134 [-2730.134, -2730.134], mean action: 2.000 [2.000, 2.000],  loss: 12847104.000000, mae: 1042.384399, mean_q: -7.782099
 1042/5000: episode: 1042, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -375.515, mean reward: -375.515 [-375.515, -375.515], mean action: 2.000 [2.000, 2.000],  loss: 16146957.000000, mae: 1242.014404, mean_q: -7.799352
 1043/5000: episode: 1043, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1195.572, mean reward: -1195.572 [-1195.572, -1195.572], mean action: 2.000 [2.000, 2.000],  loss: 22609298.000000, mae: 1427.479248, mean_q: -7.814491
 1044/5000: episode: 1044, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8837.580, mean reward: -8837.580 [-8837.580, -8837.580], mean action: 2.000 [2.000, 2.000],  loss: 27246956.000000, mae: 1591.294922, mean_q: -7.827861
 1045/5000: episode: 1045, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -58.275, mean reward: -58.275 [-58.275, -58.275], mean action: 2.000 [2.000, 2.000],  loss: 11916362.000000, mae: 1050.482788, mean_q: -7.892360
 1046/5000: episode: 1046, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1060.511, mean reward: -1060.511 [-1060.511, -1060.511], mean action: 2.000 [2.000, 2.000],  loss: 22758056.000000, mae: 1410.876465, mean_q: -7.904006
 1047/5000: episode: 1047, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2026.266, mean reward: -2026.266 [-2026.266, -2026.266], mean action: 2.000 [2.000, 2.000],  loss: 22335424.000000, mae: 1391.584595, mean_q: -7.957883
 1048/5000: episode: 1048, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -363.945, mean reward: -363.945 [-363.945, -363.945], mean action: 2.000 [2.000, 2.000],  loss: 16138061.000000, mae: 1220.307495, mean_q: -7.973303
 1049/5000: episode: 1049, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -1103.560, mean reward: -1103.560 [-1103.560, -1103.560], mean action: 2.000 [2.000, 2.000],  loss: 27044762.000000, mae: 1524.038818, mean_q: -7.964148
 1050/5000: episode: 1050, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1837.387, mean reward: -1837.387 [-1837.387, -1837.387], mean action: 2.000 [2.000, 2.000],  loss: 17074504.000000, mae: 1253.341797, mean_q: -8.052223
 1051/5000: episode: 1051, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9233.842, mean reward: -9233.842 [-9233.842, -9233.842], mean action: 2.000 [2.000, 2.000],  loss: 21180656.000000, mae: 1341.469727, mean_q: -8.051879
 1052/5000: episode: 1052, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -6828.348, mean reward: -6828.348 [-6828.348, -6828.348], mean action: 2.000 [2.000, 2.000],  loss: 15859770.000000, mae: 1231.926758, mean_q: -8.084810
 1053/5000: episode: 1053, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6068.474, mean reward: -6068.474 [-6068.474, -6068.474], mean action: 2.000 [2.000, 2.000],  loss: 24336660.000000, mae: 1482.182617, mean_q: -8.129183
 1054/5000: episode: 1054, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4063.322, mean reward: -4063.322 [-4063.322, -4063.322], mean action: 2.000 [2.000, 2.000],  loss: 17024708.000000, mae: 1195.698242, mean_q: -8.146155
 1055/5000: episode: 1055, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -712.383, mean reward: -712.383 [-712.383, -712.383], mean action: 2.000 [2.000, 2.000],  loss: 16841642.000000, mae: 1245.955811, mean_q: -8.219748
 1056/5000: episode: 1056, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5501.969, mean reward: -5501.969 [-5501.969, -5501.969], mean action: 2.000 [2.000, 2.000],  loss: 20127308.000000, mae: 1256.496338, mean_q: -8.235275
 1057/5000: episode: 1057, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1192.476, mean reward: -1192.476 [-1192.476, -1192.476], mean action: 2.000 [2.000, 2.000],  loss: 16922806.000000, mae: 1229.400757, mean_q: -8.257381
 1058/5000: episode: 1058, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3264.979, mean reward: -3264.979 [-3264.979, -3264.979], mean action: 2.000 [2.000, 2.000],  loss: 15040499.000000, mae: 1199.495117, mean_q: -8.301332
 1059/5000: episode: 1059, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1097.221, mean reward: -1097.221 [-1097.221, -1097.221], mean action: 3.000 [3.000, 3.000],  loss: 20466724.000000, mae: 1344.585571, mean_q: -8.329849
 1060/5000: episode: 1060, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1284.596, mean reward: -1284.596 [-1284.596, -1284.596], mean action: 2.000 [2.000, 2.000],  loss: 16881512.000000, mae: 1258.720459, mean_q: -8.334013
 1061/5000: episode: 1061, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4497.035, mean reward: -4497.035 [-4497.035, -4497.035], mean action: 2.000 [2.000, 2.000],  loss: 19311072.000000, mae: 1319.500122, mean_q: -8.376373
 1062/5000: episode: 1062, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -11.525, mean reward: -11.525 [-11.525, -11.525], mean action: 2.000 [2.000, 2.000],  loss: 18678946.000000, mae: 1218.966431, mean_q: -8.440418
 1063/5000: episode: 1063, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3056.684, mean reward: -3056.684 [-3056.684, -3056.684], mean action: 2.000 [2.000, 2.000],  loss: 19064450.000000, mae: 1315.453003, mean_q: -8.452676
 1064/5000: episode: 1064, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4770.391, mean reward: -4770.391 [-4770.391, -4770.391], mean action: 2.000 [2.000, 2.000],  loss: 16912434.000000, mae: 1293.847900, mean_q: -8.481391
 1065/5000: episode: 1065, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4982.264, mean reward: -4982.264 [-4982.264, -4982.264], mean action: 2.000 [2.000, 2.000],  loss: 22467696.000000, mae: 1272.349487, mean_q: -8.529357
 1066/5000: episode: 1066, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2245.614, mean reward: -2245.614 [-2245.614, -2245.614], mean action: 2.000 [2.000, 2.000],  loss: 18994442.000000, mae: 1293.073242, mean_q: -8.541946
 1067/5000: episode: 1067, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3171.169, mean reward: -3171.169 [-3171.169, -3171.169], mean action: 2.000 [2.000, 2.000],  loss: 24386844.000000, mae: 1476.738770, mean_q: -8.581038
 1068/5000: episode: 1068, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -39.217, mean reward: -39.217 [-39.217, -39.217], mean action: 2.000 [2.000, 2.000],  loss: 17059490.000000, mae: 1197.512085, mean_q: -8.624821
 1069/5000: episode: 1069, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1061.788, mean reward: -1061.788 [-1061.788, -1061.788], mean action: 2.000 [2.000, 2.000],  loss: 13316400.000000, mae: 1050.024414, mean_q: -8.715020
 1070/5000: episode: 1070, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3054.079, mean reward: -3054.079 [-3054.079, -3054.079], mean action: 2.000 [2.000, 2.000],  loss: 28585552.000000, mae: 1625.708984, mean_q: -8.671813
 1071/5000: episode: 1071, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3251.852, mean reward: -3251.852 [-3251.852, -3251.852], mean action: 2.000 [2.000, 2.000],  loss: 18523628.000000, mae: 1233.885986, mean_q: -8.733783
 1072/5000: episode: 1072, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -609.699, mean reward: -609.699 [-609.699, -609.699], mean action: 2.000 [2.000, 2.000],  loss: 21660664.000000, mae: 1444.377075, mean_q: -8.784611
 1073/5000: episode: 1073, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3844.178, mean reward: -3844.178 [-3844.178, -3844.178], mean action: 2.000 [2.000, 2.000],  loss: 15224489.000000, mae: 1110.406006, mean_q: -8.862406
 1074/5000: episode: 1074, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4398.075, mean reward: -4398.075 [-4398.075, -4398.075], mean action: 2.000 [2.000, 2.000],  loss: 20626318.000000, mae: 1292.993164, mean_q: -8.891361
 1075/5000: episode: 1075, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2020.337, mean reward: -2020.337 [-2020.337, -2020.337], mean action: 2.000 [2.000, 2.000],  loss: 24278066.000000, mae: 1521.752686, mean_q: -8.918770
 1076/5000: episode: 1076, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1743.741, mean reward: -1743.741 [-1743.741, -1743.741], mean action: 2.000 [2.000, 2.000],  loss: 15012848.000000, mae: 1192.928711, mean_q: -8.971575
 1077/5000: episode: 1077, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1557.962, mean reward: -1557.962 [-1557.962, -1557.962], mean action: 2.000 [2.000, 2.000],  loss: 17970094.000000, mae: 1296.149780, mean_q: -8.996906
 1078/5000: episode: 1078, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6288.723, mean reward: -6288.723 [-6288.723, -6288.723], mean action: 1.000 [1.000, 1.000],  loss: 28590626.000000, mae: 1581.272949, mean_q: -9.020046
 1079/5000: episode: 1079, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1622.176, mean reward: -1622.176 [-1622.176, -1622.176], mean action: 1.000 [1.000, 1.000],  loss: 19609658.000000, mae: 1264.228027, mean_q: -9.131706
 1080/5000: episode: 1080, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -10978.798, mean reward: -10978.798 [-10978.798, -10978.798], mean action: 1.000 [1.000, 1.000],  loss: 23634748.000000, mae: 1408.468262, mean_q: -9.153267
 1081/5000: episode: 1081, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2930.318, mean reward: -2930.318 [-2930.318, -2930.318], mean action: 1.000 [1.000, 1.000],  loss: 23054456.000000, mae: 1382.884521, mean_q: -9.187319
 1082/5000: episode: 1082, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1146.146, mean reward: -1146.146 [-1146.146, -1146.146], mean action: 1.000 [1.000, 1.000],  loss: 18309544.000000, mae: 1268.898071, mean_q: -9.256227
 1083/5000: episode: 1083, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7793.380, mean reward: -7793.380 [-7793.380, -7793.380], mean action: 1.000 [1.000, 1.000],  loss: 21852734.000000, mae: 1360.273438, mean_q: -9.248322
 1084/5000: episode: 1084, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6120.193, mean reward: -6120.193 [-6120.193, -6120.193], mean action: 1.000 [1.000, 1.000],  loss: 20838228.000000, mae: 1403.035522, mean_q: -9.310273
 1085/5000: episode: 1085, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6175.524, mean reward: -6175.524 [-6175.524, -6175.524], mean action: 1.000 [1.000, 1.000],  loss: 21333912.000000, mae: 1346.347412, mean_q: -9.334860
 1086/5000: episode: 1086, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -6565.946, mean reward: -6565.946 [-6565.946, -6565.946], mean action: 1.000 [1.000, 1.000],  loss: 17582374.000000, mae: 1234.109131, mean_q: -9.372878
 1087/5000: episode: 1087, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -702.501, mean reward: -702.501 [-702.501, -702.501], mean action: 1.000 [1.000, 1.000],  loss: 21231920.000000, mae: 1350.935303, mean_q: -9.424236
 1088/5000: episode: 1088, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8770.522, mean reward: -8770.522 [-8770.522, -8770.522], mean action: 1.000 [1.000, 1.000],  loss: 14378965.000000, mae: 1125.811279, mean_q: -9.476933
 1089/5000: episode: 1089, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6808.183, mean reward: -6808.183 [-6808.183, -6808.183], mean action: 1.000 [1.000, 1.000],  loss: 19646668.000000, mae: 1256.572021, mean_q: -9.525331
 1090/5000: episode: 1090, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5130.476, mean reward: -5130.476 [-5130.476, -5130.476], mean action: 1.000 [1.000, 1.000],  loss: 17530014.000000, mae: 1259.235840, mean_q: -9.597537
 1091/5000: episode: 1091, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -128.665, mean reward: -128.665 [-128.665, -128.665], mean action: 3.000 [3.000, 3.000],  loss: 19805366.000000, mae: 1290.164551, mean_q: -9.612928
 1092/5000: episode: 1092, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -391.123, mean reward: -391.123 [-391.123, -391.123], mean action: 1.000 [1.000, 1.000],  loss: 22945324.000000, mae: 1483.110352, mean_q: -9.612449
 1093/5000: episode: 1093, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7501.254, mean reward: -7501.254 [-7501.254, -7501.254], mean action: 1.000 [1.000, 1.000],  loss: 22824612.000000, mae: 1495.072754, mean_q: -9.664568
 1094/5000: episode: 1094, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5111.623, mean reward: -5111.623 [-5111.623, -5111.623], mean action: 1.000 [1.000, 1.000],  loss: 20262380.000000, mae: 1287.944092, mean_q: -9.708038
 1095/5000: episode: 1095, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2475.566, mean reward: -2475.566 [-2475.566, -2475.566], mean action: 1.000 [1.000, 1.000],  loss: 17682158.000000, mae: 1209.783203, mean_q: -9.794771
 1096/5000: episode: 1096, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3116.342, mean reward: -3116.342 [-3116.342, -3116.342], mean action: 1.000 [1.000, 1.000],  loss: 26094998.000000, mae: 1591.069580, mean_q: -9.750718
 1097/5000: episode: 1097, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2264.751, mean reward: -2264.751 [-2264.751, -2264.751], mean action: 1.000 [1.000, 1.000],  loss: 21453552.000000, mae: 1397.622070, mean_q: -9.829144
 1098/5000: episode: 1098, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3465.003, mean reward: -3465.003 [-3465.003, -3465.003], mean action: 1.000 [1.000, 1.000],  loss: 22306526.000000, mae: 1462.356812, mean_q: -9.907557
 1099/5000: episode: 1099, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4434.010, mean reward: -4434.010 [-4434.010, -4434.010], mean action: 1.000 [1.000, 1.000],  loss: 18104376.000000, mae: 1242.573730, mean_q: -9.932158
 1100/5000: episode: 1100, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7989.801, mean reward: -7989.801 [-7989.801, -7989.801], mean action: 1.000 [1.000, 1.000],  loss: 28294468.000000, mae: 1624.546387, mean_q: -9.887028
 1101/5000: episode: 1101, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3717.803, mean reward: -3717.803 [-3717.803, -3717.803], mean action: 1.000 [1.000, 1.000],  loss: 22423276.000000, mae: 1428.058838, mean_q: -9.996655
 1102/5000: episode: 1102, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6516.547, mean reward: -6516.547 [-6516.547, -6516.547], mean action: 1.000 [1.000, 1.000],  loss: 18532544.000000, mae: 1300.010376, mean_q: -10.026348
 1103/5000: episode: 1103, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2623.265, mean reward: -2623.265 [-2623.265, -2623.265], mean action: 1.000 [1.000, 1.000],  loss: 16805276.000000, mae: 1149.034180, mean_q: -10.102484
 1104/5000: episode: 1104, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7207.197, mean reward: -7207.197 [-7207.197, -7207.197], mean action: 1.000 [1.000, 1.000],  loss: 15166861.000000, mae: 1185.084717, mean_q: -10.121826
 1105/5000: episode: 1105, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4522.668, mean reward: -4522.668 [-4522.668, -4522.668], mean action: 1.000 [1.000, 1.000],  loss: 22341774.000000, mae: 1357.951660, mean_q: -10.158308
 1106/5000: episode: 1106, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -14294.598, mean reward: -14294.598 [-14294.598, -14294.598], mean action: 0.000 [0.000, 0.000],  loss: 25836684.000000, mae: 1471.405151, mean_q: -10.139027
 1107/5000: episode: 1107, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1352.074, mean reward: -1352.074 [-1352.074, -1352.074], mean action: 1.000 [1.000, 1.000],  loss: 18332322.000000, mae: 1252.900391, mean_q: -10.261578
 1108/5000: episode: 1108, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -658.656, mean reward: -658.656 [-658.656, -658.656], mean action: 1.000 [1.000, 1.000],  loss: 20168168.000000, mae: 1324.555298, mean_q: -10.250613
 1109/5000: episode: 1109, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1167.827, mean reward: -1167.827 [-1167.827, -1167.827], mean action: 1.000 [1.000, 1.000],  loss: 13756062.000000, mae: 1167.473633, mean_q: -10.346235
 1110/5000: episode: 1110, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3087.503, mean reward: -3087.503 [-3087.503, -3087.503], mean action: 1.000 [1.000, 1.000],  loss: 12775762.000000, mae: 1031.993774, mean_q: -10.426880
 1111/5000: episode: 1111, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3635.650, mean reward: -3635.650 [-3635.650, -3635.650], mean action: 1.000 [1.000, 1.000],  loss: 19985298.000000, mae: 1349.619629, mean_q: -10.426672
 1112/5000: episode: 1112, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4618.256, mean reward: -4618.256 [-4618.256, -4618.256], mean action: 1.000 [1.000, 1.000],  loss: 22256856.000000, mae: 1322.547729, mean_q: -10.444494
 1113/5000: episode: 1113, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1669.717, mean reward: -1669.717 [-1669.717, -1669.717], mean action: 3.000 [3.000, 3.000],  loss: 16552118.000000, mae: 1098.634644, mean_q: -10.534974
 1114/5000: episode: 1114, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8918.630, mean reward: -8918.630 [-8918.630, -8918.630], mean action: 1.000 [1.000, 1.000],  loss: 26246636.000000, mae: 1567.792969, mean_q: -10.503626
 1115/5000: episode: 1115, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2318.932, mean reward: -2318.932 [-2318.932, -2318.932], mean action: 1.000 [1.000, 1.000],  loss: 26168490.000000, mae: 1481.981934, mean_q: -10.535734
 1116/5000: episode: 1116, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -870.159, mean reward: -870.159 [-870.159, -870.159], mean action: 1.000 [1.000, 1.000],  loss: 15861287.000000, mae: 1229.374512, mean_q: -10.614532
 1117/5000: episode: 1117, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1588.308, mean reward: -1588.308 [-1588.308, -1588.308], mean action: 1.000 [1.000, 1.000],  loss: 15652589.000000, mae: 1190.487427, mean_q: -10.674992
 1118/5000: episode: 1118, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1252.911, mean reward: -1252.911 [-1252.911, -1252.911], mean action: 1.000 [1.000, 1.000],  loss: 23525556.000000, mae: 1327.135742, mean_q: -10.696925
 1119/5000: episode: 1119, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5707.078, mean reward: -5707.078 [-5707.078, -5707.078], mean action: 1.000 [1.000, 1.000],  loss: 22517602.000000, mae: 1411.191650, mean_q: -10.738295
 1120/5000: episode: 1120, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1189.564, mean reward: -1189.564 [-1189.564, -1189.564], mean action: 1.000 [1.000, 1.000],  loss: 19748332.000000, mae: 1271.453125, mean_q: -10.799540
 1121/5000: episode: 1121, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4386.882, mean reward: -4386.882 [-4386.882, -4386.882], mean action: 1.000 [1.000, 1.000],  loss: 20300670.000000, mae: 1333.718262, mean_q: -10.797030
 1122/5000: episode: 1122, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4750.125, mean reward: -4750.125 [-4750.125, -4750.125], mean action: 1.000 [1.000, 1.000],  loss: 20383484.000000, mae: 1302.344971, mean_q: -10.842022
 1123/5000: episode: 1123, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -593.130, mean reward: -593.130 [-593.130, -593.130], mean action: 1.000 [1.000, 1.000],  loss: 19268100.000000, mae: 1246.885254, mean_q: -10.908428
 1124/5000: episode: 1124, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -857.338, mean reward: -857.338 [-857.338, -857.338], mean action: 3.000 [3.000, 3.000],  loss: 18409626.000000, mae: 1338.938477, mean_q: -10.932678
 1125/5000: episode: 1125, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -317.068, mean reward: -317.068 [-317.068, -317.068], mean action: 3.000 [3.000, 3.000],  loss: 18005092.000000, mae: 1278.065430, mean_q: -10.982931
 1126/5000: episode: 1126, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -776.172, mean reward: -776.172 [-776.172, -776.172], mean action: 1.000 [1.000, 1.000],  loss: 22329672.000000, mae: 1446.985840, mean_q: -10.992325
 1127/5000: episode: 1127, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6643.670, mean reward: -6643.670 [-6643.670, -6643.670], mean action: 1.000 [1.000, 1.000],  loss: 26744824.000000, mae: 1626.067993, mean_q: -11.009361
 1128/5000: episode: 1128, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -875.594, mean reward: -875.594 [-875.594, -875.594], mean action: 1.000 [1.000, 1.000],  loss: 15939188.000000, mae: 1239.092529, mean_q: -11.118435
 1129/5000: episode: 1129, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5164.654, mean reward: -5164.654 [-5164.654, -5164.654], mean action: 1.000 [1.000, 1.000],  loss: 15159614.000000, mae: 1105.803955, mean_q: -11.163827
 1130/5000: episode: 1130, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2043.000, mean reward: -2043.000 [-2043.000, -2043.000], mean action: 1.000 [1.000, 1.000],  loss: 18658764.000000, mae: 1347.785889, mean_q: -11.169387
 1131/5000: episode: 1131, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2872.683, mean reward: -2872.683 [-2872.683, -2872.683], mean action: 3.000 [3.000, 3.000],  loss: 18180424.000000, mae: 1228.036621, mean_q: -11.229361
 1132/5000: episode: 1132, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2584.828, mean reward: -2584.828 [-2584.828, -2584.828], mean action: 3.000 [3.000, 3.000],  loss: 25444998.000000, mae: 1509.770630, mean_q: -11.226171
 1133/5000: episode: 1133, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10207.359, mean reward: -10207.359 [-10207.359, -10207.359], mean action: 0.000 [0.000, 0.000],  loss: 16985312.000000, mae: 1241.742920, mean_q: -11.312695
 1134/5000: episode: 1134, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -9030.882, mean reward: -9030.882 [-9030.882, -9030.882], mean action: 3.000 [3.000, 3.000],  loss: 12960761.000000, mae: 1101.205811, mean_q: -11.406929
 1135/5000: episode: 1135, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1660.876, mean reward: -1660.876 [-1660.876, -1660.876], mean action: 3.000 [3.000, 3.000],  loss: 17557408.000000, mae: 1241.202026, mean_q: -11.397374
 1136/5000: episode: 1136, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4582.142, mean reward: -4582.142 [-4582.142, -4582.142], mean action: 3.000 [3.000, 3.000],  loss: 21974804.000000, mae: 1348.821289, mean_q: -11.449992
 1137/5000: episode: 1137, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5898.496, mean reward: -5898.496 [-5898.496, -5898.496], mean action: 3.000 [3.000, 3.000],  loss: 21281140.000000, mae: 1425.229004, mean_q: -11.462717
 1138/5000: episode: 1138, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -45.674, mean reward: -45.674 [-45.674, -45.674], mean action: 3.000 [3.000, 3.000],  loss: 19824846.000000, mae: 1325.763428, mean_q: -11.553502
 1139/5000: episode: 1139, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -10390.298, mean reward: -10390.298 [-10390.298, -10390.298], mean action: 3.000 [3.000, 3.000],  loss: 12898464.000000, mae: 1018.146790, mean_q: -11.589371
 1140/5000: episode: 1140, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1436.854, mean reward: -1436.854 [-1436.854, -1436.854], mean action: 1.000 [1.000, 1.000],  loss: 16839004.000000, mae: 1280.903076, mean_q: -11.629408
 1141/5000: episode: 1141, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3947.399, mean reward: -3947.399 [-3947.399, -3947.399], mean action: 1.000 [1.000, 1.000],  loss: 15993802.000000, mae: 1258.802979, mean_q: -11.641291
 1142/5000: episode: 1142, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -940.476, mean reward: -940.476 [-940.476, -940.476], mean action: 3.000 [3.000, 3.000],  loss: 25829782.000000, mae: 1560.667236, mean_q: -11.677056
 1143/5000: episode: 1143, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -383.943, mean reward: -383.943 [-383.943, -383.943], mean action: 3.000 [3.000, 3.000],  loss: 18144442.000000, mae: 1351.832886, mean_q: -11.736513
 1144/5000: episode: 1144, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4929.186, mean reward: -4929.186 [-4929.186, -4929.186], mean action: 1.000 [1.000, 1.000],  loss: 22288516.000000, mae: 1363.489136, mean_q: -11.793202
 1145/5000: episode: 1145, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1817.339, mean reward: -1817.339 [-1817.339, -1817.339], mean action: 1.000 [1.000, 1.000],  loss: 24471496.000000, mae: 1391.563477, mean_q: -11.779516
 1146/5000: episode: 1146, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -439.602, mean reward: -439.602 [-439.602, -439.602], mean action: 3.000 [3.000, 3.000],  loss: 19203032.000000, mae: 1316.320435, mean_q: -11.863306
 1147/5000: episode: 1147, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -8842.850, mean reward: -8842.850 [-8842.850, -8842.850], mean action: 1.000 [1.000, 1.000],  loss: 13011402.000000, mae: 974.512817, mean_q: -11.971361
 1148/5000: episode: 1148, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6087.564, mean reward: -6087.564 [-6087.564, -6087.564], mean action: 1.000 [1.000, 1.000],  loss: 19486302.000000, mae: 1366.268066, mean_q: -11.951387
 1149/5000: episode: 1149, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2215.915, mean reward: -2215.915 [-2215.915, -2215.915], mean action: 1.000 [1.000, 1.000],  loss: 23423368.000000, mae: 1483.301025, mean_q: -11.946675
 1150/5000: episode: 1150, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4679.036, mean reward: -4679.036 [-4679.036, -4679.036], mean action: 1.000 [1.000, 1.000],  loss: 16391285.000000, mae: 1151.367920, mean_q: -12.086870
 1151/5000: episode: 1151, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3765.204, mean reward: -3765.204 [-3765.204, -3765.204], mean action: 1.000 [1.000, 1.000],  loss: 18506464.000000, mae: 1305.051270, mean_q: -12.108745
 1152/5000: episode: 1152, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5296.892, mean reward: -5296.892 [-5296.892, -5296.892], mean action: 1.000 [1.000, 1.000],  loss: 23058670.000000, mae: 1422.629517, mean_q: -12.104553
 1153/5000: episode: 1153, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7781.580, mean reward: -7781.580 [-7781.580, -7781.580], mean action: 1.000 [1.000, 1.000],  loss: 21505122.000000, mae: 1356.210449, mean_q: -12.140800
 1154/5000: episode: 1154, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10537.210, mean reward: -10537.210 [-10537.210, -10537.210], mean action: 1.000 [1.000, 1.000],  loss: 23076428.000000, mae: 1536.022461, mean_q: -12.157664
 1155/5000: episode: 1155, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5525.285, mean reward: -5525.285 [-5525.285, -5525.285], mean action: 1.000 [1.000, 1.000],  loss: 18192098.000000, mae: 1208.388794, mean_q: -12.241160
 1156/5000: episode: 1156, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3221.245, mean reward: -3221.245 [-3221.245, -3221.245], mean action: 1.000 [1.000, 1.000],  loss: 13120295.000000, mae: 1066.736084, mean_q: -12.316750
 1157/5000: episode: 1157, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -6827.674, mean reward: -6827.674 [-6827.674, -6827.674], mean action: 1.000 [1.000, 1.000],  loss: 16702066.000000, mae: 1204.154297, mean_q: -12.307854
 1158/5000: episode: 1158, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3482.044, mean reward: -3482.044 [-3482.044, -3482.044], mean action: 1.000 [1.000, 1.000],  loss: 33417060.000000, mae: 1831.833252, mean_q: -12.317383
 1159/5000: episode: 1159, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5073.297, mean reward: -5073.297 [-5073.297, -5073.297], mean action: 0.000 [0.000, 0.000],  loss: 14373101.000000, mae: 1079.218262, mean_q: -12.450220
 1160/5000: episode: 1160, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -8864.267, mean reward: -8864.267 [-8864.267, -8864.267], mean action: 0.000 [0.000, 0.000],  loss: 19233232.000000, mae: 1200.750854, mean_q: -12.466077
 1161/5000: episode: 1161, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2703.205, mean reward: -2703.205 [-2703.205, -2703.205], mean action: 1.000 [1.000, 1.000],  loss: 17520850.000000, mae: 1203.345093, mean_q: -12.497760
 1162/5000: episode: 1162, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2804.301, mean reward: -2804.301 [-2804.301, -2804.301], mean action: 1.000 [1.000, 1.000],  loss: 21001282.000000, mae: 1292.510010, mean_q: -12.566351
 1163/5000: episode: 1163, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -70.063, mean reward: -70.063 [-70.063, -70.063], mean action: 3.000 [3.000, 3.000],  loss: 18901420.000000, mae: 1326.287598, mean_q: -12.611484
 1164/5000: episode: 1164, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -809.876, mean reward: -809.876 [-809.876, -809.876], mean action: 1.000 [1.000, 1.000],  loss: 18604024.000000, mae: 1258.219116, mean_q: -12.670029
 1165/5000: episode: 1165, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4631.847, mean reward: -4631.847 [-4631.847, -4631.847], mean action: 3.000 [3.000, 3.000],  loss: 22619148.000000, mae: 1385.591309, mean_q: -12.688578
 1166/5000: episode: 1166, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1499.006, mean reward: -1499.006 [-1499.006, -1499.006], mean action: 3.000 [3.000, 3.000],  loss: 17535592.000000, mae: 1187.299927, mean_q: -12.795423
 1167/5000: episode: 1167, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2378.193, mean reward: -2378.193 [-2378.193, -2378.193], mean action: 3.000 [3.000, 3.000],  loss: 24620456.000000, mae: 1531.936890, mean_q: -12.720760
 1168/5000: episode: 1168, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6734.084, mean reward: -6734.084 [-6734.084, -6734.084], mean action: 3.000 [3.000, 3.000],  loss: 15834629.000000, mae: 1138.244507, mean_q: -12.753550
 1169/5000: episode: 1169, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3762.992, mean reward: -3762.992 [-3762.992, -3762.992], mean action: 3.000 [3.000, 3.000],  loss: 11967598.000000, mae: 1042.881592, mean_q: -12.881327
 1170/5000: episode: 1170, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3955.190, mean reward: -3955.190 [-3955.190, -3955.190], mean action: 1.000 [1.000, 1.000],  loss: 18620828.000000, mae: 1232.447998, mean_q: -12.901294
 1171/5000: episode: 1171, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3851.122, mean reward: -3851.122 [-3851.122, -3851.122], mean action: 1.000 [1.000, 1.000],  loss: 24836014.000000, mae: 1416.809570, mean_q: -12.940712
 1172/5000: episode: 1172, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1657.356, mean reward: -1657.356 [-1657.356, -1657.356], mean action: 1.000 [1.000, 1.000],  loss: 19598464.000000, mae: 1356.993042, mean_q: -12.954003
 1173/5000: episode: 1173, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4652.897, mean reward: -4652.897 [-4652.897, -4652.897], mean action: 1.000 [1.000, 1.000],  loss: 14980158.000000, mae: 1132.953369, mean_q: -13.070127
 1174/5000: episode: 1174, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2430.824, mean reward: -2430.824 [-2430.824, -2430.824], mean action: 1.000 [1.000, 1.000],  loss: 23353880.000000, mae: 1389.951904, mean_q: -13.042828
 1175/5000: episode: 1175, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8203.787, mean reward: -8203.787 [-8203.787, -8203.787], mean action: 1.000 [1.000, 1.000],  loss: 22216132.000000, mae: 1439.662354, mean_q: -13.103132
 1176/5000: episode: 1176, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1406.907, mean reward: -1406.907 [-1406.907, -1406.907], mean action: 1.000 [1.000, 1.000],  loss: 21761732.000000, mae: 1390.678467, mean_q: -13.151126
 1177/5000: episode: 1177, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5247.411, mean reward: -5247.411 [-5247.411, -5247.411], mean action: 1.000 [1.000, 1.000],  loss: 17966250.000000, mae: 1216.051270, mean_q: -13.243944
 1178/5000: episode: 1178, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7650.441, mean reward: -7650.441 [-7650.441, -7650.441], mean action: 1.000 [1.000, 1.000],  loss: 31213468.000000, mae: 1672.289307, mean_q: -13.218091
 1179/5000: episode: 1179, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3874.590, mean reward: -3874.590 [-3874.590, -3874.590], mean action: 1.000 [1.000, 1.000],  loss: 22245726.000000, mae: 1442.366211, mean_q: -13.330282
 1180/5000: episode: 1180, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3663.149, mean reward: -3663.149 [-3663.149, -3663.149], mean action: 1.000 [1.000, 1.000],  loss: 32015776.000000, mae: 1813.244629, mean_q: -13.265884
 1181/5000: episode: 1181, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -9366.794, mean reward: -9366.794 [-9366.794, -9366.794], mean action: 1.000 [1.000, 1.000],  loss: 11756063.000000, mae: 1027.430176, mean_q: -13.410176
 1182/5000: episode: 1182, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6457.217, mean reward: -6457.217 [-6457.217, -6457.217], mean action: 1.000 [1.000, 1.000],  loss: 14455404.000000, mae: 1199.681152, mean_q: -13.471472
 1183/5000: episode: 1183, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1285.476, mean reward: -1285.476 [-1285.476, -1285.476], mean action: 1.000 [1.000, 1.000],  loss: 21590208.000000, mae: 1441.113037, mean_q: -13.483383
 1184/5000: episode: 1184, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3789.222, mean reward: -3789.222 [-3789.222, -3789.222], mean action: 1.000 [1.000, 1.000],  loss: 20854938.000000, mae: 1391.379883, mean_q: -13.510050
 1185/5000: episode: 1185, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1826.250, mean reward: -1826.250 [-1826.250, -1826.250], mean action: 1.000 [1.000, 1.000],  loss: 24727072.000000, mae: 1421.949951, mean_q: -13.501661
 1186/5000: episode: 1186, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1403.019, mean reward: -1403.019 [-1403.019, -1403.019], mean action: 1.000 [1.000, 1.000],  loss: 16578010.000000, mae: 1213.634521, mean_q: -13.617495
 1187/5000: episode: 1187, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6862.403, mean reward: -6862.403 [-6862.403, -6862.403], mean action: 1.000 [1.000, 1.000],  loss: 27820880.000000, mae: 1702.270996, mean_q: -13.604866
 1188/5000: episode: 1188, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1861.960, mean reward: -1861.960 [-1861.960, -1861.960], mean action: 1.000 [1.000, 1.000],  loss: 15873522.000000, mae: 1179.764771, mean_q: -13.759891
 1189/5000: episode: 1189, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6444.640, mean reward: -6444.640 [-6444.640, -6444.640], mean action: 1.000 [1.000, 1.000],  loss: 13997424.000000, mae: 1069.601074, mean_q: -13.811762
 1190/5000: episode: 1190, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5420.052, mean reward: -5420.052 [-5420.052, -5420.052], mean action: 1.000 [1.000, 1.000],  loss: 19346830.000000, mae: 1378.191650, mean_q: -13.771205
 1191/5000: episode: 1191, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1400.612, mean reward: -1400.612 [-1400.612, -1400.612], mean action: 1.000 [1.000, 1.000],  loss: 15617128.000000, mae: 1208.820068, mean_q: -13.928343
 1192/5000: episode: 1192, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3313.842, mean reward: -3313.842 [-3313.842, -3313.842], mean action: 1.000 [1.000, 1.000],  loss: 21738060.000000, mae: 1374.141724, mean_q: -13.867567
 1193/5000: episode: 1193, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4222.010, mean reward: -4222.010 [-4222.010, -4222.010], mean action: 1.000 [1.000, 1.000],  loss: 20181200.000000, mae: 1341.348877, mean_q: -13.903851
 1194/5000: episode: 1194, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2349.771, mean reward: -2349.771 [-2349.771, -2349.771], mean action: 1.000 [1.000, 1.000],  loss: 18984358.000000, mae: 1347.965088, mean_q: -13.971898
 1195/5000: episode: 1195, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2247.027, mean reward: -2247.027 [-2247.027, -2247.027], mean action: 1.000 [1.000, 1.000],  loss: 17651136.000000, mae: 1319.916504, mean_q: -14.054441
 1196/5000: episode: 1196, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4774.745, mean reward: -4774.745 [-4774.745, -4774.745], mean action: 1.000 [1.000, 1.000],  loss: 14409686.000000, mae: 1177.232788, mean_q: -14.125298
 1197/5000: episode: 1197, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8979.391, mean reward: -8979.391 [-8979.391, -8979.391], mean action: 1.000 [1.000, 1.000],  loss: 16820724.000000, mae: 1249.838623, mean_q: -14.131103
 1198/5000: episode: 1198, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2291.681, mean reward: -2291.681 [-2291.681, -2291.681], mean action: 1.000 [1.000, 1.000],  loss: 23120592.000000, mae: 1445.210815, mean_q: -14.174740
 1199/5000: episode: 1199, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6955.382, mean reward: -6955.382 [-6955.382, -6955.382], mean action: 1.000 [1.000, 1.000],  loss: 15721857.000000, mae: 1168.966553, mean_q: -14.208262
 1200/5000: episode: 1200, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6671.157, mean reward: -6671.157 [-6671.157, -6671.157], mean action: 1.000 [1.000, 1.000],  loss: 31711618.000000, mae: 1722.950928, mean_q: -14.260035
 1201/5000: episode: 1201, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2210.159, mean reward: -2210.159 [-2210.159, -2210.159], mean action: 1.000 [1.000, 1.000],  loss: 16760314.000000, mae: 1229.891113, mean_q: -14.361946
 1202/5000: episode: 1202, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -6294.630, mean reward: -6294.630 [-6294.630, -6294.630], mean action: 1.000 [1.000, 1.000],  loss: 21179728.000000, mae: 1198.002686, mean_q: -14.375484
 1203/5000: episode: 1203, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7545.929, mean reward: -7545.929 [-7545.929, -7545.929], mean action: 1.000 [1.000, 1.000],  loss: 31536060.000000, mae: 1689.857422, mean_q: -14.307578
 1204/5000: episode: 1204, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8274.469, mean reward: -8274.469 [-8274.469, -8274.469], mean action: 1.000 [1.000, 1.000],  loss: 20301720.000000, mae: 1389.743042, mean_q: -14.427001
 1205/5000: episode: 1205, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2200.956, mean reward: -2200.956 [-2200.956, -2200.956], mean action: 1.000 [1.000, 1.000],  loss: 19537868.000000, mae: 1291.259033, mean_q: -14.479233
 1206/5000: episode: 1206, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3534.142, mean reward: -3534.142 [-3534.142, -3534.142], mean action: 1.000 [1.000, 1.000],  loss: 17155608.000000, mae: 1245.170898, mean_q: -14.570523
 1207/5000: episode: 1207, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6746.693, mean reward: -6746.693 [-6746.693, -6746.693], mean action: 1.000 [1.000, 1.000],  loss: 13841804.000000, mae: 1163.028198, mean_q: -14.676814
 1208/5000: episode: 1208, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5201.295, mean reward: -5201.295 [-5201.295, -5201.295], mean action: 1.000 [1.000, 1.000],  loss: 22518430.000000, mae: 1393.966797, mean_q: -14.579996
 1209/5000: episode: 1209, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6548.649, mean reward: -6548.649 [-6548.649, -6548.649], mean action: 1.000 [1.000, 1.000],  loss: 14179032.000000, mae: 1082.332031, mean_q: -14.675167
 1210/5000: episode: 1210, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1115.910, mean reward: -1115.910 [-1115.910, -1115.910], mean action: 1.000 [1.000, 1.000],  loss: 18615882.000000, mae: 1314.342651, mean_q: -14.717423
 1211/5000: episode: 1211, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4746.029, mean reward: -4746.029 [-4746.029, -4746.029], mean action: 1.000 [1.000, 1.000],  loss: 16579979.000000, mae: 1216.553223, mean_q: -14.800726
 1212/5000: episode: 1212, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -747.587, mean reward: -747.587 [-747.587, -747.587], mean action: 1.000 [1.000, 1.000],  loss: 18455384.000000, mae: 1319.607178, mean_q: -14.825126
 1213/5000: episode: 1213, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7014.601, mean reward: -7014.601 [-7014.601, -7014.601], mean action: 2.000 [2.000, 2.000],  loss: 20438636.000000, mae: 1335.978516, mean_q: -14.925182
 1214/5000: episode: 1214, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -6397.235, mean reward: -6397.235 [-6397.235, -6397.235], mean action: 1.000 [1.000, 1.000],  loss: 19711614.000000, mae: 1264.539307, mean_q: -14.917939
 1215/5000: episode: 1215, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6543.250, mean reward: -6543.250 [-6543.250, -6543.250], mean action: 1.000 [1.000, 1.000],  loss: 19700788.000000, mae: 1311.589355, mean_q: -14.952169
 1216/5000: episode: 1216, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7775.688, mean reward: -7775.688 [-7775.688, -7775.688], mean action: 1.000 [1.000, 1.000],  loss: 25186250.000000, mae: 1509.118286, mean_q: -15.018640
 1217/5000: episode: 1217, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1770.359, mean reward: -1770.359 [-1770.359, -1770.359], mean action: 1.000 [1.000, 1.000],  loss: 22020512.000000, mae: 1430.726440, mean_q: -15.087063
 1218/5000: episode: 1218, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5523.784, mean reward: -5523.784 [-5523.784, -5523.784], mean action: 1.000 [1.000, 1.000],  loss: 17787316.000000, mae: 1325.776611, mean_q: -15.134029
 1219/5000: episode: 1219, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6145.355, mean reward: -6145.355 [-6145.355, -6145.355], mean action: 1.000 [1.000, 1.000],  loss: 18109132.000000, mae: 1223.222900, mean_q: -15.237198
 1220/5000: episode: 1220, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3455.997, mean reward: -3455.997 [-3455.997, -3455.997], mean action: 1.000 [1.000, 1.000],  loss: 15383524.000000, mae: 1224.008179, mean_q: -15.283670
 1221/5000: episode: 1221, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1377.515, mean reward: -1377.515 [-1377.515, -1377.515], mean action: 3.000 [3.000, 3.000],  loss: 24405052.000000, mae: 1493.881714, mean_q: -15.307951
 1222/5000: episode: 1222, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5709.827, mean reward: -5709.827 [-5709.827, -5709.827], mean action: 1.000 [1.000, 1.000],  loss: 15677129.000000, mae: 1170.188110, mean_q: -15.352304
 1223/5000: episode: 1223, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7145.075, mean reward: -7145.075 [-7145.075, -7145.075], mean action: 1.000 [1.000, 1.000],  loss: 17641008.000000, mae: 1201.657349, mean_q: -15.418739
 1224/5000: episode: 1224, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3617.158, mean reward: -3617.158 [-3617.158, -3617.158], mean action: 1.000 [1.000, 1.000],  loss: 22473216.000000, mae: 1396.614136, mean_q: -15.407855
 1225/5000: episode: 1225, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5084.848, mean reward: -5084.848 [-5084.848, -5084.848], mean action: 1.000 [1.000, 1.000],  loss: 20573658.000000, mae: 1333.854248, mean_q: -15.515518
 1226/5000: episode: 1226, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7412.909, mean reward: -7412.909 [-7412.909, -7412.909], mean action: 1.000 [1.000, 1.000],  loss: 23402300.000000, mae: 1355.868408, mean_q: -15.503891
 1227/5000: episode: 1227, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4571.949, mean reward: -4571.949 [-4571.949, -4571.949], mean action: 1.000 [1.000, 1.000],  loss: 17722212.000000, mae: 1203.895020, mean_q: -15.625551
 1228/5000: episode: 1228, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4133.985, mean reward: -4133.985 [-4133.985, -4133.985], mean action: 1.000 [1.000, 1.000],  loss: 19421920.000000, mae: 1325.694214, mean_q: -15.618344
 1229/5000: episode: 1229, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -8275.472, mean reward: -8275.472 [-8275.472, -8275.472], mean action: 1.000 [1.000, 1.000],  loss: 25208076.000000, mae: 1504.620605, mean_q: -15.646563
 1230/5000: episode: 1230, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5159.170, mean reward: -5159.170 [-5159.170, -5159.170], mean action: 1.000 [1.000, 1.000],  loss: 19104914.000000, mae: 1272.727417, mean_q: -15.773851
 1231/5000: episode: 1231, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9585.810, mean reward: -9585.810 [-9585.810, -9585.810], mean action: 1.000 [1.000, 1.000],  loss: 17771412.000000, mae: 1283.427246, mean_q: -15.788635
 1232/5000: episode: 1232, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9465.156, mean reward: -9465.156 [-9465.156, -9465.156], mean action: 1.000 [1.000, 1.000],  loss: 16964332.000000, mae: 1249.382324, mean_q: -15.807668
 1233/5000: episode: 1233, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3146.170, mean reward: -3146.170 [-3146.170, -3146.170], mean action: 1.000 [1.000, 1.000],  loss: 11558305.000000, mae: 998.242126, mean_q: -15.933525
 1234/5000: episode: 1234, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5956.242, mean reward: -5956.242 [-5956.242, -5956.242], mean action: 1.000 [1.000, 1.000],  loss: 19278110.000000, mae: 1309.225586, mean_q: -15.876679
 1235/5000: episode: 1235, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1569.207, mean reward: -1569.207 [-1569.207, -1569.207], mean action: 1.000 [1.000, 1.000],  loss: 21486926.000000, mae: 1399.584106, mean_q: -15.911898
 1236/5000: episode: 1236, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6415.919, mean reward: -6415.919 [-6415.919, -6415.919], mean action: 1.000 [1.000, 1.000],  loss: 17503448.000000, mae: 1155.540771, mean_q: -15.978979
 1237/5000: episode: 1237, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4979.670, mean reward: -4979.670 [-4979.670, -4979.670], mean action: 1.000 [1.000, 1.000],  loss: 14947100.000000, mae: 1189.335083, mean_q: -16.094959
 1238/5000: episode: 1238, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5810.756, mean reward: -5810.756 [-5810.756, -5810.756], mean action: 1.000 [1.000, 1.000],  loss: 18054396.000000, mae: 1293.907227, mean_q: -16.108530
 1239/5000: episode: 1239, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -749.100, mean reward: -749.100 [-749.100, -749.100], mean action: 1.000 [1.000, 1.000],  loss: 13402586.000000, mae: 1043.324463, mean_q: -16.202366
 1240/5000: episode: 1240, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1485.090, mean reward: -1485.090 [-1485.090, -1485.090], mean action: 1.000 [1.000, 1.000],  loss: 17059964.000000, mae: 1176.783447, mean_q: -16.225151
 1241/5000: episode: 1241, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4752.145, mean reward: -4752.145 [-4752.145, -4752.145], mean action: 1.000 [1.000, 1.000],  loss: 15448134.000000, mae: 1103.950684, mean_q: -16.255049
 1242/5000: episode: 1242, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4555.566, mean reward: -4555.566 [-4555.566, -4555.566], mean action: 1.000 [1.000, 1.000],  loss: 21394864.000000, mae: 1479.246826, mean_q: -16.245174
 1243/5000: episode: 1243, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1656.008, mean reward: -1656.008 [-1656.008, -1656.008], mean action: 1.000 [1.000, 1.000],  loss: 18496676.000000, mae: 1312.165283, mean_q: -16.385748
 1244/5000: episode: 1244, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3170.268, mean reward: -3170.268 [-3170.268, -3170.268], mean action: 1.000 [1.000, 1.000],  loss: 15971406.000000, mae: 1186.162109, mean_q: -16.353916
 1245/5000: episode: 1245, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2183.094, mean reward: -2183.094 [-2183.094, -2183.094], mean action: 1.000 [1.000, 1.000],  loss: 18489094.000000, mae: 1155.272095, mean_q: -16.481651
 1246/5000: episode: 1246, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2076.221, mean reward: -2076.221 [-2076.221, -2076.221], mean action: 1.000 [1.000, 1.000],  loss: 19474162.000000, mae: 1379.238525, mean_q: -16.507181
 1247/5000: episode: 1247, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -456.534, mean reward: -456.534 [-456.534, -456.534], mean action: 1.000 [1.000, 1.000],  loss: 22030526.000000, mae: 1485.286255, mean_q: -16.471771
 1248/5000: episode: 1248, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6088.666, mean reward: -6088.666 [-6088.666, -6088.666], mean action: 1.000 [1.000, 1.000],  loss: 15363708.000000, mae: 1233.555908, mean_q: -16.602448
 1249/5000: episode: 1249, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4510.799, mean reward: -4510.799 [-4510.799, -4510.799], mean action: 3.000 [3.000, 3.000],  loss: 13113598.000000, mae: 975.196655, mean_q: -16.711117
 1250/5000: episode: 1250, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1898.901, mean reward: -1898.901 [-1898.901, -1898.901], mean action: 1.000 [1.000, 1.000],  loss: 25108766.000000, mae: 1520.911865, mean_q: -16.716866
 1251/5000: episode: 1251, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1691.250, mean reward: -1691.250 [-1691.250, -1691.250], mean action: 3.000 [3.000, 3.000],  loss: 13231733.000000, mae: 1078.175537, mean_q: -16.803600
 1252/5000: episode: 1252, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2236.680, mean reward: -2236.680 [-2236.680, -2236.680], mean action: 3.000 [3.000, 3.000],  loss: 20765646.000000, mae: 1375.664917, mean_q: -16.691399
 1253/5000: episode: 1253, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -138.623, mean reward: -138.623 [-138.623, -138.623], mean action: 3.000 [3.000, 3.000],  loss: 12761823.000000, mae: 1069.977295, mean_q: -16.902134
 1254/5000: episode: 1254, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3782.917, mean reward: -3782.917 [-3782.917, -3782.917], mean action: 3.000 [3.000, 3.000],  loss: 16021256.000000, mae: 1224.840454, mean_q: -16.858435
 1255/5000: episode: 1255, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2852.134, mean reward: -2852.134 [-2852.134, -2852.134], mean action: 3.000 [3.000, 3.000],  loss: 20916594.000000, mae: 1300.088379, mean_q: -16.966442
 1256/5000: episode: 1256, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9666.915, mean reward: -9666.915 [-9666.915, -9666.915], mean action: 3.000 [3.000, 3.000],  loss: 18006402.000000, mae: 1346.518433, mean_q: -16.910706
 1257/5000: episode: 1257, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -583.051, mean reward: -583.051 [-583.051, -583.051], mean action: 3.000 [3.000, 3.000],  loss: 16625292.000000, mae: 1205.589111, mean_q: -17.054855
 1258/5000: episode: 1258, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -106.295, mean reward: -106.295 [-106.295, -106.295], mean action: 3.000 [3.000, 3.000],  loss: 18335324.000000, mae: 1190.191406, mean_q: -17.101355
 1259/5000: episode: 1259, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1664.106, mean reward: -1664.106 [-1664.106, -1664.106], mean action: 3.000 [3.000, 3.000],  loss: 20362334.000000, mae: 1299.698730, mean_q: -17.106657
 1260/5000: episode: 1260, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -705.252, mean reward: -705.252 [-705.252, -705.252], mean action: 3.000 [3.000, 3.000],  loss: 23540196.000000, mae: 1496.059814, mean_q: -17.142851
 1261/5000: episode: 1261, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5409.802, mean reward: -5409.802 [-5409.802, -5409.802], mean action: 3.000 [3.000, 3.000],  loss: 18459216.000000, mae: 1268.990967, mean_q: -17.171452
 1262/5000: episode: 1262, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2588.323, mean reward: -2588.323 [-2588.323, -2588.323], mean action: 3.000 [3.000, 3.000],  loss: 20099548.000000, mae: 1295.979370, mean_q: -17.219612
 1263/5000: episode: 1263, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -372.357, mean reward: -372.357 [-372.357, -372.357], mean action: 3.000 [3.000, 3.000],  loss: 17219658.000000, mae: 1244.986816, mean_q: -17.396336
 1264/5000: episode: 1264, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1607.937, mean reward: -1607.937 [-1607.937, -1607.937], mean action: 3.000 [3.000, 3.000],  loss: 23070184.000000, mae: 1489.517090, mean_q: -17.347576
 1265/5000: episode: 1265, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -824.055, mean reward: -824.055 [-824.055, -824.055], mean action: 3.000 [3.000, 3.000],  loss: 17824492.000000, mae: 1267.197266, mean_q: -17.363844
 1266/5000: episode: 1266, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6995.575, mean reward: -6995.575 [-6995.575, -6995.575], mean action: 3.000 [3.000, 3.000],  loss: 18838972.000000, mae: 1223.849609, mean_q: -17.504391
 1267/5000: episode: 1267, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1692.781, mean reward: -1692.781 [-1692.781, -1692.781], mean action: 3.000 [3.000, 3.000],  loss: 22263724.000000, mae: 1394.991821, mean_q: -17.422878
 1268/5000: episode: 1268, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2653.331, mean reward: -2653.331 [-2653.331, -2653.331], mean action: 3.000 [3.000, 3.000],  loss: 20325930.000000, mae: 1340.556641, mean_q: -17.604877
 1269/5000: episode: 1269, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1905.137, mean reward: -1905.137 [-1905.137, -1905.137], mean action: 3.000 [3.000, 3.000],  loss: 17755320.000000, mae: 1278.519775, mean_q: -17.559219
 1270/5000: episode: 1270, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6250.721, mean reward: -6250.721 [-6250.721, -6250.721], mean action: 3.000 [3.000, 3.000],  loss: 13968353.000000, mae: 1097.696167, mean_q: -17.718334
 1271/5000: episode: 1271, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2768.832, mean reward: -2768.832 [-2768.832, -2768.832], mean action: 3.000 [3.000, 3.000],  loss: 19760840.000000, mae: 1375.969971, mean_q: -17.627073
 1272/5000: episode: 1272, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5124.526, mean reward: -5124.526 [-5124.526, -5124.526], mean action: 3.000 [3.000, 3.000],  loss: 13446229.000000, mae: 1086.577881, mean_q: -17.750408
 1273/5000: episode: 1273, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2937.584, mean reward: -2937.584 [-2937.584, -2937.584], mean action: 3.000 [3.000, 3.000],  loss: 18472672.000000, mae: 1246.276733, mean_q: -17.770737
 1274/5000: episode: 1274, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -12260.113, mean reward: -12260.113 [-12260.113, -12260.113], mean action: 3.000 [3.000, 3.000],  loss: 12918638.000000, mae: 1030.300293, mean_q: -17.863747
 1275/5000: episode: 1275, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4007.199, mean reward: -4007.199 [-4007.199, -4007.199], mean action: 3.000 [3.000, 3.000],  loss: 19990814.000000, mae: 1336.533691, mean_q: -17.852949
 1276/5000: episode: 1276, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2300.509, mean reward: -2300.509 [-2300.509, -2300.509], mean action: 3.000 [3.000, 3.000],  loss: 25028568.000000, mae: 1545.994141, mean_q: -17.883831
 1277/5000: episode: 1277, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6467.659, mean reward: -6467.659 [-6467.659, -6467.659], mean action: 3.000 [3.000, 3.000],  loss: 16806310.000000, mae: 1178.539307, mean_q: -18.047644
 1278/5000: episode: 1278, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -729.039, mean reward: -729.039 [-729.039, -729.039], mean action: 3.000 [3.000, 3.000],  loss: 17096838.000000, mae: 1205.808838, mean_q: -18.009228
 1279/5000: episode: 1279, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5227.074, mean reward: -5227.074 [-5227.074, -5227.074], mean action: 3.000 [3.000, 3.000],  loss: 18403224.000000, mae: 1341.538452, mean_q: -18.069645
 1280/5000: episode: 1280, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2240.212, mean reward: -2240.212 [-2240.212, -2240.212], mean action: 3.000 [3.000, 3.000],  loss: 14628144.000000, mae: 1183.204346, mean_q: -18.110432
 1281/5000: episode: 1281, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4018.166, mean reward: -4018.166 [-4018.166, -4018.166], mean action: 3.000 [3.000, 3.000],  loss: 26890732.000000, mae: 1599.340332, mean_q: -18.031818
 1282/5000: episode: 1282, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -407.310, mean reward: -407.310 [-407.310, -407.310], mean action: 3.000 [3.000, 3.000],  loss: 21807196.000000, mae: 1339.256592, mean_q: -18.210003
 1283/5000: episode: 1283, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -331.757, mean reward: -331.757 [-331.757, -331.757], mean action: 3.000 [3.000, 3.000],  loss: 18452788.000000, mae: 1349.144775, mean_q: -18.264812
 1284/5000: episode: 1284, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2437.256, mean reward: -2437.256 [-2437.256, -2437.256], mean action: 3.000 [3.000, 3.000],  loss: 27006256.000000, mae: 1549.117432, mean_q: -18.306601
 1285/5000: episode: 1285, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3405.532, mean reward: -3405.532 [-3405.532, -3405.532], mean action: 3.000 [3.000, 3.000],  loss: 16120260.000000, mae: 1146.072510, mean_q: -18.482594
 1286/5000: episode: 1286, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4816.012, mean reward: -4816.012 [-4816.012, -4816.012], mean action: 3.000 [3.000, 3.000],  loss: 14809192.000000, mae: 1125.524902, mean_q: -18.464203
 1287/5000: episode: 1287, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5769.617, mean reward: -5769.617 [-5769.617, -5769.617], mean action: 3.000 [3.000, 3.000],  loss: 19239834.000000, mae: 1304.105835, mean_q: -18.539394
 1288/5000: episode: 1288, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1732.891, mean reward: -1732.891 [-1732.891, -1732.891], mean action: 3.000 [3.000, 3.000],  loss: 21206060.000000, mae: 1411.583740, mean_q: -18.530809
 1289/5000: episode: 1289, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3350.731, mean reward: -3350.731 [-3350.731, -3350.731], mean action: 3.000 [3.000, 3.000],  loss: 15249382.000000, mae: 1183.340454, mean_q: -18.674372
 1290/5000: episode: 1290, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2956.941, mean reward: -2956.941 [-2956.941, -2956.941], mean action: 3.000 [3.000, 3.000],  loss: 18868468.000000, mae: 1294.990723, mean_q: -18.686214
 1291/5000: episode: 1291, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3810.800, mean reward: -3810.800 [-3810.800, -3810.800], mean action: 3.000 [3.000, 3.000],  loss: 11878676.000000, mae: 1039.275757, mean_q: -18.700615
 1292/5000: episode: 1292, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -221.311, mean reward: -221.311 [-221.311, -221.311], mean action: 3.000 [3.000, 3.000],  loss: 17882772.000000, mae: 1274.213623, mean_q: -18.759279
 1293/5000: episode: 1293, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3311.742, mean reward: -3311.742 [-3311.742, -3311.742], mean action: 3.000 [3.000, 3.000],  loss: 16668192.000000, mae: 1240.140137, mean_q: -18.831894
 1294/5000: episode: 1294, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4257.938, mean reward: -4257.938 [-4257.938, -4257.938], mean action: 3.000 [3.000, 3.000],  loss: 22372730.000000, mae: 1354.928711, mean_q: -18.892452
 1295/5000: episode: 1295, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -109.936, mean reward: -109.936 [-109.936, -109.936], mean action: 3.000 [3.000, 3.000],  loss: 18054630.000000, mae: 1281.082764, mean_q: -18.967550
 1296/5000: episode: 1296, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6252.328, mean reward: -6252.328 [-6252.328, -6252.328], mean action: 3.000 [3.000, 3.000],  loss: 18667918.000000, mae: 1197.743286, mean_q: -19.006115
 1297/5000: episode: 1297, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4704.528, mean reward: -4704.528 [-4704.528, -4704.528], mean action: 3.000 [3.000, 3.000],  loss: 20592422.000000, mae: 1235.535400, mean_q: -19.163576
 1298/5000: episode: 1298, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1123.651, mean reward: -1123.651 [-1123.651, -1123.651], mean action: 3.000 [3.000, 3.000],  loss: 20721536.000000, mae: 1390.060791, mean_q: -19.051956
 1299/5000: episode: 1299, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6153.753, mean reward: -6153.753 [-6153.753, -6153.753], mean action: 3.000 [3.000, 3.000],  loss: 20150212.000000, mae: 1439.472900, mean_q: -19.111383
 1300/5000: episode: 1300, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5464.951, mean reward: -5464.951 [-5464.951, -5464.951], mean action: 3.000 [3.000, 3.000],  loss: 18649852.000000, mae: 1220.197144, mean_q: -19.284842
 1301/5000: episode: 1301, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1966.468, mean reward: -1966.468 [-1966.468, -1966.468], mean action: 3.000 [3.000, 3.000],  loss: 18865452.000000, mae: 1303.159302, mean_q: -19.279423
 1302/5000: episode: 1302, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4864.601, mean reward: -4864.601 [-4864.601, -4864.601], mean action: 3.000 [3.000, 3.000],  loss: 22742976.000000, mae: 1405.585815, mean_q: -19.365267
 1303/5000: episode: 1303, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -2358.013, mean reward: -2358.013 [-2358.013, -2358.013], mean action: 3.000 [3.000, 3.000],  loss: 20018818.000000, mae: 1327.811523, mean_q: -19.429859
 1304/5000: episode: 1304, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6810.569, mean reward: -6810.569 [-6810.569, -6810.569], mean action: 3.000 [3.000, 3.000],  loss: 10509230.000000, mae: 989.201660, mean_q: -19.561756
 1305/5000: episode: 1305, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -126.717, mean reward: -126.717 [-126.717, -126.717], mean action: 3.000 [3.000, 3.000],  loss: 15292238.000000, mae: 1194.115356, mean_q: -19.540108
 1306/5000: episode: 1306, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -6375.201, mean reward: -6375.201 [-6375.201, -6375.201], mean action: 3.000 [3.000, 3.000],  loss: 16622752.000000, mae: 1276.907959, mean_q: -19.586063
 1307/5000: episode: 1307, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1842.364, mean reward: -1842.364 [-1842.364, -1842.364], mean action: 3.000 [3.000, 3.000],  loss: 16840336.000000, mae: 1374.522705, mean_q: -19.576988
 1308/5000: episode: 1308, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8125.487, mean reward: -8125.487 [-8125.487, -8125.487], mean action: 3.000 [3.000, 3.000],  loss: 17522112.000000, mae: 1239.518066, mean_q: -19.713749
 1309/5000: episode: 1309, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1807.707, mean reward: -1807.707 [-1807.707, -1807.707], mean action: 3.000 [3.000, 3.000],  loss: 21393788.000000, mae: 1383.948120, mean_q: -19.782875
 1310/5000: episode: 1310, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4217.160, mean reward: -4217.160 [-4217.160, -4217.160], mean action: 3.000 [3.000, 3.000],  loss: 20975268.000000, mae: 1276.701660, mean_q: -19.774191
 1311/5000: episode: 1311, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7124.901, mean reward: -7124.901 [-7124.901, -7124.901], mean action: 3.000 [3.000, 3.000],  loss: 23377576.000000, mae: 1461.253906, mean_q: -19.897175
 1312/5000: episode: 1312, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7507.932, mean reward: -7507.932 [-7507.932, -7507.932], mean action: 0.000 [0.000, 0.000],  loss: 18116418.000000, mae: 1346.947266, mean_q: -20.013435
 1313/5000: episode: 1313, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5631.000, mean reward: -5631.000 [-5631.000, -5631.000], mean action: 2.000 [2.000, 2.000],  loss: 17000414.000000, mae: 1224.263916, mean_q: -20.102139
 1314/5000: episode: 1314, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2295.894, mean reward: -2295.894 [-2295.894, -2295.894], mean action: 2.000 [2.000, 2.000],  loss: 16726140.000000, mae: 1233.292480, mean_q: -20.107323
 1315/5000: episode: 1315, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -623.985, mean reward: -623.985 [-623.985, -623.985], mean action: 2.000 [2.000, 2.000],  loss: 12287917.000000, mae: 1125.332520, mean_q: -20.191816
 1316/5000: episode: 1316, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5695.268, mean reward: -5695.268 [-5695.268, -5695.268], mean action: 3.000 [3.000, 3.000],  loss: 14285185.000000, mae: 1182.119751, mean_q: -20.236420
 1317/5000: episode: 1317, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1851.467, mean reward: -1851.467 [-1851.467, -1851.467], mean action: 2.000 [2.000, 2.000],  loss: 15565423.000000, mae: 1216.834717, mean_q: -20.241400
 1318/5000: episode: 1318, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4948.643, mean reward: -4948.643 [-4948.643, -4948.643], mean action: 2.000 [2.000, 2.000],  loss: 16662336.000000, mae: 1206.498779, mean_q: -20.273884
 1319/5000: episode: 1319, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1002.586, mean reward: -1002.586 [-1002.586, -1002.586], mean action: 2.000 [2.000, 2.000],  loss: 22878796.000000, mae: 1358.884766, mean_q: -20.282356
 1320/5000: episode: 1320, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1610.534, mean reward: -1610.534 [-1610.534, -1610.534], mean action: 2.000 [2.000, 2.000],  loss: 16443140.000000, mae: 1191.393677, mean_q: -20.373772
 1321/5000: episode: 1321, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3555.474, mean reward: -3555.474 [-3555.474, -3555.474], mean action: 2.000 [2.000, 2.000],  loss: 24541058.000000, mae: 1363.031250, mean_q: -20.430990
 1322/5000: episode: 1322, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2461.073, mean reward: -2461.073 [-2461.073, -2461.073], mean action: 2.000 [2.000, 2.000],  loss: 16712424.000000, mae: 1237.876465, mean_q: -20.473425
 1323/5000: episode: 1323, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6215.435, mean reward: -6215.435 [-6215.435, -6215.435], mean action: 2.000 [2.000, 2.000],  loss: 15668629.000000, mae: 1181.565674, mean_q: -20.495975
 1324/5000: episode: 1324, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -479.987, mean reward: -479.987 [-479.987, -479.987], mean action: 2.000 [2.000, 2.000],  loss: 14772750.000000, mae: 1128.646118, mean_q: -20.583948
 1325/5000: episode: 1325, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1540.938, mean reward: -1540.938 [-1540.938, -1540.938], mean action: 2.000 [2.000, 2.000],  loss: 24530212.000000, mae: 1435.185547, mean_q: -20.626320
 1326/5000: episode: 1326, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3553.656, mean reward: -3553.656 [-3553.656, -3553.656], mean action: 2.000 [2.000, 2.000],  loss: 16714104.000000, mae: 1292.418579, mean_q: -20.738819
 1327/5000: episode: 1327, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3079.896, mean reward: -3079.896 [-3079.896, -3079.896], mean action: 2.000 [2.000, 2.000],  loss: 15999863.000000, mae: 1180.680054, mean_q: -20.710413
 1328/5000: episode: 1328, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1211.527, mean reward: -1211.527 [-1211.527, -1211.527], mean action: 2.000 [2.000, 2.000],  loss: 11691386.000000, mae: 981.237000, mean_q: -20.991253
 1329/5000: episode: 1329, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2334.241, mean reward: -2334.241 [-2334.241, -2334.241], mean action: 2.000 [2.000, 2.000],  loss: 20658682.000000, mae: 1334.666504, mean_q: -20.857857
 1330/5000: episode: 1330, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1874.153, mean reward: -1874.153 [-1874.153, -1874.153], mean action: 2.000 [2.000, 2.000],  loss: 18188408.000000, mae: 1209.136719, mean_q: -20.876965
 1331/5000: episode: 1331, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3705.272, mean reward: -3705.272 [-3705.272, -3705.272], mean action: 3.000 [3.000, 3.000],  loss: 15181154.000000, mae: 1243.167603, mean_q: -20.992325
 1332/5000: episode: 1332, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -144.440, mean reward: -144.440 [-144.440, -144.440], mean action: 2.000 [2.000, 2.000],  loss: 20846568.000000, mae: 1349.560303, mean_q: -20.945549
 1333/5000: episode: 1333, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -1791.575, mean reward: -1791.575 [-1791.575, -1791.575], mean action: 2.000 [2.000, 2.000],  loss: 20361718.000000, mae: 1303.776367, mean_q: -21.094311
 1334/5000: episode: 1334, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5837.403, mean reward: -5837.403 [-5837.403, -5837.403], mean action: 2.000 [2.000, 2.000],  loss: 19831650.000000, mae: 1276.529663, mean_q: -21.080154
 1335/5000: episode: 1335, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -582.919, mean reward: -582.919 [-582.919, -582.919], mean action: 2.000 [2.000, 2.000],  loss: 17264956.000000, mae: 1260.291260, mean_q: -21.111212
 1336/5000: episode: 1336, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -185.341, mean reward: -185.341 [-185.341, -185.341], mean action: 2.000 [2.000, 2.000],  loss: 16000680.000000, mae: 1211.446289, mean_q: -21.209158
 1337/5000: episode: 1337, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -10937.972, mean reward: -10937.972 [-10937.972, -10937.972], mean action: 0.000 [0.000, 0.000],  loss: 23913822.000000, mae: 1485.234863, mean_q: -21.228819
 1338/5000: episode: 1338, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -2729.649, mean reward: -2729.649 [-2729.649, -2729.649], mean action: 2.000 [2.000, 2.000],  loss: 15039765.000000, mae: 1080.682983, mean_q: -21.335167
 1339/5000: episode: 1339, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4831.282, mean reward: -4831.282 [-4831.282, -4831.282], mean action: 2.000 [2.000, 2.000],  loss: 17079162.000000, mae: 1166.597412, mean_q: -21.465721
 1340/5000: episode: 1340, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2427.427, mean reward: -2427.427 [-2427.427, -2427.427], mean action: 2.000 [2.000, 2.000],  loss: 15713230.000000, mae: 1127.925781, mean_q: -21.399132
 1341/5000: episode: 1341, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3216.964, mean reward: -3216.964 [-3216.964, -3216.964], mean action: 1.000 [1.000, 1.000],  loss: 14811528.000000, mae: 1164.643433, mean_q: -21.520336
 1342/5000: episode: 1342, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1054.810, mean reward: -1054.810 [-1054.810, -1054.810], mean action: 2.000 [2.000, 2.000],  loss: 18326544.000000, mae: 1270.785767, mean_q: -21.553217
 1343/5000: episode: 1343, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1936.307, mean reward: -1936.307 [-1936.307, -1936.307], mean action: 2.000 [2.000, 2.000],  loss: 12321780.000000, mae: 1099.010742, mean_q: -21.658260
 1344/5000: episode: 1344, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2858.599, mean reward: -2858.599 [-2858.599, -2858.599], mean action: 2.000 [2.000, 2.000],  loss: 23284954.000000, mae: 1479.590576, mean_q: -21.647511
 1345/5000: episode: 1345, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2624.287, mean reward: -2624.287 [-2624.287, -2624.287], mean action: 2.000 [2.000, 2.000],  loss: 26260748.000000, mae: 1524.451050, mean_q: -21.646435
 1346/5000: episode: 1346, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -806.960, mean reward: -806.960 [-806.960, -806.960], mean action: 2.000 [2.000, 2.000],  loss: 15611444.000000, mae: 1146.714600, mean_q: -21.739815
 1347/5000: episode: 1347, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -582.730, mean reward: -582.730 [-582.730, -582.730], mean action: 2.000 [2.000, 2.000],  loss: 14758944.000000, mae: 1209.182007, mean_q: -21.829605
 1348/5000: episode: 1348, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -145.427, mean reward: -145.427 [-145.427, -145.427], mean action: 2.000 [2.000, 2.000],  loss: 19379500.000000, mae: 1373.511841, mean_q: -21.894524
 1349/5000: episode: 1349, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -889.020, mean reward: -889.020 [-889.020, -889.020], mean action: 2.000 [2.000, 2.000],  loss: 13795120.000000, mae: 1085.991577, mean_q: -21.911877
 1350/5000: episode: 1350, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -45.460, mean reward: -45.460 [-45.460, -45.460], mean action: 2.000 [2.000, 2.000],  loss: 15041564.000000, mae: 1199.775757, mean_q: -21.930763
 1351/5000: episode: 1351, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -4081.701, mean reward: -4081.701 [-4081.701, -4081.701], mean action: 2.000 [2.000, 2.000],  loss: 19845334.000000, mae: 1338.627319, mean_q: -21.976387
 1352/5000: episode: 1352, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4029.813, mean reward: -4029.813 [-4029.813, -4029.813], mean action: 2.000 [2.000, 2.000],  loss: 17549052.000000, mae: 1255.053467, mean_q: -22.042789
 1353/5000: episode: 1353, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2609.016, mean reward: -2609.016 [-2609.016, -2609.016], mean action: 2.000 [2.000, 2.000],  loss: 15491529.000000, mae: 1149.528076, mean_q: -22.210756
 1354/5000: episode: 1354, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5187.220, mean reward: -5187.220 [-5187.220, -5187.220], mean action: 2.000 [2.000, 2.000],  loss: 17816444.000000, mae: 1271.802246, mean_q: -22.177130
 1355/5000: episode: 1355, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1049.032, mean reward: -1049.032 [-1049.032, -1049.032], mean action: 2.000 [2.000, 2.000],  loss: 15475065.000000, mae: 1145.198730, mean_q: -22.283058
 1356/5000: episode: 1356, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -6582.249, mean reward: -6582.249 [-6582.249, -6582.249], mean action: 2.000 [2.000, 2.000],  loss: 19461902.000000, mae: 1274.424683, mean_q: -22.276920
 1357/5000: episode: 1357, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2918.735, mean reward: -2918.735 [-2918.735, -2918.735], mean action: 2.000 [2.000, 2.000],  loss: 15035650.000000, mae: 1162.573730, mean_q: -22.391546
 1358/5000: episode: 1358, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -6064.790, mean reward: -6064.790 [-6064.790, -6064.790], mean action: 2.000 [2.000, 2.000],  loss: 14318347.000000, mae: 1145.928223, mean_q: -22.353891
 1359/5000: episode: 1359, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -1716.257, mean reward: -1716.257 [-1716.257, -1716.257], mean action: 2.000 [2.000, 2.000],  loss: 17116534.000000, mae: 1323.213135, mean_q: -22.465374
 1360/5000: episode: 1360, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1439.611, mean reward: -1439.611 [-1439.611, -1439.611], mean action: 2.000 [2.000, 2.000],  loss: 22649500.000000, mae: 1351.056763, mean_q: -22.473618
 1361/5000: episode: 1361, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -1462.099, mean reward: -1462.099 [-1462.099, -1462.099], mean action: 2.000 [2.000, 2.000],  loss: 20506624.000000, mae: 1464.627441, mean_q: -22.603786
 1362/5000: episode: 1362, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1432.764, mean reward: -1432.764 [-1432.764, -1432.764], mean action: 2.000 [2.000, 2.000],  loss: 22581140.000000, mae: 1477.780396, mean_q: -22.622007
 1363/5000: episode: 1363, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3868.308, mean reward: -3868.308 [-3868.308, -3868.308], mean action: 2.000 [2.000, 2.000],  loss: 13292522.000000, mae: 1052.596680, mean_q: -22.701614
 1364/5000: episode: 1364, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -6581.679, mean reward: -6581.679 [-6581.679, -6581.679], mean action: 2.000 [2.000, 2.000],  loss: 21601160.000000, mae: 1428.970215, mean_q: -22.664888
 1365/5000: episode: 1365, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2565.523, mean reward: -2565.523 [-2565.523, -2565.523], mean action: 2.000 [2.000, 2.000],  loss: 22035260.000000, mae: 1424.791748, mean_q: -22.745316
 1366/5000: episode: 1366, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -753.888, mean reward: -753.888 [-753.888, -753.888], mean action: 2.000 [2.000, 2.000],  loss: 15188270.000000, mae: 1108.434570, mean_q: -22.876534
 1367/5000: episode: 1367, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5.577, mean reward: -5.577 [-5.577, -5.577], mean action: 2.000 [2.000, 2.000],  loss: 18939612.000000, mae: 1274.447510, mean_q: -22.906755
 1368/5000: episode: 1368, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4816.903, mean reward: -4816.903 [-4816.903, -4816.903], mean action: 0.000 [0.000, 0.000],  loss: 13041800.000000, mae: 1061.049438, mean_q: -22.976713
 1369/5000: episode: 1369, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3152.787, mean reward: -3152.787 [-3152.787, -3152.787], mean action: 2.000 [2.000, 2.000],  loss: 13622352.000000, mae: 1075.802246, mean_q: -23.011236
 1370/5000: episode: 1370, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2614.065, mean reward: -2614.065 [-2614.065, -2614.065], mean action: 2.000 [2.000, 2.000],  loss: 17083524.000000, mae: 1211.812744, mean_q: -23.087555
 1371/5000: episode: 1371, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2120.662, mean reward: -2120.662 [-2120.662, -2120.662], mean action: 2.000 [2.000, 2.000],  loss: 20277944.000000, mae: 1395.787354, mean_q: -23.139576
 1372/5000: episode: 1372, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6500.355, mean reward: -6500.355 [-6500.355, -6500.355], mean action: 2.000 [2.000, 2.000],  loss: 21004858.000000, mae: 1437.804565, mean_q: -23.139231
 1373/5000: episode: 1373, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4969.295, mean reward: -4969.295 [-4969.295, -4969.295], mean action: 1.000 [1.000, 1.000],  loss: 13796334.000000, mae: 1084.414307, mean_q: -23.259842
 1374/5000: episode: 1374, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5824.132, mean reward: -5824.132 [-5824.132, -5824.132], mean action: 1.000 [1.000, 1.000],  loss: 11847532.000000, mae: 982.241333, mean_q: -23.427177
 1375/5000: episode: 1375, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3712.408, mean reward: -3712.408 [-3712.408, -3712.408], mean action: 1.000 [1.000, 1.000],  loss: 17850024.000000, mae: 1212.236572, mean_q: -23.427376
 1376/5000: episode: 1376, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5176.332, mean reward: -5176.332 [-5176.332, -5176.332], mean action: 1.000 [1.000, 1.000],  loss: 21871504.000000, mae: 1357.970337, mean_q: -23.408184
 1377/5000: episode: 1377, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5957.632, mean reward: -5957.632 [-5957.632, -5957.632], mean action: 1.000 [1.000, 1.000],  loss: 21950978.000000, mae: 1317.789307, mean_q: -23.405632
 1378/5000: episode: 1378, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1721.780, mean reward: -1721.780 [-1721.780, -1721.780], mean action: 1.000 [1.000, 1.000],  loss: 11713711.000000, mae: 996.806458, mean_q: -23.710915
 1379/5000: episode: 1379, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -11556.973, mean reward: -11556.973 [-11556.973, -11556.973], mean action: 1.000 [1.000, 1.000],  loss: 14226894.000000, mae: 1165.334717, mean_q: -23.504402
 1380/5000: episode: 1380, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1382.892, mean reward: -1382.892 [-1382.892, -1382.892], mean action: 2.000 [2.000, 2.000],  loss: 18320852.000000, mae: 1306.199707, mean_q: -23.764544
 1381/5000: episode: 1381, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4745.903, mean reward: -4745.903 [-4745.903, -4745.903], mean action: 1.000 [1.000, 1.000],  loss: 12601984.000000, mae: 1017.492371, mean_q: -23.723728
 1382/5000: episode: 1382, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3204.783, mean reward: -3204.783 [-3204.783, -3204.783], mean action: 2.000 [2.000, 2.000],  loss: 21960480.000000, mae: 1479.903687, mean_q: -23.640652
 1383/5000: episode: 1383, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4400.834, mean reward: -4400.834 [-4400.834, -4400.834], mean action: 1.000 [1.000, 1.000],  loss: 16448678.000000, mae: 1208.645020, mean_q: -23.888365
 1384/5000: episode: 1384, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4142.422, mean reward: -4142.422 [-4142.422, -4142.422], mean action: 1.000 [1.000, 1.000],  loss: 16438610.000000, mae: 1187.649902, mean_q: -23.856041
 1385/5000: episode: 1385, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2270.328, mean reward: -2270.328 [-2270.328, -2270.328], mean action: 1.000 [1.000, 1.000],  loss: 19920862.000000, mae: 1341.246582, mean_q: -23.902365
 1386/5000: episode: 1386, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1758.519, mean reward: -1758.519 [-1758.519, -1758.519], mean action: 1.000 [1.000, 1.000],  loss: 15855423.000000, mae: 1159.968018, mean_q: -24.017424
 1387/5000: episode: 1387, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -1427.071, mean reward: -1427.071 [-1427.071, -1427.071], mean action: 1.000 [1.000, 1.000],  loss: 19284236.000000, mae: 1334.427002, mean_q: -24.091324
 1388/5000: episode: 1388, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5791.143, mean reward: -5791.143 [-5791.143, -5791.143], mean action: 1.000 [1.000, 1.000],  loss: 15111817.000000, mae: 1119.460449, mean_q: -24.169357
 1389/5000: episode: 1389, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -7148.847, mean reward: -7148.847 [-7148.847, -7148.847], mean action: 1.000 [1.000, 1.000],  loss: 17488592.000000, mae: 1208.391846, mean_q: -24.138306
 1390/5000: episode: 1390, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4090.502, mean reward: -4090.502 [-4090.502, -4090.502], mean action: 1.000 [1.000, 1.000],  loss: 15491189.000000, mae: 1123.305420, mean_q: -24.247902
 1391/5000: episode: 1391, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -786.240, mean reward: -786.240 [-786.240, -786.240], mean action: 2.000 [2.000, 2.000],  loss: 19868496.000000, mae: 1312.815674, mean_q: -24.285921
 1392/5000: episode: 1392, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1490.977, mean reward: -1490.977 [-1490.977, -1490.977], mean action: 1.000 [1.000, 1.000],  loss: 19707766.000000, mae: 1409.349365, mean_q: -24.330021
 1393/5000: episode: 1393, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4725.952, mean reward: -4725.952 [-4725.952, -4725.952], mean action: 1.000 [1.000, 1.000],  loss: 16797300.000000, mae: 1242.921631, mean_q: -24.334620
 1394/5000: episode: 1394, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7763.130, mean reward: -7763.130 [-7763.130, -7763.130], mean action: 1.000 [1.000, 1.000],  loss: 17829804.000000, mae: 1278.329102, mean_q: -24.480354
 1395/5000: episode: 1395, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -940.072, mean reward: -940.072 [-940.072, -940.072], mean action: 1.000 [1.000, 1.000],  loss: 21160948.000000, mae: 1319.545410, mean_q: -24.495438
 1396/5000: episode: 1396, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2884.138, mean reward: -2884.138 [-2884.138, -2884.138], mean action: 2.000 [2.000, 2.000],  loss: 15463640.000000, mae: 1217.250854, mean_q: -24.559650
 1397/5000: episode: 1397, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2098.170, mean reward: -2098.170 [-2098.170, -2098.170], mean action: 1.000 [1.000, 1.000],  loss: 17830094.000000, mae: 1276.504028, mean_q: -24.603962
 1398/5000: episode: 1398, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3589.492, mean reward: -3589.492 [-3589.492, -3589.492], mean action: 1.000 [1.000, 1.000],  loss: 15930000.000000, mae: 1099.450928, mean_q: -24.715347
 1399/5000: episode: 1399, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2065.509, mean reward: -2065.509 [-2065.509, -2065.509], mean action: 1.000 [1.000, 1.000],  loss: 19843276.000000, mae: 1315.353516, mean_q: -24.653240
 1400/5000: episode: 1400, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4695.354, mean reward: -4695.354 [-4695.354, -4695.354], mean action: 2.000 [2.000, 2.000],  loss: 19974106.000000, mae: 1387.892578, mean_q: -24.790634
 1401/5000: episode: 1401, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3718.524, mean reward: -3718.524 [-3718.524, -3718.524], mean action: 1.000 [1.000, 1.000],  loss: 21461856.000000, mae: 1401.937256, mean_q: -24.795124
 1402/5000: episode: 1402, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2840.502, mean reward: -2840.502 [-2840.502, -2840.502], mean action: 1.000 [1.000, 1.000],  loss: 17923866.000000, mae: 1288.415771, mean_q: -24.871239
 1403/5000: episode: 1403, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4096.668, mean reward: -4096.668 [-4096.668, -4096.668], mean action: 1.000 [1.000, 1.000],  loss: 17869444.000000, mae: 1270.505005, mean_q: -24.923941
 1404/5000: episode: 1404, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10368.116, mean reward: -10368.116 [-10368.116, -10368.116], mean action: 1.000 [1.000, 1.000],  loss: 15148375.000000, mae: 1170.337891, mean_q: -24.967728
 1405/5000: episode: 1405, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2266.274, mean reward: -2266.274 [-2266.274, -2266.274], mean action: 1.000 [1.000, 1.000],  loss: 12399233.000000, mae: 1058.816895, mean_q: -25.095875
 1406/5000: episode: 1406, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5716.076, mean reward: -5716.076 [-5716.076, -5716.076], mean action: 1.000 [1.000, 1.000],  loss: 16925656.000000, mae: 1239.364746, mean_q: -25.150719
 1407/5000: episode: 1407, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -9335.930, mean reward: -9335.930 [-9335.930, -9335.930], mean action: 1.000 [1.000, 1.000],  loss: 14996725.000000, mae: 1177.324951, mean_q: -25.212502
 1408/5000: episode: 1408, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -8832.327, mean reward: -8832.327 [-8832.327, -8832.327], mean action: 1.000 [1.000, 1.000],  loss: 16258747.000000, mae: 1226.321655, mean_q: -25.211769
 1409/5000: episode: 1409, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5145.493, mean reward: -5145.493 [-5145.493, -5145.493], mean action: 1.000 [1.000, 1.000],  loss: 18269550.000000, mae: 1212.375122, mean_q: -25.230501
 1410/5000: episode: 1410, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6139.423, mean reward: -6139.423 [-6139.423, -6139.423], mean action: 1.000 [1.000, 1.000],  loss: 13728092.000000, mae: 1149.831421, mean_q: -25.443630
 1411/5000: episode: 1411, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10957.736, mean reward: -10957.736 [-10957.736, -10957.736], mean action: 1.000 [1.000, 1.000],  loss: 14285520.000000, mae: 1179.290039, mean_q: -25.466154
 1412/5000: episode: 1412, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8572.066, mean reward: -8572.066 [-8572.066, -8572.066], mean action: 1.000 [1.000, 1.000],  loss: 26507992.000000, mae: 1414.294678, mean_q: -25.391308
 1413/5000: episode: 1413, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1097.847, mean reward: -1097.847 [-1097.847, -1097.847], mean action: 1.000 [1.000, 1.000],  loss: 16428948.000000, mae: 1223.716797, mean_q: -25.547447
 1414/5000: episode: 1414, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8800.228, mean reward: -8800.228 [-8800.228, -8800.228], mean action: 1.000 [1.000, 1.000],  loss: 19932064.000000, mae: 1364.790894, mean_q: -25.608673
 1415/5000: episode: 1415, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8369.195, mean reward: -8369.195 [-8369.195, -8369.195], mean action: 0.000 [0.000, 0.000],  loss: 13493620.000000, mae: 1110.891846, mean_q: -25.785992
 1416/5000: episode: 1416, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1266.183, mean reward: -1266.183 [-1266.183, -1266.183], mean action: 1.000 [1.000, 1.000],  loss: 20690256.000000, mae: 1350.364014, mean_q: -25.604927
 1417/5000: episode: 1417, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6869.094, mean reward: -6869.094 [-6869.094, -6869.094], mean action: 1.000 [1.000, 1.000],  loss: 17296946.000000, mae: 1278.625977, mean_q: -25.680916
 1418/5000: episode: 1418, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2144.814, mean reward: -2144.814 [-2144.814, -2144.814], mean action: 1.000 [1.000, 1.000],  loss: 14951426.000000, mae: 1195.380859, mean_q: -25.792820
 1419/5000: episode: 1419, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1515.464, mean reward: -1515.464 [-1515.464, -1515.464], mean action: 1.000 [1.000, 1.000],  loss: 20089956.000000, mae: 1322.935303, mean_q: -25.898724
 1420/5000: episode: 1420, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3984.187, mean reward: -3984.187 [-3984.187, -3984.187], mean action: 1.000 [1.000, 1.000],  loss: 21331668.000000, mae: 1424.979004, mean_q: -25.896074
 1421/5000: episode: 1421, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3804.418, mean reward: -3804.418 [-3804.418, -3804.418], mean action: 1.000 [1.000, 1.000],  loss: 21802082.000000, mae: 1429.635254, mean_q: -25.995955
 1422/5000: episode: 1422, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1555.842, mean reward: -1555.842 [-1555.842, -1555.842], mean action: 2.000 [2.000, 2.000],  loss: 19456292.000000, mae: 1307.291748, mean_q: -26.024628
 1423/5000: episode: 1423, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -10697.647, mean reward: -10697.647 [-10697.647, -10697.647], mean action: 1.000 [1.000, 1.000],  loss: 18855296.000000, mae: 1327.621094, mean_q: -26.188314
 1424/5000: episode: 1424, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5167.254, mean reward: -5167.254 [-5167.254, -5167.254], mean action: 1.000 [1.000, 1.000],  loss: 19821082.000000, mae: 1309.576660, mean_q: -26.191418
 1425/5000: episode: 1425, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4799.382, mean reward: -4799.382 [-4799.382, -4799.382], mean action: 1.000 [1.000, 1.000],  loss: 12223678.000000, mae: 1081.583740, mean_q: -26.396086
 1426/5000: episode: 1426, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6020.281, mean reward: -6020.281 [-6020.281, -6020.281], mean action: 1.000 [1.000, 1.000],  loss: 21266260.000000, mae: 1355.732422, mean_q: -26.385059
 1427/5000: episode: 1427, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6854.414, mean reward: -6854.414 [-6854.414, -6854.414], mean action: 1.000 [1.000, 1.000],  loss: 13253050.000000, mae: 1040.958740, mean_q: -26.437187
 1428/5000: episode: 1428, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1533.971, mean reward: -1533.971 [-1533.971, -1533.971], mean action: 1.000 [1.000, 1.000],  loss: 22631244.000000, mae: 1331.783447, mean_q: -26.372124
 1429/5000: episode: 1429, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6089.793, mean reward: -6089.793 [-6089.793, -6089.793], mean action: 1.000 [1.000, 1.000],  loss: 17269096.000000, mae: 1178.095825, mean_q: -26.588490
 1430/5000: episode: 1430, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3860.769, mean reward: -3860.769 [-3860.769, -3860.769], mean action: 1.000 [1.000, 1.000],  loss: 18931632.000000, mae: 1283.250488, mean_q: -26.574249
 1431/5000: episode: 1431, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -8813.349, mean reward: -8813.349 [-8813.349, -8813.349], mean action: 1.000 [1.000, 1.000],  loss: 16890448.000000, mae: 1223.497559, mean_q: -26.781754
 1432/5000: episode: 1432, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -12489.222, mean reward: -12489.222 [-12489.222, -12489.222], mean action: 1.000 [1.000, 1.000],  loss: 13958298.000000, mae: 1126.186768, mean_q: -26.733891
 1433/5000: episode: 1433, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -146.670, mean reward: -146.670 [-146.670, -146.670], mean action: 3.000 [3.000, 3.000],  loss: 19864280.000000, mae: 1363.486816, mean_q: -26.666504
 1434/5000: episode: 1434, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -8988.774, mean reward: -8988.774 [-8988.774, -8988.774], mean action: 1.000 [1.000, 1.000],  loss: 19797716.000000, mae: 1298.115967, mean_q: -26.828503
 1435/5000: episode: 1435, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -6767.972, mean reward: -6767.972 [-6767.972, -6767.972], mean action: 1.000 [1.000, 1.000],  loss: 15348527.000000, mae: 1257.870117, mean_q: -26.906948
 1436/5000: episode: 1436, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2225.070, mean reward: -2225.070 [-2225.070, -2225.070], mean action: 1.000 [1.000, 1.000],  loss: 16688970.000000, mae: 1233.769165, mean_q: -26.919939
 1437/5000: episode: 1437, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5385.176, mean reward: -5385.176 [-5385.176, -5385.176], mean action: 2.000 [2.000, 2.000],  loss: 25005518.000000, mae: 1360.607300, mean_q: -26.984583
 1438/5000: episode: 1438, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8015.999, mean reward: -8015.999 [-8015.999, -8015.999], mean action: 1.000 [1.000, 1.000],  loss: 17259784.000000, mae: 1260.647339, mean_q: -27.089540
 1439/5000: episode: 1439, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3232.461, mean reward: -3232.461 [-3232.461, -3232.461], mean action: 2.000 [2.000, 2.000],  loss: 11846176.000000, mae: 1073.607422, mean_q: -27.186359
 1440/5000: episode: 1440, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2101.179, mean reward: -2101.179 [-2101.179, -2101.179], mean action: 2.000 [2.000, 2.000],  loss: 16451847.000000, mae: 1215.182617, mean_q: -27.238714
 1441/5000: episode: 1441, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2868.707, mean reward: -2868.707 [-2868.707, -2868.707], mean action: 2.000 [2.000, 2.000],  loss: 21440544.000000, mae: 1372.478394, mean_q: -27.277367
 1442/5000: episode: 1442, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2215.568, mean reward: -2215.568 [-2215.568, -2215.568], mean action: 2.000 [2.000, 2.000],  loss: 16059080.000000, mae: 1183.086426, mean_q: -27.398727
 1443/5000: episode: 1443, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -840.637, mean reward: -840.637 [-840.637, -840.637], mean action: 2.000 [2.000, 2.000],  loss: 23148608.000000, mae: 1457.150391, mean_q: -27.320063
 1444/5000: episode: 1444, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2018.954, mean reward: -2018.954 [-2018.954, -2018.954], mean action: 2.000 [2.000, 2.000],  loss: 14719312.000000, mae: 1109.941772, mean_q: -27.460524
 1445/5000: episode: 1445, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -682.559, mean reward: -682.559 [-682.559, -682.559], mean action: 2.000 [2.000, 2.000],  loss: 14830059.000000, mae: 1184.967773, mean_q: -27.516901
 1446/5000: episode: 1446, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5952.231, mean reward: -5952.231 [-5952.231, -5952.231], mean action: 1.000 [1.000, 1.000],  loss: 15631302.000000, mae: 1290.362915, mean_q: -27.598164
 1447/5000: episode: 1447, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5115.009, mean reward: -5115.009 [-5115.009, -5115.009], mean action: 2.000 [2.000, 2.000],  loss: 17117372.000000, mae: 1230.989624, mean_q: -27.648556
 1448/5000: episode: 1448, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1362.296, mean reward: -1362.296 [-1362.296, -1362.296], mean action: 2.000 [2.000, 2.000],  loss: 21008248.000000, mae: 1422.143433, mean_q: -27.655300
 1449/5000: episode: 1449, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8794.567, mean reward: -8794.567 [-8794.567, -8794.567], mean action: 2.000 [2.000, 2.000],  loss: 19523056.000000, mae: 1293.399658, mean_q: -27.736469
 1450/5000: episode: 1450, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3220.979, mean reward: -3220.979 [-3220.979, -3220.979], mean action: 2.000 [2.000, 2.000],  loss: 21625970.000000, mae: 1405.768066, mean_q: -27.858807
 1451/5000: episode: 1451, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1330.953, mean reward: -1330.953 [-1330.953, -1330.953], mean action: 2.000 [2.000, 2.000],  loss: 15869702.000000, mae: 1201.564941, mean_q: -27.894585
 1452/5000: episode: 1452, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -289.374, mean reward: -289.374 [-289.374, -289.374], mean action: 2.000 [2.000, 2.000],  loss: 13389299.000000, mae: 1062.208008, mean_q: -27.910748
 1453/5000: episode: 1453, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1163.208, mean reward: -1163.208 [-1163.208, -1163.208], mean action: 2.000 [2.000, 2.000],  loss: 20127526.000000, mae: 1389.960938, mean_q: -27.915569
 1454/5000: episode: 1454, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -2503.727, mean reward: -2503.727 [-2503.727, -2503.727], mean action: 2.000 [2.000, 2.000],  loss: 22579716.000000, mae: 1485.777588, mean_q: -28.003078
 1455/5000: episode: 1455, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -742.411, mean reward: -742.411 [-742.411, -742.411], mean action: 2.000 [2.000, 2.000],  loss: 10987297.000000, mae: 959.331543, mean_q: -28.179054
 1456/5000: episode: 1456, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -2012.380, mean reward: -2012.380 [-2012.380, -2012.380], mean action: 2.000 [2.000, 2.000],  loss: 9760782.000000, mae: 911.436035, mean_q: -28.257669
 1457/5000: episode: 1457, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3223.664, mean reward: -3223.664 [-3223.664, -3223.664], mean action: 2.000 [2.000, 2.000],  loss: 18100052.000000, mae: 1302.838501, mean_q: -28.193485
 1458/5000: episode: 1458, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -264.607, mean reward: -264.607 [-264.607, -264.607], mean action: 2.000 [2.000, 2.000],  loss: 16749774.000000, mae: 1250.381958, mean_q: -28.297550
 1459/5000: episode: 1459, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2448.669, mean reward: -2448.669 [-2448.669, -2448.669], mean action: 2.000 [2.000, 2.000],  loss: 16177412.000000, mae: 1249.271240, mean_q: -28.360338
 1460/5000: episode: 1460, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2234.849, mean reward: -2234.849 [-2234.849, -2234.849], mean action: 2.000 [2.000, 2.000],  loss: 25005720.000000, mae: 1518.666626, mean_q: -28.375816
 1461/5000: episode: 1461, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2044.183, mean reward: -2044.183 [-2044.183, -2044.183], mean action: 2.000 [2.000, 2.000],  loss: 21368868.000000, mae: 1349.701904, mean_q: -28.501282
 1462/5000: episode: 1462, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -4533.931, mean reward: -4533.931 [-4533.931, -4533.931], mean action: 0.000 [0.000, 0.000],  loss: 16040965.000000, mae: 1258.390137, mean_q: -28.686882
 1463/5000: episode: 1463, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -810.573, mean reward: -810.573 [-810.573, -810.573], mean action: 2.000 [2.000, 2.000],  loss: 19308152.000000, mae: 1264.714111, mean_q: -28.637941
 1464/5000: episode: 1464, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2645.619, mean reward: -2645.619 [-2645.619, -2645.619], mean action: 2.000 [2.000, 2.000],  loss: 16445590.000000, mae: 1236.407471, mean_q: -28.614967
 1465/5000: episode: 1465, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -790.209, mean reward: -790.209 [-790.209, -790.209], mean action: 2.000 [2.000, 2.000],  loss: 20987676.000000, mae: 1316.408081, mean_q: -28.864586
 1466/5000: episode: 1466, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1998.324, mean reward: -1998.324 [-1998.324, -1998.324], mean action: 2.000 [2.000, 2.000],  loss: 19387772.000000, mae: 1352.699951, mean_q: -28.790497
 1467/5000: episode: 1467, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1847.180, mean reward: -1847.180 [-1847.180, -1847.180], mean action: 2.000 [2.000, 2.000],  loss: 16757450.000000, mae: 1149.564087, mean_q: -28.852465
 1468/5000: episode: 1468, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1468.014, mean reward: -1468.014 [-1468.014, -1468.014], mean action: 3.000 [3.000, 3.000],  loss: 12490932.000000, mae: 1014.572388, mean_q: -28.967493
 1469/5000: episode: 1469, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -962.128, mean reward: -962.128 [-962.128, -962.128], mean action: 2.000 [2.000, 2.000],  loss: 21529206.000000, mae: 1392.734985, mean_q: -28.905003
 1470/5000: episode: 1470, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2769.587, mean reward: -2769.587 [-2769.587, -2769.587], mean action: 2.000 [2.000, 2.000],  loss: 15013991.000000, mae: 1073.608765, mean_q: -29.067249
 1471/5000: episode: 1471, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3483.157, mean reward: -3483.157 [-3483.157, -3483.157], mean action: 2.000 [2.000, 2.000],  loss: 25002250.000000, mae: 1573.395752, mean_q: -29.138620
 1472/5000: episode: 1472, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1493.745, mean reward: -1493.745 [-1493.745, -1493.745], mean action: 2.000 [2.000, 2.000],  loss: 16069747.000000, mae: 1176.782715, mean_q: -29.228977
 1473/5000: episode: 1473, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -135.990, mean reward: -135.990 [-135.990, -135.990], mean action: 2.000 [2.000, 2.000],  loss: 14327040.000000, mae: 1135.648926, mean_q: -29.321228
 1474/5000: episode: 1474, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1604.465, mean reward: -1604.465 [-1604.465, -1604.465], mean action: 2.000 [2.000, 2.000],  loss: 13988576.000000, mae: 1104.404053, mean_q: -29.397516
 1475/5000: episode: 1475, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1360.896, mean reward: -1360.896 [-1360.896, -1360.896], mean action: 2.000 [2.000, 2.000],  loss: 21592400.000000, mae: 1376.299072, mean_q: -29.309299
 1476/5000: episode: 1476, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2892.689, mean reward: -2892.689 [-2892.689, -2892.689], mean action: 2.000 [2.000, 2.000],  loss: 11557447.000000, mae: 957.300903, mean_q: -29.452961
 1477/5000: episode: 1477, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2189.897, mean reward: -2189.897 [-2189.897, -2189.897], mean action: 2.000 [2.000, 2.000],  loss: 20250724.000000, mae: 1437.300293, mean_q: -29.544853
 1478/5000: episode: 1478, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9563.369, mean reward: -9563.369 [-9563.369, -9563.369], mean action: 1.000 [1.000, 1.000],  loss: 14969753.000000, mae: 1163.669678, mean_q: -29.548328
 1479/5000: episode: 1479, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -617.459, mean reward: -617.459 [-617.459, -617.459], mean action: 2.000 [2.000, 2.000],  loss: 19079608.000000, mae: 1230.142334, mean_q: -29.620287
 1480/5000: episode: 1480, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2189.907, mean reward: -2189.907 [-2189.907, -2189.907], mean action: 2.000 [2.000, 2.000],  loss: 14647533.000000, mae: 1146.983398, mean_q: -29.740479
 1481/5000: episode: 1481, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3223.663, mean reward: -3223.663 [-3223.663, -3223.663], mean action: 2.000 [2.000, 2.000],  loss: 17252672.000000, mae: 1330.715820, mean_q: -29.797462
 1482/5000: episode: 1482, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2794.914, mean reward: -2794.914 [-2794.914, -2794.914], mean action: 2.000 [2.000, 2.000],  loss: 17519452.000000, mae: 1217.639404, mean_q: -29.754181
 1483/5000: episode: 1483, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1432.078, mean reward: -1432.078 [-1432.078, -1432.078], mean action: 2.000 [2.000, 2.000],  loss: 13566967.000000, mae: 1104.185547, mean_q: -29.855015
 1484/5000: episode: 1484, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -324.129, mean reward: -324.129 [-324.129, -324.129], mean action: 2.000 [2.000, 2.000],  loss: 9929125.000000, mae: 882.672485, mean_q: -30.034613
 1485/5000: episode: 1485, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -447.030, mean reward: -447.030 [-447.030, -447.030], mean action: 2.000 [2.000, 2.000],  loss: 19435472.000000, mae: 1252.714722, mean_q: -30.014069
 1486/5000: episode: 1486, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3455.599, mean reward: -3455.599 [-3455.599, -3455.599], mean action: 1.000 [1.000, 1.000],  loss: 21099876.000000, mae: 1401.299438, mean_q: -30.084671
 1487/5000: episode: 1487, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5040.285, mean reward: -5040.285 [-5040.285, -5040.285], mean action: 1.000 [1.000, 1.000],  loss: 17360562.000000, mae: 1302.923584, mean_q: -30.183456
 1488/5000: episode: 1488, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1056.003, mean reward: -1056.003 [-1056.003, -1056.003], mean action: 2.000 [2.000, 2.000],  loss: 22269854.000000, mae: 1419.930786, mean_q: -30.127161
 1489/5000: episode: 1489, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5991.187, mean reward: -5991.187 [-5991.187, -5991.187], mean action: 1.000 [1.000, 1.000],  loss: 18737724.000000, mae: 1308.927002, mean_q: -30.315784
 1490/5000: episode: 1490, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1367.769, mean reward: -1367.769 [-1367.769, -1367.769], mean action: 1.000 [1.000, 1.000],  loss: 19701112.000000, mae: 1317.793335, mean_q: -30.339396
 1491/5000: episode: 1491, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1071.158, mean reward: -1071.158 [-1071.158, -1071.158], mean action: 1.000 [1.000, 1.000],  loss: 12586245.000000, mae: 1120.244873, mean_q: -30.535927
 1492/5000: episode: 1492, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4792.259, mean reward: -4792.259 [-4792.259, -4792.259], mean action: 1.000 [1.000, 1.000],  loss: 13719398.000000, mae: 1060.496216, mean_q: -30.397575
 1493/5000: episode: 1493, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6720.978, mean reward: -6720.978 [-6720.978, -6720.978], mean action: 3.000 [3.000, 3.000],  loss: 18230088.000000, mae: 1238.711792, mean_q: -30.576992
 1494/5000: episode: 1494, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5760.855, mean reward: -5760.855 [-5760.855, -5760.855], mean action: 1.000 [1.000, 1.000],  loss: 21834562.000000, mae: 1293.136230, mean_q: -30.589046
 1495/5000: episode: 1495, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5372.932, mean reward: -5372.932 [-5372.932, -5372.932], mean action: 1.000 [1.000, 1.000],  loss: 21398552.000000, mae: 1297.483398, mean_q: -30.815075
 1496/5000: episode: 1496, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -504.867, mean reward: -504.867 [-504.867, -504.867], mean action: 1.000 [1.000, 1.000],  loss: 16392204.000000, mae: 1230.758057, mean_q: -30.686691
 1497/5000: episode: 1497, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -7975.016, mean reward: -7975.016 [-7975.016, -7975.016], mean action: 1.000 [1.000, 1.000],  loss: 25585384.000000, mae: 1540.647095, mean_q: -30.696239
 1498/5000: episode: 1498, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6468.700, mean reward: -6468.700 [-6468.700, -6468.700], mean action: 1.000 [1.000, 1.000],  loss: 16487201.000000, mae: 1157.944580, mean_q: -30.751078
 1499/5000: episode: 1499, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2489.850, mean reward: -2489.850 [-2489.850, -2489.850], mean action: 2.000 [2.000, 2.000],  loss: 20308544.000000, mae: 1327.870850, mean_q: -30.806789
 1500/5000: episode: 1500, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -758.882, mean reward: -758.882 [-758.882, -758.882], mean action: 2.000 [2.000, 2.000],  loss: 11183112.000000, mae: 1060.078613, mean_q: -31.090714
 1501/5000: episode: 1501, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1072.343, mean reward: -1072.343 [-1072.343, -1072.343], mean action: 2.000 [2.000, 2.000],  loss: 20641208.000000, mae: 1309.925293, mean_q: -31.186098
 1502/5000: episode: 1502, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7414.822, mean reward: -7414.822 [-7414.822, -7414.822], mean action: 1.000 [1.000, 1.000],  loss: 18968856.000000, mae: 1358.499023, mean_q: -31.155157
 1503/5000: episode: 1503, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -12561.978, mean reward: -12561.978 [-12561.978, -12561.978], mean action: 1.000 [1.000, 1.000],  loss: 19086094.000000, mae: 1247.782471, mean_q: -31.198263
 1504/5000: episode: 1504, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1903.087, mean reward: -1903.087 [-1903.087, -1903.087], mean action: 2.000 [2.000, 2.000],  loss: 17385324.000000, mae: 1246.160889, mean_q: -31.169497
 1505/5000: episode: 1505, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3463.090, mean reward: -3463.090 [-3463.090, -3463.090], mean action: 0.000 [0.000, 0.000],  loss: 21794442.000000, mae: 1346.451660, mean_q: -31.433140
 1506/5000: episode: 1506, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3398.395, mean reward: -3398.395 [-3398.395, -3398.395], mean action: 1.000 [1.000, 1.000],  loss: 16435710.000000, mae: 1215.155273, mean_q: -31.335190
 1507/5000: episode: 1507, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6193.948, mean reward: -6193.948 [-6193.948, -6193.948], mean action: 1.000 [1.000, 1.000],  loss: 20116880.000000, mae: 1361.930908, mean_q: -31.414742
 1508/5000: episode: 1508, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8236.039, mean reward: -8236.039 [-8236.039, -8236.039], mean action: 1.000 [1.000, 1.000],  loss: 17217028.000000, mae: 1284.799805, mean_q: -31.632616
 1509/5000: episode: 1509, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2440.299, mean reward: -2440.299 [-2440.299, -2440.299], mean action: 1.000 [1.000, 1.000],  loss: 18800754.000000, mae: 1295.224121, mean_q: -31.559267
 1510/5000: episode: 1510, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -385.961, mean reward: -385.961 [-385.961, -385.961], mean action: 2.000 [2.000, 2.000],  loss: 24932976.000000, mae: 1535.889648, mean_q: -31.545361
 1511/5000: episode: 1511, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1503.777, mean reward: -1503.777 [-1503.777, -1503.777], mean action: 2.000 [2.000, 2.000],  loss: 17320520.000000, mae: 1281.795410, mean_q: -31.743649
 1512/5000: episode: 1512, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -823.043, mean reward: -823.043 [-823.043, -823.043], mean action: 2.000 [2.000, 2.000],  loss: 14610358.000000, mae: 1192.568237, mean_q: -31.822227
 1513/5000: episode: 1513, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6473.748, mean reward: -6473.748 [-6473.748, -6473.748], mean action: 1.000 [1.000, 1.000],  loss: 14486187.000000, mae: 1068.134277, mean_q: -31.930750
 1514/5000: episode: 1514, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -6524.497, mean reward: -6524.497 [-6524.497, -6524.497], mean action: 1.000 [1.000, 1.000],  loss: 23272836.000000, mae: 1410.839355, mean_q: -31.899410
 1515/5000: episode: 1515, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -746.641, mean reward: -746.641 [-746.641, -746.641], mean action: 2.000 [2.000, 2.000],  loss: 15477108.000000, mae: 1104.151245, mean_q: -32.127598
 1516/5000: episode: 1516, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -717.902, mean reward: -717.902 [-717.902, -717.902], mean action: 2.000 [2.000, 2.000],  loss: 17810880.000000, mae: 1162.764648, mean_q: -32.108700
 1517/5000: episode: 1517, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6711.247, mean reward: -6711.247 [-6711.247, -6711.247], mean action: 1.000 [1.000, 1.000],  loss: 19818004.000000, mae: 1331.654541, mean_q: -32.156227
 1518/5000: episode: 1518, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -466.356, mean reward: -466.356 [-466.356, -466.356], mean action: 2.000 [2.000, 2.000],  loss: 12476366.000000, mae: 1091.927734, mean_q: -32.266701
 1519/5000: episode: 1519, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2953.010, mean reward: -2953.010 [-2953.010, -2953.010], mean action: 1.000 [1.000, 1.000],  loss: 15122700.000000, mae: 1179.265137, mean_q: -32.298199
 1520/5000: episode: 1520, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -953.814, mean reward: -953.814 [-953.814, -953.814], mean action: 2.000 [2.000, 2.000],  loss: 16926402.000000, mae: 1187.824219, mean_q: -32.527634
 1521/5000: episode: 1521, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7747.054, mean reward: -7747.054 [-7747.054, -7747.054], mean action: 1.000 [1.000, 1.000],  loss: 22218722.000000, mae: 1398.917725, mean_q: -32.515663
 1522/5000: episode: 1522, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -779.381, mean reward: -779.381 [-779.381, -779.381], mean action: 2.000 [2.000, 2.000],  loss: 14291779.000000, mae: 1131.577148, mean_q: -32.508095
 1523/5000: episode: 1523, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -9740.996, mean reward: -9740.996 [-9740.996, -9740.996], mean action: 1.000 [1.000, 1.000],  loss: 19493940.000000, mae: 1320.804199, mean_q: -32.609230
 1524/5000: episode: 1524, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7386.457, mean reward: -7386.457 [-7386.457, -7386.457], mean action: 1.000 [1.000, 1.000],  loss: 20300536.000000, mae: 1311.171387, mean_q: -32.645527
 1525/5000: episode: 1525, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3428.370, mean reward: -3428.370 [-3428.370, -3428.370], mean action: 2.000 [2.000, 2.000],  loss: 20732650.000000, mae: 1322.367432, mean_q: -32.697800
 1526/5000: episode: 1526, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5732.892, mean reward: -5732.892 [-5732.892, -5732.892], mean action: 2.000 [2.000, 2.000],  loss: 14871176.000000, mae: 1146.730835, mean_q: -32.826832
 1527/5000: episode: 1527, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -997.034, mean reward: -997.034 [-997.034, -997.034], mean action: 2.000 [2.000, 2.000],  loss: 20445768.000000, mae: 1315.487549, mean_q: -32.839447
 1528/5000: episode: 1528, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -3296.568, mean reward: -3296.568 [-3296.568, -3296.568], mean action: 2.000 [2.000, 2.000],  loss: 14300072.000000, mae: 1191.212891, mean_q: -32.927795
 1529/5000: episode: 1529, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6207.087, mean reward: -6207.087 [-6207.087, -6207.087], mean action: 2.000 [2.000, 2.000],  loss: 17274016.000000, mae: 1255.009766, mean_q: -33.038197
 1530/5000: episode: 1530, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6072.696, mean reward: -6072.696 [-6072.696, -6072.696], mean action: 2.000 [2.000, 2.000],  loss: 17068108.000000, mae: 1289.833008, mean_q: -32.992249
 1531/5000: episode: 1531, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3280.650, mean reward: -3280.650 [-3280.650, -3280.650], mean action: 2.000 [2.000, 2.000],  loss: 14357575.000000, mae: 1152.202637, mean_q: -33.152016
 1532/5000: episode: 1532, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1728.063, mean reward: -1728.063 [-1728.063, -1728.063], mean action: 2.000 [2.000, 2.000],  loss: 11310766.000000, mae: 997.776733, mean_q: -33.392933
 1533/5000: episode: 1533, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2756.040, mean reward: -2756.040 [-2756.040, -2756.040], mean action: 2.000 [2.000, 2.000],  loss: 14980148.000000, mae: 1167.484497, mean_q: -33.219543
 1534/5000: episode: 1534, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4917.276, mean reward: -4917.276 [-4917.276, -4917.276], mean action: 2.000 [2.000, 2.000],  loss: 15971092.000000, mae: 1097.553711, mean_q: -33.239151
 1535/5000: episode: 1535, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3512.393, mean reward: -3512.393 [-3512.393, -3512.393], mean action: 2.000 [2.000, 2.000],  loss: 20712378.000000, mae: 1388.305664, mean_q: -33.393726
 1536/5000: episode: 1536, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1486.518, mean reward: -1486.518 [-1486.518, -1486.518], mean action: 2.000 [2.000, 2.000],  loss: 24423452.000000, mae: 1502.309570, mean_q: -33.402138
 1537/5000: episode: 1537, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4419.062, mean reward: -4419.062 [-4419.062, -4419.062], mean action: 2.000 [2.000, 2.000],  loss: 26725024.000000, mae: 1561.853760, mean_q: -33.403343
 1538/5000: episode: 1538, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3093.206, mean reward: -3093.206 [-3093.206, -3093.206], mean action: 2.000 [2.000, 2.000],  loss: 15499148.000000, mae: 1192.049438, mean_q: -33.541733
 1539/5000: episode: 1539, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2447.287, mean reward: -2447.287 [-2447.287, -2447.287], mean action: 2.000 [2.000, 2.000],  loss: 19782288.000000, mae: 1391.622559, mean_q: -33.635956
 1540/5000: episode: 1540, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -247.520, mean reward: -247.520 [-247.520, -247.520], mean action: 2.000 [2.000, 2.000],  loss: 16979398.000000, mae: 1252.154297, mean_q: -33.623230
 1541/5000: episode: 1541, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1701.056, mean reward: -1701.056 [-1701.056, -1701.056], mean action: 2.000 [2.000, 2.000],  loss: 16437545.000000, mae: 1099.254883, mean_q: -33.791245
 1542/5000: episode: 1542, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -243.285, mean reward: -243.285 [-243.285, -243.285], mean action: 2.000 [2.000, 2.000],  loss: 12872758.000000, mae: 1029.180664, mean_q: -33.813515
 1543/5000: episode: 1543, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -793.895, mean reward: -793.895 [-793.895, -793.895], mean action: 2.000 [2.000, 2.000],  loss: 15126380.000000, mae: 1197.563354, mean_q: -33.880299
 1544/5000: episode: 1544, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4723.967, mean reward: -4723.967 [-4723.967, -4723.967], mean action: 2.000 [2.000, 2.000],  loss: 11011553.000000, mae: 968.867798, mean_q: -33.998825
 1545/5000: episode: 1545, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4210.619, mean reward: -4210.619 [-4210.619, -4210.619], mean action: 2.000 [2.000, 2.000],  loss: 20826358.000000, mae: 1356.006104, mean_q: -34.020153
 1546/5000: episode: 1546, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2194.001, mean reward: -2194.001 [-2194.001, -2194.001], mean action: 2.000 [2.000, 2.000],  loss: 16210080.000000, mae: 1174.696655, mean_q: -34.247494
 1547/5000: episode: 1547, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -354.744, mean reward: -354.744 [-354.744, -354.744], mean action: 2.000 [2.000, 2.000],  loss: 15767809.000000, mae: 1149.828003, mean_q: -34.248150
 1548/5000: episode: 1548, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1057.074, mean reward: -1057.074 [-1057.074, -1057.074], mean action: 2.000 [2.000, 2.000],  loss: 17023344.000000, mae: 1267.200439, mean_q: -34.353592
 1549/5000: episode: 1549, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1941.713, mean reward: -1941.713 [-1941.713, -1941.713], mean action: 2.000 [2.000, 2.000],  loss: 13701821.000000, mae: 1123.895874, mean_q: -34.362072
 1550/5000: episode: 1550, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1623.663, mean reward: -1623.663 [-1623.663, -1623.663], mean action: 2.000 [2.000, 2.000],  loss: 24028936.000000, mae: 1449.088379, mean_q: -34.408417
 1551/5000: episode: 1551, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2943.345, mean reward: -2943.345 [-2943.345, -2943.345], mean action: 2.000 [2.000, 2.000],  loss: 15752878.000000, mae: 1174.269409, mean_q: -34.496864
 1552/5000: episode: 1552, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1828.917, mean reward: -1828.917 [-1828.917, -1828.917], mean action: 2.000 [2.000, 2.000],  loss: 15294447.000000, mae: 1092.782471, mean_q: -34.488178
 1553/5000: episode: 1553, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -96.534, mean reward: -96.534 [-96.534, -96.534], mean action: 2.000 [2.000, 2.000],  loss: 16499536.000000, mae: 1211.446533, mean_q: -34.413872
 1554/5000: episode: 1554, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3322.479, mean reward: -3322.479 [-3322.479, -3322.479], mean action: 2.000 [2.000, 2.000],  loss: 19317652.000000, mae: 1321.961182, mean_q: -34.903973
 1555/5000: episode: 1555, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2804.003, mean reward: -2804.003 [-2804.003, -2804.003], mean action: 2.000 [2.000, 2.000],  loss: 10956744.000000, mae: 1004.816040, mean_q: -34.909599
 1556/5000: episode: 1556, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6440.869, mean reward: -6440.869 [-6440.869, -6440.869], mean action: 2.000 [2.000, 2.000],  loss: 19622510.000000, mae: 1327.709595, mean_q: -34.782257
 1557/5000: episode: 1557, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3254.146, mean reward: -3254.146 [-3254.146, -3254.146], mean action: 2.000 [2.000, 2.000],  loss: 16724524.000000, mae: 1233.167603, mean_q: -35.010048
 1558/5000: episode: 1558, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -105.829, mean reward: -105.829 [-105.829, -105.829], mean action: 2.000 [2.000, 2.000],  loss: 14959030.000000, mae: 1171.690063, mean_q: -35.113937
 1559/5000: episode: 1559, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2224.075, mean reward: -2224.075 [-2224.075, -2224.075], mean action: 2.000 [2.000, 2.000],  loss: 21018632.000000, mae: 1387.973389, mean_q: -35.106438
 1560/5000: episode: 1560, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7162.797, mean reward: -7162.797 [-7162.797, -7162.797], mean action: 2.000 [2.000, 2.000],  loss: 23000372.000000, mae: 1430.938232, mean_q: -35.228470
 1561/5000: episode: 1561, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -157.805, mean reward: -157.805 [-157.805, -157.805], mean action: 2.000 [2.000, 2.000],  loss: 16628886.000000, mae: 1133.710938, mean_q: -35.179504
 1562/5000: episode: 1562, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2616.228, mean reward: -2616.228 [-2616.228, -2616.228], mean action: 2.000 [2.000, 2.000],  loss: 21881874.000000, mae: 1418.139771, mean_q: -35.372265
 1563/5000: episode: 1563, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3116.552, mean reward: -3116.552 [-3116.552, -3116.552], mean action: 2.000 [2.000, 2.000],  loss: 19034398.000000, mae: 1316.613281, mean_q: -35.308506
 1564/5000: episode: 1564, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -307.720, mean reward: -307.720 [-307.720, -307.720], mean action: 2.000 [2.000, 2.000],  loss: 16739178.000000, mae: 1206.852051, mean_q: -35.408043
 1565/5000: episode: 1565, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2325.991, mean reward: -2325.991 [-2325.991, -2325.991], mean action: 2.000 [2.000, 2.000],  loss: 17786508.000000, mae: 1240.420166, mean_q: -35.544250
 1566/5000: episode: 1566, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1492.891, mean reward: -1492.891 [-1492.891, -1492.891], mean action: 2.000 [2.000, 2.000],  loss: 19631976.000000, mae: 1201.659424, mean_q: -35.608665
 1567/5000: episode: 1567, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4393.622, mean reward: -4393.622 [-4393.622, -4393.622], mean action: 1.000 [1.000, 1.000],  loss: 13317774.000000, mae: 1143.547852, mean_q: -35.703339
 1568/5000: episode: 1568, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -360.740, mean reward: -360.740 [-360.740, -360.740], mean action: 2.000 [2.000, 2.000],  loss: 18266200.000000, mae: 1270.985474, mean_q: -35.828159
 1569/5000: episode: 1569, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -33.798, mean reward: -33.798 [-33.798, -33.798], mean action: 2.000 [2.000, 2.000],  loss: 17363020.000000, mae: 1278.470947, mean_q: -35.889366
 1570/5000: episode: 1570, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -707.944, mean reward: -707.944 [-707.944, -707.944], mean action: 2.000 [2.000, 2.000],  loss: 17346974.000000, mae: 1299.873047, mean_q: -35.949165
 1571/5000: episode: 1571, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -506.433, mean reward: -506.433 [-506.433, -506.433], mean action: 2.000 [2.000, 2.000],  loss: 18381652.000000, mae: 1328.658936, mean_q: -35.700947
 1572/5000: episode: 1572, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -305.491, mean reward: -305.491 [-305.491, -305.491], mean action: 2.000 [2.000, 2.000],  loss: 20942470.000000, mae: 1428.393555, mean_q: -35.969395
 1573/5000: episode: 1573, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1838.700, mean reward: -1838.700 [-1838.700, -1838.700], mean action: 2.000 [2.000, 2.000],  loss: 20210692.000000, mae: 1415.639160, mean_q: -35.968925
 1574/5000: episode: 1574, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1682.393, mean reward: -1682.393 [-1682.393, -1682.393], mean action: 2.000 [2.000, 2.000],  loss: 19164068.000000, mae: 1327.896362, mean_q: -36.174004
 1575/5000: episode: 1575, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3382.805, mean reward: -3382.805 [-3382.805, -3382.805], mean action: 2.000 [2.000, 2.000],  loss: 14025411.000000, mae: 1034.481934, mean_q: -36.258385
 1576/5000: episode: 1576, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1015.067, mean reward: -1015.067 [-1015.067, -1015.067], mean action: 2.000 [2.000, 2.000],  loss: 16772187.000000, mae: 1264.220947, mean_q: -36.417126
 1577/5000: episode: 1577, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6147.083, mean reward: -6147.083 [-6147.083, -6147.083], mean action: 2.000 [2.000, 2.000],  loss: 11609577.000000, mae: 993.354065, mean_q: -36.551659
 1578/5000: episode: 1578, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -1159.337, mean reward: -1159.337 [-1159.337, -1159.337], mean action: 2.000 [2.000, 2.000],  loss: 18064708.000000, mae: 1241.149414, mean_q: -36.458534
 1579/5000: episode: 1579, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5179.494, mean reward: -5179.494 [-5179.494, -5179.494], mean action: 1.000 [1.000, 1.000],  loss: 23563924.000000, mae: 1479.172852, mean_q: -36.556107
 1580/5000: episode: 1580, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2045.338, mean reward: -2045.338 [-2045.338, -2045.338], mean action: 2.000 [2.000, 2.000],  loss: 12486770.000000, mae: 1080.743896, mean_q: -36.781765
 1581/5000: episode: 1581, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -924.162, mean reward: -924.162 [-924.162, -924.162], mean action: 2.000 [2.000, 2.000],  loss: 16232502.000000, mae: 1227.288452, mean_q: -36.708111
 1582/5000: episode: 1582, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9301.232, mean reward: -9301.232 [-9301.232, -9301.232], mean action: 1.000 [1.000, 1.000],  loss: 13897304.000000, mae: 1150.756104, mean_q: -36.998085
 1583/5000: episode: 1583, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -979.524, mean reward: -979.524 [-979.524, -979.524], mean action: 2.000 [2.000, 2.000],  loss: 22316308.000000, mae: 1406.591431, mean_q: -36.837433
 1584/5000: episode: 1584, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -933.951, mean reward: -933.951 [-933.951, -933.951], mean action: 2.000 [2.000, 2.000],  loss: 24923696.000000, mae: 1493.510986, mean_q: -36.962139
 1585/5000: episode: 1585, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1060.830, mean reward: -1060.830 [-1060.830, -1060.830], mean action: 2.000 [2.000, 2.000],  loss: 13795376.000000, mae: 1114.248291, mean_q: -37.151520
 1586/5000: episode: 1586, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1247.349, mean reward: -1247.349 [-1247.349, -1247.349], mean action: 2.000 [2.000, 2.000],  loss: 18099316.000000, mae: 1301.230225, mean_q: -37.097813
 1587/5000: episode: 1587, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2793.709, mean reward: -2793.709 [-2793.709, -2793.709], mean action: 2.000 [2.000, 2.000],  loss: 13492832.000000, mae: 1111.578735, mean_q: -37.343613
 1588/5000: episode: 1588, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1393.864, mean reward: -1393.864 [-1393.864, -1393.864], mean action: 2.000 [2.000, 2.000],  loss: 17461568.000000, mae: 1254.702148, mean_q: -37.455330
 1589/5000: episode: 1589, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2934.648, mean reward: -2934.648 [-2934.648, -2934.648], mean action: 2.000 [2.000, 2.000],  loss: 21779702.000000, mae: 1404.540039, mean_q: -37.281826
 1590/5000: episode: 1590, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2179.852, mean reward: -2179.852 [-2179.852, -2179.852], mean action: 3.000 [3.000, 3.000],  loss: 15434449.000000, mae: 1225.885498, mean_q: -37.558033
 1591/5000: episode: 1591, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4512.922, mean reward: -4512.922 [-4512.922, -4512.922], mean action: 2.000 [2.000, 2.000],  loss: 17799668.000000, mae: 1225.987427, mean_q: -37.607658
 1592/5000: episode: 1592, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2160.603, mean reward: -2160.603 [-2160.603, -2160.603], mean action: 3.000 [3.000, 3.000],  loss: 13835686.000000, mae: 1111.643188, mean_q: -37.582451
 1593/5000: episode: 1593, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7804.754, mean reward: -7804.754 [-7804.754, -7804.754], mean action: 3.000 [3.000, 3.000],  loss: 18707812.000000, mae: 1324.860229, mean_q: -37.595860
 1594/5000: episode: 1594, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9017.278, mean reward: -9017.278 [-9017.278, -9017.278], mean action: 3.000 [3.000, 3.000],  loss: 21754126.000000, mae: 1381.195435, mean_q: -37.845955
 1595/5000: episode: 1595, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2291.379, mean reward: -2291.379 [-2291.379, -2291.379], mean action: 3.000 [3.000, 3.000],  loss: 15068109.000000, mae: 1168.466553, mean_q: -37.993671
 1596/5000: episode: 1596, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1635.076, mean reward: -1635.076 [-1635.076, -1635.076], mean action: 3.000 [3.000, 3.000],  loss: 26823392.000000, mae: 1384.625854, mean_q: -37.905205
 1597/5000: episode: 1597, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4553.863, mean reward: -4553.863 [-4553.863, -4553.863], mean action: 3.000 [3.000, 3.000],  loss: 10409946.000000, mae: 938.652100, mean_q: -38.305771
 1598/5000: episode: 1598, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2719.563, mean reward: -2719.563 [-2719.563, -2719.563], mean action: 3.000 [3.000, 3.000],  loss: 14235653.000000, mae: 1103.696045, mean_q: -38.175121
 1599/5000: episode: 1599, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1369.826, mean reward: -1369.826 [-1369.826, -1369.826], mean action: 3.000 [3.000, 3.000],  loss: 26454414.000000, mae: 1611.151733, mean_q: -37.956306
 1600/5000: episode: 1600, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1279.753, mean reward: -1279.753 [-1279.753, -1279.753], mean action: 3.000 [3.000, 3.000],  loss: 23382284.000000, mae: 1485.114868, mean_q: -38.027451
 1601/5000: episode: 1601, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10125.194, mean reward: -10125.194 [-10125.194, -10125.194], mean action: 3.000 [3.000, 3.000],  loss: 19365154.000000, mae: 1345.773926, mean_q: -38.262928
 1602/5000: episode: 1602, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1517.090, mean reward: -1517.090 [-1517.090, -1517.090], mean action: 3.000 [3.000, 3.000],  loss: 16092020.000000, mae: 1161.930664, mean_q: -38.313789
 1603/5000: episode: 1603, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2936.085, mean reward: -2936.085 [-2936.085, -2936.085], mean action: 3.000 [3.000, 3.000],  loss: 11586157.000000, mae: 1023.277222, mean_q: -38.537598
 1604/5000: episode: 1604, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6126.836, mean reward: -6126.836 [-6126.836, -6126.836], mean action: 3.000 [3.000, 3.000],  loss: 14007811.000000, mae: 1068.731445, mean_q: -38.455894
 1605/5000: episode: 1605, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -309.057, mean reward: -309.057 [-309.057, -309.057], mean action: 3.000 [3.000, 3.000],  loss: 12007844.000000, mae: 1058.948975, mean_q: -38.609047
 1606/5000: episode: 1606, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4918.531, mean reward: -4918.531 [-4918.531, -4918.531], mean action: 3.000 [3.000, 3.000],  loss: 21313764.000000, mae: 1435.411377, mean_q: -38.596985
 1607/5000: episode: 1607, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -127.489, mean reward: -127.489 [-127.489, -127.489], mean action: 3.000 [3.000, 3.000],  loss: 13384824.000000, mae: 1110.690674, mean_q: -38.807327
 1608/5000: episode: 1608, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5013.828, mean reward: -5013.828 [-5013.828, -5013.828], mean action: 3.000 [3.000, 3.000],  loss: 10800528.000000, mae: 984.540344, mean_q: -38.772392
 1609/5000: episode: 1609, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1289.738, mean reward: -1289.738 [-1289.738, -1289.738], mean action: 3.000 [3.000, 3.000],  loss: 13504182.000000, mae: 1098.846069, mean_q: -38.806614
 1610/5000: episode: 1610, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3680.823, mean reward: -3680.823 [-3680.823, -3680.823], mean action: 3.000 [3.000, 3.000],  loss: 17707312.000000, mae: 1301.063232, mean_q: -39.022316
 1611/5000: episode: 1611, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -68.383, mean reward: -68.383 [-68.383, -68.383], mean action: 3.000 [3.000, 3.000],  loss: 11715044.000000, mae: 982.868958, mean_q: -39.144402
 1612/5000: episode: 1612, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -808.894, mean reward: -808.894 [-808.894, -808.894], mean action: 1.000 [1.000, 1.000],  loss: 23022972.000000, mae: 1351.044434, mean_q: -39.069298
 1613/5000: episode: 1613, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2848.098, mean reward: -2848.098 [-2848.098, -2848.098], mean action: 2.000 [2.000, 2.000],  loss: 17323992.000000, mae: 1278.802246, mean_q: -39.252285
 1614/5000: episode: 1614, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4530.034, mean reward: -4530.034 [-4530.034, -4530.034], mean action: 3.000 [3.000, 3.000],  loss: 10667573.000000, mae: 989.085693, mean_q: -39.281029
 1615/5000: episode: 1615, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1512.518, mean reward: -1512.518 [-1512.518, -1512.518], mean action: 3.000 [3.000, 3.000],  loss: 16688214.000000, mae: 1267.210449, mean_q: -39.201725
 1616/5000: episode: 1616, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -6424.124, mean reward: -6424.124 [-6424.124, -6424.124], mean action: 3.000 [3.000, 3.000],  loss: 24457172.000000, mae: 1494.340332, mean_q: -39.300262
 1617/5000: episode: 1617, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1121.761, mean reward: -1121.761 [-1121.761, -1121.761], mean action: 3.000 [3.000, 3.000],  loss: 13350666.000000, mae: 1085.692993, mean_q: -39.301910
 1618/5000: episode: 1618, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -6.659, mean reward: -6.659 [-6.659, -6.659], mean action: 3.000 [3.000, 3.000],  loss: 19769040.000000, mae: 1337.661133, mean_q: -39.461563
 1619/5000: episode: 1619, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1551.606, mean reward: -1551.606 [-1551.606, -1551.606], mean action: 3.000 [3.000, 3.000],  loss: 17594636.000000, mae: 1178.463623, mean_q: -39.641006
 1620/5000: episode: 1620, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -524.375, mean reward: -524.375 [-524.375, -524.375], mean action: 3.000 [3.000, 3.000],  loss: 25079534.000000, mae: 1541.857422, mean_q: -39.548592
 1621/5000: episode: 1621, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3362.293, mean reward: -3362.293 [-3362.293, -3362.293], mean action: 3.000 [3.000, 3.000],  loss: 16342910.000000, mae: 1269.036133, mean_q: -39.857967
 1622/5000: episode: 1622, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2350.799, mean reward: -2350.799 [-2350.799, -2350.799], mean action: 3.000 [3.000, 3.000],  loss: 20950076.000000, mae: 1305.745361, mean_q: -39.957058
 1623/5000: episode: 1623, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1454.118, mean reward: -1454.118 [-1454.118, -1454.118], mean action: 3.000 [3.000, 3.000],  loss: 15737602.000000, mae: 1257.081787, mean_q: -39.977230
 1624/5000: episode: 1624, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3685.004, mean reward: -3685.004 [-3685.004, -3685.004], mean action: 3.000 [3.000, 3.000],  loss: 12577836.000000, mae: 1011.579956, mean_q: -39.913971
 1625/5000: episode: 1625, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3330.048, mean reward: -3330.048 [-3330.048, -3330.048], mean action: 3.000 [3.000, 3.000],  loss: 16460278.000000, mae: 1231.533081, mean_q: -40.105301
 1626/5000: episode: 1626, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2285.974, mean reward: -2285.974 [-2285.974, -2285.974], mean action: 3.000 [3.000, 3.000],  loss: 20889262.000000, mae: 1378.819946, mean_q: -40.109337
 1627/5000: episode: 1627, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1435.328, mean reward: -1435.328 [-1435.328, -1435.328], mean action: 3.000 [3.000, 3.000],  loss: 17966506.000000, mae: 1326.348389, mean_q: -40.186180
 1628/5000: episode: 1628, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -1186.556, mean reward: -1186.556 [-1186.556, -1186.556], mean action: 3.000 [3.000, 3.000],  loss: 20877536.000000, mae: 1406.080566, mean_q: -40.218151
 1629/5000: episode: 1629, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6086.759, mean reward: -6086.759 [-6086.759, -6086.759], mean action: 3.000 [3.000, 3.000],  loss: 11132669.000000, mae: 995.371582, mean_q: -40.462532
 1630/5000: episode: 1630, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1474.863, mean reward: -1474.863 [-1474.863, -1474.863], mean action: 3.000 [3.000, 3.000],  loss: 16490968.000000, mae: 1177.449585, mean_q: -40.443592
 1631/5000: episode: 1631, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3532.666, mean reward: -3532.666 [-3532.666, -3532.666], mean action: 1.000 [1.000, 1.000],  loss: 24266898.000000, mae: 1502.754883, mean_q: -40.371552
 1632/5000: episode: 1632, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2254.098, mean reward: -2254.098 [-2254.098, -2254.098], mean action: 3.000 [3.000, 3.000],  loss: 11044806.000000, mae: 1003.906494, mean_q: -40.913227
 1633/5000: episode: 1633, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -153.212, mean reward: -153.212 [-153.212, -153.212], mean action: 3.000 [3.000, 3.000],  loss: 14103282.000000, mae: 1097.170410, mean_q: -40.710857
 1634/5000: episode: 1634, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4302.418, mean reward: -4302.418 [-4302.418, -4302.418], mean action: 3.000 [3.000, 3.000],  loss: 29170774.000000, mae: 1575.471069, mean_q: -40.590389
 1635/5000: episode: 1635, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2473.690, mean reward: -2473.690 [-2473.690, -2473.690], mean action: 3.000 [3.000, 3.000],  loss: 19575032.000000, mae: 1272.894165, mean_q: -40.810200
 1636/5000: episode: 1636, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2994.314, mean reward: -2994.314 [-2994.314, -2994.314], mean action: 3.000 [3.000, 3.000],  loss: 18662780.000000, mae: 1292.279053, mean_q: -40.880165
 1637/5000: episode: 1637, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1116.220, mean reward: -1116.220 [-1116.220, -1116.220], mean action: 3.000 [3.000, 3.000],  loss: 16594431.000000, mae: 1282.802734, mean_q: -40.888416
 1638/5000: episode: 1638, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3614.388, mean reward: -3614.388 [-3614.388, -3614.388], mean action: 3.000 [3.000, 3.000],  loss: 16285148.000000, mae: 1250.709717, mean_q: -41.122887
 1639/5000: episode: 1639, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -865.921, mean reward: -865.921 [-865.921, -865.921], mean action: 3.000 [3.000, 3.000],  loss: 12338352.000000, mae: 1073.515381, mean_q: -41.225700
 1640/5000: episode: 1640, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5411.772, mean reward: -5411.772 [-5411.772, -5411.772], mean action: 1.000 [1.000, 1.000],  loss: 16326314.000000, mae: 1208.218140, mean_q: -41.141758
 1641/5000: episode: 1641, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6137.299, mean reward: -6137.299 [-6137.299, -6137.299], mean action: 1.000 [1.000, 1.000],  loss: 16345307.000000, mae: 1206.163818, mean_q: -41.310711
 1642/5000: episode: 1642, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -91.793, mean reward: -91.793 [-91.793, -91.793], mean action: 3.000 [3.000, 3.000],  loss: 16564225.000000, mae: 1238.253662, mean_q: -41.411049
 1643/5000: episode: 1643, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6972.310, mean reward: -6972.310 [-6972.310, -6972.310], mean action: 1.000 [1.000, 1.000],  loss: 19497954.000000, mae: 1333.768921, mean_q: -41.510864
 1644/5000: episode: 1644, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7860.294, mean reward: -7860.294 [-7860.294, -7860.294], mean action: 1.000 [1.000, 1.000],  loss: 22599714.000000, mae: 1458.745117, mean_q: -41.456711
 1645/5000: episode: 1645, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8425.338, mean reward: -8425.338 [-8425.338, -8425.338], mean action: 1.000 [1.000, 1.000],  loss: 16360174.000000, mae: 1111.484619, mean_q: -41.466896
 1646/5000: episode: 1646, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4083.341, mean reward: -4083.341 [-4083.341, -4083.341], mean action: 1.000 [1.000, 1.000],  loss: 18519164.000000, mae: 1403.075928, mean_q: -41.707336
 1647/5000: episode: 1647, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1211.731, mean reward: -1211.731 [-1211.731, -1211.731], mean action: 1.000 [1.000, 1.000],  loss: 15730970.000000, mae: 1122.997070, mean_q: -41.805031
 1648/5000: episode: 1648, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2169.618, mean reward: -2169.618 [-2169.618, -2169.618], mean action: 1.000 [1.000, 1.000],  loss: 15351878.000000, mae: 1120.911865, mean_q: -41.823952
 1649/5000: episode: 1649, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -7537.518, mean reward: -7537.518 [-7537.518, -7537.518], mean action: 1.000 [1.000, 1.000],  loss: 17176766.000000, mae: 1275.604736, mean_q: -41.871510
 1650/5000: episode: 1650, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2810.446, mean reward: -2810.446 [-2810.446, -2810.446], mean action: 1.000 [1.000, 1.000],  loss: 11210205.000000, mae: 1020.288696, mean_q: -42.179749
 1651/5000: episode: 1651, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5523.141, mean reward: -5523.141 [-5523.141, -5523.141], mean action: 1.000 [1.000, 1.000],  loss: 12580742.000000, mae: 1069.588745, mean_q: -42.418312
 1652/5000: episode: 1652, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -13077.094, mean reward: -13077.094 [-13077.094, -13077.094], mean action: 1.000 [1.000, 1.000],  loss: 23481420.000000, mae: 1447.275024, mean_q: -42.119537
 1653/5000: episode: 1653, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -11346.761, mean reward: -11346.761 [-11346.761, -11346.761], mean action: 1.000 [1.000, 1.000],  loss: 12883492.000000, mae: 1119.962646, mean_q: -42.457436
 1654/5000: episode: 1654, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3184.801, mean reward: -3184.801 [-3184.801, -3184.801], mean action: 1.000 [1.000, 1.000],  loss: 22354176.000000, mae: 1326.585083, mean_q: -42.286758
 1655/5000: episode: 1655, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4132.740, mean reward: -4132.740 [-4132.740, -4132.740], mean action: 1.000 [1.000, 1.000],  loss: 14821763.000000, mae: 1161.690674, mean_q: -42.476273
 1656/5000: episode: 1656, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6910.748, mean reward: -6910.748 [-6910.748, -6910.748], mean action: 0.000 [0.000, 0.000],  loss: 17599522.000000, mae: 1350.851685, mean_q: -42.337311
 1657/5000: episode: 1657, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7520.384, mean reward: -7520.384 [-7520.384, -7520.384], mean action: 1.000 [1.000, 1.000],  loss: 14490367.000000, mae: 1075.177124, mean_q: -42.542641
 1658/5000: episode: 1658, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4252.278, mean reward: -4252.278 [-4252.278, -4252.278], mean action: 0.000 [0.000, 0.000],  loss: 11398078.000000, mae: 995.795837, mean_q: -42.697552
 1659/5000: episode: 1659, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1100.529, mean reward: -1100.529 [-1100.529, -1100.529], mean action: 1.000 [1.000, 1.000],  loss: 17215280.000000, mae: 1303.657837, mean_q: -42.704765
 1660/5000: episode: 1660, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7625.224, mean reward: -7625.224 [-7625.224, -7625.224], mean action: 1.000 [1.000, 1.000],  loss: 20394790.000000, mae: 1369.291260, mean_q: -42.785286
 1661/5000: episode: 1661, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8180.089, mean reward: -8180.089 [-8180.089, -8180.089], mean action: 1.000 [1.000, 1.000],  loss: 15460576.000000, mae: 1224.629150, mean_q: -42.956741
 1662/5000: episode: 1662, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2226.091, mean reward: -2226.091 [-2226.091, -2226.091], mean action: 1.000 [1.000, 1.000],  loss: 21081736.000000, mae: 1343.142700, mean_q: -42.856598
 1663/5000: episode: 1663, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1856.263, mean reward: -1856.263 [-1856.263, -1856.263], mean action: 3.000 [3.000, 3.000],  loss: 20532456.000000, mae: 1454.602051, mean_q: -42.889503
 1664/5000: episode: 1664, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4807.183, mean reward: -4807.183 [-4807.183, -4807.183], mean action: 1.000 [1.000, 1.000],  loss: 21203068.000000, mae: 1319.506836, mean_q: -43.060841
 1665/5000: episode: 1665, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2250.558, mean reward: -2250.558 [-2250.558, -2250.558], mean action: 3.000 [3.000, 3.000],  loss: 13891632.000000, mae: 1036.973633, mean_q: -43.395451
 1666/5000: episode: 1666, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4358.633, mean reward: -4358.633 [-4358.633, -4358.633], mean action: 3.000 [3.000, 3.000],  loss: 12674859.000000, mae: 1046.386108, mean_q: -43.240852
 1667/5000: episode: 1667, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3295.187, mean reward: -3295.187 [-3295.187, -3295.187], mean action: 3.000 [3.000, 3.000],  loss: 16103892.000000, mae: 1120.953491, mean_q: -43.406250
 1668/5000: episode: 1668, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10906.265, mean reward: -10906.265 [-10906.265, -10906.265], mean action: 3.000 [3.000, 3.000],  loss: 17912656.000000, mae: 1271.822021, mean_q: -43.393749
 1669/5000: episode: 1669, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6938.056, mean reward: -6938.056 [-6938.056, -6938.056], mean action: 3.000 [3.000, 3.000],  loss: 17999312.000000, mae: 1310.460449, mean_q: -43.367630
 1670/5000: episode: 1670, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -648.796, mean reward: -648.796 [-648.796, -648.796], mean action: 3.000 [3.000, 3.000],  loss: 22029896.000000, mae: 1390.316162, mean_q: -43.550556
 1671/5000: episode: 1671, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1088.144, mean reward: -1088.144 [-1088.144, -1088.144], mean action: 3.000 [3.000, 3.000],  loss: 15356666.000000, mae: 1184.291992, mean_q: -43.696968
 1672/5000: episode: 1672, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1632.664, mean reward: -1632.664 [-1632.664, -1632.664], mean action: 3.000 [3.000, 3.000],  loss: 15222274.000000, mae: 1125.340088, mean_q: -43.618614
 1673/5000: episode: 1673, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7753.820, mean reward: -7753.820 [-7753.820, -7753.820], mean action: 3.000 [3.000, 3.000],  loss: 19383406.000000, mae: 1287.739868, mean_q: -43.867332
 1674/5000: episode: 1674, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5095.801, mean reward: -5095.801 [-5095.801, -5095.801], mean action: 3.000 [3.000, 3.000],  loss: 14633116.000000, mae: 1142.910034, mean_q: -43.921593
 1675/5000: episode: 1675, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6806.882, mean reward: -6806.882 [-6806.882, -6806.882], mean action: 3.000 [3.000, 3.000],  loss: 16160786.000000, mae: 1213.352661, mean_q: -43.958843
 1676/5000: episode: 1676, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4920.199, mean reward: -4920.199 [-4920.199, -4920.199], mean action: 3.000 [3.000, 3.000],  loss: 14001055.000000, mae: 1066.863525, mean_q: -43.944511
 1677/5000: episode: 1677, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2852.153, mean reward: -2852.153 [-2852.153, -2852.153], mean action: 3.000 [3.000, 3.000],  loss: 15059312.000000, mae: 1128.345459, mean_q: -44.125427
 1678/5000: episode: 1678, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3941.914, mean reward: -3941.914 [-3941.914, -3941.914], mean action: 1.000 [1.000, 1.000],  loss: 17692024.000000, mae: 1208.844604, mean_q: -44.235977
 1679/5000: episode: 1679, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -573.064, mean reward: -573.064 [-573.064, -573.064], mean action: 3.000 [3.000, 3.000],  loss: 15660204.000000, mae: 1154.893188, mean_q: -44.131424
 1680/5000: episode: 1680, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -620.319, mean reward: -620.319 [-620.319, -620.319], mean action: 3.000 [3.000, 3.000],  loss: 20392392.000000, mae: 1435.560547, mean_q: -44.217567
 1681/5000: episode: 1681, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1549.084, mean reward: -1549.084 [-1549.084, -1549.084], mean action: 3.000 [3.000, 3.000],  loss: 15928732.000000, mae: 1108.783691, mean_q: -44.326294
 1682/5000: episode: 1682, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -9134.580, mean reward: -9134.580 [-9134.580, -9134.580], mean action: 3.000 [3.000, 3.000],  loss: 13798379.000000, mae: 1142.516479, mean_q: -44.668846
 1683/5000: episode: 1683, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2973.140, mean reward: -2973.140 [-2973.140, -2973.140], mean action: 3.000 [3.000, 3.000],  loss: 19307484.000000, mae: 1302.911011, mean_q: -44.591763
 1684/5000: episode: 1684, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2634.522, mean reward: -2634.522 [-2634.522, -2634.522], mean action: 2.000 [2.000, 2.000],  loss: 22153096.000000, mae: 1501.703857, mean_q: -44.665924
 1685/5000: episode: 1685, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -11603.408, mean reward: -11603.408 [-11603.408, -11603.408], mean action: 0.000 [0.000, 0.000],  loss: 20722244.000000, mae: 1350.921265, mean_q: -44.569714
 1686/5000: episode: 1686, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -893.974, mean reward: -893.974 [-893.974, -893.974], mean action: 3.000 [3.000, 3.000],  loss: 11677650.000000, mae: 1078.601562, mean_q: -44.810070
 1687/5000: episode: 1687, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1783.977, mean reward: -1783.977 [-1783.977, -1783.977], mean action: 2.000 [2.000, 2.000],  loss: 10789096.000000, mae: 956.682312, mean_q: -44.905777
 1688/5000: episode: 1688, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5751.804, mean reward: -5751.804 [-5751.804, -5751.804], mean action: 3.000 [3.000, 3.000],  loss: 21819300.000000, mae: 1413.669922, mean_q: -44.967361
 1689/5000: episode: 1689, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -727.943, mean reward: -727.943 [-727.943, -727.943], mean action: 3.000 [3.000, 3.000],  loss: 16584684.000000, mae: 1175.212280, mean_q: -44.992409
 1690/5000: episode: 1690, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2016.384, mean reward: -2016.384 [-2016.384, -2016.384], mean action: 3.000 [3.000, 3.000],  loss: 12981100.000000, mae: 1088.503906, mean_q: -45.452744
 1691/5000: episode: 1691, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -237.671, mean reward: -237.671 [-237.671, -237.671], mean action: 3.000 [3.000, 3.000],  loss: 15145634.000000, mae: 1091.969482, mean_q: -45.226570
 1692/5000: episode: 1692, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2897.299, mean reward: -2897.299 [-2897.299, -2897.299], mean action: 2.000 [2.000, 2.000],  loss: 14949327.000000, mae: 1202.803101, mean_q: -45.264957
 1693/5000: episode: 1693, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2920.931, mean reward: -2920.931 [-2920.931, -2920.931], mean action: 3.000 [3.000, 3.000],  loss: 15804209.000000, mae: 1191.601318, mean_q: -45.458141
 1694/5000: episode: 1694, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6930.488, mean reward: -6930.488 [-6930.488, -6930.488], mean action: 3.000 [3.000, 3.000],  loss: 15722232.000000, mae: 1223.570068, mean_q: -45.431274
 1695/5000: episode: 1695, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1165.468, mean reward: -1165.468 [-1165.468, -1165.468], mean action: 3.000 [3.000, 3.000],  loss: 17653482.000000, mae: 1282.886597, mean_q: -45.665230
 1696/5000: episode: 1696, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -175.738, mean reward: -175.738 [-175.738, -175.738], mean action: 3.000 [3.000, 3.000],  loss: 14504303.000000, mae: 1051.842163, mean_q: -45.568962
 1697/5000: episode: 1697, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5681.510, mean reward: -5681.510 [-5681.510, -5681.510], mean action: 3.000 [3.000, 3.000],  loss: 19208542.000000, mae: 1352.437012, mean_q: -45.382687
 1698/5000: episode: 1698, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2785.675, mean reward: -2785.675 [-2785.675, -2785.675], mean action: 3.000 [3.000, 3.000],  loss: 20794132.000000, mae: 1384.149780, mean_q: -45.636608
 1699/5000: episode: 1699, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3496.464, mean reward: -3496.464 [-3496.464, -3496.464], mean action: 3.000 [3.000, 3.000],  loss: 13213157.000000, mae: 991.039246, mean_q: -45.933830
 1700/5000: episode: 1700, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5522.816, mean reward: -5522.816 [-5522.816, -5522.816], mean action: 3.000 [3.000, 3.000],  loss: 11600055.000000, mae: 978.915649, mean_q: -46.047901
 1701/5000: episode: 1701, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -12364.592, mean reward: -12364.592 [-12364.592, -12364.592], mean action: 0.000 [0.000, 0.000],  loss: 12181526.000000, mae: 1046.102783, mean_q: -46.156036
 1702/5000: episode: 1702, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4044.736, mean reward: -4044.736 [-4044.736, -4044.736], mean action: 3.000 [3.000, 3.000],  loss: 18234274.000000, mae: 1276.330078, mean_q: -46.003662
 1703/5000: episode: 1703, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6288.362, mean reward: -6288.362 [-6288.362, -6288.362], mean action: 3.000 [3.000, 3.000],  loss: 22375676.000000, mae: 1387.455322, mean_q: -46.073021
 1704/5000: episode: 1704, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2756.823, mean reward: -2756.823 [-2756.823, -2756.823], mean action: 3.000 [3.000, 3.000],  loss: 13695243.000000, mae: 1069.863037, mean_q: -46.333305
 1705/5000: episode: 1705, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7530.037, mean reward: -7530.037 [-7530.037, -7530.037], mean action: 1.000 [1.000, 1.000],  loss: 17230096.000000, mae: 1201.259033, mean_q: -46.187065
 1706/5000: episode: 1706, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7542.775, mean reward: -7542.775 [-7542.775, -7542.775], mean action: 1.000 [1.000, 1.000],  loss: 11951807.000000, mae: 1027.252441, mean_q: -46.510727
 1707/5000: episode: 1707, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -748.104, mean reward: -748.104 [-748.104, -748.104], mean action: 3.000 [3.000, 3.000],  loss: 25610694.000000, mae: 1499.054688, mean_q: -46.417038
 1708/5000: episode: 1708, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3453.455, mean reward: -3453.455 [-3453.455, -3453.455], mean action: 3.000 [3.000, 3.000],  loss: 15726056.000000, mae: 1049.919922, mean_q: -46.400673
 1709/5000: episode: 1709, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -1614.184, mean reward: -1614.184 [-1614.184, -1614.184], mean action: 3.000 [3.000, 3.000],  loss: 10800353.000000, mae: 965.852051, mean_q: -46.706398
 1710/5000: episode: 1710, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5280.214, mean reward: -5280.214 [-5280.214, -5280.214], mean action: 3.000 [3.000, 3.000],  loss: 22032094.000000, mae: 1418.750488, mean_q: -46.649170
 1711/5000: episode: 1711, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3067.504, mean reward: -3067.504 [-3067.504, -3067.504], mean action: 3.000 [3.000, 3.000],  loss: 12976602.000000, mae: 1030.804688, mean_q: -46.810417
 1712/5000: episode: 1712, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5561.450, mean reward: -5561.450 [-5561.450, -5561.450], mean action: 3.000 [3.000, 3.000],  loss: 15441151.000000, mae: 1245.395752, mean_q: -46.843849
 1713/5000: episode: 1713, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3332.491, mean reward: -3332.491 [-3332.491, -3332.491], mean action: 3.000 [3.000, 3.000],  loss: 15312498.000000, mae: 1186.180176, mean_q: -46.937054
 1714/5000: episode: 1714, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2538.497, mean reward: -2538.497 [-2538.497, -2538.497], mean action: 3.000 [3.000, 3.000],  loss: 12190628.000000, mae: 1063.215698, mean_q: -47.295605
 1715/5000: episode: 1715, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1247.409, mean reward: -1247.409 [-1247.409, -1247.409], mean action: 3.000 [3.000, 3.000],  loss: 12246891.000000, mae: 1027.618164, mean_q: -47.380238
 1716/5000: episode: 1716, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2610.293, mean reward: -2610.293 [-2610.293, -2610.293], mean action: 3.000 [3.000, 3.000],  loss: 15406252.000000, mae: 1201.935547, mean_q: -47.184540
 1717/5000: episode: 1717, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -665.592, mean reward: -665.592 [-665.592, -665.592], mean action: 3.000 [3.000, 3.000],  loss: 15431658.000000, mae: 1191.552490, mean_q: -47.256454
 1718/5000: episode: 1718, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4451.820, mean reward: -4451.820 [-4451.820, -4451.820], mean action: 3.000 [3.000, 3.000],  loss: 21014738.000000, mae: 1437.877686, mean_q: -47.358543
 1719/5000: episode: 1719, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6485.736, mean reward: -6485.736 [-6485.736, -6485.736], mean action: 3.000 [3.000, 3.000],  loss: 10261123.000000, mae: 925.669678, mean_q: -47.597572
 1720/5000: episode: 1720, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -829.545, mean reward: -829.545 [-829.545, -829.545], mean action: 2.000 [2.000, 2.000],  loss: 22461778.000000, mae: 1364.859619, mean_q: -47.366932
 1721/5000: episode: 1721, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5300.174, mean reward: -5300.174 [-5300.174, -5300.174], mean action: 3.000 [3.000, 3.000],  loss: 20218108.000000, mae: 1288.056763, mean_q: -47.773273
 1722/5000: episode: 1722, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -14.820, mean reward: -14.820 [-14.820, -14.820], mean action: 3.000 [3.000, 3.000],  loss: 19618130.000000, mae: 1328.377686, mean_q: -47.519493
 1723/5000: episode: 1723, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1321.963, mean reward: -1321.963 [-1321.963, -1321.963], mean action: 3.000 [3.000, 3.000],  loss: 17152136.000000, mae: 1188.805176, mean_q: -47.773491
 1724/5000: episode: 1724, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -2931.875, mean reward: -2931.875 [-2931.875, -2931.875], mean action: 3.000 [3.000, 3.000],  loss: 12540527.000000, mae: 1061.044312, mean_q: -47.771126
 1725/5000: episode: 1725, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2974.605, mean reward: -2974.605 [-2974.605, -2974.605], mean action: 3.000 [3.000, 3.000],  loss: 20956466.000000, mae: 1390.528320, mean_q: -47.837173
 1726/5000: episode: 1726, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -125.700, mean reward: -125.700 [-125.700, -125.700], mean action: 3.000 [3.000, 3.000],  loss: 16103823.000000, mae: 1109.870605, mean_q: -48.164017
 1727/5000: episode: 1727, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3189.549, mean reward: -3189.549 [-3189.549, -3189.549], mean action: 2.000 [2.000, 2.000],  loss: 21139356.000000, mae: 1288.636963, mean_q: -48.192307
 1728/5000: episode: 1728, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -757.269, mean reward: -757.269 [-757.269, -757.269], mean action: 3.000 [3.000, 3.000],  loss: 16116045.000000, mae: 1225.070190, mean_q: -48.149094
 1729/5000: episode: 1729, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3771.886, mean reward: -3771.886 [-3771.886, -3771.886], mean action: 3.000 [3.000, 3.000],  loss: 11846590.000000, mae: 1033.015625, mean_q: -48.531967
 1730/5000: episode: 1730, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3527.457, mean reward: -3527.457 [-3527.457, -3527.457], mean action: 3.000 [3.000, 3.000],  loss: 13239193.000000, mae: 1041.959106, mean_q: -48.387436
 1731/5000: episode: 1731, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -389.701, mean reward: -389.701 [-389.701, -389.701], mean action: 2.000 [2.000, 2.000],  loss: 13471378.000000, mae: 1094.413330, mean_q: -48.345078
 1732/5000: episode: 1732, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1684.215, mean reward: -1684.215 [-1684.215, -1684.215], mean action: 2.000 [2.000, 2.000],  loss: 15551691.000000, mae: 1129.595703, mean_q: -48.657791
 1733/5000: episode: 1733, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5477.059, mean reward: -5477.059 [-5477.059, -5477.059], mean action: 2.000 [2.000, 2.000],  loss: 20697468.000000, mae: 1437.979004, mean_q: -48.571892
 1734/5000: episode: 1734, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4662.140, mean reward: -4662.140 [-4662.140, -4662.140], mean action: 2.000 [2.000, 2.000],  loss: 9014088.000000, mae: 869.752808, mean_q: -48.831249
 1735/5000: episode: 1735, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -486.094, mean reward: -486.094 [-486.094, -486.094], mean action: 2.000 [2.000, 2.000],  loss: 22904992.000000, mae: 1414.576660, mean_q: -48.678379
 1736/5000: episode: 1736, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1936.725, mean reward: -1936.725 [-1936.725, -1936.725], mean action: 2.000 [2.000, 2.000],  loss: 15233748.000000, mae: 1133.357422, mean_q: -48.730980
 1737/5000: episode: 1737, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1589.377, mean reward: -1589.377 [-1589.377, -1589.377], mean action: 2.000 [2.000, 2.000],  loss: 16178297.000000, mae: 1198.284912, mean_q: -48.995914
 1738/5000: episode: 1738, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -260.800, mean reward: -260.800 [-260.800, -260.800], mean action: 2.000 [2.000, 2.000],  loss: 16096654.000000, mae: 1187.869751, mean_q: -49.105476
 1739/5000: episode: 1739, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2867.053, mean reward: -2867.053 [-2867.053, -2867.053], mean action: 2.000 [2.000, 2.000],  loss: 15347420.000000, mae: 1135.058594, mean_q: -49.132172
 1740/5000: episode: 1740, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1275.524, mean reward: -1275.524 [-1275.524, -1275.524], mean action: 2.000 [2.000, 2.000],  loss: 21703680.000000, mae: 1313.270874, mean_q: -49.162258
 1741/5000: episode: 1741, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2952.808, mean reward: -2952.808 [-2952.808, -2952.808], mean action: 2.000 [2.000, 2.000],  loss: 9443952.000000, mae: 822.722778, mean_q: -49.238621
 1742/5000: episode: 1742, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5210.689, mean reward: -5210.689 [-5210.689, -5210.689], mean action: 2.000 [2.000, 2.000],  loss: 13803938.000000, mae: 1098.448975, mean_q: -49.238609
 1743/5000: episode: 1743, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1184.519, mean reward: -1184.519 [-1184.519, -1184.519], mean action: 2.000 [2.000, 2.000],  loss: 8761383.000000, mae: 868.385193, mean_q: -49.499908
 1744/5000: episode: 1744, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -633.495, mean reward: -633.495 [-633.495, -633.495], mean action: 2.000 [2.000, 2.000],  loss: 11865191.000000, mae: 1002.926514, mean_q: -49.544224
 1745/5000: episode: 1745, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -95.453, mean reward: -95.453 [-95.453, -95.453], mean action: 2.000 [2.000, 2.000],  loss: 24595396.000000, mae: 1495.454590, mean_q: -49.440681
 1746/5000: episode: 1746, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -753.755, mean reward: -753.755 [-753.755, -753.755], mean action: 2.000 [2.000, 2.000],  loss: 15620954.000000, mae: 1160.894165, mean_q: -49.809669
 1747/5000: episode: 1747, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3905.211, mean reward: -3905.211 [-3905.211, -3905.211], mean action: 2.000 [2.000, 2.000],  loss: 11840042.000000, mae: 1031.327393, mean_q: -49.840508
 1748/5000: episode: 1748, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6312.161, mean reward: -6312.161 [-6312.161, -6312.161], mean action: 1.000 [1.000, 1.000],  loss: 14461723.000000, mae: 1127.534424, mean_q: -49.942913
 1749/5000: episode: 1749, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3414.116, mean reward: -3414.116 [-3414.116, -3414.116], mean action: 2.000 [2.000, 2.000],  loss: 14325814.000000, mae: 1092.172852, mean_q: -49.943054
 1750/5000: episode: 1750, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5498.575, mean reward: -5498.575 [-5498.575, -5498.575], mean action: 2.000 [2.000, 2.000],  loss: 14117020.000000, mae: 1130.695557, mean_q: -50.159630
 1751/5000: episode: 1751, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -999.624, mean reward: -999.624 [-999.624, -999.624], mean action: 2.000 [2.000, 2.000],  loss: 12765056.000000, mae: 1027.705078, mean_q: -50.322495
 1752/5000: episode: 1752, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -909.339, mean reward: -909.339 [-909.339, -909.339], mean action: 2.000 [2.000, 2.000],  loss: 12236185.000000, mae: 1084.840454, mean_q: -50.228779
 1753/5000: episode: 1753, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6.237, mean reward: -6.237 [-6.237, -6.237], mean action: 2.000 [2.000, 2.000],  loss: 16428274.000000, mae: 1207.737183, mean_q: -50.355598
 1754/5000: episode: 1754, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4988.478, mean reward: -4988.478 [-4988.478, -4988.478], mean action: 2.000 [2.000, 2.000],  loss: 16644942.000000, mae: 1240.073242, mean_q: -50.389843
 1755/5000: episode: 1755, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1842.880, mean reward: -1842.880 [-1842.880, -1842.880], mean action: 2.000 [2.000, 2.000],  loss: 17859880.000000, mae: 1192.368652, mean_q: -50.463127
 1756/5000: episode: 1756, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2175.471, mean reward: -2175.471 [-2175.471, -2175.471], mean action: 2.000 [2.000, 2.000],  loss: 18376736.000000, mae: 1295.583862, mean_q: -50.501778
 1757/5000: episode: 1757, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2984.130, mean reward: -2984.130 [-2984.130, -2984.130], mean action: 2.000 [2.000, 2.000],  loss: 25320800.000000, mae: 1556.851074, mean_q: -50.646053
 1758/5000: episode: 1758, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -646.805, mean reward: -646.805 [-646.805, -646.805], mean action: 3.000 [3.000, 3.000],  loss: 10900455.000000, mae: 973.578491, mean_q: -50.932453
 1759/5000: episode: 1759, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -1890.139, mean reward: -1890.139 [-1890.139, -1890.139], mean action: 2.000 [2.000, 2.000],  loss: 13385118.000000, mae: 1116.283691, mean_q: -50.678726
 1760/5000: episode: 1760, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3772.179, mean reward: -3772.179 [-3772.179, -3772.179], mean action: 2.000 [2.000, 2.000],  loss: 11792419.000000, mae: 1046.216553, mean_q: -50.953606
 1761/5000: episode: 1761, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2013.749, mean reward: -2013.749 [-2013.749, -2013.749], mean action: 2.000 [2.000, 2.000],  loss: 14745382.000000, mae: 1165.793213, mean_q: -50.914101
 1762/5000: episode: 1762, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -875.332, mean reward: -875.332 [-875.332, -875.332], mean action: 2.000 [2.000, 2.000],  loss: 17130714.000000, mae: 1287.227539, mean_q: -50.920658
 1763/5000: episode: 1763, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1555.762, mean reward: -1555.762 [-1555.762, -1555.762], mean action: 2.000 [2.000, 2.000],  loss: 15688810.000000, mae: 1225.258423, mean_q: -50.979191
 1764/5000: episode: 1764, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1958.684, mean reward: -1958.684 [-1958.684, -1958.684], mean action: 2.000 [2.000, 2.000],  loss: 20652708.000000, mae: 1345.745972, mean_q: -51.103489
 1765/5000: episode: 1765, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -678.492, mean reward: -678.492 [-678.492, -678.492], mean action: 2.000 [2.000, 2.000],  loss: 18200980.000000, mae: 1207.800293, mean_q: -51.175941
 1766/5000: episode: 1766, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3008.490, mean reward: -3008.490 [-3008.490, -3008.490], mean action: 2.000 [2.000, 2.000],  loss: 12312838.000000, mae: 1005.653748, mean_q: -51.396492
 1767/5000: episode: 1767, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4818.041, mean reward: -4818.041 [-4818.041, -4818.041], mean action: 2.000 [2.000, 2.000],  loss: 17679560.000000, mae: 1183.786987, mean_q: -51.479748
 1768/5000: episode: 1768, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5006.900, mean reward: -5006.900 [-5006.900, -5006.900], mean action: 2.000 [2.000, 2.000],  loss: 17664016.000000, mae: 1235.634277, mean_q: -51.479332
 1769/5000: episode: 1769, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -733.550, mean reward: -733.550 [-733.550, -733.550], mean action: 2.000 [2.000, 2.000],  loss: 18516152.000000, mae: 1213.462402, mean_q: -51.433567
 1770/5000: episode: 1770, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5625.580, mean reward: -5625.580 [-5625.580, -5625.580], mean action: 2.000 [2.000, 2.000],  loss: 18757286.000000, mae: 1177.285522, mean_q: -51.588333
 1771/5000: episode: 1771, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4558.149, mean reward: -4558.149 [-4558.149, -4558.149], mean action: 2.000 [2.000, 2.000],  loss: 22282252.000000, mae: 1240.562500, mean_q: -51.725998
 1772/5000: episode: 1772, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1160.107, mean reward: -1160.107 [-1160.107, -1160.107], mean action: 2.000 [2.000, 2.000],  loss: 17300120.000000, mae: 1278.332031, mean_q: -52.014854
 1773/5000: episode: 1773, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -316.968, mean reward: -316.968 [-316.968, -316.968], mean action: 2.000 [2.000, 2.000],  loss: 18625216.000000, mae: 1339.671387, mean_q: -51.807671
 1774/5000: episode: 1774, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6317.107, mean reward: -6317.107 [-6317.107, -6317.107], mean action: 2.000 [2.000, 2.000],  loss: 22601858.000000, mae: 1393.291260, mean_q: -52.263908
 1775/5000: episode: 1775, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2582.812, mean reward: -2582.812 [-2582.812, -2582.812], mean action: 2.000 [2.000, 2.000],  loss: 11516468.000000, mae: 1027.548828, mean_q: -52.077057
 1776/5000: episode: 1776, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1257.011, mean reward: -1257.011 [-1257.011, -1257.011], mean action: 2.000 [2.000, 2.000],  loss: 13191976.000000, mae: 1079.902222, mean_q: -52.192417
 1777/5000: episode: 1777, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1938.904, mean reward: -1938.904 [-1938.904, -1938.904], mean action: 2.000 [2.000, 2.000],  loss: 20214748.000000, mae: 1303.944092, mean_q: -52.222698
 1778/5000: episode: 1778, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2713.594, mean reward: -2713.594 [-2713.594, -2713.594], mean action: 2.000 [2.000, 2.000],  loss: 18038266.000000, mae: 1258.123535, mean_q: -52.394562
 1779/5000: episode: 1779, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1537.533, mean reward: -1537.533 [-1537.533, -1537.533], mean action: 2.000 [2.000, 2.000],  loss: 20228504.000000, mae: 1350.598145, mean_q: -52.312328
 1780/5000: episode: 1780, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1876.880, mean reward: -1876.880 [-1876.880, -1876.880], mean action: 2.000 [2.000, 2.000],  loss: 14305332.000000, mae: 1179.605713, mean_q: -52.695847
 1781/5000: episode: 1781, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -266.085, mean reward: -266.085 [-266.085, -266.085], mean action: 2.000 [2.000, 2.000],  loss: 12203319.000000, mae: 1130.587402, mean_q: -52.849655
 1782/5000: episode: 1782, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1891.562, mean reward: -1891.562 [-1891.562, -1891.562], mean action: 2.000 [2.000, 2.000],  loss: 19624924.000000, mae: 1296.271240, mean_q: -52.767109
 1783/5000: episode: 1783, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9849.654, mean reward: -9849.654 [-9849.654, -9849.654], mean action: 2.000 [2.000, 2.000],  loss: 16400553.000000, mae: 1137.868042, mean_q: -53.111755
 1784/5000: episode: 1784, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -183.818, mean reward: -183.818 [-183.818, -183.818], mean action: 3.000 [3.000, 3.000],  loss: 13764009.000000, mae: 1077.669678, mean_q: -52.928513
 1785/5000: episode: 1785, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2167.335, mean reward: -2167.335 [-2167.335, -2167.335], mean action: 2.000 [2.000, 2.000],  loss: 17353324.000000, mae: 1293.898071, mean_q: -52.845238
 1786/5000: episode: 1786, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2308.069, mean reward: -2308.069 [-2308.069, -2308.069], mean action: 2.000 [2.000, 2.000],  loss: 21322744.000000, mae: 1412.801758, mean_q: -52.870354
 1787/5000: episode: 1787, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6458.848, mean reward: -6458.848 [-6458.848, -6458.848], mean action: 2.000 [2.000, 2.000],  loss: 15019211.000000, mae: 1159.636719, mean_q: -52.931213
 1788/5000: episode: 1788, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2053.026, mean reward: -2053.026 [-2053.026, -2053.026], mean action: 2.000 [2.000, 2.000],  loss: 17836144.000000, mae: 1187.477051, mean_q: -53.436249
 1789/5000: episode: 1789, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -1173.883, mean reward: -1173.883 [-1173.883, -1173.883], mean action: 2.000 [2.000, 2.000],  loss: 21231196.000000, mae: 1307.974121, mean_q: -53.398403
 1790/5000: episode: 1790, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2175.362, mean reward: -2175.362 [-2175.362, -2175.362], mean action: 2.000 [2.000, 2.000],  loss: 13353952.000000, mae: 1005.059448, mean_q: -53.605179
 1791/5000: episode: 1791, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6153.921, mean reward: -6153.921 [-6153.921, -6153.921], mean action: 2.000 [2.000, 2.000],  loss: 21086492.000000, mae: 1485.650879, mean_q: -53.581360
 1792/5000: episode: 1792, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -167.597, mean reward: -167.597 [-167.597, -167.597], mean action: 2.000 [2.000, 2.000],  loss: 16038850.000000, mae: 1146.276367, mean_q: -53.586647
 1793/5000: episode: 1793, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -7023.243, mean reward: -7023.243 [-7023.243, -7023.243], mean action: 2.000 [2.000, 2.000],  loss: 21759680.000000, mae: 1463.859863, mean_q: -53.777435
 1794/5000: episode: 1794, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -883.321, mean reward: -883.321 [-883.321, -883.321], mean action: 2.000 [2.000, 2.000],  loss: 10601054.000000, mae: 912.304688, mean_q: -54.070797
 1795/5000: episode: 1795, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3492.952, mean reward: -3492.952 [-3492.952, -3492.952], mean action: 0.000 [0.000, 0.000],  loss: 12362589.000000, mae: 980.398804, mean_q: -53.984375
 1796/5000: episode: 1796, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1515.060, mean reward: -1515.060 [-1515.060, -1515.060], mean action: 2.000 [2.000, 2.000],  loss: 13335122.000000, mae: 1130.937134, mean_q: -54.324661
 1797/5000: episode: 1797, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4764.753, mean reward: -4764.753 [-4764.753, -4764.753], mean action: 2.000 [2.000, 2.000],  loss: 16012982.000000, mae: 1223.201782, mean_q: -53.997017
 1798/5000: episode: 1798, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -516.077, mean reward: -516.077 [-516.077, -516.077], mean action: 2.000 [2.000, 2.000],  loss: 13974116.000000, mae: 1022.781982, mean_q: -54.519226
 1799/5000: episode: 1799, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2435.834, mean reward: -2435.834 [-2435.834, -2435.834], mean action: 2.000 [2.000, 2.000],  loss: 20333308.000000, mae: 1322.825562, mean_q: -54.398861
 1800/5000: episode: 1800, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4401.440, mean reward: -4401.440 [-4401.440, -4401.440], mean action: 2.000 [2.000, 2.000],  loss: 9094400.000000, mae: 890.875305, mean_q: -54.608753
 1801/5000: episode: 1801, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -862.610, mean reward: -862.610 [-862.610, -862.610], mean action: 2.000 [2.000, 2.000],  loss: 14726853.000000, mae: 1181.908447, mean_q: -54.493374
 1802/5000: episode: 1802, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -319.737, mean reward: -319.737 [-319.737, -319.737], mean action: 2.000 [2.000, 2.000],  loss: 14690305.000000, mae: 1175.670654, mean_q: -54.879757
 1803/5000: episode: 1803, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -3657.773, mean reward: -3657.773 [-3657.773, -3657.773], mean action: 2.000 [2.000, 2.000],  loss: 8279830.000000, mae: 893.112671, mean_q: -54.849613
 1804/5000: episode: 1804, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1212.966, mean reward: -1212.966 [-1212.966, -1212.966], mean action: 2.000 [2.000, 2.000],  loss: 11849869.000000, mae: 926.207031, mean_q: -54.989197
 1805/5000: episode: 1805, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1478.049, mean reward: -1478.049 [-1478.049, -1478.049], mean action: 2.000 [2.000, 2.000],  loss: 14951780.000000, mae: 1216.852051, mean_q: -55.049694
 1806/5000: episode: 1806, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -6141.074, mean reward: -6141.074 [-6141.074, -6141.074], mean action: 2.000 [2.000, 2.000],  loss: 13712930.000000, mae: 1114.804688, mean_q: -54.999134
 1807/5000: episode: 1807, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1633.880, mean reward: -1633.880 [-1633.880, -1633.880], mean action: 3.000 [3.000, 3.000],  loss: 14691297.000000, mae: 1134.359985, mean_q: -55.160347
 1808/5000: episode: 1808, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3105.717, mean reward: -3105.717 [-3105.717, -3105.717], mean action: 2.000 [2.000, 2.000],  loss: 12657728.000000, mae: 1064.920410, mean_q: -55.231300
 1809/5000: episode: 1809, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8281.981, mean reward: -8281.981 [-8281.981, -8281.981], mean action: 1.000 [1.000, 1.000],  loss: 28698072.000000, mae: 1661.721802, mean_q: -55.145844
 1810/5000: episode: 1810, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12053.064, mean reward: -12053.064 [-12053.064, -12053.064], mean action: 0.000 [0.000, 0.000],  loss: 23931658.000000, mae: 1408.338623, mean_q: -55.325485
 1811/5000: episode: 1811, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5169.293, mean reward: -5169.293 [-5169.293, -5169.293], mean action: 1.000 [1.000, 1.000],  loss: 19495440.000000, mae: 1367.005859, mean_q: -55.486977
 1812/5000: episode: 1812, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1569.708, mean reward: -1569.708 [-1569.708, -1569.708], mean action: 2.000 [2.000, 2.000],  loss: 16654630.000000, mae: 1229.717041, mean_q: -55.688473
 1813/5000: episode: 1813, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1096.872, mean reward: -1096.872 [-1096.872, -1096.872], mean action: 1.000 [1.000, 1.000],  loss: 16343014.000000, mae: 1216.143433, mean_q: -55.454914
 1814/5000: episode: 1814, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5918.552, mean reward: -5918.552 [-5918.552, -5918.552], mean action: 2.000 [2.000, 2.000],  loss: 18546314.000000, mae: 1240.198975, mean_q: -55.781414
 1815/5000: episode: 1815, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1870.734, mean reward: -1870.734 [-1870.734, -1870.734], mean action: 2.000 [2.000, 2.000],  loss: 14986900.000000, mae: 1117.132812, mean_q: -55.937294
 1816/5000: episode: 1816, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3133.481, mean reward: -3133.481 [-3133.481, -3133.481], mean action: 2.000 [2.000, 2.000],  loss: 14928996.000000, mae: 1162.223877, mean_q: -55.796375
 1817/5000: episode: 1817, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6676.192, mean reward: -6676.192 [-6676.192, -6676.192], mean action: 1.000 [1.000, 1.000],  loss: 16875648.000000, mae: 1278.782471, mean_q: -55.967216
 1818/5000: episode: 1818, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3055.889, mean reward: -3055.889 [-3055.889, -3055.889], mean action: 2.000 [2.000, 2.000],  loss: 20410082.000000, mae: 1355.712158, mean_q: -56.052216
 1819/5000: episode: 1819, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -660.883, mean reward: -660.883 [-660.883, -660.883], mean action: 2.000 [2.000, 2.000],  loss: 13553815.000000, mae: 1036.397827, mean_q: -56.113300
 1820/5000: episode: 1820, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2314.626, mean reward: -2314.626 [-2314.626, -2314.626], mean action: 2.000 [2.000, 2.000],  loss: 10381384.000000, mae: 905.938538, mean_q: -56.344490
 1821/5000: episode: 1821, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9556.998, mean reward: -9556.998 [-9556.998, -9556.998], mean action: 0.000 [0.000, 0.000],  loss: 19157988.000000, mae: 1330.546753, mean_q: -56.305931
 1822/5000: episode: 1822, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -3040.358, mean reward: -3040.358 [-3040.358, -3040.358], mean action: 2.000 [2.000, 2.000],  loss: 15914942.000000, mae: 1162.107788, mean_q: -56.689747
 1823/5000: episode: 1823, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1308.293, mean reward: -1308.293 [-1308.293, -1308.293], mean action: 2.000 [2.000, 2.000],  loss: 12706388.000000, mae: 1129.710693, mean_q: -56.721607
 1824/5000: episode: 1824, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2161.395, mean reward: -2161.395 [-2161.395, -2161.395], mean action: 2.000 [2.000, 2.000],  loss: 20844342.000000, mae: 1400.029053, mean_q: -56.304516
 1825/5000: episode: 1825, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4585.329, mean reward: -4585.329 [-4585.329, -4585.329], mean action: 2.000 [2.000, 2.000],  loss: 9497932.000000, mae: 939.723755, mean_q: -56.840340
 1826/5000: episode: 1826, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1703.616, mean reward: -1703.616 [-1703.616, -1703.616], mean action: 2.000 [2.000, 2.000],  loss: 18098182.000000, mae: 1285.759521, mean_q: -56.853249
 1827/5000: episode: 1827, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -409.041, mean reward: -409.041 [-409.041, -409.041], mean action: 2.000 [2.000, 2.000],  loss: 19452804.000000, mae: 1300.383057, mean_q: -56.748180
 1828/5000: episode: 1828, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3665.558, mean reward: -3665.558 [-3665.558, -3665.558], mean action: 1.000 [1.000, 1.000],  loss: 19179414.000000, mae: 1352.735962, mean_q: -56.921173
 1829/5000: episode: 1829, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3411.364, mean reward: -3411.364 [-3411.364, -3411.364], mean action: 1.000 [1.000, 1.000],  loss: 7938316.000000, mae: 877.349243, mean_q: -57.463287
 1830/5000: episode: 1830, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1671.450, mean reward: -1671.450 [-1671.450, -1671.450], mean action: 2.000 [2.000, 2.000],  loss: 13787359.000000, mae: 1111.873169, mean_q: -57.472595
 1831/5000: episode: 1831, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3062.858, mean reward: -3062.858 [-3062.858, -3062.858], mean action: 1.000 [1.000, 1.000],  loss: 15059975.000000, mae: 1217.589355, mean_q: -57.369957
 1832/5000: episode: 1832, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -6265.477, mean reward: -6265.477 [-6265.477, -6265.477], mean action: 1.000 [1.000, 1.000],  loss: 16162010.000000, mae: 1135.108032, mean_q: -57.514126
 1833/5000: episode: 1833, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2980.976, mean reward: -2980.976 [-2980.976, -2980.976], mean action: 2.000 [2.000, 2.000],  loss: 16612185.000000, mae: 1226.416504, mean_q: -57.409771
 1834/5000: episode: 1834, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -842.961, mean reward: -842.961 [-842.961, -842.961], mean action: 2.000 [2.000, 2.000],  loss: 18072012.000000, mae: 1281.273682, mean_q: -57.473366
 1835/5000: episode: 1835, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2578.059, mean reward: -2578.059 [-2578.059, -2578.059], mean action: 2.000 [2.000, 2.000],  loss: 11891094.000000, mae: 894.347717, mean_q: -57.805244
 1836/5000: episode: 1836, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -11028.736, mean reward: -11028.736 [-11028.736, -11028.736], mean action: 3.000 [3.000, 3.000],  loss: 12122245.000000, mae: 1043.671021, mean_q: -57.964420
 1837/5000: episode: 1837, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8116.498, mean reward: -8116.498 [-8116.498, -8116.498], mean action: 1.000 [1.000, 1.000],  loss: 22102156.000000, mae: 1389.408325, mean_q: -57.797607
 1838/5000: episode: 1838, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1920.682, mean reward: -1920.682 [-1920.682, -1920.682], mean action: 1.000 [1.000, 1.000],  loss: 17047550.000000, mae: 1232.198730, mean_q: -58.091965
 1839/5000: episode: 1839, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1193.355, mean reward: -1193.355 [-1193.355, -1193.355], mean action: 1.000 [1.000, 1.000],  loss: 18015176.000000, mae: 1249.448975, mean_q: -57.959164
 1840/5000: episode: 1840, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9669.759, mean reward: -9669.759 [-9669.759, -9669.759], mean action: 1.000 [1.000, 1.000],  loss: 10901019.000000, mae: 993.559326, mean_q: -58.132751
 1841/5000: episode: 1841, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -10272.159, mean reward: -10272.159 [-10272.159, -10272.159], mean action: 1.000 [1.000, 1.000],  loss: 16604302.000000, mae: 1243.778931, mean_q: -58.104942
 1842/5000: episode: 1842, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6226.067, mean reward: -6226.067 [-6226.067, -6226.067], mean action: 1.000 [1.000, 1.000],  loss: 19419526.000000, mae: 1352.666748, mean_q: -58.020493
 1843/5000: episode: 1843, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4603.558, mean reward: -4603.558 [-4603.558, -4603.558], mean action: 1.000 [1.000, 1.000],  loss: 13845911.000000, mae: 1137.624512, mean_q: -58.386166
 1844/5000: episode: 1844, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1816.695, mean reward: -1816.695 [-1816.695, -1816.695], mean action: 1.000 [1.000, 1.000],  loss: 15236926.000000, mae: 1059.875244, mean_q: -58.788700
 1845/5000: episode: 1845, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -827.178, mean reward: -827.178 [-827.178, -827.178], mean action: 1.000 [1.000, 1.000],  loss: 20822960.000000, mae: 1352.404541, mean_q: -58.333397
 1846/5000: episode: 1846, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5183.797, mean reward: -5183.797 [-5183.797, -5183.797], mean action: 1.000 [1.000, 1.000],  loss: 14728350.000000, mae: 1121.424316, mean_q: -58.628761
 1847/5000: episode: 1847, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5534.150, mean reward: -5534.150 [-5534.150, -5534.150], mean action: 1.000 [1.000, 1.000],  loss: 10264635.000000, mae: 974.787109, mean_q: -58.974209
 1848/5000: episode: 1848, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6764.013, mean reward: -6764.013 [-6764.013, -6764.013], mean action: 1.000 [1.000, 1.000],  loss: 15221198.000000, mae: 1217.322998, mean_q: -58.825302
 1849/5000: episode: 1849, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1071.577, mean reward: -1071.577 [-1071.577, -1071.577], mean action: 2.000 [2.000, 2.000],  loss: 13884454.000000, mae: 1118.397095, mean_q: -59.013969
 1850/5000: episode: 1850, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5441.252, mean reward: -5441.252 [-5441.252, -5441.252], mean action: 1.000 [1.000, 1.000],  loss: 17816770.000000, mae: 1302.829590, mean_q: -59.024185
 1851/5000: episode: 1851, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2819.022, mean reward: -2819.022 [-2819.022, -2819.022], mean action: 1.000 [1.000, 1.000],  loss: 14301742.000000, mae: 1147.016113, mean_q: -59.357658
 1852/5000: episode: 1852, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8377.723, mean reward: -8377.723 [-8377.723, -8377.723], mean action: 1.000 [1.000, 1.000],  loss: 14548776.000000, mae: 1120.870117, mean_q: -59.373032
 1853/5000: episode: 1853, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2675.693, mean reward: -2675.693 [-2675.693, -2675.693], mean action: 1.000 [1.000, 1.000],  loss: 18033112.000000, mae: 1311.337402, mean_q: -59.310913
 1854/5000: episode: 1854, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1292.004, mean reward: -1292.004 [-1292.004, -1292.004], mean action: 1.000 [1.000, 1.000],  loss: 16644778.000000, mae: 1199.799072, mean_q: -59.442917
 1855/5000: episode: 1855, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -6143.462, mean reward: -6143.462 [-6143.462, -6143.462], mean action: 1.000 [1.000, 1.000],  loss: 20478676.000000, mae: 1445.054321, mean_q: -59.552757
 1856/5000: episode: 1856, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -9676.525, mean reward: -9676.525 [-9676.525, -9676.525], mean action: 1.000 [1.000, 1.000],  loss: 22998218.000000, mae: 1519.090576, mean_q: -59.491959
 1857/5000: episode: 1857, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2534.187, mean reward: -2534.187 [-2534.187, -2534.187], mean action: 1.000 [1.000, 1.000],  loss: 16435851.000000, mae: 1245.415527, mean_q: -59.657425
 1858/5000: episode: 1858, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1175.535, mean reward: -1175.535 [-1175.535, -1175.535], mean action: 1.000 [1.000, 1.000],  loss: 14432016.000000, mae: 1177.441406, mean_q: -59.901970
 1859/5000: episode: 1859, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -3574.499, mean reward: -3574.499 [-3574.499, -3574.499], mean action: 1.000 [1.000, 1.000],  loss: 16587192.000000, mae: 1115.312988, mean_q: -59.815071
 1860/5000: episode: 1860, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -491.569, mean reward: -491.569 [-491.569, -491.569], mean action: 1.000 [1.000, 1.000],  loss: 16764932.000000, mae: 1180.631714, mean_q: -60.295719
 1861/5000: episode: 1861, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4922.295, mean reward: -4922.295 [-4922.295, -4922.295], mean action: 3.000 [3.000, 3.000],  loss: 11860044.000000, mae: 996.607239, mean_q: -60.136490
 1862/5000: episode: 1862, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2473.974, mean reward: -2473.974 [-2473.974, -2473.974], mean action: 1.000 [1.000, 1.000],  loss: 11188008.000000, mae: 1017.421326, mean_q: -60.212967
 1863/5000: episode: 1863, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2372.910, mean reward: -2372.910 [-2372.910, -2372.910], mean action: 3.000 [3.000, 3.000],  loss: 11993420.000000, mae: 1043.849609, mean_q: -60.413139
 1864/5000: episode: 1864, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -15.755, mean reward: -15.755 [-15.755, -15.755], mean action: 3.000 [3.000, 3.000],  loss: 12545486.000000, mae: 1013.979553, mean_q: -60.233597
 1865/5000: episode: 1865, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -629.341, mean reward: -629.341 [-629.341, -629.341], mean action: 3.000 [3.000, 3.000],  loss: 15120764.000000, mae: 1188.475220, mean_q: -60.490089
 1866/5000: episode: 1866, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1675.483, mean reward: -1675.483 [-1675.483, -1675.483], mean action: 3.000 [3.000, 3.000],  loss: 12125322.000000, mae: 1073.663330, mean_q: -60.682407
 1867/5000: episode: 1867, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -1383.444, mean reward: -1383.444 [-1383.444, -1383.444], mean action: 2.000 [2.000, 2.000],  loss: 18604036.000000, mae: 1335.907471, mean_q: -60.390858
 1868/5000: episode: 1868, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6964.897, mean reward: -6964.897 [-6964.897, -6964.897], mean action: 3.000 [3.000, 3.000],  loss: 9307294.000000, mae: 935.461548, mean_q: -60.734207
 1869/5000: episode: 1869, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3856.174, mean reward: -3856.174 [-3856.174, -3856.174], mean action: 0.000 [0.000, 0.000],  loss: 15130775.000000, mae: 1176.043335, mean_q: -60.655212
 1870/5000: episode: 1870, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4253.288, mean reward: -4253.288 [-4253.288, -4253.288], mean action: 3.000 [3.000, 3.000],  loss: 18632156.000000, mae: 1288.561768, mean_q: -60.911404
 1871/5000: episode: 1871, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1341.813, mean reward: -1341.813 [-1341.813, -1341.813], mean action: 3.000 [3.000, 3.000],  loss: 12278082.000000, mae: 1013.649292, mean_q: -60.947781
 1872/5000: episode: 1872, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5260.996, mean reward: -5260.996 [-5260.996, -5260.996], mean action: 3.000 [3.000, 3.000],  loss: 17002968.000000, mae: 1281.050293, mean_q: -61.129036
 1873/5000: episode: 1873, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1138.852, mean reward: -1138.852 [-1138.852, -1138.852], mean action: 3.000 [3.000, 3.000],  loss: 11170926.000000, mae: 1015.168457, mean_q: -61.208817
 1874/5000: episode: 1874, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4863.056, mean reward: -4863.056 [-4863.056, -4863.056], mean action: 3.000 [3.000, 3.000],  loss: 17014778.000000, mae: 1264.707397, mean_q: -61.284943
 1875/5000: episode: 1875, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3658.785, mean reward: -3658.785 [-3658.785, -3658.785], mean action: 2.000 [2.000, 2.000],  loss: 14504067.000000, mae: 1206.001709, mean_q: -61.226097
 1876/5000: episode: 1876, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2319.893, mean reward: -2319.893 [-2319.893, -2319.893], mean action: 3.000 [3.000, 3.000],  loss: 12153804.000000, mae: 1012.702820, mean_q: -61.488647
 1877/5000: episode: 1877, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3515.844, mean reward: -3515.844 [-3515.844, -3515.844], mean action: 3.000 [3.000, 3.000],  loss: 16563418.000000, mae: 1271.948730, mean_q: -61.230904
 1878/5000: episode: 1878, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8376.077, mean reward: -8376.077 [-8376.077, -8376.077], mean action: 3.000 [3.000, 3.000],  loss: 21366724.000000, mae: 1417.683960, mean_q: -61.500885
 1879/5000: episode: 1879, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -116.632, mean reward: -116.632 [-116.632, -116.632], mean action: 3.000 [3.000, 3.000],  loss: 24113838.000000, mae: 1480.577026, mean_q: -61.505672
 1880/5000: episode: 1880, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -277.186, mean reward: -277.186 [-277.186, -277.186], mean action: 3.000 [3.000, 3.000],  loss: 22242360.000000, mae: 1491.418213, mean_q: -61.826088
 1881/5000: episode: 1881, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4618.676, mean reward: -4618.676 [-4618.676, -4618.676], mean action: 3.000 [3.000, 3.000],  loss: 15759498.000000, mae: 1267.873413, mean_q: -61.999809
 1882/5000: episode: 1882, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5540.791, mean reward: -5540.791 [-5540.791, -5540.791], mean action: 3.000 [3.000, 3.000],  loss: 17284092.000000, mae: 1334.363281, mean_q: -61.894367
 1883/5000: episode: 1883, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2477.651, mean reward: -2477.651 [-2477.651, -2477.651], mean action: 2.000 [2.000, 2.000],  loss: 11956601.000000, mae: 1003.361694, mean_q: -62.044441
 1884/5000: episode: 1884, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5361.391, mean reward: -5361.391 [-5361.391, -5361.391], mean action: 3.000 [3.000, 3.000],  loss: 12720296.000000, mae: 1050.129395, mean_q: -62.395119
 1885/5000: episode: 1885, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -278.825, mean reward: -278.825 [-278.825, -278.825], mean action: 2.000 [2.000, 2.000],  loss: 16606890.000000, mae: 1218.369019, mean_q: -62.453758
 1886/5000: episode: 1886, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6593.216, mean reward: -6593.216 [-6593.216, -6593.216], mean action: 1.000 [1.000, 1.000],  loss: 12472889.000000, mae: 1109.165894, mean_q: -62.415653
 1887/5000: episode: 1887, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3765.277, mean reward: -3765.277 [-3765.277, -3765.277], mean action: 3.000 [3.000, 3.000],  loss: 13409860.000000, mae: 1132.219482, mean_q: -62.454376
 1888/5000: episode: 1888, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -192.096, mean reward: -192.096 [-192.096, -192.096], mean action: 2.000 [2.000, 2.000],  loss: 10417971.000000, mae: 946.736633, mean_q: -62.715958
 1889/5000: episode: 1889, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4617.432, mean reward: -4617.432 [-4617.432, -4617.432], mean action: 3.000 [3.000, 3.000],  loss: 14576512.000000, mae: 1103.780762, mean_q: -62.403019
 1890/5000: episode: 1890, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1612.795, mean reward: -1612.795 [-1612.795, -1612.795], mean action: 3.000 [3.000, 3.000],  loss: 20624036.000000, mae: 1311.156860, mean_q: -62.834206
 1891/5000: episode: 1891, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -559.386, mean reward: -559.386 [-559.386, -559.386], mean action: 3.000 [3.000, 3.000],  loss: 15569390.000000, mae: 1093.747070, mean_q: -62.671715
 1892/5000: episode: 1892, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3912.573, mean reward: -3912.573 [-3912.573, -3912.573], mean action: 3.000 [3.000, 3.000],  loss: 16075913.000000, mae: 1188.904297, mean_q: -62.812069
 1893/5000: episode: 1893, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1700.797, mean reward: -1700.797 [-1700.797, -1700.797], mean action: 3.000 [3.000, 3.000],  loss: 11435122.000000, mae: 998.435608, mean_q: -62.977459
 1894/5000: episode: 1894, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3167.901, mean reward: -3167.901 [-3167.901, -3167.901], mean action: 3.000 [3.000, 3.000],  loss: 23745432.000000, mae: 1412.271973, mean_q: -62.820908
 1895/5000: episode: 1895, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2147.234, mean reward: -2147.234 [-2147.234, -2147.234], mean action: 3.000 [3.000, 3.000],  loss: 20236570.000000, mae: 1418.675781, mean_q: -62.987408
 1896/5000: episode: 1896, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1653.310, mean reward: -1653.310 [-1653.310, -1653.310], mean action: 1.000 [1.000, 1.000],  loss: 24413128.000000, mae: 1529.816406, mean_q: -63.084503
 1897/5000: episode: 1897, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1153.727, mean reward: -1153.727 [-1153.727, -1153.727], mean action: 3.000 [3.000, 3.000],  loss: 14357214.000000, mae: 1165.824463, mean_q: -63.442284
 1898/5000: episode: 1898, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10121.964, mean reward: -10121.964 [-10121.964, -10121.964], mean action: 0.000 [0.000, 0.000],  loss: 18841976.000000, mae: 1303.317505, mean_q: -63.519135
 1899/5000: episode: 1899, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -40.279, mean reward: -40.279 [-40.279, -40.279], mean action: 3.000 [3.000, 3.000],  loss: 14483857.000000, mae: 1191.710938, mean_q: -63.609978
 1900/5000: episode: 1900, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1893.388, mean reward: -1893.388 [-1893.388, -1893.388], mean action: 3.000 [3.000, 3.000],  loss: 20758512.000000, mae: 1398.647217, mean_q: -63.597382
 1901/5000: episode: 1901, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7864.304, mean reward: -7864.304 [-7864.304, -7864.304], mean action: 3.000 [3.000, 3.000],  loss: 17069232.000000, mae: 1215.435059, mean_q: -63.704601
 1902/5000: episode: 1902, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5140.746, mean reward: -5140.746 [-5140.746, -5140.746], mean action: 3.000 [3.000, 3.000],  loss: 17999804.000000, mae: 1330.698975, mean_q: -63.768692
 1903/5000: episode: 1903, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -244.115, mean reward: -244.115 [-244.115, -244.115], mean action: 3.000 [3.000, 3.000],  loss: 14380920.000000, mae: 1079.083740, mean_q: -64.157654
 1904/5000: episode: 1904, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5.533, mean reward: -5.533 [-5.533, -5.533], mean action: 3.000 [3.000, 3.000],  loss: 22785410.000000, mae: 1451.684082, mean_q: -63.696365
 1905/5000: episode: 1905, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3249.170, mean reward: -3249.170 [-3249.170, -3249.170], mean action: 3.000 [3.000, 3.000],  loss: 12576340.000000, mae: 1032.069824, mean_q: -64.164795
 1906/5000: episode: 1906, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9859.858, mean reward: -9859.858 [-9859.858, -9859.858], mean action: 3.000 [3.000, 3.000],  loss: 10404378.000000, mae: 993.561340, mean_q: -64.340454
 1907/5000: episode: 1907, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -202.776, mean reward: -202.776 [-202.776, -202.776], mean action: 3.000 [3.000, 3.000],  loss: 16158374.000000, mae: 1266.419678, mean_q: -64.246765
 1908/5000: episode: 1908, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10653.889, mean reward: -10653.889 [-10653.889, -10653.889], mean action: 3.000 [3.000, 3.000],  loss: 17043646.000000, mae: 1293.238770, mean_q: -64.485046
 1909/5000: episode: 1909, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -137.357, mean reward: -137.357 [-137.357, -137.357], mean action: 3.000 [3.000, 3.000],  loss: 18861020.000000, mae: 1239.713745, mean_q: -64.436714
 1910/5000: episode: 1910, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6482.835, mean reward: -6482.835 [-6482.835, -6482.835], mean action: 3.000 [3.000, 3.000],  loss: 14529336.000000, mae: 1125.678589, mean_q: -64.780022
 1911/5000: episode: 1911, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -417.973, mean reward: -417.973 [-417.973, -417.973], mean action: 2.000 [2.000, 2.000],  loss: 15978762.000000, mae: 1243.023193, mean_q: -64.473991
 1912/5000: episode: 1912, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6143.089, mean reward: -6143.089 [-6143.089, -6143.089], mean action: 3.000 [3.000, 3.000],  loss: 15452744.000000, mae: 1118.802490, mean_q: -64.974236
 1913/5000: episode: 1913, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -8750.612, mean reward: -8750.612 [-8750.612, -8750.612], mean action: 3.000 [3.000, 3.000],  loss: 10617359.000000, mae: 945.337158, mean_q: -64.912117
 1914/5000: episode: 1914, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6994.965, mean reward: -6994.965 [-6994.965, -6994.965], mean action: 3.000 [3.000, 3.000],  loss: 13992230.000000, mae: 1208.347168, mean_q: -64.996994
 1915/5000: episode: 1915, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -414.844, mean reward: -414.844 [-414.844, -414.844], mean action: 1.000 [1.000, 1.000],  loss: 23392128.000000, mae: 1428.308838, mean_q: -64.897331
 1916/5000: episode: 1916, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -179.759, mean reward: -179.759 [-179.759, -179.759], mean action: 1.000 [1.000, 1.000],  loss: 13869331.000000, mae: 1134.857544, mean_q: -65.420685
 1917/5000: episode: 1917, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2976.129, mean reward: -2976.129 [-2976.129, -2976.129], mean action: 1.000 [1.000, 1.000],  loss: 17903826.000000, mae: 1274.296143, mean_q: -65.575958
 1918/5000: episode: 1918, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -370.798, mean reward: -370.798 [-370.798, -370.798], mean action: 1.000 [1.000, 1.000],  loss: 16577662.000000, mae: 1153.015991, mean_q: -65.387245
 1919/5000: episode: 1919, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1731.414, mean reward: -1731.414 [-1731.414, -1731.414], mean action: 1.000 [1.000, 1.000],  loss: 14986952.000000, mae: 1124.096924, mean_q: -65.621399
 1920/5000: episode: 1920, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4321.549, mean reward: -4321.549 [-4321.549, -4321.549], mean action: 1.000 [1.000, 1.000],  loss: 13286648.000000, mae: 1086.206177, mean_q: -65.585167
 1921/5000: episode: 1921, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1741.990, mean reward: -1741.990 [-1741.990, -1741.990], mean action: 1.000 [1.000, 1.000],  loss: 11303148.000000, mae: 1012.695679, mean_q: -65.833878
 1922/5000: episode: 1922, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4439.441, mean reward: -4439.441 [-4439.441, -4439.441], mean action: 1.000 [1.000, 1.000],  loss: 11603084.000000, mae: 1111.307861, mean_q: -65.915321
 1923/5000: episode: 1923, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1604.286, mean reward: -1604.286 [-1604.286, -1604.286], mean action: 1.000 [1.000, 1.000],  loss: 17190048.000000, mae: 1300.726929, mean_q: -66.161606
 1924/5000: episode: 1924, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4022.529, mean reward: -4022.529 [-4022.529, -4022.529], mean action: 1.000 [1.000, 1.000],  loss: 10262930.000000, mae: 996.273682, mean_q: -66.026718
 1925/5000: episode: 1925, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -9065.470, mean reward: -9065.470 [-9065.470, -9065.470], mean action: 1.000 [1.000, 1.000],  loss: 19460304.000000, mae: 1331.421875, mean_q: -66.018272
 1926/5000: episode: 1926, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6852.993, mean reward: -6852.993 [-6852.993, -6852.993], mean action: 1.000 [1.000, 1.000],  loss: 11916141.000000, mae: 1049.169678, mean_q: -66.560730
 1927/5000: episode: 1927, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1187.103, mean reward: -1187.103 [-1187.103, -1187.103], mean action: 1.000 [1.000, 1.000],  loss: 15249317.000000, mae: 1210.670166, mean_q: -66.471504
 1928/5000: episode: 1928, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -305.570, mean reward: -305.570 [-305.570, -305.570], mean action: 1.000 [1.000, 1.000],  loss: 16084420.000000, mae: 1178.862671, mean_q: -66.082512
 1929/5000: episode: 1929, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1527.552, mean reward: -1527.552 [-1527.552, -1527.552], mean action: 1.000 [1.000, 1.000],  loss: 16396780.000000, mae: 1186.502197, mean_q: -66.427017
 1930/5000: episode: 1930, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5334.219, mean reward: -5334.219 [-5334.219, -5334.219], mean action: 1.000 [1.000, 1.000],  loss: 14898514.000000, mae: 1217.731567, mean_q: -66.405655
 1931/5000: episode: 1931, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3101.412, mean reward: -3101.412 [-3101.412, -3101.412], mean action: 1.000 [1.000, 1.000],  loss: 16574800.000000, mae: 1164.575439, mean_q: -66.628349
 1932/5000: episode: 1932, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1947.949, mean reward: -1947.949 [-1947.949, -1947.949], mean action: 1.000 [1.000, 1.000],  loss: 17568810.000000, mae: 1320.957275, mean_q: -66.867859
 1933/5000: episode: 1933, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -3674.662, mean reward: -3674.662 [-3674.662, -3674.662], mean action: 1.000 [1.000, 1.000],  loss: 13619989.000000, mae: 1122.116455, mean_q: -66.942169
 1934/5000: episode: 1934, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5481.381, mean reward: -5481.381 [-5481.381, -5481.381], mean action: 1.000 [1.000, 1.000],  loss: 13201957.000000, mae: 1047.092041, mean_q: -66.747452
 1935/5000: episode: 1935, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2945.353, mean reward: -2945.353 [-2945.353, -2945.353], mean action: 0.000 [0.000, 0.000],  loss: 16476954.000000, mae: 1228.544800, mean_q: -66.820282
 1936/5000: episode: 1936, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4275.197, mean reward: -4275.197 [-4275.197, -4275.197], mean action: 1.000 [1.000, 1.000],  loss: 9787562.000000, mae: 980.436401, mean_q: -67.238121
 1937/5000: episode: 1937, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -7786.158, mean reward: -7786.158 [-7786.158, -7786.158], mean action: 1.000 [1.000, 1.000],  loss: 13590444.000000, mae: 1080.150269, mean_q: -67.297852
 1938/5000: episode: 1938, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1572.230, mean reward: -1572.230 [-1572.230, -1572.230], mean action: 1.000 [1.000, 1.000],  loss: 16398984.000000, mae: 1244.367310, mean_q: -67.173111
 1939/5000: episode: 1939, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4097.754, mean reward: -4097.754 [-4097.754, -4097.754], mean action: 1.000 [1.000, 1.000],  loss: 18654400.000000, mae: 1265.173828, mean_q: -67.535141
 1940/5000: episode: 1940, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7223.097, mean reward: -7223.097 [-7223.097, -7223.097], mean action: 1.000 [1.000, 1.000],  loss: 14248105.000000, mae: 1213.493896, mean_q: -67.601746
 1941/5000: episode: 1941, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5511.119, mean reward: -5511.119 [-5511.119, -5511.119], mean action: 1.000 [1.000, 1.000],  loss: 13982351.000000, mae: 1181.122925, mean_q: -67.543503
 1942/5000: episode: 1942, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3272.019, mean reward: -3272.019 [-3272.019, -3272.019], mean action: 1.000 [1.000, 1.000],  loss: 11137298.000000, mae: 1023.268433, mean_q: -67.486145
 1943/5000: episode: 1943, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -8626.945, mean reward: -8626.945 [-8626.945, -8626.945], mean action: 1.000 [1.000, 1.000],  loss: 15975604.000000, mae: 1148.439209, mean_q: -67.581329
 1944/5000: episode: 1944, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3211.799, mean reward: -3211.799 [-3211.799, -3211.799], mean action: 1.000 [1.000, 1.000],  loss: 13470628.000000, mae: 1162.870117, mean_q: -67.751434
 1945/5000: episode: 1945, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1913.162, mean reward: -1913.162 [-1913.162, -1913.162], mean action: 1.000 [1.000, 1.000],  loss: 27795502.000000, mae: 1682.731934, mean_q: -67.821793
 1946/5000: episode: 1946, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2036.685, mean reward: -2036.685 [-2036.685, -2036.685], mean action: 1.000 [1.000, 1.000],  loss: 14507267.000000, mae: 1181.774902, mean_q: -68.370003
 1947/5000: episode: 1947, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3645.269, mean reward: -3645.269 [-3645.269, -3645.269], mean action: 1.000 [1.000, 1.000],  loss: 14665238.000000, mae: 1092.062378, mean_q: -68.126465
 1948/5000: episode: 1948, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4385.283, mean reward: -4385.283 [-4385.283, -4385.283], mean action: 1.000 [1.000, 1.000],  loss: 18371854.000000, mae: 1302.927368, mean_q: -68.033592
 1949/5000: episode: 1949, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -393.025, mean reward: -393.025 [-393.025, -393.025], mean action: 1.000 [1.000, 1.000],  loss: 20845864.000000, mae: 1370.410278, mean_q: -68.182915
 1950/5000: episode: 1950, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1344.062, mean reward: -1344.062 [-1344.062, -1344.062], mean action: 1.000 [1.000, 1.000],  loss: 13821996.000000, mae: 1114.707397, mean_q: -68.390686
 1951/5000: episode: 1951, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -6128.108, mean reward: -6128.108 [-6128.108, -6128.108], mean action: 3.000 [3.000, 3.000],  loss: 14110256.000000, mae: 1132.214844, mean_q: -68.355560
 1952/5000: episode: 1952, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -3150.608, mean reward: -3150.608 [-3150.608, -3150.608], mean action: 1.000 [1.000, 1.000],  loss: 17536296.000000, mae: 1202.378052, mean_q: -68.443283
 1953/5000: episode: 1953, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4141.220, mean reward: -4141.220 [-4141.220, -4141.220], mean action: 1.000 [1.000, 1.000],  loss: 12899464.000000, mae: 1096.489014, mean_q: -68.579994
 1954/5000: episode: 1954, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3318.724, mean reward: -3318.724 [-3318.724, -3318.724], mean action: 1.000 [1.000, 1.000],  loss: 16068935.000000, mae: 1198.593018, mean_q: -68.946320
 1955/5000: episode: 1955, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1813.359, mean reward: -1813.359 [-1813.359, -1813.359], mean action: 1.000 [1.000, 1.000],  loss: 22282380.000000, mae: 1427.489868, mean_q: -68.633926
 1956/5000: episode: 1956, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3760.229, mean reward: -3760.229 [-3760.229, -3760.229], mean action: 1.000 [1.000, 1.000],  loss: 17033828.000000, mae: 1246.953857, mean_q: -69.585556
 1957/5000: episode: 1957, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2199.282, mean reward: -2199.282 [-2199.282, -2199.282], mean action: 1.000 [1.000, 1.000],  loss: 9990462.000000, mae: 992.005554, mean_q: -69.497307
 1958/5000: episode: 1958, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1255.297, mean reward: -1255.297 [-1255.297, -1255.297], mean action: 1.000 [1.000, 1.000],  loss: 13062237.000000, mae: 1107.224854, mean_q: -69.407288
 1959/5000: episode: 1959, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5244.027, mean reward: -5244.027 [-5244.027, -5244.027], mean action: 0.000 [0.000, 0.000],  loss: 14673909.000000, mae: 1110.717407, mean_q: -69.489441
 1960/5000: episode: 1960, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1567.551, mean reward: -1567.551 [-1567.551, -1567.551], mean action: 1.000 [1.000, 1.000],  loss: 19577464.000000, mae: 1372.440918, mean_q: -69.440422
 1961/5000: episode: 1961, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2794.023, mean reward: -2794.023 [-2794.023, -2794.023], mean action: 1.000 [1.000, 1.000],  loss: 16915096.000000, mae: 1176.143677, mean_q: -69.714798
 1962/5000: episode: 1962, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4227.676, mean reward: -4227.676 [-4227.676, -4227.676], mean action: 1.000 [1.000, 1.000],  loss: 15770920.000000, mae: 1130.049072, mean_q: -69.804901
 1963/5000: episode: 1963, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4316.539, mean reward: -4316.539 [-4316.539, -4316.539], mean action: 1.000 [1.000, 1.000],  loss: 8930330.000000, mae: 901.176392, mean_q: -70.054924
 1964/5000: episode: 1964, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8170.336, mean reward: -8170.336 [-8170.336, -8170.336], mean action: 1.000 [1.000, 1.000],  loss: 15569610.000000, mae: 1191.230713, mean_q: -69.900093
 1965/5000: episode: 1965, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4227.738, mean reward: -4227.738 [-4227.738, -4227.738], mean action: 1.000 [1.000, 1.000],  loss: 21481594.000000, mae: 1499.144531, mean_q: -70.058556
 1966/5000: episode: 1966, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -11058.965, mean reward: -11058.965 [-11058.965, -11058.965], mean action: 1.000 [1.000, 1.000],  loss: 9435480.000000, mae: 963.570679, mean_q: -70.272690
 1967/5000: episode: 1967, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6957.407, mean reward: -6957.407 [-6957.407, -6957.407], mean action: 1.000 [1.000, 1.000],  loss: 10816320.000000, mae: 1074.852417, mean_q: -70.465942
 1968/5000: episode: 1968, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3738.787, mean reward: -3738.787 [-3738.787, -3738.787], mean action: 1.000 [1.000, 1.000],  loss: 15316254.000000, mae: 1128.080322, mean_q: -70.522034
 1969/5000: episode: 1969, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5536.813, mean reward: -5536.813 [-5536.813, -5536.813], mean action: 1.000 [1.000, 1.000],  loss: 20827688.000000, mae: 1325.759644, mean_q: -70.209549
 1970/5000: episode: 1970, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3521.241, mean reward: -3521.241 [-3521.241, -3521.241], mean action: 1.000 [1.000, 1.000],  loss: 11122351.000000, mae: 1037.192017, mean_q: -70.610046
 1971/5000: episode: 1971, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5358.861, mean reward: -5358.861 [-5358.861, -5358.861], mean action: 1.000 [1.000, 1.000],  loss: 11635094.000000, mae: 1023.493164, mean_q: -70.832817
 1972/5000: episode: 1972, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2491.088, mean reward: -2491.088 [-2491.088, -2491.088], mean action: 1.000 [1.000, 1.000],  loss: 14784540.000000, mae: 1178.276611, mean_q: -70.607498
 1973/5000: episode: 1973, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3986.538, mean reward: -3986.538 [-3986.538, -3986.538], mean action: 1.000 [1.000, 1.000],  loss: 16453098.000000, mae: 1208.518433, mean_q: -70.771156
 1974/5000: episode: 1974, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2743.871, mean reward: -2743.871 [-2743.871, -2743.871], mean action: 1.000 [1.000, 1.000],  loss: 16814456.000000, mae: 1192.187012, mean_q: -71.014236
 1975/5000: episode: 1975, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3340.210, mean reward: -3340.210 [-3340.210, -3340.210], mean action: 1.000 [1.000, 1.000],  loss: 15585207.000000, mae: 1250.656494, mean_q: -70.938560
 1976/5000: episode: 1976, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9081.445, mean reward: -9081.445 [-9081.445, -9081.445], mean action: 1.000 [1.000, 1.000],  loss: 11062955.000000, mae: 1055.844971, mean_q: -71.060341
 1977/5000: episode: 1977, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -6406.920, mean reward: -6406.920 [-6406.920, -6406.920], mean action: 1.000 [1.000, 1.000],  loss: 15207110.000000, mae: 1219.789062, mean_q: -70.817116
 1978/5000: episode: 1978, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -8357.742, mean reward: -8357.742 [-8357.742, -8357.742], mean action: 1.000 [1.000, 1.000],  loss: 16709814.000000, mae: 1265.303467, mean_q: -71.485916
 1979/5000: episode: 1979, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2699.572, mean reward: -2699.572 [-2699.572, -2699.572], mean action: 1.000 [1.000, 1.000],  loss: 14503044.000000, mae: 1130.616455, mean_q: -71.238647
 1980/5000: episode: 1980, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4917.591, mean reward: -4917.591 [-4917.591, -4917.591], mean action: 1.000 [1.000, 1.000],  loss: 22216242.000000, mae: 1428.797974, mean_q: -71.345139
 1981/5000: episode: 1981, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1689.008, mean reward: -1689.008 [-1689.008, -1689.008], mean action: 1.000 [1.000, 1.000],  loss: 8635038.000000, mae: 900.951904, mean_q: -71.914268
 1982/5000: episode: 1982, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3684.119, mean reward: -3684.119 [-3684.119, -3684.119], mean action: 1.000 [1.000, 1.000],  loss: 14768839.000000, mae: 1214.462646, mean_q: -71.689018
 1983/5000: episode: 1983, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1121.682, mean reward: -1121.682 [-1121.682, -1121.682], mean action: 1.000 [1.000, 1.000],  loss: 16107797.000000, mae: 1189.180420, mean_q: -71.840561
 1984/5000: episode: 1984, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -10846.649, mean reward: -10846.649 [-10846.649, -10846.649], mean action: 1.000 [1.000, 1.000],  loss: 15310172.000000, mae: 1135.884277, mean_q: -71.923111
 1985/5000: episode: 1985, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -3841.287, mean reward: -3841.287 [-3841.287, -3841.287], mean action: 1.000 [1.000, 1.000],  loss: 14754782.000000, mae: 1042.209106, mean_q: -72.026680
 1986/5000: episode: 1986, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4162.096, mean reward: -4162.096 [-4162.096, -4162.096], mean action: 1.000 [1.000, 1.000],  loss: 19325804.000000, mae: 1304.102295, mean_q: -71.936882
 1987/5000: episode: 1987, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -841.159, mean reward: -841.159 [-841.159, -841.159], mean action: 1.000 [1.000, 1.000],  loss: 18530364.000000, mae: 1326.147949, mean_q: -71.961685
 1988/5000: episode: 1988, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4307.688, mean reward: -4307.688 [-4307.688, -4307.688], mean action: 1.000 [1.000, 1.000],  loss: 14660434.000000, mae: 1075.383911, mean_q: -72.431793
 1989/5000: episode: 1989, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -720.134, mean reward: -720.134 [-720.134, -720.134], mean action: 1.000 [1.000, 1.000],  loss: 8755836.000000, mae: 915.383789, mean_q: -72.799416
 1990/5000: episode: 1990, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2301.800, mean reward: -2301.800 [-2301.800, -2301.800], mean action: 1.000 [1.000, 1.000],  loss: 14791386.000000, mae: 1157.842529, mean_q: -72.270859
 1991/5000: episode: 1991, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -11171.586, mean reward: -11171.586 [-11171.586, -11171.586], mean action: 0.000 [0.000, 0.000],  loss: 14589520.000000, mae: 1096.544189, mean_q: -72.555008
 1992/5000: episode: 1992, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1986.605, mean reward: -1986.605 [-1986.605, -1986.605], mean action: 1.000 [1.000, 1.000],  loss: 9037730.000000, mae: 906.522034, mean_q: -72.943390
 1993/5000: episode: 1993, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3910.964, mean reward: -3910.964 [-3910.964, -3910.964], mean action: 1.000 [1.000, 1.000],  loss: 10968773.000000, mae: 959.191162, mean_q: -72.944580
 1994/5000: episode: 1994, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -11036.819, mean reward: -11036.819 [-11036.819, -11036.819], mean action: 1.000 [1.000, 1.000],  loss: 13532944.000000, mae: 1035.871094, mean_q: -72.672905
 1995/5000: episode: 1995, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9453.907, mean reward: -9453.907 [-9453.907, -9453.907], mean action: 1.000 [1.000, 1.000],  loss: 13334926.000000, mae: 1125.765503, mean_q: -73.111221
 1996/5000: episode: 1996, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1379.087, mean reward: -1379.087 [-1379.087, -1379.087], mean action: 2.000 [2.000, 2.000],  loss: 15512490.000000, mae: 1232.317871, mean_q: -72.941994
 1997/5000: episode: 1997, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4968.898, mean reward: -4968.898 [-4968.898, -4968.898], mean action: 1.000 [1.000, 1.000],  loss: 15388478.000000, mae: 1106.668945, mean_q: -73.417236
 1998/5000: episode: 1998, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3906.657, mean reward: -3906.657 [-3906.657, -3906.657], mean action: 1.000 [1.000, 1.000],  loss: 12744122.000000, mae: 1124.895142, mean_q: -73.208664
 1999/5000: episode: 1999, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4920.321, mean reward: -4920.321 [-4920.321, -4920.321], mean action: 1.000 [1.000, 1.000],  loss: 19260426.000000, mae: 1318.574341, mean_q: -73.325714
 2000/5000: episode: 2000, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6904.007, mean reward: -6904.007 [-6904.007, -6904.007], mean action: 1.000 [1.000, 1.000],  loss: 16742106.000000, mae: 1227.185059, mean_q: -73.084625
 2001/5000: episode: 2001, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2281.536, mean reward: -2281.536 [-2281.536, -2281.536], mean action: 3.000 [3.000, 3.000],  loss: 14242032.000000, mae: 1153.373413, mean_q: -73.662117
 2002/5000: episode: 2002, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10830.537, mean reward: -10830.537 [-10830.537, -10830.537], mean action: 1.000 [1.000, 1.000],  loss: 18974348.000000, mae: 1357.934082, mean_q: -73.478996
 2003/5000: episode: 2003, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4322.997, mean reward: -4322.997 [-4322.997, -4322.997], mean action: 1.000 [1.000, 1.000],  loss: 19841494.000000, mae: 1424.905518, mean_q: -73.382484
 2004/5000: episode: 2004, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4496.636, mean reward: -4496.636 [-4496.636, -4496.636], mean action: 1.000 [1.000, 1.000],  loss: 19157768.000000, mae: 1296.374268, mean_q: -73.839294
 2005/5000: episode: 2005, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5408.813, mean reward: -5408.813 [-5408.813, -5408.813], mean action: 1.000 [1.000, 1.000],  loss: 15039149.000000, mae: 1126.446411, mean_q: -73.767670
 2006/5000: episode: 2006, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2681.766, mean reward: -2681.766 [-2681.766, -2681.766], mean action: 1.000 [1.000, 1.000],  loss: 11567948.000000, mae: 1026.382202, mean_q: -73.950386
 2007/5000: episode: 2007, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4553.233, mean reward: -4553.233 [-4553.233, -4553.233], mean action: 1.000 [1.000, 1.000],  loss: 12919462.000000, mae: 1023.275146, mean_q: -74.347702
 2008/5000: episode: 2008, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6442.876, mean reward: -6442.876 [-6442.876, -6442.876], mean action: 1.000 [1.000, 1.000],  loss: 21241462.000000, mae: 1421.261108, mean_q: -74.062073
 2009/5000: episode: 2009, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5852.740, mean reward: -5852.740 [-5852.740, -5852.740], mean action: 0.000 [0.000, 0.000],  loss: 20058566.000000, mae: 1365.802734, mean_q: -74.325043
 2010/5000: episode: 2010, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1940.502, mean reward: -1940.502 [-1940.502, -1940.502], mean action: 1.000 [1.000, 1.000],  loss: 16432916.000000, mae: 1190.145752, mean_q: -74.356049
 2011/5000: episode: 2011, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3480.841, mean reward: -3480.841 [-3480.841, -3480.841], mean action: 1.000 [1.000, 1.000],  loss: 17185336.000000, mae: 1278.562500, mean_q: -74.409058
 2012/5000: episode: 2012, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3042.186, mean reward: -3042.186 [-3042.186, -3042.186], mean action: 1.000 [1.000, 1.000],  loss: 11145323.000000, mae: 970.733521, mean_q: -74.667137
 2013/5000: episode: 2013, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -11118.455, mean reward: -11118.455 [-11118.455, -11118.455], mean action: 1.000 [1.000, 1.000],  loss: 12716650.000000, mae: 1025.656738, mean_q: -74.762581
 2014/5000: episode: 2014, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3360.717, mean reward: -3360.717 [-3360.717, -3360.717], mean action: 1.000 [1.000, 1.000],  loss: 18738084.000000, mae: 1256.921387, mean_q: -74.850189
 2015/5000: episode: 2015, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2244.443, mean reward: -2244.443 [-2244.443, -2244.443], mean action: 3.000 [3.000, 3.000],  loss: 11732378.000000, mae: 1000.767822, mean_q: -75.248337
 2016/5000: episode: 2016, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -2435.244, mean reward: -2435.244 [-2435.244, -2435.244], mean action: 1.000 [1.000, 1.000],  loss: 12947431.000000, mae: 1024.850098, mean_q: -75.119957
 2017/5000: episode: 2017, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5896.567, mean reward: -5896.567 [-5896.567, -5896.567], mean action: 1.000 [1.000, 1.000],  loss: 20403384.000000, mae: 1399.111816, mean_q: -75.062897
 2018/5000: episode: 2018, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2011.768, mean reward: -2011.768 [-2011.768, -2011.768], mean action: 1.000 [1.000, 1.000],  loss: 18198038.000000, mae: 1221.920044, mean_q: -75.264374
 2019/5000: episode: 2019, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4096.039, mean reward: -4096.039 [-4096.039, -4096.039], mean action: 1.000 [1.000, 1.000],  loss: 14450588.000000, mae: 1094.194092, mean_q: -75.615280
 2020/5000: episode: 2020, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4916.792, mean reward: -4916.792 [-4916.792, -4916.792], mean action: 1.000 [1.000, 1.000],  loss: 18716900.000000, mae: 1280.197388, mean_q: -75.526917
 2021/5000: episode: 2021, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1431.251, mean reward: -1431.251 [-1431.251, -1431.251], mean action: 1.000 [1.000, 1.000],  loss: 18020918.000000, mae: 1311.295898, mean_q: -75.520691
 2022/5000: episode: 2022, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -630.504, mean reward: -630.504 [-630.504, -630.504], mean action: 2.000 [2.000, 2.000],  loss: 10772012.000000, mae: 1000.439331, mean_q: -75.957870
 2023/5000: episode: 2023, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -818.759, mean reward: -818.759 [-818.759, -818.759], mean action: 1.000 [1.000, 1.000],  loss: 13300026.000000, mae: 1127.202881, mean_q: -76.148010
 2024/5000: episode: 2024, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7711.986, mean reward: -7711.986 [-7711.986, -7711.986], mean action: 1.000 [1.000, 1.000],  loss: 14201305.000000, mae: 1102.065186, mean_q: -76.042824
 2025/5000: episode: 2025, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5359.424, mean reward: -5359.424 [-5359.424, -5359.424], mean action: 1.000 [1.000, 1.000],  loss: 14673578.000000, mae: 1277.765137, mean_q: -76.035934
 2026/5000: episode: 2026, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1781.626, mean reward: -1781.626 [-1781.626, -1781.626], mean action: 1.000 [1.000, 1.000],  loss: 15942924.000000, mae: 1199.260254, mean_q: -76.161797
 2027/5000: episode: 2027, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3265.260, mean reward: -3265.260 [-3265.260, -3265.260], mean action: 2.000 [2.000, 2.000],  loss: 11964869.000000, mae: 1086.442139, mean_q: -76.209442
 2028/5000: episode: 2028, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4253.439, mean reward: -4253.439 [-4253.439, -4253.439], mean action: 1.000 [1.000, 1.000],  loss: 15223023.000000, mae: 1222.548950, mean_q: -76.263062
 2029/5000: episode: 2029, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -549.017, mean reward: -549.017 [-549.017, -549.017], mean action: 1.000 [1.000, 1.000],  loss: 12500752.000000, mae: 1089.065186, mean_q: -76.658752
 2030/5000: episode: 2030, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1739.271, mean reward: -1739.271 [-1739.271, -1739.271], mean action: 1.000 [1.000, 1.000],  loss: 13522410.000000, mae: 1116.936768, mean_q: -76.357422
 2031/5000: episode: 2031, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -883.915, mean reward: -883.915 [-883.915, -883.915], mean action: 1.000 [1.000, 1.000],  loss: 15174376.000000, mae: 1095.237061, mean_q: -76.659454
 2032/5000: episode: 2032, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4879.674, mean reward: -4879.674 [-4879.674, -4879.674], mean action: 1.000 [1.000, 1.000],  loss: 14170287.000000, mae: 1098.387085, mean_q: -76.811005
 2033/5000: episode: 2033, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9218.970, mean reward: -9218.970 [-9218.970, -9218.970], mean action: 1.000 [1.000, 1.000],  loss: 14339412.000000, mae: 1205.762207, mean_q: -76.785446
 2034/5000: episode: 2034, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -10679.143, mean reward: -10679.143 [-10679.143, -10679.143], mean action: 1.000 [1.000, 1.000],  loss: 16770031.000000, mae: 1243.669434, mean_q: -77.163513
 2035/5000: episode: 2035, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1620.602, mean reward: -1620.602 [-1620.602, -1620.602], mean action: 1.000 [1.000, 1.000],  loss: 19777988.000000, mae: 1296.648193, mean_q: -76.838104
 2036/5000: episode: 2036, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5178.785, mean reward: -5178.785 [-5178.785, -5178.785], mean action: 1.000 [1.000, 1.000],  loss: 15394930.000000, mae: 1238.218750, mean_q: -76.908463
 2037/5000: episode: 2037, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1825.774, mean reward: -1825.774 [-1825.774, -1825.774], mean action: 1.000 [1.000, 1.000],  loss: 12968956.000000, mae: 1095.565552, mean_q: -77.303909
 2038/5000: episode: 2038, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2815.981, mean reward: -2815.981 [-2815.981, -2815.981], mean action: 1.000 [1.000, 1.000],  loss: 15142031.000000, mae: 1212.215820, mean_q: -77.196808
 2039/5000: episode: 2039, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3396.878, mean reward: -3396.878 [-3396.878, -3396.878], mean action: 1.000 [1.000, 1.000],  loss: 9366896.000000, mae: 920.108704, mean_q: -77.721458
 2040/5000: episode: 2040, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2092.647, mean reward: -2092.647 [-2092.647, -2092.647], mean action: 1.000 [1.000, 1.000],  loss: 13874480.000000, mae: 1188.883667, mean_q: -77.765778
 2041/5000: episode: 2041, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5837.042, mean reward: -5837.042 [-5837.042, -5837.042], mean action: 1.000 [1.000, 1.000],  loss: 12646742.000000, mae: 991.171265, mean_q: -77.615585
 2042/5000: episode: 2042, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5777.523, mean reward: -5777.523 [-5777.523, -5777.523], mean action: 1.000 [1.000, 1.000],  loss: 9422998.000000, mae: 975.279663, mean_q: -78.253952
 2043/5000: episode: 2043, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5156.276, mean reward: -5156.276 [-5156.276, -5156.276], mean action: 1.000 [1.000, 1.000],  loss: 16335214.000000, mae: 1258.529663, mean_q: -77.810585
 2044/5000: episode: 2044, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5389.692, mean reward: -5389.692 [-5389.692, -5389.692], mean action: 1.000 [1.000, 1.000],  loss: 11295973.000000, mae: 1036.946777, mean_q: -77.745583
 2045/5000: episode: 2045, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8124.539, mean reward: -8124.539 [-8124.539, -8124.539], mean action: 1.000 [1.000, 1.000],  loss: 12518252.000000, mae: 1092.284424, mean_q: -78.017876
 2046/5000: episode: 2046, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3427.059, mean reward: -3427.059 [-3427.059, -3427.059], mean action: 1.000 [1.000, 1.000],  loss: 11899842.000000, mae: 1053.419678, mean_q: -78.254501
 2047/5000: episode: 2047, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -1493.667, mean reward: -1493.667 [-1493.667, -1493.667], mean action: 1.000 [1.000, 1.000],  loss: 16351116.000000, mae: 1152.819458, mean_q: -78.327209
 2048/5000: episode: 2048, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2202.412, mean reward: -2202.412 [-2202.412, -2202.412], mean action: 1.000 [1.000, 1.000],  loss: 15605038.000000, mae: 1218.229248, mean_q: -77.836906
 2049/5000: episode: 2049, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5554.200, mean reward: -5554.200 [-5554.200, -5554.200], mean action: 0.000 [0.000, 0.000],  loss: 14586072.000000, mae: 1225.277832, mean_q: -78.616714
 2050/5000: episode: 2050, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1028.506, mean reward: -1028.506 [-1028.506, -1028.506], mean action: 1.000 [1.000, 1.000],  loss: 18625648.000000, mae: 1205.943970, mean_q: -78.494659
 2051/5000: episode: 2051, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1072.554, mean reward: -1072.554 [-1072.554, -1072.554], mean action: 1.000 [1.000, 1.000],  loss: 15196525.000000, mae: 1138.736816, mean_q: -78.452797
 2052/5000: episode: 2052, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5226.088, mean reward: -5226.088 [-5226.088, -5226.088], mean action: 1.000 [1.000, 1.000],  loss: 12725188.000000, mae: 1134.991699, mean_q: -78.582779
 2053/5000: episode: 2053, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -735.112, mean reward: -735.112 [-735.112, -735.112], mean action: 1.000 [1.000, 1.000],  loss: 8767472.000000, mae: 918.653198, mean_q: -78.860184
 2054/5000: episode: 2054, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1770.872, mean reward: -1770.872 [-1770.872, -1770.872], mean action: 1.000 [1.000, 1.000],  loss: 19754826.000000, mae: 1335.710327, mean_q: -78.827499
 2055/5000: episode: 2055, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1012.009, mean reward: -1012.009 [-1012.009, -1012.009], mean action: 1.000 [1.000, 1.000],  loss: 15274587.000000, mae: 1137.785645, mean_q: -79.129677
 2056/5000: episode: 2056, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2568.080, mean reward: -2568.080 [-2568.080, -2568.080], mean action: 1.000 [1.000, 1.000],  loss: 13168094.000000, mae: 1102.686768, mean_q: -79.327301
 2057/5000: episode: 2057, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3005.881, mean reward: -3005.881 [-3005.881, -3005.881], mean action: 1.000 [1.000, 1.000],  loss: 15652823.000000, mae: 1223.326782, mean_q: -79.177963
 2058/5000: episode: 2058, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -1314.200, mean reward: -1314.200 [-1314.200, -1314.200], mean action: 2.000 [2.000, 2.000],  loss: 13919365.000000, mae: 1140.346436, mean_q: -79.506371
 2059/5000: episode: 2059, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1382.169, mean reward: -1382.169 [-1382.169, -1382.169], mean action: 1.000 [1.000, 1.000],  loss: 14432587.000000, mae: 1099.018799, mean_q: -79.338043
 2060/5000: episode: 2060, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1735.376, mean reward: -1735.376 [-1735.376, -1735.376], mean action: 1.000 [1.000, 1.000],  loss: 12851622.000000, mae: 1046.967407, mean_q: -79.733246
 2061/5000: episode: 2061, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7015.483, mean reward: -7015.483 [-7015.483, -7015.483], mean action: 1.000 [1.000, 1.000],  loss: 13176708.000000, mae: 1123.386230, mean_q: -79.725624
 2062/5000: episode: 2062, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6738.715, mean reward: -6738.715 [-6738.715, -6738.715], mean action: 1.000 [1.000, 1.000],  loss: 14261716.000000, mae: 1145.328857, mean_q: -79.661392
 2063/5000: episode: 2063, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2791.435, mean reward: -2791.435 [-2791.435, -2791.435], mean action: 1.000 [1.000, 1.000],  loss: 18002304.000000, mae: 1291.538574, mean_q: -79.936867
 2064/5000: episode: 2064, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7679.412, mean reward: -7679.412 [-7679.412, -7679.412], mean action: 1.000 [1.000, 1.000],  loss: 10270944.000000, mae: 983.227417, mean_q: -79.903755
 2065/5000: episode: 2065, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2231.974, mean reward: -2231.974 [-2231.974, -2231.974], mean action: 1.000 [1.000, 1.000],  loss: 21694892.000000, mae: 1433.883545, mean_q: -79.746185
 2066/5000: episode: 2066, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -10843.416, mean reward: -10843.416 [-10843.416, -10843.416], mean action: 1.000 [1.000, 1.000],  loss: 11834260.000000, mae: 1109.108032, mean_q: -80.184792
 2067/5000: episode: 2067, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2486.065, mean reward: -2486.065 [-2486.065, -2486.065], mean action: 1.000 [1.000, 1.000],  loss: 10245094.000000, mae: 1021.213318, mean_q: -80.572899
 2068/5000: episode: 2068, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -528.456, mean reward: -528.456 [-528.456, -528.456], mean action: 1.000 [1.000, 1.000],  loss: 15528548.000000, mae: 1156.133301, mean_q: -80.303284
 2069/5000: episode: 2069, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10209.492, mean reward: -10209.492 [-10209.492, -10209.492], mean action: 1.000 [1.000, 1.000],  loss: 10591646.000000, mae: 1067.678101, mean_q: -80.552734
 2070/5000: episode: 2070, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2641.866, mean reward: -2641.866 [-2641.866, -2641.866], mean action: 1.000 [1.000, 1.000],  loss: 18023612.000000, mae: 1346.720947, mean_q: -80.607788
 2071/5000: episode: 2071, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1765.983, mean reward: -1765.983 [-1765.983, -1765.983], mean action: 1.000 [1.000, 1.000],  loss: 15382482.000000, mae: 1143.038086, mean_q: -80.790085
 2072/5000: episode: 2072, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1969.835, mean reward: -1969.835 [-1969.835, -1969.835], mean action: 1.000 [1.000, 1.000],  loss: 12077641.000000, mae: 1017.771729, mean_q: -80.891144
 2073/5000: episode: 2073, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2962.625, mean reward: -2962.625 [-2962.625, -2962.625], mean action: 1.000 [1.000, 1.000],  loss: 19410658.000000, mae: 1389.742310, mean_q: -81.084549
 2074/5000: episode: 2074, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6831.454, mean reward: -6831.454 [-6831.454, -6831.454], mean action: 1.000 [1.000, 1.000],  loss: 22675636.000000, mae: 1498.329712, mean_q: -80.797791
 2075/5000: episode: 2075, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6748.085, mean reward: -6748.085 [-6748.085, -6748.085], mean action: 1.000 [1.000, 1.000],  loss: 18403722.000000, mae: 1262.722656, mean_q: -81.155296
 2076/5000: episode: 2076, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2625.700, mean reward: -2625.700 [-2625.700, -2625.700], mean action: 1.000 [1.000, 1.000],  loss: 11411922.000000, mae: 1082.464600, mean_q: -81.160271
 2077/5000: episode: 2077, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -1195.429, mean reward: -1195.429 [-1195.429, -1195.429], mean action: 1.000 [1.000, 1.000],  loss: 18564496.000000, mae: 1275.464478, mean_q: -81.107323
 2078/5000: episode: 2078, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2808.238, mean reward: -2808.238 [-2808.238, -2808.238], mean action: 1.000 [1.000, 1.000],  loss: 14457764.000000, mae: 1211.251831, mean_q: -81.450241
 2079/5000: episode: 2079, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3422.671, mean reward: -3422.671 [-3422.671, -3422.671], mean action: 2.000 [2.000, 2.000],  loss: 9524226.000000, mae: 943.408447, mean_q: -81.845505
 2080/5000: episode: 2080, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2252.369, mean reward: -2252.369 [-2252.369, -2252.369], mean action: 1.000 [1.000, 1.000],  loss: 17011496.000000, mae: 1309.550537, mean_q: -81.572472
 2081/5000: episode: 2081, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -6238.621, mean reward: -6238.621 [-6238.621, -6238.621], mean action: 1.000 [1.000, 1.000],  loss: 22482048.000000, mae: 1369.357422, mean_q: -82.018860
 2082/5000: episode: 2082, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1708.418, mean reward: -1708.418 [-1708.418, -1708.418], mean action: 2.000 [2.000, 2.000],  loss: 16318891.000000, mae: 1088.164062, mean_q: -81.958206
 2083/5000: episode: 2083, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5717.122, mean reward: -5717.122 [-5717.122, -5717.122], mean action: 2.000 [2.000, 2.000],  loss: 21998230.000000, mae: 1504.702637, mean_q: -81.832077
 2084/5000: episode: 2084, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10792.795, mean reward: -10792.795 [-10792.795, -10792.795], mean action: 3.000 [3.000, 3.000],  loss: 14419371.000000, mae: 1212.402344, mean_q: -82.307999
 2085/5000: episode: 2085, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -8878.417, mean reward: -8878.417 [-8878.417, -8878.417], mean action: 1.000 [1.000, 1.000],  loss: 12627724.000000, mae: 1082.051514, mean_q: -82.573090
 2086/5000: episode: 2086, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1921.678, mean reward: -1921.678 [-1921.678, -1921.678], mean action: 2.000 [2.000, 2.000],  loss: 18983644.000000, mae: 1366.848633, mean_q: -82.299637
 2087/5000: episode: 2087, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2510.618, mean reward: -2510.618 [-2510.618, -2510.618], mean action: 2.000 [2.000, 2.000],  loss: 9036491.000000, mae: 976.642578, mean_q: -82.814621
 2088/5000: episode: 2088, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1155.456, mean reward: -1155.456 [-1155.456, -1155.456], mean action: 2.000 [2.000, 2.000],  loss: 12913247.000000, mae: 1093.814453, mean_q: -82.585655
 2089/5000: episode: 2089, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3374.607, mean reward: -3374.607 [-3374.607, -3374.607], mean action: 2.000 [2.000, 2.000],  loss: 17984166.000000, mae: 1283.320435, mean_q: -82.768364
 2090/5000: episode: 2090, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4235.108, mean reward: -4235.108 [-4235.108, -4235.108], mean action: 2.000 [2.000, 2.000],  loss: 14967452.000000, mae: 1212.964844, mean_q: -82.720787
 2091/5000: episode: 2091, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4952.357, mean reward: -4952.357 [-4952.357, -4952.357], mean action: 2.000 [2.000, 2.000],  loss: 16557707.000000, mae: 1270.740601, mean_q: -82.897995
 2092/5000: episode: 2092, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3513.792, mean reward: -3513.792 [-3513.792, -3513.792], mean action: 3.000 [3.000, 3.000],  loss: 18162180.000000, mae: 1287.137939, mean_q: -82.739449
 2093/5000: episode: 2093, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3784.777, mean reward: -3784.777 [-3784.777, -3784.777], mean action: 2.000 [2.000, 2.000],  loss: 12148806.000000, mae: 1070.077515, mean_q: -82.957993
 2094/5000: episode: 2094, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -830.129, mean reward: -830.129 [-830.129, -830.129], mean action: 2.000 [2.000, 2.000],  loss: 13951250.000000, mae: 1035.519287, mean_q: -83.083870
 2095/5000: episode: 2095, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5679.219, mean reward: -5679.219 [-5679.219, -5679.219], mean action: 2.000 [2.000, 2.000],  loss: 14135907.000000, mae: 1159.769531, mean_q: -83.289185
 2096/5000: episode: 2096, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1444.545, mean reward: -1444.545 [-1444.545, -1444.545], mean action: 2.000 [2.000, 2.000],  loss: 9778864.000000, mae: 985.684082, mean_q: -83.602745
 2097/5000: episode: 2097, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1917.921, mean reward: -1917.921 [-1917.921, -1917.921], mean action: 2.000 [2.000, 2.000],  loss: 14633903.000000, mae: 1248.084351, mean_q: -83.720871
 2098/5000: episode: 2098, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2046.480, mean reward: -2046.480 [-2046.480, -2046.480], mean action: 0.000 [0.000, 0.000],  loss: 12994426.000000, mae: 1131.348145, mean_q: -83.747284
 2099/5000: episode: 2099, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -524.875, mean reward: -524.875 [-524.875, -524.875], mean action: 2.000 [2.000, 2.000],  loss: 13665336.000000, mae: 1161.385010, mean_q: -83.543869
 2100/5000: episode: 2100, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2694.593, mean reward: -2694.593 [-2694.593, -2694.593], mean action: 2.000 [2.000, 2.000],  loss: 18117952.000000, mae: 1290.448120, mean_q: -83.831413
 2101/5000: episode: 2101, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -2840.668, mean reward: -2840.668 [-2840.668, -2840.668], mean action: 2.000 [2.000, 2.000],  loss: 12452652.000000, mae: 1104.284424, mean_q: -83.920166
 2102/5000: episode: 2102, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8169.529, mean reward: -8169.529 [-8169.529, -8169.529], mean action: 2.000 [2.000, 2.000],  loss: 15921811.000000, mae: 1265.181152, mean_q: -83.788818
 2103/5000: episode: 2103, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1446.894, mean reward: -1446.894 [-1446.894, -1446.894], mean action: 3.000 [3.000, 3.000],  loss: 16046092.000000, mae: 1202.474487, mean_q: -84.076668
 2104/5000: episode: 2104, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1543.638, mean reward: -1543.638 [-1543.638, -1543.638], mean action: 2.000 [2.000, 2.000],  loss: 11770430.000000, mae: 1098.090820, mean_q: -84.368729
 2105/5000: episode: 2105, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -949.799, mean reward: -949.799 [-949.799, -949.799], mean action: 2.000 [2.000, 2.000],  loss: 18015060.000000, mae: 1282.240967, mean_q: -84.389832
 2106/5000: episode: 2106, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10811.387, mean reward: -10811.387 [-10811.387, -10811.387], mean action: 0.000 [0.000, 0.000],  loss: 16074121.000000, mae: 1100.084229, mean_q: -84.381790
 2107/5000: episode: 2107, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -864.801, mean reward: -864.801 [-864.801, -864.801], mean action: 2.000 [2.000, 2.000],  loss: 15827985.000000, mae: 1228.694946, mean_q: -84.911118
 2108/5000: episode: 2108, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5749.337, mean reward: -5749.337 [-5749.337, -5749.337], mean action: 2.000 [2.000, 2.000],  loss: 14359258.000000, mae: 1114.818970, mean_q: -84.685081
 2109/5000: episode: 2109, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2761.157, mean reward: -2761.157 [-2761.157, -2761.157], mean action: 2.000 [2.000, 2.000],  loss: 24454872.000000, mae: 1481.055420, mean_q: -84.154305
 2110/5000: episode: 2110, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -878.722, mean reward: -878.722 [-878.722, -878.722], mean action: 2.000 [2.000, 2.000],  loss: 15886435.000000, mae: 1180.818115, mean_q: -84.767700
 2111/5000: episode: 2111, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2695.552, mean reward: -2695.552 [-2695.552, -2695.552], mean action: 2.000 [2.000, 2.000],  loss: 17609888.000000, mae: 1318.087402, mean_q: -85.149811
 2112/5000: episode: 2112, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1054.712, mean reward: -1054.712 [-1054.712, -1054.712], mean action: 2.000 [2.000, 2.000],  loss: 17507472.000000, mae: 1135.469482, mean_q: -85.117027
 2113/5000: episode: 2113, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2178.472, mean reward: -2178.472 [-2178.472, -2178.472], mean action: 2.000 [2.000, 2.000],  loss: 12344028.000000, mae: 1086.083984, mean_q: -85.313759
 2114/5000: episode: 2114, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1074.082, mean reward: -1074.082 [-1074.082, -1074.082], mean action: 2.000 [2.000, 2.000],  loss: 21532936.000000, mae: 1435.885742, mean_q: -85.205048
 2115/5000: episode: 2115, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1737.982, mean reward: -1737.982 [-1737.982, -1737.982], mean action: 2.000 [2.000, 2.000],  loss: 17781510.000000, mae: 1251.829468, mean_q: -85.352158
 2116/5000: episode: 2116, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -115.329, mean reward: -115.329 [-115.329, -115.329], mean action: 2.000 [2.000, 2.000],  loss: 13656874.000000, mae: 1163.488403, mean_q: -85.273651
 2117/5000: episode: 2117, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1122.205, mean reward: -1122.205 [-1122.205, -1122.205], mean action: 2.000 [2.000, 2.000],  loss: 16291327.000000, mae: 1264.271484, mean_q: -85.390541
 2118/5000: episode: 2118, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1294.175, mean reward: -1294.175 [-1294.175, -1294.175], mean action: 2.000 [2.000, 2.000],  loss: 14968190.000000, mae: 1143.317139, mean_q: -86.064468
 2119/5000: episode: 2119, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -618.406, mean reward: -618.406 [-618.406, -618.406], mean action: 2.000 [2.000, 2.000],  loss: 8585220.000000, mae: 895.221130, mean_q: -86.112297
 2120/5000: episode: 2120, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2573.936, mean reward: -2573.936 [-2573.936, -2573.936], mean action: 2.000 [2.000, 2.000],  loss: 20106548.000000, mae: 1376.236572, mean_q: -85.894737
 2121/5000: episode: 2121, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1625.932, mean reward: -1625.932 [-1625.932, -1625.932], mean action: 2.000 [2.000, 2.000],  loss: 21045388.000000, mae: 1431.696533, mean_q: -85.670227
 2122/5000: episode: 2122, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1621.795, mean reward: -1621.795 [-1621.795, -1621.795], mean action: 2.000 [2.000, 2.000],  loss: 15482459.000000, mae: 1198.236816, mean_q: -85.932518
 2123/5000: episode: 2123, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1170.367, mean reward: -1170.367 [-1170.367, -1170.367], mean action: 2.000 [2.000, 2.000],  loss: 14667282.000000, mae: 1142.050781, mean_q: -86.293205
 2124/5000: episode: 2124, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1022.574, mean reward: -1022.574 [-1022.574, -1022.574], mean action: 2.000 [2.000, 2.000],  loss: 7067532.000000, mae: 846.592773, mean_q: -86.539314
 2125/5000: episode: 2125, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2088.153, mean reward: -2088.153 [-2088.153, -2088.153], mean action: 2.000 [2.000, 2.000],  loss: 11602844.000000, mae: 1005.546631, mean_q: -86.699280
 2126/5000: episode: 2126, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1888.186, mean reward: -1888.186 [-1888.186, -1888.186], mean action: 2.000 [2.000, 2.000],  loss: 13637416.000000, mae: 1155.144775, mean_q: -86.580162
 2127/5000: episode: 2127, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2834.689, mean reward: -2834.689 [-2834.689, -2834.689], mean action: 2.000 [2.000, 2.000],  loss: 10076164.000000, mae: 1037.457275, mean_q: -86.816338
 2128/5000: episode: 2128, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2070.399, mean reward: -2070.399 [-2070.399, -2070.399], mean action: 2.000 [2.000, 2.000],  loss: 12297854.000000, mae: 1022.449524, mean_q: -86.970245
 2129/5000: episode: 2129, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3956.779, mean reward: -3956.779 [-3956.779, -3956.779], mean action: 2.000 [2.000, 2.000],  loss: 12228274.000000, mae: 1093.877441, mean_q: -87.058258
 2130/5000: episode: 2130, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2438.102, mean reward: -2438.102 [-2438.102, -2438.102], mean action: 2.000 [2.000, 2.000],  loss: 17551974.000000, mae: 1274.331909, mean_q: -86.628113
 2131/5000: episode: 2131, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -255.287, mean reward: -255.287 [-255.287, -255.287], mean action: 2.000 [2.000, 2.000],  loss: 13462482.000000, mae: 1168.589844, mean_q: -87.264496
 2132/5000: episode: 2132, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -42.519, mean reward: -42.519 [-42.519, -42.519], mean action: 2.000 [2.000, 2.000],  loss: 10561264.000000, mae: 1049.005737, mean_q: -87.224045
 2133/5000: episode: 2133, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2485.295, mean reward: -2485.295 [-2485.295, -2485.295], mean action: 2.000 [2.000, 2.000],  loss: 13815138.000000, mae: 1087.109253, mean_q: -87.548409
 2134/5000: episode: 2134, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1042.889, mean reward: -1042.889 [-1042.889, -1042.889], mean action: 2.000 [2.000, 2.000],  loss: 14579259.000000, mae: 1186.098633, mean_q: -87.370590
 2135/5000: episode: 2135, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -215.459, mean reward: -215.459 [-215.459, -215.459], mean action: 2.000 [2.000, 2.000],  loss: 11905409.000000, mae: 1019.042603, mean_q: -87.976082
 2136/5000: episode: 2136, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3678.619, mean reward: -3678.619 [-3678.619, -3678.619], mean action: 2.000 [2.000, 2.000],  loss: 15121266.000000, mae: 1208.519043, mean_q: -87.673279
 2137/5000: episode: 2137, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3515.290, mean reward: -3515.290 [-3515.290, -3515.290], mean action: 2.000 [2.000, 2.000],  loss: 13369798.000000, mae: 1097.782227, mean_q: -88.070755
 2138/5000: episode: 2138, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2813.304, mean reward: -2813.304 [-2813.304, -2813.304], mean action: 2.000 [2.000, 2.000],  loss: 22091756.000000, mae: 1399.538452, mean_q: -87.497849
 2139/5000: episode: 2139, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5039.691, mean reward: -5039.691 [-5039.691, -5039.691], mean action: 2.000 [2.000, 2.000],  loss: 17069022.000000, mae: 1274.680420, mean_q: -87.997314
 2140/5000: episode: 2140, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -528.213, mean reward: -528.213 [-528.213, -528.213], mean action: 2.000 [2.000, 2.000],  loss: 15351734.000000, mae: 1170.773926, mean_q: -88.206497
 2141/5000: episode: 2141, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2062.353, mean reward: -2062.353 [-2062.353, -2062.353], mean action: 2.000 [2.000, 2.000],  loss: 14094286.000000, mae: 1127.484131, mean_q: -88.036545
 2142/5000: episode: 2142, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1179.019, mean reward: -1179.019 [-1179.019, -1179.019], mean action: 2.000 [2.000, 2.000],  loss: 17294364.000000, mae: 1328.951538, mean_q: -88.386963
 2143/5000: episode: 2143, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4946.895, mean reward: -4946.895 [-4946.895, -4946.895], mean action: 1.000 [1.000, 1.000],  loss: 10766934.000000, mae: 965.298340, mean_q: -88.526428
 2144/5000: episode: 2144, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1290.739, mean reward: -1290.739 [-1290.739, -1290.739], mean action: 3.000 [3.000, 3.000],  loss: 19783508.000000, mae: 1399.109009, mean_q: -88.745689
 2145/5000: episode: 2145, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1117.352, mean reward: -1117.352 [-1117.352, -1117.352], mean action: 2.000 [2.000, 2.000],  loss: 16626901.000000, mae: 1175.280029, mean_q: -88.566757
 2146/5000: episode: 2146, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8104.725, mean reward: -8104.725 [-8104.725, -8104.725], mean action: 2.000 [2.000, 2.000],  loss: 14325865.000000, mae: 1176.599976, mean_q: -88.796410
 2147/5000: episode: 2147, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -546.123, mean reward: -546.123 [-546.123, -546.123], mean action: 2.000 [2.000, 2.000],  loss: 18989148.000000, mae: 1328.747314, mean_q: -88.718735
 2148/5000: episode: 2148, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -217.388, mean reward: -217.388 [-217.388, -217.388], mean action: 2.000 [2.000, 2.000],  loss: 13560307.000000, mae: 1130.053833, mean_q: -89.026390
 2149/5000: episode: 2149, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7046.150, mean reward: -7046.150 [-7046.150, -7046.150], mean action: 3.000 [3.000, 3.000],  loss: 16831572.000000, mae: 1254.844482, mean_q: -88.972580
 2150/5000: episode: 2150, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -2140.700, mean reward: -2140.700 [-2140.700, -2140.700], mean action: 2.000 [2.000, 2.000],  loss: 13343979.000000, mae: 1149.529663, mean_q: -88.676849
 2151/5000: episode: 2151, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3995.711, mean reward: -3995.711 [-3995.711, -3995.711], mean action: 3.000 [3.000, 3.000],  loss: 14422098.000000, mae: 1148.717285, mean_q: -89.596130
 2152/5000: episode: 2152, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -240.537, mean reward: -240.537 [-240.537, -240.537], mean action: 3.000 [3.000, 3.000],  loss: 17079136.000000, mae: 1215.085938, mean_q: -89.451614
 2153/5000: episode: 2153, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1094.859, mean reward: -1094.859 [-1094.859, -1094.859], mean action: 3.000 [3.000, 3.000],  loss: 13447898.000000, mae: 1002.966370, mean_q: -90.007355
 2154/5000: episode: 2154, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6569.014, mean reward: -6569.014 [-6569.014, -6569.014], mean action: 2.000 [2.000, 2.000],  loss: 12453502.000000, mae: 1010.041809, mean_q: -89.786942
 2155/5000: episode: 2155, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3830.039, mean reward: -3830.039 [-3830.039, -3830.039], mean action: 1.000 [1.000, 1.000],  loss: 9648744.000000, mae: 945.690735, mean_q: -90.096558
 2156/5000: episode: 2156, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2379.210, mean reward: -2379.210 [-2379.210, -2379.210], mean action: 2.000 [2.000, 2.000],  loss: 11717116.000000, mae: 1077.444580, mean_q: -89.646606
 2157/5000: episode: 2157, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3004.637, mean reward: -3004.637 [-3004.637, -3004.637], mean action: 2.000 [2.000, 2.000],  loss: 18350368.000000, mae: 1284.235352, mean_q: -90.108818
 2158/5000: episode: 2158, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -847.048, mean reward: -847.048 [-847.048, -847.048], mean action: 3.000 [3.000, 3.000],  loss: 18061508.000000, mae: 1262.886963, mean_q: -89.710876
 2159/5000: episode: 2159, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -749.729, mean reward: -749.729 [-749.729, -749.729], mean action: 2.000 [2.000, 2.000],  loss: 16164092.000000, mae: 1155.262451, mean_q: -90.227234
 2160/5000: episode: 2160, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8132.108, mean reward: -8132.108 [-8132.108, -8132.108], mean action: 3.000 [3.000, 3.000],  loss: 15144164.000000, mae: 1258.072021, mean_q: -90.542099
 2161/5000: episode: 2161, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -7642.107, mean reward: -7642.107 [-7642.107, -7642.107], mean action: 3.000 [3.000, 3.000],  loss: 13485829.000000, mae: 1079.666260, mean_q: -90.442688
 2162/5000: episode: 2162, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1098.197, mean reward: -1098.197 [-1098.197, -1098.197], mean action: 2.000 [2.000, 2.000],  loss: 11784237.000000, mae: 1029.328369, mean_q: -90.438538
 2163/5000: episode: 2163, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4044.974, mean reward: -4044.974 [-4044.974, -4044.974], mean action: 3.000 [3.000, 3.000],  loss: 18067024.000000, mae: 1296.960449, mean_q: -90.476456
 2164/5000: episode: 2164, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -961.522, mean reward: -961.522 [-961.522, -961.522], mean action: 2.000 [2.000, 2.000],  loss: 18470820.000000, mae: 1305.208740, mean_q: -90.672043
 2165/5000: episode: 2165, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4586.059, mean reward: -4586.059 [-4586.059, -4586.059], mean action: 2.000 [2.000, 2.000],  loss: 16248465.000000, mae: 1220.125244, mean_q: -90.737267
 2166/5000: episode: 2166, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2603.928, mean reward: -2603.928 [-2603.928, -2603.928], mean action: 2.000 [2.000, 2.000],  loss: 20375932.000000, mae: 1324.783203, mean_q: -90.941498
 2167/5000: episode: 2167, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -449.319, mean reward: -449.319 [-449.319, -449.319], mean action: 2.000 [2.000, 2.000],  loss: 14570052.000000, mae: 1166.133911, mean_q: -90.928955
 2168/5000: episode: 2168, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1314.667, mean reward: -1314.667 [-1314.667, -1314.667], mean action: 2.000 [2.000, 2.000],  loss: 15843318.000000, mae: 1151.553467, mean_q: -90.856506
 2169/5000: episode: 2169, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2189.540, mean reward: -2189.540 [-2189.540, -2189.540], mean action: 2.000 [2.000, 2.000],  loss: 7261698.000000, mae: 869.460815, mean_q: -91.379898
 2170/5000: episode: 2170, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3558.729, mean reward: -3558.729 [-3558.729, -3558.729], mean action: 2.000 [2.000, 2.000],  loss: 16035130.000000, mae: 1187.365234, mean_q: -91.511353
 2171/5000: episode: 2171, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3251.240, mean reward: -3251.240 [-3251.240, -3251.240], mean action: 2.000 [2.000, 2.000],  loss: 18814806.000000, mae: 1297.506348, mean_q: -91.312538
 2172/5000: episode: 2172, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2046.214, mean reward: -2046.214 [-2046.214, -2046.214], mean action: 2.000 [2.000, 2.000],  loss: 13733324.000000, mae: 1150.409180, mean_q: -91.802177
 2173/5000: episode: 2173, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -253.841, mean reward: -253.841 [-253.841, -253.841], mean action: 2.000 [2.000, 2.000],  loss: 10278979.000000, mae: 957.484009, mean_q: -91.364265
 2174/5000: episode: 2174, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1791.505, mean reward: -1791.505 [-1791.505, -1791.505], mean action: 2.000 [2.000, 2.000],  loss: 21049396.000000, mae: 1356.882080, mean_q: -91.529541
 2175/5000: episode: 2175, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6205.661, mean reward: -6205.661 [-6205.661, -6205.661], mean action: 2.000 [2.000, 2.000],  loss: 17333282.000000, mae: 1275.031738, mean_q: -91.979233
 2176/5000: episode: 2176, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1582.865, mean reward: -1582.865 [-1582.865, -1582.865], mean action: 2.000 [2.000, 2.000],  loss: 5224564.000000, mae: 755.724731, mean_q: -92.444824
 2177/5000: episode: 2177, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1014.205, mean reward: -1014.205 [-1014.205, -1014.205], mean action: 2.000 [2.000, 2.000],  loss: 9579772.000000, mae: 976.324829, mean_q: -91.723114
 2178/5000: episode: 2178, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3931.475, mean reward: -3931.475 [-3931.475, -3931.475], mean action: 2.000 [2.000, 2.000],  loss: 19309164.000000, mae: 1411.973145, mean_q: -91.619476
 2179/5000: episode: 2179, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -493.230, mean reward: -493.230 [-493.230, -493.230], mean action: 2.000 [2.000, 2.000],  loss: 19095892.000000, mae: 1321.891357, mean_q: -91.950378
 2180/5000: episode: 2180, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -851.798, mean reward: -851.798 [-851.798, -851.798], mean action: 2.000 [2.000, 2.000],  loss: 20971046.000000, mae: 1382.681030, mean_q: -92.120018
 2181/5000: episode: 2181, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -543.418, mean reward: -543.418 [-543.418, -543.418], mean action: 2.000 [2.000, 2.000],  loss: 12851399.000000, mae: 1119.070557, mean_q: -92.427750
 2182/5000: episode: 2182, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -4699.897, mean reward: -4699.897 [-4699.897, -4699.897], mean action: 2.000 [2.000, 2.000],  loss: 12274464.000000, mae: 1089.179565, mean_q: -92.358017
 2183/5000: episode: 2183, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -177.364, mean reward: -177.364 [-177.364, -177.364], mean action: 2.000 [2.000, 2.000],  loss: 17925956.000000, mae: 1174.635498, mean_q: -92.890862
 2184/5000: episode: 2184, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2236.001, mean reward: -2236.001 [-2236.001, -2236.001], mean action: 2.000 [2.000, 2.000],  loss: 15865100.000000, mae: 1240.981445, mean_q: -92.918335
 2185/5000: episode: 2185, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -155.445, mean reward: -155.445 [-155.445, -155.445], mean action: 2.000 [2.000, 2.000],  loss: 8026172.500000, mae: 858.805664, mean_q: -93.219345
 2186/5000: episode: 2186, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5176.408, mean reward: -5176.408 [-5176.408, -5176.408], mean action: 2.000 [2.000, 2.000],  loss: 12938287.000000, mae: 1136.130859, mean_q: -93.162430
 2187/5000: episode: 2187, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -1090.405, mean reward: -1090.405 [-1090.405, -1090.405], mean action: 2.000 [2.000, 2.000],  loss: 13744816.000000, mae: 1110.760986, mean_q: -93.448616
 2188/5000: episode: 2188, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3443.770, mean reward: -3443.770 [-3443.770, -3443.770], mean action: 3.000 [3.000, 3.000],  loss: 12525304.000000, mae: 1099.025757, mean_q: -93.370117
 2189/5000: episode: 2189, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3172.762, mean reward: -3172.762 [-3172.762, -3172.762], mean action: 3.000 [3.000, 3.000],  loss: 13627981.000000, mae: 1111.152832, mean_q: -93.552200
 2190/5000: episode: 2190, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -104.246, mean reward: -104.246 [-104.246, -104.246], mean action: 2.000 [2.000, 2.000],  loss: 9886508.000000, mae: 997.543518, mean_q: -93.762939
 2191/5000: episode: 2191, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6854.254, mean reward: -6854.254 [-6854.254, -6854.254], mean action: 3.000 [3.000, 3.000],  loss: 17436560.000000, mae: 1247.877441, mean_q: -93.580284
 2192/5000: episode: 2192, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -609.878, mean reward: -609.878 [-609.878, -609.878], mean action: 3.000 [3.000, 3.000],  loss: 7166068.000000, mae: 856.705017, mean_q: -94.025467
 2193/5000: episode: 2193, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1217.963, mean reward: -1217.963 [-1217.963, -1217.963], mean action: 2.000 [2.000, 2.000],  loss: 13465190.000000, mae: 1094.333740, mean_q: -94.004349
 2194/5000: episode: 2194, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1867.727, mean reward: -1867.727 [-1867.727, -1867.727], mean action: 3.000 [3.000, 3.000],  loss: 15253463.000000, mae: 1172.944580, mean_q: -93.790817
 2195/5000: episode: 2195, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4976.864, mean reward: -4976.864 [-4976.864, -4976.864], mean action: 3.000 [3.000, 3.000],  loss: 19357760.000000, mae: 1337.860352, mean_q: -93.705147
 2196/5000: episode: 2196, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1548.110, mean reward: -1548.110 [-1548.110, -1548.110], mean action: 3.000 [3.000, 3.000],  loss: 12343398.000000, mae: 1091.090820, mean_q: -94.196121
 2197/5000: episode: 2197, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -166.058, mean reward: -166.058 [-166.058, -166.058], mean action: 3.000 [3.000, 3.000],  loss: 8797392.000000, mae: 822.144653, mean_q: -94.873505
 2198/5000: episode: 2198, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -7326.346, mean reward: -7326.346 [-7326.346, -7326.346], mean action: 3.000 [3.000, 3.000],  loss: 11589646.000000, mae: 1087.367676, mean_q: -94.760651
 2199/5000: episode: 2199, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5802.691, mean reward: -5802.691 [-5802.691, -5802.691], mean action: 3.000 [3.000, 3.000],  loss: 19584316.000000, mae: 1376.155151, mean_q: -94.539429
 2200/5000: episode: 2200, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -70.039, mean reward: -70.039 [-70.039, -70.039], mean action: 3.000 [3.000, 3.000],  loss: 16567166.000000, mae: 1193.449463, mean_q: -94.623947
 2201/5000: episode: 2201, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -197.229, mean reward: -197.229 [-197.229, -197.229], mean action: 3.000 [3.000, 3.000],  loss: 11925954.000000, mae: 1025.037476, mean_q: -95.265671
 2202/5000: episode: 2202, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5969.868, mean reward: -5969.868 [-5969.868, -5969.868], mean action: 2.000 [2.000, 2.000],  loss: 16911428.000000, mae: 1300.135864, mean_q: -94.713577
 2203/5000: episode: 2203, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2792.575, mean reward: -2792.575 [-2792.575, -2792.575], mean action: 3.000 [3.000, 3.000],  loss: 10359868.000000, mae: 1051.854980, mean_q: -95.392166
 2204/5000: episode: 2204, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3429.074, mean reward: -3429.074 [-3429.074, -3429.074], mean action: 3.000 [3.000, 3.000],  loss: 11224556.000000, mae: 988.126892, mean_q: -95.384834
 2205/5000: episode: 2205, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -8521.634, mean reward: -8521.634 [-8521.634, -8521.634], mean action: 3.000 [3.000, 3.000],  loss: 18481724.000000, mae: 1215.677734, mean_q: -95.623001
 2206/5000: episode: 2206, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -297.436, mean reward: -297.436 [-297.436, -297.436], mean action: 3.000 [3.000, 3.000],  loss: 10292210.000000, mae: 989.081299, mean_q: -95.774483
 2207/5000: episode: 2207, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3739.981, mean reward: -3739.981 [-3739.981, -3739.981], mean action: 3.000 [3.000, 3.000],  loss: 21365510.000000, mae: 1426.643066, mean_q: -95.225204
 2208/5000: episode: 2208, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6283.222, mean reward: -6283.222 [-6283.222, -6283.222], mean action: 3.000 [3.000, 3.000],  loss: 10022882.000000, mae: 902.817871, mean_q: -96.467697
 2209/5000: episode: 2209, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -3075.287, mean reward: -3075.287 [-3075.287, -3075.287], mean action: 3.000 [3.000, 3.000],  loss: 14007952.000000, mae: 1159.913574, mean_q: -95.710732
 2210/5000: episode: 2210, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5352.579, mean reward: -5352.579 [-5352.579, -5352.579], mean action: 3.000 [3.000, 3.000],  loss: 11840894.000000, mae: 1135.281738, mean_q: -96.089668
 2211/5000: episode: 2211, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6988.567, mean reward: -6988.567 [-6988.567, -6988.567], mean action: 3.000 [3.000, 3.000],  loss: 11645862.000000, mae: 1069.746704, mean_q: -95.911072
 2212/5000: episode: 2212, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1054.754, mean reward: -1054.754 [-1054.754, -1054.754], mean action: 3.000 [3.000, 3.000],  loss: 15930707.000000, mae: 1218.858887, mean_q: -95.946671
 2213/5000: episode: 2213, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6671.304, mean reward: -6671.304 [-6671.304, -6671.304], mean action: 3.000 [3.000, 3.000],  loss: 13538742.000000, mae: 1165.070312, mean_q: -96.237709
 2214/5000: episode: 2214, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -897.447, mean reward: -897.447 [-897.447, -897.447], mean action: 3.000 [3.000, 3.000],  loss: 12942238.000000, mae: 1075.631104, mean_q: -96.596504
 2215/5000: episode: 2215, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7895.127, mean reward: -7895.127 [-7895.127, -7895.127], mean action: 3.000 [3.000, 3.000],  loss: 18089726.000000, mae: 1282.680420, mean_q: -95.893349
 2216/5000: episode: 2216, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2517.049, mean reward: -2517.049 [-2517.049, -2517.049], mean action: 3.000 [3.000, 3.000],  loss: 14988280.000000, mae: 1233.311523, mean_q: -96.356018
 2217/5000: episode: 2217, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1251.314, mean reward: -1251.314 [-1251.314, -1251.314], mean action: 3.000 [3.000, 3.000],  loss: 8999156.000000, mae: 990.284851, mean_q: -96.586990
 2218/5000: episode: 2218, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7340.153, mean reward: -7340.153 [-7340.153, -7340.153], mean action: 3.000 [3.000, 3.000],  loss: 12101280.000000, mae: 992.789551, mean_q: -96.993401
 2219/5000: episode: 2219, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -3560.914, mean reward: -3560.914 [-3560.914, -3560.914], mean action: 3.000 [3.000, 3.000],  loss: 8241298.000000, mae: 849.158936, mean_q: -97.423553
 2220/5000: episode: 2220, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -88.523, mean reward: -88.523 [-88.523, -88.523], mean action: 3.000 [3.000, 3.000],  loss: 18024102.000000, mae: 1243.817139, mean_q: -97.085579
 2221/5000: episode: 2221, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -8471.240, mean reward: -8471.240 [-8471.240, -8471.240], mean action: 3.000 [3.000, 3.000],  loss: 18395404.000000, mae: 1338.544800, mean_q: -96.775864
 2222/5000: episode: 2222, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1539.400, mean reward: -1539.400 [-1539.400, -1539.400], mean action: 3.000 [3.000, 3.000],  loss: 14522137.000000, mae: 1179.105469, mean_q: -97.068939
 2223/5000: episode: 2223, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3110.685, mean reward: -3110.685 [-3110.685, -3110.685], mean action: 3.000 [3.000, 3.000],  loss: 13121494.000000, mae: 1072.260986, mean_q: -97.258194
 2224/5000: episode: 2224, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2999.581, mean reward: -2999.581 [-2999.581, -2999.581], mean action: 3.000 [3.000, 3.000],  loss: 8911142.000000, mae: 898.865112, mean_q: -98.161423
 2225/5000: episode: 2225, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6569.596, mean reward: -6569.596 [-6569.596, -6569.596], mean action: 3.000 [3.000, 3.000],  loss: 21488052.000000, mae: 1316.579590, mean_q: -97.054024
 2226/5000: episode: 2226, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4918.193, mean reward: -4918.193 [-4918.193, -4918.193], mean action: 3.000 [3.000, 3.000],  loss: 13240617.000000, mae: 1087.966064, mean_q: -97.627396
 2227/5000: episode: 2227, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3021.155, mean reward: -3021.155 [-3021.155, -3021.155], mean action: 3.000 [3.000, 3.000],  loss: 17402860.000000, mae: 1132.141357, mean_q: -97.747246
 2228/5000: episode: 2228, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -10267.662, mean reward: -10267.662 [-10267.662, -10267.662], mean action: 3.000 [3.000, 3.000],  loss: 7936430.000000, mae: 931.649536, mean_q: -98.232498
 2229/5000: episode: 2229, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2518.154, mean reward: -2518.154 [-2518.154, -2518.154], mean action: 2.000 [2.000, 2.000],  loss: 14254389.000000, mae: 1107.800049, mean_q: -98.071899
 2230/5000: episode: 2230, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -956.335, mean reward: -956.335 [-956.335, -956.335], mean action: 3.000 [3.000, 3.000],  loss: 17877400.000000, mae: 1230.199463, mean_q: -98.003845
 2231/5000: episode: 2231, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4002.713, mean reward: -4002.713 [-4002.713, -4002.713], mean action: 3.000 [3.000, 3.000],  loss: 12976456.000000, mae: 1043.648438, mean_q: -98.233322
 2232/5000: episode: 2232, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -892.900, mean reward: -892.900 [-892.900, -892.900], mean action: 3.000 [3.000, 3.000],  loss: 10730117.000000, mae: 970.293335, mean_q: -98.637985
 2233/5000: episode: 2233, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1775.317, mean reward: -1775.317 [-1775.317, -1775.317], mean action: 2.000 [2.000, 2.000],  loss: 13226493.000000, mae: 1214.787842, mean_q: -98.470718
 2234/5000: episode: 2234, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6646.898, mean reward: -6646.898 [-6646.898, -6646.898], mean action: 0.000 [0.000, 0.000],  loss: 10518662.000000, mae: 1007.878235, mean_q: -98.743958
 2235/5000: episode: 2235, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4730.430, mean reward: -4730.430 [-4730.430, -4730.430], mean action: 3.000 [3.000, 3.000],  loss: 15693754.000000, mae: 1222.958984, mean_q: -98.932343
 2236/5000: episode: 2236, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -504.635, mean reward: -504.635 [-504.635, -504.635], mean action: 1.000 [1.000, 1.000],  loss: 8108439.500000, mae: 831.901001, mean_q: -98.925278
 2237/5000: episode: 2237, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5500.253, mean reward: -5500.253 [-5500.253, -5500.253], mean action: 3.000 [3.000, 3.000],  loss: 14250437.000000, mae: 1107.219360, mean_q: -98.988770
 2238/5000: episode: 2238, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2781.022, mean reward: -2781.022 [-2781.022, -2781.022], mean action: 3.000 [3.000, 3.000],  loss: 12705689.000000, mae: 1120.057861, mean_q: -99.122986
 2239/5000: episode: 2239, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1508.248, mean reward: -1508.248 [-1508.248, -1508.248], mean action: 3.000 [3.000, 3.000],  loss: 13767420.000000, mae: 1156.751709, mean_q: -99.444359
 2240/5000: episode: 2240, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -3997.415, mean reward: -3997.415 [-3997.415, -3997.415], mean action: 3.000 [3.000, 3.000],  loss: 10373876.000000, mae: 1020.458496, mean_q: -99.462639
 2241/5000: episode: 2241, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2152.033, mean reward: -2152.033 [-2152.033, -2152.033], mean action: 3.000 [3.000, 3.000],  loss: 13392164.000000, mae: 1146.099609, mean_q: -99.281570
 2242/5000: episode: 2242, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2671.952, mean reward: -2671.952 [-2671.952, -2671.952], mean action: 2.000 [2.000, 2.000],  loss: 13281754.000000, mae: 1027.710815, mean_q: -99.886505
 2243/5000: episode: 2243, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9.246, mean reward: -9.246 [-9.246, -9.246], mean action: 3.000 [3.000, 3.000],  loss: 11239145.000000, mae: 987.227600, mean_q: -100.221352
 2244/5000: episode: 2244, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6308.027, mean reward: -6308.027 [-6308.027, -6308.027], mean action: 2.000 [2.000, 2.000],  loss: 10844919.000000, mae: 1008.822998, mean_q: -99.601601
 2245/5000: episode: 2245, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3452.070, mean reward: -3452.070 [-3452.070, -3452.070], mean action: 2.000 [2.000, 2.000],  loss: 13615344.000000, mae: 1127.016724, mean_q: -99.731308
 2246/5000: episode: 2246, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1323.969, mean reward: -1323.969 [-1323.969, -1323.969], mean action: 2.000 [2.000, 2.000],  loss: 20533018.000000, mae: 1227.249512, mean_q: -99.615303
 2247/5000: episode: 2247, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -323.405, mean reward: -323.405 [-323.405, -323.405], mean action: 2.000 [2.000, 2.000],  loss: 16920372.000000, mae: 1217.974365, mean_q: -99.633453
 2248/5000: episode: 2248, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -774.240, mean reward: -774.240 [-774.240, -774.240], mean action: 2.000 [2.000, 2.000],  loss: 20402752.000000, mae: 1473.626587, mean_q: -100.151657
 2249/5000: episode: 2249, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2575.598, mean reward: -2575.598 [-2575.598, -2575.598], mean action: 3.000 [3.000, 3.000],  loss: 11624182.000000, mae: 962.266846, mean_q: -99.949158
 2250/5000: episode: 2250, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5817.736, mean reward: -5817.736 [-5817.736, -5817.736], mean action: 2.000 [2.000, 2.000],  loss: 7374825.000000, mae: 798.736694, mean_q: -100.568665
 2251/5000: episode: 2251, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3523.361, mean reward: -3523.361 [-3523.361, -3523.361], mean action: 2.000 [2.000, 2.000],  loss: 10271952.000000, mae: 985.231323, mean_q: -100.296646
 2252/5000: episode: 2252, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1619.140, mean reward: -1619.140 [-1619.140, -1619.140], mean action: 2.000 [2.000, 2.000],  loss: 12199814.000000, mae: 1084.077515, mean_q: -100.725021
 2253/5000: episode: 2253, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -1572.924, mean reward: -1572.924 [-1572.924, -1572.924], mean action: 2.000 [2.000, 2.000],  loss: 23337934.000000, mae: 1429.974609, mean_q: -100.218140
 2254/5000: episode: 2254, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2225.408, mean reward: -2225.408 [-2225.408, -2225.408], mean action: 2.000 [2.000, 2.000],  loss: 16808618.000000, mae: 1289.796509, mean_q: -100.876030
 2255/5000: episode: 2255, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6887.616, mean reward: -6887.616 [-6887.616, -6887.616], mean action: 2.000 [2.000, 2.000],  loss: 8847931.000000, mae: 914.911011, mean_q: -101.191635
 2256/5000: episode: 2256, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -3128.539, mean reward: -3128.539 [-3128.539, -3128.539], mean action: 2.000 [2.000, 2.000],  loss: 13355316.000000, mae: 1150.792603, mean_q: -101.202988
 2257/5000: episode: 2257, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2471.082, mean reward: -2471.082 [-2471.082, -2471.082], mean action: 2.000 [2.000, 2.000],  loss: 16430707.000000, mae: 1214.463867, mean_q: -101.095322
 2258/5000: episode: 2258, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1001.044, mean reward: -1001.044 [-1001.044, -1001.044], mean action: 2.000 [2.000, 2.000],  loss: 9619206.000000, mae: 951.839478, mean_q: -101.556572
 2259/5000: episode: 2259, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -434.732, mean reward: -434.732 [-434.732, -434.732], mean action: 2.000 [2.000, 2.000],  loss: 8586154.000000, mae: 879.394531, mean_q: -101.429031
 2260/5000: episode: 2260, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3064.564, mean reward: -3064.564 [-3064.564, -3064.564], mean action: 2.000 [2.000, 2.000],  loss: 12176521.000000, mae: 1107.057861, mean_q: -101.979622
 2261/5000: episode: 2261, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2018.572, mean reward: -2018.572 [-2018.572, -2018.572], mean action: 2.000 [2.000, 2.000],  loss: 19134102.000000, mae: 1252.129150, mean_q: -101.227180
 2262/5000: episode: 2262, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1276.560, mean reward: -1276.560 [-1276.560, -1276.560], mean action: 3.000 [3.000, 3.000],  loss: 20808480.000000, mae: 1302.976807, mean_q: -101.753799
 2263/5000: episode: 2263, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2952.593, mean reward: -2952.593 [-2952.593, -2952.593], mean action: 2.000 [2.000, 2.000],  loss: 24126876.000000, mae: 1496.994263, mean_q: -101.403595
 2264/5000: episode: 2264, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -449.559, mean reward: -449.559 [-449.559, -449.559], mean action: 2.000 [2.000, 2.000],  loss: 9450827.000000, mae: 966.136963, mean_q: -102.320175
 2265/5000: episode: 2265, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2564.281, mean reward: -2564.281 [-2564.281, -2564.281], mean action: 2.000 [2.000, 2.000],  loss: 12082078.000000, mae: 1090.054810, mean_q: -102.266464
 2266/5000: episode: 2266, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -558.874, mean reward: -558.874 [-558.874, -558.874], mean action: 2.000 [2.000, 2.000],  loss: 11783170.000000, mae: 1050.973511, mean_q: -102.174591
 2267/5000: episode: 2267, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -341.290, mean reward: -341.290 [-341.290, -341.290], mean action: 2.000 [2.000, 2.000],  loss: 13501440.000000, mae: 1106.452026, mean_q: -102.274628
 2268/5000: episode: 2268, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -198.965, mean reward: -198.965 [-198.965, -198.965], mean action: 2.000 [2.000, 2.000],  loss: 10521997.000000, mae: 1017.801636, mean_q: -102.256325
 2269/5000: episode: 2269, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -932.801, mean reward: -932.801 [-932.801, -932.801], mean action: 2.000 [2.000, 2.000],  loss: 15684120.000000, mae: 1065.458252, mean_q: -102.897247
 2270/5000: episode: 2270, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3983.371, mean reward: -3983.371 [-3983.371, -3983.371], mean action: 2.000 [2.000, 2.000],  loss: 12360850.000000, mae: 983.751465, mean_q: -102.936493
 2271/5000: episode: 2271, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1593.005, mean reward: -1593.005 [-1593.005, -1593.005], mean action: 2.000 [2.000, 2.000],  loss: 12205128.000000, mae: 984.772522, mean_q: -102.886894
 2272/5000: episode: 2272, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3942.229, mean reward: -3942.229 [-3942.229, -3942.229], mean action: 2.000 [2.000, 2.000],  loss: 19515094.000000, mae: 1384.118530, mean_q: -102.473984
 2273/5000: episode: 2273, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -447.831, mean reward: -447.831 [-447.831, -447.831], mean action: 2.000 [2.000, 2.000],  loss: 15258438.000000, mae: 1174.010010, mean_q: -103.257980
 2274/5000: episode: 2274, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2045.832, mean reward: -2045.832 [-2045.832, -2045.832], mean action: 2.000 [2.000, 2.000],  loss: 21379544.000000, mae: 1494.529663, mean_q: -103.001266
 2275/5000: episode: 2275, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7953.974, mean reward: -7953.974 [-7953.974, -7953.974], mean action: 1.000 [1.000, 1.000],  loss: 12105238.000000, mae: 1086.809937, mean_q: -103.272278
 2276/5000: episode: 2276, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2350.708, mean reward: -2350.708 [-2350.708, -2350.708], mean action: 2.000 [2.000, 2.000],  loss: 11080339.000000, mae: 1047.084473, mean_q: -103.639496
 2277/5000: episode: 2277, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3577.307, mean reward: -3577.307 [-3577.307, -3577.307], mean action: 2.000 [2.000, 2.000],  loss: 9727066.000000, mae: 958.031189, mean_q: -103.543106
 2278/5000: episode: 2278, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5441.469, mean reward: -5441.469 [-5441.469, -5441.469], mean action: 2.000 [2.000, 2.000],  loss: 16460966.000000, mae: 1223.237061, mean_q: -103.725212
 2279/5000: episode: 2279, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -180.251, mean reward: -180.251 [-180.251, -180.251], mean action: 2.000 [2.000, 2.000],  loss: 14379608.000000, mae: 1188.868774, mean_q: -103.474533
 2280/5000: episode: 2280, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -941.090, mean reward: -941.090 [-941.090, -941.090], mean action: 2.000 [2.000, 2.000],  loss: 10999332.000000, mae: 965.304688, mean_q: -103.841454
 2281/5000: episode: 2281, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2748.922, mean reward: -2748.922 [-2748.922, -2748.922], mean action: 2.000 [2.000, 2.000],  loss: 13374718.000000, mae: 1177.384155, mean_q: -104.282066
 2282/5000: episode: 2282, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1596.117, mean reward: -1596.117 [-1596.117, -1596.117], mean action: 2.000 [2.000, 2.000],  loss: 11787744.000000, mae: 1069.752319, mean_q: -104.061417
 2283/5000: episode: 2283, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6905.458, mean reward: -6905.458 [-6905.458, -6905.458], mean action: 3.000 [3.000, 3.000],  loss: 14920636.000000, mae: 1083.210938, mean_q: -104.535477
 2284/5000: episode: 2284, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -83.708, mean reward: -83.708 [-83.708, -83.708], mean action: 2.000 [2.000, 2.000],  loss: 14771123.000000, mae: 1122.766602, mean_q: -104.670853
 2285/5000: episode: 2285, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5779.548, mean reward: -5779.548 [-5779.548, -5779.548], mean action: 0.000 [0.000, 0.000],  loss: 17107348.000000, mae: 1329.417236, mean_q: -104.276360
 2286/5000: episode: 2286, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -9440.173, mean reward: -9440.173 [-9440.173, -9440.173], mean action: 3.000 [3.000, 3.000],  loss: 15479190.000000, mae: 1248.566650, mean_q: -104.331085
 2287/5000: episode: 2287, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5032.886, mean reward: -5032.886 [-5032.886, -5032.886], mean action: 3.000 [3.000, 3.000],  loss: 10708082.000000, mae: 965.574341, mean_q: -104.899361
 2288/5000: episode: 2288, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5225.978, mean reward: -5225.978 [-5225.978, -5225.978], mean action: 3.000 [3.000, 3.000],  loss: 11556818.000000, mae: 1123.166748, mean_q: -105.146156
 2289/5000: episode: 2289, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2186.436, mean reward: -2186.436 [-2186.436, -2186.436], mean action: 3.000 [3.000, 3.000],  loss: 18397856.000000, mae: 1263.189697, mean_q: -104.693123
 2290/5000: episode: 2290, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3683.238, mean reward: -3683.238 [-3683.238, -3683.238], mean action: 3.000 [3.000, 3.000],  loss: 14612308.000000, mae: 1218.599854, mean_q: -105.449615
 2291/5000: episode: 2291, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7629.660, mean reward: -7629.660 [-7629.660, -7629.660], mean action: 3.000 [3.000, 3.000],  loss: 10146314.000000, mae: 952.635010, mean_q: -105.048164
 2292/5000: episode: 2292, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5758.498, mean reward: -5758.498 [-5758.498, -5758.498], mean action: 3.000 [3.000, 3.000],  loss: 17563208.000000, mae: 1361.315552, mean_q: -104.857376
 2293/5000: episode: 2293, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7871.939, mean reward: -7871.939 [-7871.939, -7871.939], mean action: 3.000 [3.000, 3.000],  loss: 14089493.000000, mae: 1173.432617, mean_q: -105.429352
 2294/5000: episode: 2294, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -4131.594, mean reward: -4131.594 [-4131.594, -4131.594], mean action: 3.000 [3.000, 3.000],  loss: 11422278.000000, mae: 1044.102783, mean_q: -105.396759
 2295/5000: episode: 2295, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2995.268, mean reward: -2995.268 [-2995.268, -2995.268], mean action: 3.000 [3.000, 3.000],  loss: 12175062.000000, mae: 1070.878906, mean_q: -105.534805
 2296/5000: episode: 2296, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -718.703, mean reward: -718.703 [-718.703, -718.703], mean action: 3.000 [3.000, 3.000],  loss: 12026572.000000, mae: 1113.156494, mean_q: -106.187363
 2297/5000: episode: 2297, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -841.164, mean reward: -841.164 [-841.164, -841.164], mean action: 3.000 [3.000, 3.000],  loss: 15066546.000000, mae: 1222.508545, mean_q: -105.423561
 2298/5000: episode: 2298, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2298.310, mean reward: -2298.310 [-2298.310, -2298.310], mean action: 3.000 [3.000, 3.000],  loss: 15486286.000000, mae: 1087.906982, mean_q: -105.714272
 2299/5000: episode: 2299, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1910.141, mean reward: -1910.141 [-1910.141, -1910.141], mean action: 2.000 [2.000, 2.000],  loss: 11034494.000000, mae: 1069.616089, mean_q: -106.132874
 2300/5000: episode: 2300, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3311.614, mean reward: -3311.614 [-3311.614, -3311.614], mean action: 2.000 [2.000, 2.000],  loss: 12213224.000000, mae: 1058.354492, mean_q: -106.109726
 2301/5000: episode: 2301, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7563.407, mean reward: -7563.407 [-7563.407, -7563.407], mean action: 3.000 [3.000, 3.000],  loss: 16242343.000000, mae: 1228.806641, mean_q: -105.916550
 2302/5000: episode: 2302, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5045.865, mean reward: -5045.865 [-5045.865, -5045.865], mean action: 3.000 [3.000, 3.000],  loss: 13798876.000000, mae: 1077.163818, mean_q: -106.271622
 2303/5000: episode: 2303, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1405.663, mean reward: -1405.663 [-1405.663, -1405.663], mean action: 2.000 [2.000, 2.000],  loss: 6634759.500000, mae: 830.539795, mean_q: -107.208778
 2304/5000: episode: 2304, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1001.119, mean reward: -1001.119 [-1001.119, -1001.119], mean action: 3.000 [3.000, 3.000],  loss: 12796000.000000, mae: 1034.223633, mean_q: -106.838432
 2305/5000: episode: 2305, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7190.481, mean reward: -7190.481 [-7190.481, -7190.481], mean action: 3.000 [3.000, 3.000],  loss: 17724432.000000, mae: 1148.745728, mean_q: -106.792206
 2306/5000: episode: 2306, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2933.818, mean reward: -2933.818 [-2933.818, -2933.818], mean action: 3.000 [3.000, 3.000],  loss: 12104832.000000, mae: 1052.015869, mean_q: -107.049950
 2307/5000: episode: 2307, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -468.278, mean reward: -468.278 [-468.278, -468.278], mean action: 3.000 [3.000, 3.000],  loss: 14586482.000000, mae: 1135.816895, mean_q: -106.986191
 2308/5000: episode: 2308, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -7596.086, mean reward: -7596.086 [-7596.086, -7596.086], mean action: 3.000 [3.000, 3.000],  loss: 17193610.000000, mae: 1275.214355, mean_q: -107.147591
 2309/5000: episode: 2309, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5532.011, mean reward: -5532.011 [-5532.011, -5532.011], mean action: 3.000 [3.000, 3.000],  loss: 12995336.000000, mae: 1060.448486, mean_q: -107.102295
 2310/5000: episode: 2310, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3315.355, mean reward: -3315.355 [-3315.355, -3315.355], mean action: 3.000 [3.000, 3.000],  loss: 14587576.000000, mae: 1146.821045, mean_q: -107.455261
 2311/5000: episode: 2311, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6452.783, mean reward: -6452.783 [-6452.783, -6452.783], mean action: 3.000 [3.000, 3.000],  loss: 14563911.000000, mae: 1107.717529, mean_q: -107.818893
 2312/5000: episode: 2312, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1285.269, mean reward: -1285.269 [-1285.269, -1285.269], mean action: 3.000 [3.000, 3.000],  loss: 11604703.000000, mae: 1043.510498, mean_q: -107.586090
 2313/5000: episode: 2313, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -1677.545, mean reward: -1677.545 [-1677.545, -1677.545], mean action: 1.000 [1.000, 1.000],  loss: 14120633.000000, mae: 1118.106812, mean_q: -107.787460
 2314/5000: episode: 2314, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1614.914, mean reward: -1614.914 [-1614.914, -1614.914], mean action: 2.000 [2.000, 2.000],  loss: 10913053.000000, mae: 1004.522827, mean_q: -107.230927
 2315/5000: episode: 2315, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5358.317, mean reward: -5358.317 [-5358.317, -5358.317], mean action: 3.000 [3.000, 3.000],  loss: 16478468.000000, mae: 1229.969116, mean_q: -107.395706
 2316/5000: episode: 2316, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2781.940, mean reward: -2781.940 [-2781.940, -2781.940], mean action: 1.000 [1.000, 1.000],  loss: 14293751.000000, mae: 1217.632202, mean_q: -107.782791
 2317/5000: episode: 2317, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6985.981, mean reward: -6985.981 [-6985.981, -6985.981], mean action: 0.000 [0.000, 0.000],  loss: 17422944.000000, mae: 1287.273682, mean_q: -108.005600
 2318/5000: episode: 2318, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -10809.004, mean reward: -10809.004 [-10809.004, -10809.004], mean action: 0.000 [0.000, 0.000],  loss: 13885807.000000, mae: 1152.951172, mean_q: -108.542618
 2319/5000: episode: 2319, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -10837.156, mean reward: -10837.156 [-10837.156, -10837.156], mean action: 1.000 [1.000, 1.000],  loss: 9449777.000000, mae: 963.887573, mean_q: -108.554382
 2320/5000: episode: 2320, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -2373.975, mean reward: -2373.975 [-2373.975, -2373.975], mean action: 1.000 [1.000, 1.000],  loss: 13710050.000000, mae: 1106.726685, mean_q: -108.516983
 2321/5000: episode: 2321, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6070.113, mean reward: -6070.113 [-6070.113, -6070.113], mean action: 1.000 [1.000, 1.000],  loss: 9294185.000000, mae: 894.772095, mean_q: -108.898056
 2322/5000: episode: 2322, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -6869.130, mean reward: -6869.130 [-6869.130, -6869.130], mean action: 1.000 [1.000, 1.000],  loss: 13121612.000000, mae: 1071.212891, mean_q: -109.193375
 2323/5000: episode: 2323, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -3538.887, mean reward: -3538.887 [-3538.887, -3538.887], mean action: 1.000 [1.000, 1.000],  loss: 12548568.000000, mae: 1126.140137, mean_q: -109.063095
 2324/5000: episode: 2324, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -8025.720, mean reward: -8025.720 [-8025.720, -8025.720], mean action: 1.000 [1.000, 1.000],  loss: 16392342.000000, mae: 1292.576538, mean_q: -108.615105
 2325/5000: episode: 2325, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6989.418, mean reward: -6989.418 [-6989.418, -6989.418], mean action: 1.000 [1.000, 1.000],  loss: 13639436.000000, mae: 1070.261963, mean_q: -109.321808
 2326/5000: episode: 2326, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7234.141, mean reward: -7234.141 [-7234.141, -7234.141], mean action: 1.000 [1.000, 1.000],  loss: 14805066.000000, mae: 1211.856201, mean_q: -108.877007
 2327/5000: episode: 2327, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1109.920, mean reward: -1109.920 [-1109.920, -1109.920], mean action: 1.000 [1.000, 1.000],  loss: 11654966.000000, mae: 1054.048462, mean_q: -109.494423
 2328/5000: episode: 2328, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1020.800, mean reward: -1020.800 [-1020.800, -1020.800], mean action: 1.000 [1.000, 1.000],  loss: 13435785.000000, mae: 1128.905029, mean_q: -109.104225
 2329/5000: episode: 2329, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2396.197, mean reward: -2396.197 [-2396.197, -2396.197], mean action: 1.000 [1.000, 1.000],  loss: 8564038.000000, mae: 893.342896, mean_q: -110.066895
 2330/5000: episode: 2330, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3319.716, mean reward: -3319.716 [-3319.716, -3319.716], mean action: 2.000 [2.000, 2.000],  loss: 13626649.000000, mae: 1174.807617, mean_q: -109.528931
 2331/5000: episode: 2331, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4953.491, mean reward: -4953.491 [-4953.491, -4953.491], mean action: 1.000 [1.000, 1.000],  loss: 16764439.000000, mae: 1262.183105, mean_q: -109.218910
 2332/5000: episode: 2332, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8791.304, mean reward: -8791.304 [-8791.304, -8791.304], mean action: 1.000 [1.000, 1.000],  loss: 14928393.000000, mae: 1207.287842, mean_q: -109.974640
 2333/5000: episode: 2333, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6595.391, mean reward: -6595.391 [-6595.391, -6595.391], mean action: 1.000 [1.000, 1.000],  loss: 14611835.000000, mae: 1125.511353, mean_q: -110.267624
 2334/5000: episode: 2334, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3721.402, mean reward: -3721.402 [-3721.402, -3721.402], mean action: 1.000 [1.000, 1.000],  loss: 13117536.000000, mae: 1159.089844, mean_q: -110.149246
 2335/5000: episode: 2335, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4894.287, mean reward: -4894.287 [-4894.287, -4894.287], mean action: 1.000 [1.000, 1.000],  loss: 12874008.000000, mae: 1069.652832, mean_q: -110.435524
 2336/5000: episode: 2336, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2319.016, mean reward: -2319.016 [-2319.016, -2319.016], mean action: 1.000 [1.000, 1.000],  loss: 10647086.000000, mae: 1054.953003, mean_q: -109.894585
 2337/5000: episode: 2337, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2184.084, mean reward: -2184.084 [-2184.084, -2184.084], mean action: 1.000 [1.000, 1.000],  loss: 13840728.000000, mae: 1105.582520, mean_q: -110.094765
 2338/5000: episode: 2338, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4725.020, mean reward: -4725.020 [-4725.020, -4725.020], mean action: 1.000 [1.000, 1.000],  loss: 9885220.000000, mae: 996.504089, mean_q: -110.313721
 2339/5000: episode: 2339, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1933.222, mean reward: -1933.222 [-1933.222, -1933.222], mean action: 1.000 [1.000, 1.000],  loss: 18925772.000000, mae: 1258.294922, mean_q: -110.945465
 2340/5000: episode: 2340, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1766.357, mean reward: -1766.357 [-1766.357, -1766.357], mean action: 1.000 [1.000, 1.000],  loss: 10416948.000000, mae: 1013.189697, mean_q: -111.040413
 2341/5000: episode: 2341, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3630.882, mean reward: -3630.882 [-3630.882, -3630.882], mean action: 1.000 [1.000, 1.000],  loss: 14836813.000000, mae: 1120.840942, mean_q: -110.853836
 2342/5000: episode: 2342, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6192.728, mean reward: -6192.728 [-6192.728, -6192.728], mean action: 1.000 [1.000, 1.000],  loss: 16117875.000000, mae: 1190.625977, mean_q: -111.092636
 2343/5000: episode: 2343, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3304.561, mean reward: -3304.561 [-3304.561, -3304.561], mean action: 1.000 [1.000, 1.000],  loss: 13737052.000000, mae: 1233.207031, mean_q: -111.205185
 2344/5000: episode: 2344, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6838.754, mean reward: -6838.754 [-6838.754, -6838.754], mean action: 1.000 [1.000, 1.000],  loss: 12850516.000000, mae: 1156.505859, mean_q: -111.213478
 2345/5000: episode: 2345, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5370.512, mean reward: -5370.512 [-5370.512, -5370.512], mean action: 1.000 [1.000, 1.000],  loss: 17104208.000000, mae: 1218.623047, mean_q: -110.602707
 2346/5000: episode: 2346, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6101.991, mean reward: -6101.991 [-6101.991, -6101.991], mean action: 1.000 [1.000, 1.000],  loss: 14899238.000000, mae: 1242.498291, mean_q: -111.815910
 2347/5000: episode: 2347, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -2636.582, mean reward: -2636.582 [-2636.582, -2636.582], mean action: 1.000 [1.000, 1.000],  loss: 9510898.000000, mae: 953.984680, mean_q: -111.568283
 2348/5000: episode: 2348, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5344.879, mean reward: -5344.879 [-5344.879, -5344.879], mean action: 1.000 [1.000, 1.000],  loss: 16579710.000000, mae: 1234.667114, mean_q: -111.611526
 2349/5000: episode: 2349, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7250.897, mean reward: -7250.897 [-7250.897, -7250.897], mean action: 1.000 [1.000, 1.000],  loss: 15269748.000000, mae: 1236.345581, mean_q: -111.047028
 2350/5000: episode: 2350, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5152.664, mean reward: -5152.664 [-5152.664, -5152.664], mean action: 1.000 [1.000, 1.000],  loss: 21620532.000000, mae: 1414.609131, mean_q: -111.582550
 2351/5000: episode: 2351, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1891.251, mean reward: -1891.251 [-1891.251, -1891.251], mean action: 3.000 [3.000, 3.000],  loss: 18596568.000000, mae: 1296.775879, mean_q: -112.168823
 2352/5000: episode: 2352, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7343.660, mean reward: -7343.660 [-7343.660, -7343.660], mean action: 1.000 [1.000, 1.000],  loss: 15206303.000000, mae: 1082.049561, mean_q: -112.084335
 2353/5000: episode: 2353, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4161.595, mean reward: -4161.595 [-4161.595, -4161.595], mean action: 1.000 [1.000, 1.000],  loss: 15175720.000000, mae: 1153.694702, mean_q: -112.225945
 2354/5000: episode: 2354, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -905.454, mean reward: -905.454 [-905.454, -905.454], mean action: 1.000 [1.000, 1.000],  loss: 16768507.000000, mae: 1219.522095, mean_q: -112.188171
 2355/5000: episode: 2355, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5835.517, mean reward: -5835.517 [-5835.517, -5835.517], mean action: 1.000 [1.000, 1.000],  loss: 14093858.000000, mae: 1164.516846, mean_q: -111.826347
 2356/5000: episode: 2356, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6567.037, mean reward: -6567.037 [-6567.037, -6567.037], mean action: 1.000 [1.000, 1.000],  loss: 9141838.000000, mae: 964.976440, mean_q: -113.034943
 2357/5000: episode: 2357, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6987.852, mean reward: -6987.852 [-6987.852, -6987.852], mean action: 1.000 [1.000, 1.000],  loss: 17653340.000000, mae: 1319.443604, mean_q: -112.101883
 2358/5000: episode: 2358, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1194.801, mean reward: -1194.801 [-1194.801, -1194.801], mean action: 1.000 [1.000, 1.000],  loss: 19626784.000000, mae: 1313.459473, mean_q: -112.471619
 2359/5000: episode: 2359, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3303.363, mean reward: -3303.363 [-3303.363, -3303.363], mean action: 1.000 [1.000, 1.000],  loss: 14211643.000000, mae: 1099.537598, mean_q: -112.879662
 2360/5000: episode: 2360, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4501.502, mean reward: -4501.502 [-4501.502, -4501.502], mean action: 1.000 [1.000, 1.000],  loss: 16713159.000000, mae: 1226.695923, mean_q: -112.816689
 2361/5000: episode: 2361, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1349.300, mean reward: -1349.300 [-1349.300, -1349.300], mean action: 1.000 [1.000, 1.000],  loss: 10222481.000000, mae: 963.560425, mean_q: -112.951584
 2362/5000: episode: 2362, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2857.324, mean reward: -2857.324 [-2857.324, -2857.324], mean action: 1.000 [1.000, 1.000],  loss: 19080662.000000, mae: 1349.966675, mean_q: -113.087967
 2363/5000: episode: 2363, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7420.455, mean reward: -7420.455 [-7420.455, -7420.455], mean action: 1.000 [1.000, 1.000],  loss: 16217680.000000, mae: 1192.015503, mean_q: -113.271103
 2364/5000: episode: 2364, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1543.370, mean reward: -1543.370 [-1543.370, -1543.370], mean action: 1.000 [1.000, 1.000],  loss: 16937078.000000, mae: 1277.304199, mean_q: -113.019264
 2365/5000: episode: 2365, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -206.389, mean reward: -206.389 [-206.389, -206.389], mean action: 3.000 [3.000, 3.000],  loss: 13706442.000000, mae: 1134.515137, mean_q: -113.615005
 2366/5000: episode: 2366, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4559.832, mean reward: -4559.832 [-4559.832, -4559.832], mean action: 1.000 [1.000, 1.000],  loss: 14283112.000000, mae: 1133.942017, mean_q: -113.417549
 2367/5000: episode: 2367, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2913.979, mean reward: -2913.979 [-2913.979, -2913.979], mean action: 1.000 [1.000, 1.000],  loss: 7073927.500000, mae: 840.021118, mean_q: -113.671021
 2368/5000: episode: 2368, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -842.641, mean reward: -842.641 [-842.641, -842.641], mean action: 1.000 [1.000, 1.000],  loss: 16741050.000000, mae: 1233.990479, mean_q: -113.752014
 2369/5000: episode: 2369, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -13527.217, mean reward: -13527.217 [-13527.217, -13527.217], mean action: 1.000 [1.000, 1.000],  loss: 16832204.000000, mae: 1304.152710, mean_q: -114.044434
 2370/5000: episode: 2370, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1196.416, mean reward: -1196.416 [-1196.416, -1196.416], mean action: 1.000 [1.000, 1.000],  loss: 19146406.000000, mae: 1335.847900, mean_q: -113.781052
 2371/5000: episode: 2371, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8409.717, mean reward: -8409.717 [-8409.717, -8409.717], mean action: 1.000 [1.000, 1.000],  loss: 11398746.000000, mae: 1035.841064, mean_q: -114.598877
 2372/5000: episode: 2372, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5850.714, mean reward: -5850.714 [-5850.714, -5850.714], mean action: 1.000 [1.000, 1.000],  loss: 14088006.000000, mae: 1140.080322, mean_q: -114.932220
 2373/5000: episode: 2373, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7894.676, mean reward: -7894.676 [-7894.676, -7894.676], mean action: 1.000 [1.000, 1.000],  loss: 16713047.000000, mae: 1269.507568, mean_q: -114.351440
 2374/5000: episode: 2374, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4154.266, mean reward: -4154.266 [-4154.266, -4154.266], mean action: 0.000 [0.000, 0.000],  loss: 11401610.000000, mae: 982.425293, mean_q: -114.884628
 2375/5000: episode: 2375, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2262.267, mean reward: -2262.267 [-2262.267, -2262.267], mean action: 1.000 [1.000, 1.000],  loss: 12263555.000000, mae: 1108.152466, mean_q: -114.902420
 2376/5000: episode: 2376, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -827.052, mean reward: -827.052 [-827.052, -827.052], mean action: 1.000 [1.000, 1.000],  loss: 10461215.000000, mae: 995.456116, mean_q: -115.481804
 2377/5000: episode: 2377, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2414.441, mean reward: -2414.441 [-2414.441, -2414.441], mean action: 1.000 [1.000, 1.000],  loss: 14960662.000000, mae: 1194.744141, mean_q: -114.835709
 2378/5000: episode: 2378, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -346.655, mean reward: -346.655 [-346.655, -346.655], mean action: 3.000 [3.000, 3.000],  loss: 10750297.000000, mae: 1134.668213, mean_q: -115.661957
 2379/5000: episode: 2379, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2560.277, mean reward: -2560.277 [-2560.277, -2560.277], mean action: 1.000 [1.000, 1.000],  loss: 12330498.000000, mae: 1069.283691, mean_q: -115.363815
 2380/5000: episode: 2380, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6844.409, mean reward: -6844.409 [-6844.409, -6844.409], mean action: 1.000 [1.000, 1.000],  loss: 13481070.000000, mae: 1117.383789, mean_q: -115.894363
 2381/5000: episode: 2381, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1692.367, mean reward: -1692.367 [-1692.367, -1692.367], mean action: 1.000 [1.000, 1.000],  loss: 17059134.000000, mae: 1280.102905, mean_q: -115.519691
 2382/5000: episode: 2382, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5771.082, mean reward: -5771.082 [-5771.082, -5771.082], mean action: 1.000 [1.000, 1.000],  loss: 16551362.000000, mae: 1183.588867, mean_q: -115.454681
 2383/5000: episode: 2383, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5591.787, mean reward: -5591.787 [-5591.787, -5591.787], mean action: 1.000 [1.000, 1.000],  loss: 15459834.000000, mae: 1220.625366, mean_q: -116.245529
 2384/5000: episode: 2384, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3887.724, mean reward: -3887.724 [-3887.724, -3887.724], mean action: 1.000 [1.000, 1.000],  loss: 15402383.000000, mae: 1216.536743, mean_q: -116.199356
 2385/5000: episode: 2385, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8746.813, mean reward: -8746.813 [-8746.813, -8746.813], mean action: 1.000 [1.000, 1.000],  loss: 15586253.000000, mae: 1273.674561, mean_q: -115.634659
 2386/5000: episode: 2386, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2387.845, mean reward: -2387.845 [-2387.845, -2387.845], mean action: 1.000 [1.000, 1.000],  loss: 11177838.000000, mae: 984.810913, mean_q: -115.587112
 2387/5000: episode: 2387, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6288.083, mean reward: -6288.083 [-6288.083, -6288.083], mean action: 1.000 [1.000, 1.000],  loss: 16404137.000000, mae: 1307.877563, mean_q: -116.167267
 2388/5000: episode: 2388, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7675.039, mean reward: -7675.039 [-7675.039, -7675.039], mean action: 1.000 [1.000, 1.000],  loss: 9532956.000000, mae: 941.524963, mean_q: -116.539291
 2389/5000: episode: 2389, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8205.016, mean reward: -8205.016 [-8205.016, -8205.016], mean action: 1.000 [1.000, 1.000],  loss: 11531699.000000, mae: 987.848206, mean_q: -117.181831
 2390/5000: episode: 2390, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -808.098, mean reward: -808.098 [-808.098, -808.098], mean action: 1.000 [1.000, 1.000],  loss: 8880792.000000, mae: 880.654053, mean_q: -117.161827
 2391/5000: episode: 2391, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5426.616, mean reward: -5426.616 [-5426.616, -5426.616], mean action: 1.000 [1.000, 1.000],  loss: 15759615.000000, mae: 1242.255005, mean_q: -116.813530
 2392/5000: episode: 2392, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7371.574, mean reward: -7371.574 [-7371.574, -7371.574], mean action: 1.000 [1.000, 1.000],  loss: 16862578.000000, mae: 1294.286743, mean_q: -116.827034
 2393/5000: episode: 2393, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5143.179, mean reward: -5143.179 [-5143.179, -5143.179], mean action: 1.000 [1.000, 1.000],  loss: 18031658.000000, mae: 1309.138062, mean_q: -116.600128
 2394/5000: episode: 2394, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10490.825, mean reward: -10490.825 [-10490.825, -10490.825], mean action: 1.000 [1.000, 1.000],  loss: 12601796.000000, mae: 1179.566895, mean_q: -117.102051
 2395/5000: episode: 2395, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6873.053, mean reward: -6873.053 [-6873.053, -6873.053], mean action: 1.000 [1.000, 1.000],  loss: 11177150.000000, mae: 1049.777100, mean_q: -117.492874
 2396/5000: episode: 2396, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2953.589, mean reward: -2953.589 [-2953.589, -2953.589], mean action: 0.000 [0.000, 0.000],  loss: 8825282.000000, mae: 899.648071, mean_q: -117.997009
 2397/5000: episode: 2397, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5077.744, mean reward: -5077.744 [-5077.744, -5077.744], mean action: 1.000 [1.000, 1.000],  loss: 14050806.000000, mae: 1069.438965, mean_q: -117.887199
 2398/5000: episode: 2398, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6220.039, mean reward: -6220.039 [-6220.039, -6220.039], mean action: 1.000 [1.000, 1.000],  loss: 15588071.000000, mae: 1251.566040, mean_q: -117.130157
 2399/5000: episode: 2399, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5095.425, mean reward: -5095.425 [-5095.425, -5095.425], mean action: 1.000 [1.000, 1.000],  loss: 9925630.000000, mae: 824.140320, mean_q: -118.078865
 2400/5000: episode: 2400, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -781.397, mean reward: -781.397 [-781.397, -781.397], mean action: 1.000 [1.000, 1.000],  loss: 10193021.000000, mae: 1012.820984, mean_q: -117.767166
 2401/5000: episode: 2401, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -991.172, mean reward: -991.172 [-991.172, -991.172], mean action: 1.000 [1.000, 1.000],  loss: 9394059.000000, mae: 949.248657, mean_q: -118.333534
 2402/5000: episode: 2402, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3748.891, mean reward: -3748.891 [-3748.891, -3748.891], mean action: 1.000 [1.000, 1.000],  loss: 9432952.000000, mae: 1030.122070, mean_q: -118.452713
 2403/5000: episode: 2403, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1264.017, mean reward: -1264.017 [-1264.017, -1264.017], mean action: 1.000 [1.000, 1.000],  loss: 11246074.000000, mae: 1027.385742, mean_q: -118.335602
 2404/5000: episode: 2404, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3844.320, mean reward: -3844.320 [-3844.320, -3844.320], mean action: 1.000 [1.000, 1.000],  loss: 17218256.000000, mae: 1258.740479, mean_q: -118.172768
 2405/5000: episode: 2405, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3942.234, mean reward: -3942.234 [-3942.234, -3942.234], mean action: 1.000 [1.000, 1.000],  loss: 12912591.000000, mae: 1065.196045, mean_q: -118.220261
 2406/5000: episode: 2406, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9568.787, mean reward: -9568.787 [-9568.787, -9568.787], mean action: 1.000 [1.000, 1.000],  loss: 10425758.000000, mae: 1072.771362, mean_q: -118.520836
 2407/5000: episode: 2407, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2559.085, mean reward: -2559.085 [-2559.085, -2559.085], mean action: 1.000 [1.000, 1.000],  loss: 12526168.000000, mae: 1070.928955, mean_q: -118.556732
 2408/5000: episode: 2408, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -3705.613, mean reward: -3705.613 [-3705.613, -3705.613], mean action: 1.000 [1.000, 1.000],  loss: 14331643.000000, mae: 1196.415894, mean_q: -118.695602
 2409/5000: episode: 2409, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8218.093, mean reward: -8218.093 [-8218.093, -8218.093], mean action: 1.000 [1.000, 1.000],  loss: 21824130.000000, mae: 1337.930908, mean_q: -118.977951
 2410/5000: episode: 2410, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4637.833, mean reward: -4637.833 [-4637.833, -4637.833], mean action: 1.000 [1.000, 1.000],  loss: 13122716.000000, mae: 1153.821533, mean_q: -118.950851
 2411/5000: episode: 2411, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1071.065, mean reward: -1071.065 [-1071.065, -1071.065], mean action: 2.000 [2.000, 2.000],  loss: 12430683.000000, mae: 1033.904907, mean_q: -118.766464
 2412/5000: episode: 2412, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -806.365, mean reward: -806.365 [-806.365, -806.365], mean action: 1.000 [1.000, 1.000],  loss: 9846462.000000, mae: 1018.877686, mean_q: -119.163033
 2413/5000: episode: 2413, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -566.550, mean reward: -566.550 [-566.550, -566.550], mean action: 2.000 [2.000, 2.000],  loss: 14682760.000000, mae: 1190.013916, mean_q: -119.506790
 2414/5000: episode: 2414, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -2944.054, mean reward: -2944.054 [-2944.054, -2944.054], mean action: 1.000 [1.000, 1.000],  loss: 14374839.000000, mae: 1146.843018, mean_q: -119.406601
 2415/5000: episode: 2415, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8709.851, mean reward: -8709.851 [-8709.851, -8709.851], mean action: 1.000 [1.000, 1.000],  loss: 13772436.000000, mae: 1191.398193, mean_q: -119.764297
 2416/5000: episode: 2416, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -353.824, mean reward: -353.824 [-353.824, -353.824], mean action: 2.000 [2.000, 2.000],  loss: 12578530.000000, mae: 1073.940918, mean_q: -119.990280
 2417/5000: episode: 2417, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5337.172, mean reward: -5337.172 [-5337.172, -5337.172], mean action: 2.000 [2.000, 2.000],  loss: 8403644.000000, mae: 851.008118, mean_q: -120.289124
 2418/5000: episode: 2418, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6636.307, mean reward: -6636.307 [-6636.307, -6636.307], mean action: 1.000 [1.000, 1.000],  loss: 9634010.000000, mae: 915.410828, mean_q: -120.123642
 2419/5000: episode: 2419, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2481.311, mean reward: -2481.311 [-2481.311, -2481.311], mean action: 2.000 [2.000, 2.000],  loss: 15774052.000000, mae: 1304.846069, mean_q: -120.048050
 2420/5000: episode: 2420, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -757.494, mean reward: -757.494 [-757.494, -757.494], mean action: 2.000 [2.000, 2.000],  loss: 10409086.000000, mae: 1006.820251, mean_q: -120.181320
 2421/5000: episode: 2421, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2781.789, mean reward: -2781.789 [-2781.789, -2781.789], mean action: 2.000 [2.000, 2.000],  loss: 11510382.000000, mae: 1045.061768, mean_q: -120.082367
 2422/5000: episode: 2422, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -4455.921, mean reward: -4455.921 [-4455.921, -4455.921], mean action: 2.000 [2.000, 2.000],  loss: 13636560.000000, mae: 1115.606201, mean_q: -120.054260
 2423/5000: episode: 2423, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4095.118, mean reward: -4095.118 [-4095.118, -4095.118], mean action: 2.000 [2.000, 2.000],  loss: 15899180.000000, mae: 1226.234863, mean_q: -120.645012
 2424/5000: episode: 2424, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -763.209, mean reward: -763.209 [-763.209, -763.209], mean action: 2.000 [2.000, 2.000],  loss: 14462408.000000, mae: 1178.646484, mean_q: -120.573456
 2425/5000: episode: 2425, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2642.971, mean reward: -2642.971 [-2642.971, -2642.971], mean action: 2.000 [2.000, 2.000],  loss: 13389854.000000, mae: 1105.364502, mean_q: -120.956856
 2426/5000: episode: 2426, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2690.689, mean reward: -2690.689 [-2690.689, -2690.689], mean action: 2.000 [2.000, 2.000],  loss: 20812792.000000, mae: 1417.508789, mean_q: -120.731171
 2427/5000: episode: 2427, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4002.015, mean reward: -4002.015 [-4002.015, -4002.015], mean action: 2.000 [2.000, 2.000],  loss: 14271432.000000, mae: 1213.553467, mean_q: -121.250031
 2428/5000: episode: 2428, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -2207.957, mean reward: -2207.957 [-2207.957, -2207.957], mean action: 2.000 [2.000, 2.000],  loss: 17039822.000000, mae: 1184.130859, mean_q: -120.878899
 2429/5000: episode: 2429, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3149.405, mean reward: -3149.405 [-3149.405, -3149.405], mean action: 2.000 [2.000, 2.000],  loss: 11569747.000000, mae: 970.277222, mean_q: -121.580429
 2430/5000: episode: 2430, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4935.763, mean reward: -4935.763 [-4935.763, -4935.763], mean action: 2.000 [2.000, 2.000],  loss: 15411670.000000, mae: 1237.376465, mean_q: -121.277443
 2431/5000: episode: 2431, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -960.437, mean reward: -960.437 [-960.437, -960.437], mean action: 2.000 [2.000, 2.000],  loss: 11957494.000000, mae: 1101.275391, mean_q: -121.274948
 2432/5000: episode: 2432, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -55.965, mean reward: -55.965 [-55.965, -55.965], mean action: 2.000 [2.000, 2.000],  loss: 14283511.000000, mae: 1092.588379, mean_q: -121.508224
 2433/5000: episode: 2433, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1759.897, mean reward: -1759.897 [-1759.897, -1759.897], mean action: 2.000 [2.000, 2.000],  loss: 17816090.000000, mae: 1317.553833, mean_q: -121.260658
 2434/5000: episode: 2434, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5779.062, mean reward: -5779.062 [-5779.062, -5779.062], mean action: 2.000 [2.000, 2.000],  loss: 12912772.000000, mae: 1094.086914, mean_q: -122.181236
 2435/5000: episode: 2435, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1056.612, mean reward: -1056.612 [-1056.612, -1056.612], mean action: 2.000 [2.000, 2.000],  loss: 14346398.000000, mae: 1164.634521, mean_q: -121.953796
 2436/5000: episode: 2436, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5594.458, mean reward: -5594.458 [-5594.458, -5594.458], mean action: 2.000 [2.000, 2.000],  loss: 13667069.000000, mae: 1071.836182, mean_q: -121.947006
 2437/5000: episode: 2437, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1078.455, mean reward: -1078.455 [-1078.455, -1078.455], mean action: 2.000 [2.000, 2.000],  loss: 14514879.000000, mae: 1158.696777, mean_q: -122.350906
 2438/5000: episode: 2438, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4791.042, mean reward: -4791.042 [-4791.042, -4791.042], mean action: 2.000 [2.000, 2.000],  loss: 13147316.000000, mae: 1151.385742, mean_q: -121.983917
 2439/5000: episode: 2439, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6902.360, mean reward: -6902.360 [-6902.360, -6902.360], mean action: 2.000 [2.000, 2.000],  loss: 12767618.000000, mae: 1167.765869, mean_q: -122.585938
 2440/5000: episode: 2440, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2095.613, mean reward: -2095.613 [-2095.613, -2095.613], mean action: 2.000 [2.000, 2.000],  loss: 15569894.000000, mae: 1224.549316, mean_q: -122.554932
 2441/5000: episode: 2441, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -691.505, mean reward: -691.505 [-691.505, -691.505], mean action: 2.000 [2.000, 2.000],  loss: 15214886.000000, mae: 1215.781006, mean_q: -122.672974
 2442/5000: episode: 2442, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2118.784, mean reward: -2118.784 [-2118.784, -2118.784], mean action: 2.000 [2.000, 2.000],  loss: 18784588.000000, mae: 1385.139404, mean_q: -122.929413
 2443/5000: episode: 2443, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3548.508, mean reward: -3548.508 [-3548.508, -3548.508], mean action: 2.000 [2.000, 2.000],  loss: 12229060.000000, mae: 1020.940796, mean_q: -122.775223
 2444/5000: episode: 2444, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7954.174, mean reward: -7954.174 [-7954.174, -7954.174], mean action: 2.000 [2.000, 2.000],  loss: 16693728.000000, mae: 1186.993164, mean_q: -122.954132
 2445/5000: episode: 2445, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5837.499, mean reward: -5837.499 [-5837.499, -5837.499], mean action: 2.000 [2.000, 2.000],  loss: 14875673.000000, mae: 1111.927368, mean_q: -122.958824
 2446/5000: episode: 2446, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2933.510, mean reward: -2933.510 [-2933.510, -2933.510], mean action: 2.000 [2.000, 2.000],  loss: 14129724.000000, mae: 1207.047607, mean_q: -123.220222
 2447/5000: episode: 2447, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2989.577, mean reward: -2989.577 [-2989.577, -2989.577], mean action: 2.000 [2.000, 2.000],  loss: 11931546.000000, mae: 1031.915894, mean_q: -123.440559
 2448/5000: episode: 2448, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2096.407, mean reward: -2096.407 [-2096.407, -2096.407], mean action: 2.000 [2.000, 2.000],  loss: 15116809.000000, mae: 1180.444092, mean_q: -123.533043
 2449/5000: episode: 2449, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4550.581, mean reward: -4550.581 [-4550.581, -4550.581], mean action: 2.000 [2.000, 2.000],  loss: 19868476.000000, mae: 1364.303467, mean_q: -123.240219
 2450/5000: episode: 2450, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6614.047, mean reward: -6614.047 [-6614.047, -6614.047], mean action: 2.000 [2.000, 2.000],  loss: 9916024.000000, mae: 1023.815308, mean_q: -123.620102
 2451/5000: episode: 2451, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1590.268, mean reward: -1590.268 [-1590.268, -1590.268], mean action: 2.000 [2.000, 2.000],  loss: 12942819.000000, mae: 1214.102539, mean_q: -123.701363
 2452/5000: episode: 2452, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -197.753, mean reward: -197.753 [-197.753, -197.753], mean action: 2.000 [2.000, 2.000],  loss: 18830112.000000, mae: 1362.478394, mean_q: -123.566483
 2453/5000: episode: 2453, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -389.506, mean reward: -389.506 [-389.506, -389.506], mean action: 2.000 [2.000, 2.000],  loss: 12411057.000000, mae: 1135.913208, mean_q: -124.207108
 2454/5000: episode: 2454, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2368.934, mean reward: -2368.934 [-2368.934, -2368.934], mean action: 2.000 [2.000, 2.000],  loss: 13773204.000000, mae: 1106.287354, mean_q: -124.283287
 2455/5000: episode: 2455, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2990.458, mean reward: -2990.458 [-2990.458, -2990.458], mean action: 2.000 [2.000, 2.000],  loss: 8777617.000000, mae: 959.004028, mean_q: -124.463257
 2456/5000: episode: 2456, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5295.898, mean reward: -5295.898 [-5295.898, -5295.898], mean action: 2.000 [2.000, 2.000],  loss: 13911441.000000, mae: 1193.376465, mean_q: -124.319542
 2457/5000: episode: 2457, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6867.935, mean reward: -6867.935 [-6867.935, -6867.935], mean action: 0.000 [0.000, 0.000],  loss: 24232914.000000, mae: 1440.353516, mean_q: -124.354973
 2458/5000: episode: 2458, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4341.345, mean reward: -4341.345 [-4341.345, -4341.345], mean action: 3.000 [3.000, 3.000],  loss: 18748454.000000, mae: 1425.595337, mean_q: -124.647774
 2459/5000: episode: 2459, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1692.350, mean reward: -1692.350 [-1692.350, -1692.350], mean action: 2.000 [2.000, 2.000],  loss: 16245317.000000, mae: 1324.106689, mean_q: -124.978790
 2460/5000: episode: 2460, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3653.103, mean reward: -3653.103 [-3653.103, -3653.103], mean action: 2.000 [2.000, 2.000],  loss: 15313836.000000, mae: 1161.866455, mean_q: -124.785233
 2461/5000: episode: 2461, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2352.654, mean reward: -2352.654 [-2352.654, -2352.654], mean action: 2.000 [2.000, 2.000],  loss: 14194176.000000, mae: 1226.557495, mean_q: -124.656631
 2462/5000: episode: 2462, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1786.555, mean reward: -1786.555 [-1786.555, -1786.555], mean action: 2.000 [2.000, 2.000],  loss: 15452920.000000, mae: 1250.177734, mean_q: -124.919327
 2463/5000: episode: 2463, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1141.797, mean reward: -1141.797 [-1141.797, -1141.797], mean action: 3.000 [3.000, 3.000],  loss: 14308985.000000, mae: 1086.999023, mean_q: -126.105453
 2464/5000: episode: 2464, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1618.074, mean reward: -1618.074 [-1618.074, -1618.074], mean action: 2.000 [2.000, 2.000],  loss: 12475344.000000, mae: 1073.668091, mean_q: -125.334274
 2465/5000: episode: 2465, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -342.133, mean reward: -342.133 [-342.133, -342.133], mean action: 2.000 [2.000, 2.000],  loss: 12345615.000000, mae: 1150.703613, mean_q: -125.228317
 2466/5000: episode: 2466, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2001.754, mean reward: -2001.754 [-2001.754, -2001.754], mean action: 2.000 [2.000, 2.000],  loss: 11954614.000000, mae: 1088.982056, mean_q: -125.997932
 2467/5000: episode: 2467, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2906.159, mean reward: -2906.159 [-2906.159, -2906.159], mean action: 2.000 [2.000, 2.000],  loss: 13458005.000000, mae: 1118.396362, mean_q: -126.188950
 2468/5000: episode: 2468, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1543.784, mean reward: -1543.784 [-1543.784, -1543.784], mean action: 2.000 [2.000, 2.000],  loss: 14128538.000000, mae: 1199.028320, mean_q: -125.758423
 2469/5000: episode: 2469, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1570.405, mean reward: -1570.405 [-1570.405, -1570.405], mean action: 2.000 [2.000, 2.000],  loss: 7384980.000000, mae: 924.776184, mean_q: -126.363922
 2470/5000: episode: 2470, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -93.714, mean reward: -93.714 [-93.714, -93.714], mean action: 2.000 [2.000, 2.000],  loss: 13027895.000000, mae: 1130.183716, mean_q: -126.624413
 2471/5000: episode: 2471, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -2627.458, mean reward: -2627.458 [-2627.458, -2627.458], mean action: 0.000 [0.000, 0.000],  loss: 18686524.000000, mae: 1241.109619, mean_q: -126.137962
 2472/5000: episode: 2472, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1997.611, mean reward: -1997.611 [-1997.611, -1997.611], mean action: 2.000 [2.000, 2.000],  loss: 8881374.000000, mae: 902.192017, mean_q: -126.439598
 2473/5000: episode: 2473, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2467.446, mean reward: -2467.446 [-2467.446, -2467.446], mean action: 2.000 [2.000, 2.000],  loss: 17376000.000000, mae: 1280.155762, mean_q: -126.011063
 2474/5000: episode: 2474, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7195.893, mean reward: -7195.893 [-7195.893, -7195.893], mean action: 2.000 [2.000, 2.000],  loss: 14555987.000000, mae: 1231.032349, mean_q: -126.803619
 2475/5000: episode: 2475, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3045.645, mean reward: -3045.645 [-3045.645, -3045.645], mean action: 2.000 [2.000, 2.000],  loss: 10498033.000000, mae: 1050.762451, mean_q: -127.128693
 2476/5000: episode: 2476, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1275.887, mean reward: -1275.887 [-1275.887, -1275.887], mean action: 2.000 [2.000, 2.000],  loss: 21645200.000000, mae: 1379.125488, mean_q: -127.383316
 2477/5000: episode: 2477, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5513.379, mean reward: -5513.379 [-5513.379, -5513.379], mean action: 2.000 [2.000, 2.000],  loss: 8140737.500000, mae: 917.530334, mean_q: -127.463448
 2478/5000: episode: 2478, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2194.682, mean reward: -2194.682 [-2194.682, -2194.682], mean action: 2.000 [2.000, 2.000],  loss: 15730606.000000, mae: 1243.107788, mean_q: -127.243645
 2479/5000: episode: 2479, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -191.782, mean reward: -191.782 [-191.782, -191.782], mean action: 2.000 [2.000, 2.000],  loss: 19268144.000000, mae: 1336.575928, mean_q: -126.910645
 2480/5000: episode: 2480, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2935.220, mean reward: -2935.220 [-2935.220, -2935.220], mean action: 2.000 [2.000, 2.000],  loss: 13469255.000000, mae: 1151.631104, mean_q: -127.685349
 2481/5000: episode: 2481, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2209.480, mean reward: -2209.480 [-2209.480, -2209.480], mean action: 2.000 [2.000, 2.000],  loss: 20126386.000000, mae: 1390.423340, mean_q: -127.493408
 2482/5000: episode: 2482, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -864.337, mean reward: -864.337 [-864.337, -864.337], mean action: 2.000 [2.000, 2.000],  loss: 11843404.000000, mae: 1083.529663, mean_q: -127.786568
 2483/5000: episode: 2483, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -638.376, mean reward: -638.376 [-638.376, -638.376], mean action: 3.000 [3.000, 3.000],  loss: 13058058.000000, mae: 1081.742676, mean_q: -127.924591
 2484/5000: episode: 2484, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8576.655, mean reward: -8576.655 [-8576.655, -8576.655], mean action: 2.000 [2.000, 2.000],  loss: 11559735.000000, mae: 1098.896851, mean_q: -127.439941
 2485/5000: episode: 2485, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2715.275, mean reward: -2715.275 [-2715.275, -2715.275], mean action: 2.000 [2.000, 2.000],  loss: 10741478.000000, mae: 967.690186, mean_q: -128.452042
 2486/5000: episode: 2486, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -337.359, mean reward: -337.359 [-337.359, -337.359], mean action: 2.000 [2.000, 2.000],  loss: 9148639.000000, mae: 945.414551, mean_q: -128.623856
 2487/5000: episode: 2487, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5746.713, mean reward: -5746.713 [-5746.713, -5746.713], mean action: 1.000 [1.000, 1.000],  loss: 11250752.000000, mae: 1009.269775, mean_q: -128.474625
 2488/5000: episode: 2488, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2397.150, mean reward: -2397.150 [-2397.150, -2397.150], mean action: 2.000 [2.000, 2.000],  loss: 19910096.000000, mae: 1326.951172, mean_q: -128.637817
 2489/5000: episode: 2489, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -443.840, mean reward: -443.840 [-443.840, -443.840], mean action: 2.000 [2.000, 2.000],  loss: 13661652.000000, mae: 1084.536865, mean_q: -128.695282
 2490/5000: episode: 2490, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3111.167, mean reward: -3111.167 [-3111.167, -3111.167], mean action: 2.000 [2.000, 2.000],  loss: 18074224.000000, mae: 1406.466309, mean_q: -128.562485
 2491/5000: episode: 2491, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2411.616, mean reward: -2411.616 [-2411.616, -2411.616], mean action: 2.000 [2.000, 2.000],  loss: 20316236.000000, mae: 1361.417725, mean_q: -129.083145
 2492/5000: episode: 2492, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4651.321, mean reward: -4651.321 [-4651.321, -4651.321], mean action: 2.000 [2.000, 2.000],  loss: 12950248.000000, mae: 1092.338623, mean_q: -129.195999
 2493/5000: episode: 2493, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -211.889, mean reward: -211.889 [-211.889, -211.889], mean action: 2.000 [2.000, 2.000],  loss: 14647067.000000, mae: 1182.300659, mean_q: -128.944870
 2494/5000: episode: 2494, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -9071.227, mean reward: -9071.227 [-9071.227, -9071.227], mean action: 2.000 [2.000, 2.000],  loss: 18651020.000000, mae: 1358.282471, mean_q: -129.056213
 2495/5000: episode: 2495, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -557.154, mean reward: -557.154 [-557.154, -557.154], mean action: 2.000 [2.000, 2.000],  loss: 17221616.000000, mae: 1293.329468, mean_q: -129.252777
 2496/5000: episode: 2496, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3759.225, mean reward: -3759.225 [-3759.225, -3759.225], mean action: 2.000 [2.000, 2.000],  loss: 10054368.000000, mae: 956.254089, mean_q: -129.376190
 2497/5000: episode: 2497, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -869.368, mean reward: -869.368 [-869.368, -869.368], mean action: 2.000 [2.000, 2.000],  loss: 15910726.000000, mae: 1242.642090, mean_q: -129.685669
 2498/5000: episode: 2498, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3967.893, mean reward: -3967.893 [-3967.893, -3967.893], mean action: 2.000 [2.000, 2.000],  loss: 17155510.000000, mae: 1288.972412, mean_q: -129.886612
 2499/5000: episode: 2499, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -73.438, mean reward: -73.438 [-73.438, -73.438], mean action: 2.000 [2.000, 2.000],  loss: 14666766.000000, mae: 1136.108643, mean_q: -129.952789
 2500/5000: episode: 2500, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3498.573, mean reward: -3498.573 [-3498.573, -3498.573], mean action: 2.000 [2.000, 2.000],  loss: 8343450.500000, mae: 930.637695, mean_q: -130.584702
 2501/5000: episode: 2501, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3306.204, mean reward: -3306.204 [-3306.204, -3306.204], mean action: 2.000 [2.000, 2.000],  loss: 16778920.000000, mae: 1330.770996, mean_q: -130.152802
 2502/5000: episode: 2502, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3653.845, mean reward: -3653.845 [-3653.845, -3653.845], mean action: 2.000 [2.000, 2.000],  loss: 18390836.000000, mae: 1303.319214, mean_q: -130.039001
 2503/5000: episode: 2503, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3212.319, mean reward: -3212.319 [-3212.319, -3212.319], mean action: 2.000 [2.000, 2.000],  loss: 17863968.000000, mae: 1335.797607, mean_q: -130.564468
 2504/5000: episode: 2504, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1631.273, mean reward: -1631.273 [-1631.273, -1631.273], mean action: 2.000 [2.000, 2.000],  loss: 8070450.000000, mae: 928.921875, mean_q: -130.960571
 2505/5000: episode: 2505, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4554.771, mean reward: -4554.771 [-4554.771, -4554.771], mean action: 2.000 [2.000, 2.000],  loss: 11588550.000000, mae: 1085.423828, mean_q: -130.611084
 2506/5000: episode: 2506, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1755.471, mean reward: -1755.471 [-1755.471, -1755.471], mean action: 2.000 [2.000, 2.000],  loss: 21089482.000000, mae: 1435.022827, mean_q: -130.218964
 2507/5000: episode: 2507, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2853.707, mean reward: -2853.707 [-2853.707, -2853.707], mean action: 2.000 [2.000, 2.000],  loss: 14603304.000000, mae: 1195.046509, mean_q: -130.799255
 2508/5000: episode: 2508, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10235.232, mean reward: -10235.232 [-10235.232, -10235.232], mean action: 2.000 [2.000, 2.000],  loss: 14116757.000000, mae: 1194.537476, mean_q: -131.447876
 2509/5000: episode: 2509, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -630.009, mean reward: -630.009 [-630.009, -630.009], mean action: 2.000 [2.000, 2.000],  loss: 16254065.000000, mae: 1170.870850, mean_q: -131.129639
 2510/5000: episode: 2510, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1353.002, mean reward: -1353.002 [-1353.002, -1353.002], mean action: 2.000 [2.000, 2.000],  loss: 15999258.000000, mae: 1283.846680, mean_q: -131.072189
 2511/5000: episode: 2511, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3431.735, mean reward: -3431.735 [-3431.735, -3431.735], mean action: 1.000 [1.000, 1.000],  loss: 17187056.000000, mae: 1214.721436, mean_q: -131.455292
 2512/5000: episode: 2512, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1695.752, mean reward: -1695.752 [-1695.752, -1695.752], mean action: 2.000 [2.000, 2.000],  loss: 11905586.000000, mae: 972.670715, mean_q: -132.197632
 2513/5000: episode: 2513, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -12161.632, mean reward: -12161.632 [-12161.632, -12161.632], mean action: 1.000 [1.000, 1.000],  loss: 18541520.000000, mae: 1302.408447, mean_q: -131.805374
 2514/5000: episode: 2514, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4553.981, mean reward: -4553.981 [-4553.981, -4553.981], mean action: 1.000 [1.000, 1.000],  loss: 17884476.000000, mae: 1282.668091, mean_q: -131.998962
 2515/5000: episode: 2515, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7879.201, mean reward: -7879.201 [-7879.201, -7879.201], mean action: 1.000 [1.000, 1.000],  loss: 14574972.000000, mae: 1243.132202, mean_q: -132.132568
 2516/5000: episode: 2516, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5802.714, mean reward: -5802.714 [-5802.714, -5802.714], mean action: 2.000 [2.000, 2.000],  loss: 18643734.000000, mae: 1297.640137, mean_q: -132.147766
 2517/5000: episode: 2517, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5690.547, mean reward: -5690.547 [-5690.547, -5690.547], mean action: 2.000 [2.000, 2.000],  loss: 11372081.000000, mae: 1100.159424, mean_q: -132.747467
 2518/5000: episode: 2518, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3250.219, mean reward: -3250.219 [-3250.219, -3250.219], mean action: 3.000 [3.000, 3.000],  loss: 17407352.000000, mae: 1329.136963, mean_q: -132.279663
 2519/5000: episode: 2519, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3800.027, mean reward: -3800.027 [-3800.027, -3800.027], mean action: 1.000 [1.000, 1.000],  loss: 12920690.000000, mae: 1147.180908, mean_q: -132.317673
 2520/5000: episode: 2520, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -26.103, mean reward: -26.103 [-26.103, -26.103], mean action: 2.000 [2.000, 2.000],  loss: 10773178.000000, mae: 1018.086792, mean_q: -132.774475
 2521/5000: episode: 2521, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -28.287, mean reward: -28.287 [-28.287, -28.287], mean action: 2.000 [2.000, 2.000],  loss: 20876408.000000, mae: 1486.453613, mean_q: -132.679962
 2522/5000: episode: 2522, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6609.478, mean reward: -6609.478 [-6609.478, -6609.478], mean action: 3.000 [3.000, 3.000],  loss: 15535856.000000, mae: 1191.473389, mean_q: -132.882217
 2523/5000: episode: 2523, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5451.796, mean reward: -5451.796 [-5451.796, -5451.796], mean action: 2.000 [2.000, 2.000],  loss: 17596728.000000, mae: 1156.842285, mean_q: -133.296982
 2524/5000: episode: 2524, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1650.805, mean reward: -1650.805 [-1650.805, -1650.805], mean action: 2.000 [2.000, 2.000],  loss: 16067442.000000, mae: 1313.981445, mean_q: -132.634354
 2525/5000: episode: 2525, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3074.354, mean reward: -3074.354 [-3074.354, -3074.354], mean action: 2.000 [2.000, 2.000],  loss: 12129419.000000, mae: 1136.071533, mean_q: -133.942307
 2526/5000: episode: 2526, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5118.865, mean reward: -5118.865 [-5118.865, -5118.865], mean action: 2.000 [2.000, 2.000],  loss: 14145537.000000, mae: 1095.249268, mean_q: -133.605225
 2527/5000: episode: 2527, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2129.463, mean reward: -2129.463 [-2129.463, -2129.463], mean action: 3.000 [3.000, 3.000],  loss: 19021164.000000, mae: 1444.174072, mean_q: -133.056641
 2528/5000: episode: 2528, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -246.254, mean reward: -246.254 [-246.254, -246.254], mean action: 2.000 [2.000, 2.000],  loss: 20299456.000000, mae: 1356.802002, mean_q: -133.718124
 2529/5000: episode: 2529, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10576.089, mean reward: -10576.089 [-10576.089, -10576.089], mean action: 0.000 [0.000, 0.000],  loss: 10644346.000000, mae: 986.971252, mean_q: -133.793045
 2530/5000: episode: 2530, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1864.093, mean reward: -1864.093 [-1864.093, -1864.093], mean action: 2.000 [2.000, 2.000],  loss: 19700784.000000, mae: 1399.395020, mean_q: -133.698761
 2531/5000: episode: 2531, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3624.546, mean reward: -3624.546 [-3624.546, -3624.546], mean action: 2.000 [2.000, 2.000],  loss: 10137777.000000, mae: 1037.568848, mean_q: -133.804977
 2532/5000: episode: 2532, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2955.400, mean reward: -2955.400 [-2955.400, -2955.400], mean action: 2.000 [2.000, 2.000],  loss: 16938360.000000, mae: 1328.860840, mean_q: -134.581726
 2533/5000: episode: 2533, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -266.268, mean reward: -266.268 [-266.268, -266.268], mean action: 2.000 [2.000, 2.000],  loss: 21210484.000000, mae: 1428.229370, mean_q: -133.613770
 2534/5000: episode: 2534, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1449.469, mean reward: -1449.469 [-1449.469, -1449.469], mean action: 2.000 [2.000, 2.000],  loss: 20390992.000000, mae: 1408.012451, mean_q: -134.220734
 2535/5000: episode: 2535, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2960.108, mean reward: -2960.108 [-2960.108, -2960.108], mean action: 2.000 [2.000, 2.000],  loss: 15999775.000000, mae: 1295.918213, mean_q: -134.090591
 2536/5000: episode: 2536, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3708.239, mean reward: -3708.239 [-3708.239, -3708.239], mean action: 2.000 [2.000, 2.000],  loss: 12639384.000000, mae: 1068.429199, mean_q: -134.673279
 2537/5000: episode: 2537, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2591.641, mean reward: -2591.641 [-2591.641, -2591.641], mean action: 0.000 [0.000, 0.000],  loss: 16430298.000000, mae: 1287.527832, mean_q: -135.118881
 2538/5000: episode: 2538, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1394.633, mean reward: -1394.633 [-1394.633, -1394.633], mean action: 2.000 [2.000, 2.000],  loss: 11644025.000000, mae: 1096.366211, mean_q: -135.493774
 2539/5000: episode: 2539, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7938.633, mean reward: -7938.633 [-7938.633, -7938.633], mean action: 2.000 [2.000, 2.000],  loss: 13067329.000000, mae: 1188.434570, mean_q: -135.238205
 2540/5000: episode: 2540, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4202.497, mean reward: -4202.497 [-4202.497, -4202.497], mean action: 2.000 [2.000, 2.000],  loss: 13142852.000000, mae: 1083.001221, mean_q: -135.239166
 2541/5000: episode: 2541, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5564.249, mean reward: -5564.249 [-5564.249, -5564.249], mean action: 2.000 [2.000, 2.000],  loss: 13539355.000000, mae: 1103.282349, mean_q: -135.788345
 2542/5000: episode: 2542, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -657.587, mean reward: -657.587 [-657.587, -657.587], mean action: 2.000 [2.000, 2.000],  loss: 10475360.000000, mae: 1024.105225, mean_q: -136.108765
 2543/5000: episode: 2543, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1959.054, mean reward: -1959.054 [-1959.054, -1959.054], mean action: 2.000 [2.000, 2.000],  loss: 10127098.000000, mae: 1024.368042, mean_q: -135.925583
 2544/5000: episode: 2544, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3254.570, mean reward: -3254.570 [-3254.570, -3254.570], mean action: 1.000 [1.000, 1.000],  loss: 13743776.000000, mae: 1232.324707, mean_q: -135.516586
 2545/5000: episode: 2545, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5267.064, mean reward: -5267.064 [-5267.064, -5267.064], mean action: 1.000 [1.000, 1.000],  loss: 7800825.000000, mae: 843.294556, mean_q: -136.055496
 2546/5000: episode: 2546, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5169.551, mean reward: -5169.551 [-5169.551, -5169.551], mean action: 1.000 [1.000, 1.000],  loss: 15423789.000000, mae: 1244.292725, mean_q: -136.246368
 2547/5000: episode: 2547, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -840.001, mean reward: -840.001 [-840.001, -840.001], mean action: 1.000 [1.000, 1.000],  loss: 11745286.000000, mae: 1111.072021, mean_q: -136.373703
 2548/5000: episode: 2548, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3874.105, mean reward: -3874.105 [-3874.105, -3874.105], mean action: 1.000 [1.000, 1.000],  loss: 13966009.000000, mae: 1147.598022, mean_q: -136.284515
 2549/5000: episode: 2549, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4465.500, mean reward: -4465.500 [-4465.500, -4465.500], mean action: 2.000 [2.000, 2.000],  loss: 9526995.000000, mae: 973.960205, mean_q: -136.693848
 2550/5000: episode: 2550, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4002.587, mean reward: -4002.587 [-4002.587, -4002.587], mean action: 1.000 [1.000, 1.000],  loss: 13244420.000000, mae: 1179.090820, mean_q: -136.172256
 2551/5000: episode: 2551, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2248.377, mean reward: -2248.377 [-2248.377, -2248.377], mean action: 1.000 [1.000, 1.000],  loss: 14927466.000000, mae: 1155.220459, mean_q: -137.042923
 2552/5000: episode: 2552, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3841.288, mean reward: -3841.288 [-3841.288, -3841.288], mean action: 1.000 [1.000, 1.000],  loss: 24958522.000000, mae: 1441.726440, mean_q: -136.839142
 2553/5000: episode: 2553, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -492.663, mean reward: -492.663 [-492.663, -492.663], mean action: 1.000 [1.000, 1.000],  loss: 11800874.000000, mae: 1054.602539, mean_q: -137.267044
 2554/5000: episode: 2554, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1848.789, mean reward: -1848.789 [-1848.789, -1848.789], mean action: 1.000 [1.000, 1.000],  loss: 17746168.000000, mae: 1343.879395, mean_q: -136.836578
 2555/5000: episode: 2555, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -10261.648, mean reward: -10261.648 [-10261.648, -10261.648], mean action: 1.000 [1.000, 1.000],  loss: 23844974.000000, mae: 1571.056885, mean_q: -136.783417
 2556/5000: episode: 2556, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6011.538, mean reward: -6011.538 [-6011.538, -6011.538], mean action: 1.000 [1.000, 1.000],  loss: 10488546.000000, mae: 1002.347046, mean_q: -137.329941
 2557/5000: episode: 2557, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3889.303, mean reward: -3889.303 [-3889.303, -3889.303], mean action: 1.000 [1.000, 1.000],  loss: 16592944.000000, mae: 1170.220093, mean_q: -137.287415
 2558/5000: episode: 2558, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5015.089, mean reward: -5015.089 [-5015.089, -5015.089], mean action: 1.000 [1.000, 1.000],  loss: 9539146.000000, mae: 922.952026, mean_q: -137.627594
 2559/5000: episode: 2559, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6116.349, mean reward: -6116.349 [-6116.349, -6116.349], mean action: 1.000 [1.000, 1.000],  loss: 13356771.000000, mae: 1135.398438, mean_q: -137.775711
 2560/5000: episode: 2560, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1662.193, mean reward: -1662.193 [-1662.193, -1662.193], mean action: 1.000 [1.000, 1.000],  loss: 12049066.000000, mae: 1124.299561, mean_q: -137.426819
 2561/5000: episode: 2561, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3599.034, mean reward: -3599.034 [-3599.034, -3599.034], mean action: 1.000 [1.000, 1.000],  loss: 16428119.000000, mae: 1320.508057, mean_q: -138.188080
 2562/5000: episode: 2562, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1301.639, mean reward: -1301.639 [-1301.639, -1301.639], mean action: 1.000 [1.000, 1.000],  loss: 20427764.000000, mae: 1483.284424, mean_q: -137.722717
 2563/5000: episode: 2563, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6188.826, mean reward: -6188.826 [-6188.826, -6188.826], mean action: 1.000 [1.000, 1.000],  loss: 13651136.000000, mae: 1188.934570, mean_q: -138.050201
 2564/5000: episode: 2564, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1248.865, mean reward: -1248.865 [-1248.865, -1248.865], mean action: 1.000 [1.000, 1.000],  loss: 10420950.000000, mae: 1077.111816, mean_q: -138.820343
 2565/5000: episode: 2565, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1992.907, mean reward: -1992.907 [-1992.907, -1992.907], mean action: 1.000 [1.000, 1.000],  loss: 18477942.000000, mae: 1371.763794, mean_q: -138.365082
 2566/5000: episode: 2566, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3749.736, mean reward: -3749.736 [-3749.736, -3749.736], mean action: 0.000 [0.000, 0.000],  loss: 11848634.000000, mae: 1075.019897, mean_q: -138.581635
 2567/5000: episode: 2567, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6975.824, mean reward: -6975.824 [-6975.824, -6975.824], mean action: 1.000 [1.000, 1.000],  loss: 12362386.000000, mae: 1105.245117, mean_q: -138.982727
 2568/5000: episode: 2568, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2773.441, mean reward: -2773.441 [-2773.441, -2773.441], mean action: 1.000 [1.000, 1.000],  loss: 15107737.000000, mae: 1271.226807, mean_q: -138.474625
 2569/5000: episode: 2569, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7079.721, mean reward: -7079.721 [-7079.721, -7079.721], mean action: 1.000 [1.000, 1.000],  loss: 18161196.000000, mae: 1335.249023, mean_q: -139.207489
 2570/5000: episode: 2570, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4480.926, mean reward: -4480.926 [-4480.926, -4480.926], mean action: 1.000 [1.000, 1.000],  loss: 12663888.000000, mae: 1197.886719, mean_q: -139.179428
 2571/5000: episode: 2571, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1146.619, mean reward: -1146.619 [-1146.619, -1146.619], mean action: 1.000 [1.000, 1.000],  loss: 7628906.500000, mae: 877.598938, mean_q: -139.494736
 2572/5000: episode: 2572, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -8246.021, mean reward: -8246.021 [-8246.021, -8246.021], mean action: 1.000 [1.000, 1.000],  loss: 6643819.000000, mae: 837.074219, mean_q: -140.209854
 2573/5000: episode: 2573, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2936.314, mean reward: -2936.314 [-2936.314, -2936.314], mean action: 1.000 [1.000, 1.000],  loss: 11347886.000000, mae: 1107.632446, mean_q: -139.300400
 2574/5000: episode: 2574, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1445.539, mean reward: -1445.539 [-1445.539, -1445.539], mean action: 1.000 [1.000, 1.000],  loss: 13162600.000000, mae: 1142.070679, mean_q: -139.840729
 2575/5000: episode: 2575, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3148.493, mean reward: -3148.493 [-3148.493, -3148.493], mean action: 1.000 [1.000, 1.000],  loss: 11421816.000000, mae: 1085.523926, mean_q: -140.228943
 2576/5000: episode: 2576, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4802.054, mean reward: -4802.054 [-4802.054, -4802.054], mean action: 1.000 [1.000, 1.000],  loss: 17944336.000000, mae: 1421.353760, mean_q: -140.089905
 2577/5000: episode: 2577, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1009.188, mean reward: -1009.188 [-1009.188, -1009.188], mean action: 1.000 [1.000, 1.000],  loss: 13532235.000000, mae: 1102.398926, mean_q: -140.177505
 2578/5000: episode: 2578, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5337.831, mean reward: -5337.831 [-5337.831, -5337.831], mean action: 1.000 [1.000, 1.000],  loss: 16678666.000000, mae: 1298.244873, mean_q: -140.246552
 2579/5000: episode: 2579, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8626.998, mean reward: -8626.998 [-8626.998, -8626.998], mean action: 1.000 [1.000, 1.000],  loss: 18736702.000000, mae: 1303.782959, mean_q: -140.941650
 2580/5000: episode: 2580, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3471.775, mean reward: -3471.775 [-3471.775, -3471.775], mean action: 1.000 [1.000, 1.000],  loss: 11138086.000000, mae: 1063.432617, mean_q: -140.789597
 2581/5000: episode: 2581, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1820.790, mean reward: -1820.790 [-1820.790, -1820.790], mean action: 1.000 [1.000, 1.000],  loss: 11256068.000000, mae: 1074.841675, mean_q: -140.619690
 2582/5000: episode: 2582, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3181.960, mean reward: -3181.960 [-3181.960, -3181.960], mean action: 1.000 [1.000, 1.000],  loss: 12825763.000000, mae: 1113.733398, mean_q: -141.070984
 2583/5000: episode: 2583, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6610.869, mean reward: -6610.869 [-6610.869, -6610.869], mean action: 1.000 [1.000, 1.000],  loss: 14581626.000000, mae: 1207.801758, mean_q: -141.550781
 2584/5000: episode: 2584, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1442.937, mean reward: -1442.937 [-1442.937, -1442.937], mean action: 1.000 [1.000, 1.000],  loss: 10087290.000000, mae: 1027.369385, mean_q: -141.469437
 2585/5000: episode: 2585, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1827.097, mean reward: -1827.097 [-1827.097, -1827.097], mean action: 1.000 [1.000, 1.000],  loss: 15092192.000000, mae: 1177.974365, mean_q: -141.057053
 2586/5000: episode: 2586, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4406.816, mean reward: -4406.816 [-4406.816, -4406.816], mean action: 1.000 [1.000, 1.000],  loss: 12051308.000000, mae: 1147.526001, mean_q: -141.940140
 2587/5000: episode: 2587, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5292.782, mean reward: -5292.782 [-5292.782, -5292.782], mean action: 1.000 [1.000, 1.000],  loss: 18033132.000000, mae: 1383.466797, mean_q: -140.875122
 2588/5000: episode: 2588, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4486.372, mean reward: -4486.372 [-4486.372, -4486.372], mean action: 1.000 [1.000, 1.000],  loss: 12732039.000000, mae: 1079.165405, mean_q: -141.776138
 2589/5000: episode: 2589, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5949.870, mean reward: -5949.870 [-5949.870, -5949.870], mean action: 1.000 [1.000, 1.000],  loss: 12289946.000000, mae: 1180.293945, mean_q: -141.722504
 2590/5000: episode: 2590, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4402.599, mean reward: -4402.599 [-4402.599, -4402.599], mean action: 1.000 [1.000, 1.000],  loss: 8885362.000000, mae: 972.614380, mean_q: -142.339020
 2591/5000: episode: 2591, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3406.714, mean reward: -3406.714 [-3406.714, -3406.714], mean action: 1.000 [1.000, 1.000],  loss: 13038790.000000, mae: 1153.266602, mean_q: -142.161102
 2592/5000: episode: 2592, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7992.848, mean reward: -7992.848 [-7992.848, -7992.848], mean action: 1.000 [1.000, 1.000],  loss: 14279634.000000, mae: 1275.592285, mean_q: -142.195679
 2593/5000: episode: 2593, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3786.695, mean reward: -3786.695 [-3786.695, -3786.695], mean action: 1.000 [1.000, 1.000],  loss: 15666586.000000, mae: 1208.595459, mean_q: -142.868439
 2594/5000: episode: 2594, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8017.027, mean reward: -8017.027 [-8017.027, -8017.027], mean action: 1.000 [1.000, 1.000],  loss: 12649072.000000, mae: 1069.239258, mean_q: -142.284363
 2595/5000: episode: 2595, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3094.656, mean reward: -3094.656 [-3094.656, -3094.656], mean action: 1.000 [1.000, 1.000],  loss: 9090144.000000, mae: 987.631958, mean_q: -142.616760
 2596/5000: episode: 2596, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3677.818, mean reward: -3677.818 [-3677.818, -3677.818], mean action: 1.000 [1.000, 1.000],  loss: 10221502.000000, mae: 1066.978271, mean_q: -142.980316
 2597/5000: episode: 2597, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6954.898, mean reward: -6954.898 [-6954.898, -6954.898], mean action: 1.000 [1.000, 1.000],  loss: 12321642.000000, mae: 1168.426270, mean_q: -142.348770
 2598/5000: episode: 2598, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2186.707, mean reward: -2186.707 [-2186.707, -2186.707], mean action: 1.000 [1.000, 1.000],  loss: 16256806.000000, mae: 1167.664429, mean_q: -142.734039
 2599/5000: episode: 2599, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5128.454, mean reward: -5128.454 [-5128.454, -5128.454], mean action: 3.000 [3.000, 3.000],  loss: 12834915.000000, mae: 1083.577759, mean_q: -143.195099
 2600/5000: episode: 2600, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -6153.363, mean reward: -6153.363 [-6153.363, -6153.363], mean action: 3.000 [3.000, 3.000],  loss: 21995052.000000, mae: 1461.978394, mean_q: -142.944153
 2601/5000: episode: 2601, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -171.959, mean reward: -171.959 [-171.959, -171.959], mean action: 3.000 [3.000, 3.000],  loss: 10834106.000000, mae: 1081.300659, mean_q: -143.594238
 2602/5000: episode: 2602, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1417.268, mean reward: -1417.268 [-1417.268, -1417.268], mean action: 1.000 [1.000, 1.000],  loss: 13608416.000000, mae: 1061.800537, mean_q: -143.057526
 2603/5000: episode: 2603, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1325.395, mean reward: -1325.395 [-1325.395, -1325.395], mean action: 3.000 [3.000, 3.000],  loss: 14960878.000000, mae: 1232.619751, mean_q: -143.166229
 2604/5000: episode: 2604, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -7728.081, mean reward: -7728.081 [-7728.081, -7728.081], mean action: 3.000 [3.000, 3.000],  loss: 14068430.000000, mae: 1113.885742, mean_q: -144.067154
 2605/5000: episode: 2605, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3426.189, mean reward: -3426.189 [-3426.189, -3426.189], mean action: 3.000 [3.000, 3.000],  loss: 13853672.000000, mae: 1185.829224, mean_q: -144.065292
 2606/5000: episode: 2606, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1670.351, mean reward: -1670.351 [-1670.351, -1670.351], mean action: 3.000 [3.000, 3.000],  loss: 14865636.000000, mae: 1209.962158, mean_q: -143.997437
 2607/5000: episode: 2607, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -626.850, mean reward: -626.850 [-626.850, -626.850], mean action: 3.000 [3.000, 3.000],  loss: 11119101.000000, mae: 1000.658691, mean_q: -144.047180
 2608/5000: episode: 2608, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -12776.921, mean reward: -12776.921 [-12776.921, -12776.921], mean action: 0.000 [0.000, 0.000],  loss: 18265138.000000, mae: 1256.647705, mean_q: -144.806915
 2609/5000: episode: 2609, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -841.244, mean reward: -841.244 [-841.244, -841.244], mean action: 3.000 [3.000, 3.000],  loss: 10286084.000000, mae: 960.797913, mean_q: -144.325439
 2610/5000: episode: 2610, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -82.022, mean reward: -82.022 [-82.022, -82.022], mean action: 3.000 [3.000, 3.000],  loss: 10561688.000000, mae: 979.168030, mean_q: -144.149048
 2611/5000: episode: 2611, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9477.291, mean reward: -9477.291 [-9477.291, -9477.291], mean action: 3.000 [3.000, 3.000],  loss: 10035867.000000, mae: 862.963379, mean_q: -145.244476
 2612/5000: episode: 2612, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -788.078, mean reward: -788.078 [-788.078, -788.078], mean action: 3.000 [3.000, 3.000],  loss: 7756983.000000, mae: 791.145142, mean_q: -145.469421
 2613/5000: episode: 2613, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5969.545, mean reward: -5969.545 [-5969.545, -5969.545], mean action: 3.000 [3.000, 3.000],  loss: 8776170.000000, mae: 938.733215, mean_q: -145.122787
 2614/5000: episode: 2614, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7541.536, mean reward: -7541.536 [-7541.536, -7541.536], mean action: 3.000 [3.000, 3.000],  loss: 17381234.000000, mae: 1183.709473, mean_q: -144.692017
 2615/5000: episode: 2615, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2284.321, mean reward: -2284.321 [-2284.321, -2284.321], mean action: 3.000 [3.000, 3.000],  loss: 12806489.000000, mae: 1047.256714, mean_q: -145.030655
 2616/5000: episode: 2616, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -208.862, mean reward: -208.862 [-208.862, -208.862], mean action: 3.000 [3.000, 3.000],  loss: 13056380.000000, mae: 1181.028687, mean_q: -145.326569
 2617/5000: episode: 2617, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1658.735, mean reward: -1658.735 [-1658.735, -1658.735], mean action: 3.000 [3.000, 3.000],  loss: 15713895.000000, mae: 1185.859375, mean_q: -146.056946
 2618/5000: episode: 2618, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -6233.986, mean reward: -6233.986 [-6233.986, -6233.986], mean action: 3.000 [3.000, 3.000],  loss: 18423808.000000, mae: 1388.183472, mean_q: -145.192139
 2619/5000: episode: 2619, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -415.949, mean reward: -415.949 [-415.949, -415.949], mean action: 3.000 [3.000, 3.000],  loss: 9746074.000000, mae: 960.715820, mean_q: -145.941437
 2620/5000: episode: 2620, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4812.574, mean reward: -4812.574 [-4812.574, -4812.574], mean action: 3.000 [3.000, 3.000],  loss: 9269602.000000, mae: 997.731628, mean_q: -146.201752
 2621/5000: episode: 2621, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3682.221, mean reward: -3682.221 [-3682.221, -3682.221], mean action: 3.000 [3.000, 3.000],  loss: 9974656.000000, mae: 1008.182861, mean_q: -145.711456
 2622/5000: episode: 2622, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -367.809, mean reward: -367.809 [-367.809, -367.809], mean action: 3.000 [3.000, 3.000],  loss: 13242282.000000, mae: 1075.160156, mean_q: -146.206940
 2623/5000: episode: 2623, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -284.997, mean reward: -284.997 [-284.997, -284.997], mean action: 3.000 [3.000, 3.000],  loss: 20203396.000000, mae: 1486.008545, mean_q: -145.942993
 2624/5000: episode: 2624, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1934.363, mean reward: -1934.363 [-1934.363, -1934.363], mean action: 3.000 [3.000, 3.000],  loss: 17256168.000000, mae: 1273.808228, mean_q: -146.676559
 2625/5000: episode: 2625, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -517.291, mean reward: -517.291 [-517.291, -517.291], mean action: 3.000 [3.000, 3.000],  loss: 14521844.000000, mae: 1210.557129, mean_q: -146.762375
 2626/5000: episode: 2626, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -9635.015, mean reward: -9635.015 [-9635.015, -9635.015], mean action: 3.000 [3.000, 3.000],  loss: 12242473.000000, mae: 1106.527100, mean_q: -146.740326
 2627/5000: episode: 2627, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -421.918, mean reward: -421.918 [-421.918, -421.918], mean action: 3.000 [3.000, 3.000],  loss: 17231584.000000, mae: 1245.382935, mean_q: -146.463501
 2628/5000: episode: 2628, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2737.067, mean reward: -2737.067 [-2737.067, -2737.067], mean action: 3.000 [3.000, 3.000],  loss: 17272290.000000, mae: 1318.368652, mean_q: -146.566986
 2629/5000: episode: 2629, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1845.245, mean reward: -1845.245 [-1845.245, -1845.245], mean action: 3.000 [3.000, 3.000],  loss: 14574638.000000, mae: 1269.382324, mean_q: -146.388199
 2630/5000: episode: 2630, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -712.533, mean reward: -712.533 [-712.533, -712.533], mean action: 3.000 [3.000, 3.000],  loss: 10635674.000000, mae: 938.137024, mean_q: -147.919983
 2631/5000: episode: 2631, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4260.592, mean reward: -4260.592 [-4260.592, -4260.592], mean action: 3.000 [3.000, 3.000],  loss: 12635011.000000, mae: 1208.355957, mean_q: -147.050873
 2632/5000: episode: 2632, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -384.058, mean reward: -384.058 [-384.058, -384.058], mean action: 3.000 [3.000, 3.000],  loss: 12465108.000000, mae: 1105.431519, mean_q: -148.232178
 2633/5000: episode: 2633, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3601.872, mean reward: -3601.872 [-3601.872, -3601.872], mean action: 3.000 [3.000, 3.000],  loss: 17088544.000000, mae: 1315.008789, mean_q: -147.604675
 2634/5000: episode: 2634, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1498.988, mean reward: -1498.988 [-1498.988, -1498.988], mean action: 3.000 [3.000, 3.000],  loss: 11359312.000000, mae: 1076.994385, mean_q: -147.905243
 2635/5000: episode: 2635, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7140.224, mean reward: -7140.224 [-7140.224, -7140.224], mean action: 3.000 [3.000, 3.000],  loss: 17839918.000000, mae: 1303.765747, mean_q: -147.456512
 2636/5000: episode: 2636, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3993.275, mean reward: -3993.275 [-3993.275, -3993.275], mean action: 3.000 [3.000, 3.000],  loss: 13110920.000000, mae: 1215.170776, mean_q: -147.735291
 2637/5000: episode: 2637, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7614.782, mean reward: -7614.782 [-7614.782, -7614.782], mean action: 3.000 [3.000, 3.000],  loss: 11717517.000000, mae: 1156.217285, mean_q: -148.182739
 2638/5000: episode: 2638, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3603.478, mean reward: -3603.478 [-3603.478, -3603.478], mean action: 3.000 [3.000, 3.000],  loss: 11724376.000000, mae: 1075.539795, mean_q: -148.352798
 2639/5000: episode: 2639, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9787.276, mean reward: -9787.276 [-9787.276, -9787.276], mean action: 3.000 [3.000, 3.000],  loss: 14985780.000000, mae: 1221.857666, mean_q: -148.029602
 2640/5000: episode: 2640, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5505.625, mean reward: -5505.625 [-5505.625, -5505.625], mean action: 3.000 [3.000, 3.000],  loss: 20304552.000000, mae: 1366.738281, mean_q: -148.137497
 2641/5000: episode: 2641, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3269.561, mean reward: -3269.561 [-3269.561, -3269.561], mean action: 3.000 [3.000, 3.000],  loss: 13837942.000000, mae: 1237.366333, mean_q: -148.685562
 2642/5000: episode: 2642, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -509.596, mean reward: -509.596 [-509.596, -509.596], mean action: 1.000 [1.000, 1.000],  loss: 13129475.000000, mae: 1130.483521, mean_q: -148.969421
 2643/5000: episode: 2643, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1810.703, mean reward: -1810.703 [-1810.703, -1810.703], mean action: 3.000 [3.000, 3.000],  loss: 14539180.000000, mae: 1183.185791, mean_q: -148.714066
 2644/5000: episode: 2644, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3757.528, mean reward: -3757.528 [-3757.528, -3757.528], mean action: 3.000 [3.000, 3.000],  loss: 14390790.000000, mae: 1209.793701, mean_q: -149.543839
 2645/5000: episode: 2645, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1841.677, mean reward: -1841.677 [-1841.677, -1841.677], mean action: 3.000 [3.000, 3.000],  loss: 16522998.000000, mae: 1294.118286, mean_q: -149.128159
 2646/5000: episode: 2646, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6339.022, mean reward: -6339.022 [-6339.022, -6339.022], mean action: 3.000 [3.000, 3.000],  loss: 10171583.000000, mae: 1040.226074, mean_q: -149.148804
 2647/5000: episode: 2647, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2872.282, mean reward: -2872.282 [-2872.282, -2872.282], mean action: 3.000 [3.000, 3.000],  loss: 12418296.000000, mae: 1077.816406, mean_q: -149.216537
 2648/5000: episode: 2648, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3458.989, mean reward: -3458.989 [-3458.989, -3458.989], mean action: 3.000 [3.000, 3.000],  loss: 8661119.000000, mae: 951.962036, mean_q: -149.728485
 2649/5000: episode: 2649, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1403.701, mean reward: -1403.701 [-1403.701, -1403.701], mean action: 3.000 [3.000, 3.000],  loss: 14134705.000000, mae: 1121.420776, mean_q: -149.230652
 2650/5000: episode: 2650, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5462.433, mean reward: -5462.433 [-5462.433, -5462.433], mean action: 3.000 [3.000, 3.000],  loss: 15765800.000000, mae: 1213.294434, mean_q: -149.907181
 2651/5000: episode: 2651, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -15.308, mean reward: -15.308 [-15.308, -15.308], mean action: 3.000 [3.000, 3.000],  loss: 14428822.000000, mae: 1150.371460, mean_q: -150.388596
 2652/5000: episode: 2652, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -677.230, mean reward: -677.230 [-677.230, -677.230], mean action: 3.000 [3.000, 3.000],  loss: 17469730.000000, mae: 1310.562988, mean_q: -150.274536
 2653/5000: episode: 2653, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -11534.273, mean reward: -11534.273 [-11534.273, -11534.273], mean action: 3.000 [3.000, 3.000],  loss: 9374680.000000, mae: 996.488647, mean_q: -150.853760
 2654/5000: episode: 2654, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1360.246, mean reward: -1360.246 [-1360.246, -1360.246], mean action: 1.000 [1.000, 1.000],  loss: 14510547.000000, mae: 1246.945801, mean_q: -150.052277
 2655/5000: episode: 2655, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -129.657, mean reward: -129.657 [-129.657, -129.657], mean action: 1.000 [1.000, 1.000],  loss: 11541703.000000, mae: 1112.273560, mean_q: -151.297379
 2656/5000: episode: 2656, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5932.853, mean reward: -5932.853 [-5932.853, -5932.853], mean action: 3.000 [3.000, 3.000],  loss: 17810588.000000, mae: 1399.098022, mean_q: -150.661804
 2657/5000: episode: 2657, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7366.218, mean reward: -7366.218 [-7366.218, -7366.218], mean action: 3.000 [3.000, 3.000],  loss: 13375838.000000, mae: 1102.013428, mean_q: -150.628082
 2658/5000: episode: 2658, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -969.635, mean reward: -969.635 [-969.635, -969.635], mean action: 3.000 [3.000, 3.000],  loss: 12263707.000000, mae: 1073.667725, mean_q: -150.529434
 2659/5000: episode: 2659, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -925.657, mean reward: -925.657 [-925.657, -925.657], mean action: 3.000 [3.000, 3.000],  loss: 14889580.000000, mae: 1204.411865, mean_q: -150.690567
 2660/5000: episode: 2660, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1359.822, mean reward: -1359.822 [-1359.822, -1359.822], mean action: 3.000 [3.000, 3.000],  loss: 15674916.000000, mae: 1218.429688, mean_q: -151.256317
 2661/5000: episode: 2661, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2070.247, mean reward: -2070.247 [-2070.247, -2070.247], mean action: 3.000 [3.000, 3.000],  loss: 9980080.000000, mae: 1021.441589, mean_q: -151.373047
 2662/5000: episode: 2662, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3181.887, mean reward: -3181.887 [-3181.887, -3181.887], mean action: 3.000 [3.000, 3.000],  loss: 15353105.000000, mae: 1279.407837, mean_q: -151.833069
 2663/5000: episode: 2663, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6404.118, mean reward: -6404.118 [-6404.118, -6404.118], mean action: 3.000 [3.000, 3.000],  loss: 17294020.000000, mae: 1229.801636, mean_q: -151.501709
 2664/5000: episode: 2664, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -131.133, mean reward: -131.133 [-131.133, -131.133], mean action: 3.000 [3.000, 3.000],  loss: 11121942.000000, mae: 1059.966064, mean_q: -151.555359
 2665/5000: episode: 2665, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6109.796, mean reward: -6109.796 [-6109.796, -6109.796], mean action: 3.000 [3.000, 3.000],  loss: 16132188.000000, mae: 1268.573486, mean_q: -151.317902
 2666/5000: episode: 2666, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7022.079, mean reward: -7022.079 [-7022.079, -7022.079], mean action: 3.000 [3.000, 3.000],  loss: 13079000.000000, mae: 1216.567139, mean_q: -152.100891
 2667/5000: episode: 2667, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2548.924, mean reward: -2548.924 [-2548.924, -2548.924], mean action: 3.000 [3.000, 3.000],  loss: 8150942.500000, mae: 954.416687, mean_q: -151.928162
 2668/5000: episode: 2668, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -945.973, mean reward: -945.973 [-945.973, -945.973], mean action: 2.000 [2.000, 2.000],  loss: 10059380.000000, mae: 998.930054, mean_q: -152.147720
 2669/5000: episode: 2669, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -8908.155, mean reward: -8908.155 [-8908.155, -8908.155], mean action: 3.000 [3.000, 3.000],  loss: 16825488.000000, mae: 1334.620605, mean_q: -152.033234
 2670/5000: episode: 2670, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -246.313, mean reward: -246.313 [-246.313, -246.313], mean action: 3.000 [3.000, 3.000],  loss: 10715550.000000, mae: 1156.458374, mean_q: -152.347900
 2671/5000: episode: 2671, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1729.287, mean reward: -1729.287 [-1729.287, -1729.287], mean action: 0.000 [0.000, 0.000],  loss: 11608442.000000, mae: 1063.159912, mean_q: -153.376282
 2672/5000: episode: 2672, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1599.075, mean reward: -1599.075 [-1599.075, -1599.075], mean action: 3.000 [3.000, 3.000],  loss: 13131758.000000, mae: 1094.468262, mean_q: -152.972336
 2673/5000: episode: 2673, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1895.378, mean reward: -1895.378 [-1895.378, -1895.378], mean action: 3.000 [3.000, 3.000],  loss: 11435699.000000, mae: 1109.620850, mean_q: -153.567581
 2674/5000: episode: 2674, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1234.067, mean reward: -1234.067 [-1234.067, -1234.067], mean action: 3.000 [3.000, 3.000],  loss: 11154206.000000, mae: 1051.539062, mean_q: -153.652786
 2675/5000: episode: 2675, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8459.369, mean reward: -8459.369 [-8459.369, -8459.369], mean action: 3.000 [3.000, 3.000],  loss: 19614680.000000, mae: 1420.782715, mean_q: -153.416107
 2676/5000: episode: 2676, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5029.096, mean reward: -5029.096 [-5029.096, -5029.096], mean action: 3.000 [3.000, 3.000],  loss: 10600293.000000, mae: 1008.568665, mean_q: -153.199310
 2677/5000: episode: 2677, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7422.718, mean reward: -7422.718 [-7422.718, -7422.718], mean action: 3.000 [3.000, 3.000],  loss: 14800168.000000, mae: 1290.987183, mean_q: -153.653564
 2678/5000: episode: 2678, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1560.508, mean reward: -1560.508 [-1560.508, -1560.508], mean action: 3.000 [3.000, 3.000],  loss: 13647480.000000, mae: 1227.537964, mean_q: -153.577209
 2679/5000: episode: 2679, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1321.005, mean reward: -1321.005 [-1321.005, -1321.005], mean action: 3.000 [3.000, 3.000],  loss: 9742336.000000, mae: 941.678162, mean_q: -153.822083
 2680/5000: episode: 2680, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1788.315, mean reward: -1788.315 [-1788.315, -1788.315], mean action: 3.000 [3.000, 3.000],  loss: 12910605.000000, mae: 1220.156494, mean_q: -154.218536
 2681/5000: episode: 2681, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3314.476, mean reward: -3314.476 [-3314.476, -3314.476], mean action: 3.000 [3.000, 3.000],  loss: 17373970.000000, mae: 1320.779053, mean_q: -153.869141
 2682/5000: episode: 2682, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -7354.892, mean reward: -7354.892 [-7354.892, -7354.892], mean action: 3.000 [3.000, 3.000],  loss: 20119764.000000, mae: 1395.035645, mean_q: -153.633896
 2683/5000: episode: 2683, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -367.139, mean reward: -367.139 [-367.139, -367.139], mean action: 3.000 [3.000, 3.000],  loss: 5187552.000000, mae: 762.377197, mean_q: -154.809784
 2684/5000: episode: 2684, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7691.901, mean reward: -7691.901 [-7691.901, -7691.901], mean action: 3.000 [3.000, 3.000],  loss: 18591252.000000, mae: 1421.226685, mean_q: -154.021484
 2685/5000: episode: 2685, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -2117.049, mean reward: -2117.049 [-2117.049, -2117.049], mean action: 3.000 [3.000, 3.000],  loss: 14108584.000000, mae: 1194.234863, mean_q: -155.164764
 2686/5000: episode: 2686, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -68.012, mean reward: -68.012 [-68.012, -68.012], mean action: 3.000 [3.000, 3.000],  loss: 11238817.000000, mae: 1103.278442, mean_q: -154.385315
 2687/5000: episode: 2687, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3671.628, mean reward: -3671.628 [-3671.628, -3671.628], mean action: 3.000 [3.000, 3.000],  loss: 22922504.000000, mae: 1612.023193, mean_q: -154.291718
 2688/5000: episode: 2688, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3836.736, mean reward: -3836.736 [-3836.736, -3836.736], mean action: 3.000 [3.000, 3.000],  loss: 16672329.000000, mae: 1242.399414, mean_q: -155.165787
 2689/5000: episode: 2689, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3957.059, mean reward: -3957.059 [-3957.059, -3957.059], mean action: 3.000 [3.000, 3.000],  loss: 11985764.000000, mae: 1161.653564, mean_q: -155.969803
 2690/5000: episode: 2690, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -12597.715, mean reward: -12597.715 [-12597.715, -12597.715], mean action: 3.000 [3.000, 3.000],  loss: 13447426.000000, mae: 1231.676392, mean_q: -155.359131
 2691/5000: episode: 2691, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6098.121, mean reward: -6098.121 [-6098.121, -6098.121], mean action: 3.000 [3.000, 3.000],  loss: 16032063.000000, mae: 1250.857178, mean_q: -154.935608
 2692/5000: episode: 2692, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -99.904, mean reward: -99.904 [-99.904, -99.904], mean action: 3.000 [3.000, 3.000],  loss: 9158378.000000, mae: 949.839478, mean_q: -156.136520
 2693/5000: episode: 2693, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -4495.683, mean reward: -4495.683 [-4495.683, -4495.683], mean action: 3.000 [3.000, 3.000],  loss: 8536314.000000, mae: 962.290527, mean_q: -156.769714
 2694/5000: episode: 2694, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1303.048, mean reward: -1303.048 [-1303.048, -1303.048], mean action: 3.000 [3.000, 3.000],  loss: 10272412.000000, mae: 1025.368164, mean_q: -155.218063
 2695/5000: episode: 2695, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6881.059, mean reward: -6881.059 [-6881.059, -6881.059], mean action: 3.000 [3.000, 3.000],  loss: 11972324.000000, mae: 1124.221802, mean_q: -156.991119
 2696/5000: episode: 2696, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2868.759, mean reward: -2868.759 [-2868.759, -2868.759], mean action: 3.000 [3.000, 3.000],  loss: 11870768.000000, mae: 1060.486328, mean_q: -156.085312
 2697/5000: episode: 2697, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1999.620, mean reward: -1999.620 [-1999.620, -1999.620], mean action: 3.000 [3.000, 3.000],  loss: 15495790.000000, mae: 1226.294189, mean_q: -156.270660
 2698/5000: episode: 2698, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -69.317, mean reward: -69.317 [-69.317, -69.317], mean action: 3.000 [3.000, 3.000],  loss: 15096865.000000, mae: 1266.357422, mean_q: -156.916412
 2699/5000: episode: 2699, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3494.986, mean reward: -3494.986 [-3494.986, -3494.986], mean action: 3.000 [3.000, 3.000],  loss: 12797932.000000, mae: 1189.840820, mean_q: -156.490631
 2700/5000: episode: 2700, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4351.106, mean reward: -4351.106 [-4351.106, -4351.106], mean action: 3.000 [3.000, 3.000],  loss: 17881400.000000, mae: 1337.793213, mean_q: -157.339020
 2701/5000: episode: 2701, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3025.142, mean reward: -3025.142 [-3025.142, -3025.142], mean action: 3.000 [3.000, 3.000],  loss: 12421298.000000, mae: 1150.198242, mean_q: -157.117081
 2702/5000: episode: 2702, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -598.956, mean reward: -598.956 [-598.956, -598.956], mean action: 3.000 [3.000, 3.000],  loss: 17092444.000000, mae: 1350.878174, mean_q: -157.007904
 2703/5000: episode: 2703, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4400.839, mean reward: -4400.839 [-4400.839, -4400.839], mean action: 3.000 [3.000, 3.000],  loss: 14355644.000000, mae: 1103.290527, mean_q: -157.441315
 2704/5000: episode: 2704, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1091.746, mean reward: -1091.746 [-1091.746, -1091.746], mean action: 3.000 [3.000, 3.000],  loss: 9976426.000000, mae: 1043.079590, mean_q: -156.785675
 2705/5000: episode: 2705, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -421.126, mean reward: -421.126 [-421.126, -421.126], mean action: 3.000 [3.000, 3.000],  loss: 8299497.500000, mae: 938.055054, mean_q: -157.727173
 2706/5000: episode: 2706, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6293.457, mean reward: -6293.457 [-6293.457, -6293.457], mean action: 3.000 [3.000, 3.000],  loss: 17032668.000000, mae: 1318.557617, mean_q: -157.478958
 2707/5000: episode: 2707, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4670.878, mean reward: -4670.878 [-4670.878, -4670.878], mean action: 3.000 [3.000, 3.000],  loss: 16311198.000000, mae: 1217.413818, mean_q: -157.406555
 2708/5000: episode: 2708, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1175.041, mean reward: -1175.041 [-1175.041, -1175.041], mean action: 3.000 [3.000, 3.000],  loss: 9980510.000000, mae: 1051.157471, mean_q: -157.898087
 2709/5000: episode: 2709, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4364.186, mean reward: -4364.186 [-4364.186, -4364.186], mean action: 3.000 [3.000, 3.000],  loss: 13104774.000000, mae: 1187.524658, mean_q: -158.277344
 2710/5000: episode: 2710, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9372.817, mean reward: -9372.817 [-9372.817, -9372.817], mean action: 3.000 [3.000, 3.000],  loss: 10273604.000000, mae: 1094.960938, mean_q: -158.161438
 2711/5000: episode: 2711, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -12191.281, mean reward: -12191.281 [-12191.281, -12191.281], mean action: 0.000 [0.000, 0.000],  loss: 14510146.000000, mae: 1199.223877, mean_q: -157.576965
 2712/5000: episode: 2712, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6400.766, mean reward: -6400.766 [-6400.766, -6400.766], mean action: 3.000 [3.000, 3.000],  loss: 17957328.000000, mae: 1302.930542, mean_q: -158.034897
 2713/5000: episode: 2713, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -507.110, mean reward: -507.110 [-507.110, -507.110], mean action: 3.000 [3.000, 3.000],  loss: 12384016.000000, mae: 1117.738525, mean_q: -157.956818
 2714/5000: episode: 2714, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -14.174, mean reward: -14.174 [-14.174, -14.174], mean action: 3.000 [3.000, 3.000],  loss: 14343898.000000, mae: 1261.820068, mean_q: -159.129501
 2715/5000: episode: 2715, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -7800.207, mean reward: -7800.207 [-7800.207, -7800.207], mean action: 3.000 [3.000, 3.000],  loss: 15776888.000000, mae: 1258.082031, mean_q: -158.939087
 2716/5000: episode: 2716, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6366.273, mean reward: -6366.273 [-6366.273, -6366.273], mean action: 3.000 [3.000, 3.000],  loss: 22283288.000000, mae: 1538.356689, mean_q: -158.416000
 2717/5000: episode: 2717, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -853.203, mean reward: -853.203 [-853.203, -853.203], mean action: 3.000 [3.000, 3.000],  loss: 14290498.000000, mae: 1180.304443, mean_q: -159.176483
 2718/5000: episode: 2718, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7158.316, mean reward: -7158.316 [-7158.316, -7158.316], mean action: 3.000 [3.000, 3.000],  loss: 14783565.000000, mae: 1184.285156, mean_q: -159.267334
 2719/5000: episode: 2719, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -737.270, mean reward: -737.270 [-737.270, -737.270], mean action: 3.000 [3.000, 3.000],  loss: 16420233.000000, mae: 1198.470093, mean_q: -158.825195
 2720/5000: episode: 2720, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -10711.328, mean reward: -10711.328 [-10711.328, -10711.328], mean action: 0.000 [0.000, 0.000],  loss: 15222523.000000, mae: 1300.434082, mean_q: -159.490997
 2721/5000: episode: 2721, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4003.995, mean reward: -4003.995 [-4003.995, -4003.995], mean action: 3.000 [3.000, 3.000],  loss: 10067171.000000, mae: 1054.411499, mean_q: -160.482468
 2722/5000: episode: 2722, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4513.250, mean reward: -4513.250 [-4513.250, -4513.250], mean action: 3.000 [3.000, 3.000],  loss: 9168350.000000, mae: 975.180847, mean_q: -159.509750
 2723/5000: episode: 2723, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5392.869, mean reward: -5392.869 [-5392.869, -5392.869], mean action: 1.000 [1.000, 1.000],  loss: 9996709.000000, mae: 941.685242, mean_q: -160.387619
 2724/5000: episode: 2724, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -788.151, mean reward: -788.151 [-788.151, -788.151], mean action: 1.000 [1.000, 1.000],  loss: 10211166.000000, mae: 1012.232239, mean_q: -159.782272
 2725/5000: episode: 2725, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3345.348, mean reward: -3345.348 [-3345.348, -3345.348], mean action: 1.000 [1.000, 1.000],  loss: 14285213.000000, mae: 1240.094238, mean_q: -159.414520
 2726/5000: episode: 2726, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6357.903, mean reward: -6357.903 [-6357.903, -6357.903], mean action: 1.000 [1.000, 1.000],  loss: 12036846.000000, mae: 1056.510986, mean_q: -160.372101
 2727/5000: episode: 2727, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2692.194, mean reward: -2692.194 [-2692.194, -2692.194], mean action: 1.000 [1.000, 1.000],  loss: 9244283.000000, mae: 986.193542, mean_q: -161.177551
 2728/5000: episode: 2728, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2831.619, mean reward: -2831.619 [-2831.619, -2831.619], mean action: 1.000 [1.000, 1.000],  loss: 14700107.000000, mae: 1250.482788, mean_q: -160.533859
 2729/5000: episode: 2729, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7371.367, mean reward: -7371.367 [-7371.367, -7371.367], mean action: 1.000 [1.000, 1.000],  loss: 11614100.000000, mae: 1067.472290, mean_q: -161.120087
 2730/5000: episode: 2730, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4916.423, mean reward: -4916.423 [-4916.423, -4916.423], mean action: 0.000 [0.000, 0.000],  loss: 10366019.000000, mae: 1056.996216, mean_q: -161.381210
 2731/5000: episode: 2731, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2134.575, mean reward: -2134.575 [-2134.575, -2134.575], mean action: 1.000 [1.000, 1.000],  loss: 12726322.000000, mae: 1132.234863, mean_q: -160.902863
 2732/5000: episode: 2732, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -417.035, mean reward: -417.035 [-417.035, -417.035], mean action: 3.000 [3.000, 3.000],  loss: 15470666.000000, mae: 1219.397949, mean_q: -161.244659
 2733/5000: episode: 2733, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3215.619, mean reward: -3215.619 [-3215.619, -3215.619], mean action: 1.000 [1.000, 1.000],  loss: 15719228.000000, mae: 1247.558472, mean_q: -161.487930
 2734/5000: episode: 2734, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3367.943, mean reward: -3367.943 [-3367.943, -3367.943], mean action: 1.000 [1.000, 1.000],  loss: 14492244.000000, mae: 1144.248169, mean_q: -161.087555
 2735/5000: episode: 2735, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6978.555, mean reward: -6978.555 [-6978.555, -6978.555], mean action: 1.000 [1.000, 1.000],  loss: 10710002.000000, mae: 1036.974854, mean_q: -161.734375
 2736/5000: episode: 2736, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1310.103, mean reward: -1310.103 [-1310.103, -1310.103], mean action: 2.000 [2.000, 2.000],  loss: 9315430.000000, mae: 1012.200500, mean_q: -162.238480
 2737/5000: episode: 2737, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3907.565, mean reward: -3907.565 [-3907.565, -3907.565], mean action: 1.000 [1.000, 1.000],  loss: 18500978.000000, mae: 1441.645020, mean_q: -161.675903
 2738/5000: episode: 2738, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5036.667, mean reward: -5036.667 [-5036.667, -5036.667], mean action: 2.000 [2.000, 2.000],  loss: 8338146.500000, mae: 941.260315, mean_q: -161.971619
 2739/5000: episode: 2739, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5232.586, mean reward: -5232.586 [-5232.586, -5232.586], mean action: 1.000 [1.000, 1.000],  loss: 18762216.000000, mae: 1399.632568, mean_q: -161.978729
 2740/5000: episode: 2740, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -976.049, mean reward: -976.049 [-976.049, -976.049], mean action: 2.000 [2.000, 2.000],  loss: 16937720.000000, mae: 1297.923828, mean_q: -162.384903
 2741/5000: episode: 2741, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2348.968, mean reward: -2348.968 [-2348.968, -2348.968], mean action: 2.000 [2.000, 2.000],  loss: 17541008.000000, mae: 1286.926758, mean_q: -161.926422
 2742/5000: episode: 2742, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3782.536, mean reward: -3782.536 [-3782.536, -3782.536], mean action: 2.000 [2.000, 2.000],  loss: 20422490.000000, mae: 1359.832642, mean_q: -162.105988
 2743/5000: episode: 2743, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2566.395, mean reward: -2566.395 [-2566.395, -2566.395], mean action: 2.000 [2.000, 2.000],  loss: 12546976.000000, mae: 1087.408081, mean_q: -162.562454
 2744/5000: episode: 2744, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1061.042, mean reward: -1061.042 [-1061.042, -1061.042], mean action: 2.000 [2.000, 2.000],  loss: 12849699.000000, mae: 1128.041382, mean_q: -163.446991
 2745/5000: episode: 2745, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6499.515, mean reward: -6499.515 [-6499.515, -6499.515], mean action: 2.000 [2.000, 2.000],  loss: 11024399.000000, mae: 1087.537109, mean_q: -163.460007
 2746/5000: episode: 2746, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1129.707, mean reward: -1129.707 [-1129.707, -1129.707], mean action: 2.000 [2.000, 2.000],  loss: 16147137.000000, mae: 1279.528687, mean_q: -163.820786
 2747/5000: episode: 2747, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2625.999, mean reward: -2625.999 [-2625.999, -2625.999], mean action: 2.000 [2.000, 2.000],  loss: 10664680.000000, mae: 1074.582886, mean_q: -162.772064
 2748/5000: episode: 2748, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1576.945, mean reward: -1576.945 [-1576.945, -1576.945], mean action: 2.000 [2.000, 2.000],  loss: 10711598.000000, mae: 1059.600952, mean_q: -164.482285
 2749/5000: episode: 2749, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5331.382, mean reward: -5331.382 [-5331.382, -5331.382], mean action: 2.000 [2.000, 2.000],  loss: 8482295.000000, mae: 901.271606, mean_q: -163.560455
 2750/5000: episode: 2750, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3349.105, mean reward: -3349.105 [-3349.105, -3349.105], mean action: 1.000 [1.000, 1.000],  loss: 8294282.500000, mae: 991.237366, mean_q: -163.283203
 2751/5000: episode: 2751, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7535.531, mean reward: -7535.531 [-7535.531, -7535.531], mean action: 1.000 [1.000, 1.000],  loss: 13315433.000000, mae: 1180.987549, mean_q: -164.270966
 2752/5000: episode: 2752, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9113.842, mean reward: -9113.842 [-9113.842, -9113.842], mean action: 1.000 [1.000, 1.000],  loss: 19611946.000000, mae: 1415.736328, mean_q: -163.312347
 2753/5000: episode: 2753, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3512.312, mean reward: -3512.312 [-3512.312, -3512.312], mean action: 1.000 [1.000, 1.000],  loss: 11039040.000000, mae: 1096.164917, mean_q: -164.259705
 2754/5000: episode: 2754, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1703.877, mean reward: -1703.877 [-1703.877, -1703.877], mean action: 1.000 [1.000, 1.000],  loss: 16301573.000000, mae: 1304.306641, mean_q: -164.276291
 2755/5000: episode: 2755, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7895.151, mean reward: -7895.151 [-7895.151, -7895.151], mean action: 1.000 [1.000, 1.000],  loss: 8390118.000000, mae: 896.451660, mean_q: -164.604828
 2756/5000: episode: 2756, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3051.323, mean reward: -3051.323 [-3051.323, -3051.323], mean action: 1.000 [1.000, 1.000],  loss: 8604715.000000, mae: 1004.984924, mean_q: -164.758972
 2757/5000: episode: 2757, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -7835.893, mean reward: -7835.893 [-7835.893, -7835.893], mean action: 1.000 [1.000, 1.000],  loss: 7644688.000000, mae: 921.551453, mean_q: -164.873978
 2758/5000: episode: 2758, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2862.373, mean reward: -2862.373 [-2862.373, -2862.373], mean action: 1.000 [1.000, 1.000],  loss: 15846422.000000, mae: 1327.064453, mean_q: -164.677444
 2759/5000: episode: 2759, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3797.239, mean reward: -3797.239 [-3797.239, -3797.239], mean action: 1.000 [1.000, 1.000],  loss: 10965460.000000, mae: 1109.358398, mean_q: -164.452194
 2760/5000: episode: 2760, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3118.907, mean reward: -3118.907 [-3118.907, -3118.907], mean action: 1.000 [1.000, 1.000],  loss: 18895612.000000, mae: 1377.471436, mean_q: -164.547516
 2761/5000: episode: 2761, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1805.595, mean reward: -1805.595 [-1805.595, -1805.595], mean action: 1.000 [1.000, 1.000],  loss: 13513862.000000, mae: 1194.356201, mean_q: -165.040619
 2762/5000: episode: 2762, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5073.976, mean reward: -5073.976 [-5073.976, -5073.976], mean action: 1.000 [1.000, 1.000],  loss: 8146092.500000, mae: 911.343384, mean_q: -165.357178
 2763/5000: episode: 2763, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3417.842, mean reward: -3417.842 [-3417.842, -3417.842], mean action: 1.000 [1.000, 1.000],  loss: 22312676.000000, mae: 1505.586182, mean_q: -164.792862
 2764/5000: episode: 2764, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5475.452, mean reward: -5475.452 [-5475.452, -5475.452], mean action: 1.000 [1.000, 1.000],  loss: 9040514.000000, mae: 1004.374146, mean_q: -165.700226
 2765/5000: episode: 2765, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -926.308, mean reward: -926.308 [-926.308, -926.308], mean action: 1.000 [1.000, 1.000],  loss: 12087912.000000, mae: 1101.708130, mean_q: -166.123352
 2766/5000: episode: 2766, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1400.291, mean reward: -1400.291 [-1400.291, -1400.291], mean action: 1.000 [1.000, 1.000],  loss: 14310632.000000, mae: 1250.959473, mean_q: -165.524750
 2767/5000: episode: 2767, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1566.499, mean reward: -1566.499 [-1566.499, -1566.499], mean action: 1.000 [1.000, 1.000],  loss: 12686755.000000, mae: 1131.702271, mean_q: -166.046844
 2768/5000: episode: 2768, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7845.935, mean reward: -7845.935 [-7845.935, -7845.935], mean action: 1.000 [1.000, 1.000],  loss: 10177395.000000, mae: 1004.188110, mean_q: -166.403229
 2769/5000: episode: 2769, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1515.365, mean reward: -1515.365 [-1515.365, -1515.365], mean action: 1.000 [1.000, 1.000],  loss: 15310609.000000, mae: 1291.587646, mean_q: -165.085129
 2770/5000: episode: 2770, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7253.799, mean reward: -7253.799 [-7253.799, -7253.799], mean action: 1.000 [1.000, 1.000],  loss: 17888028.000000, mae: 1256.418823, mean_q: -165.218781
 2771/5000: episode: 2771, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2736.199, mean reward: -2736.199 [-2736.199, -2736.199], mean action: 1.000 [1.000, 1.000],  loss: 15046055.000000, mae: 1161.081421, mean_q: -167.037842
 2772/5000: episode: 2772, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6135.938, mean reward: -6135.938 [-6135.938, -6135.938], mean action: 1.000 [1.000, 1.000],  loss: 11923975.000000, mae: 1136.790283, mean_q: -167.014587
 2773/5000: episode: 2773, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5374.717, mean reward: -5374.717 [-5374.717, -5374.717], mean action: 1.000 [1.000, 1.000],  loss: 17463090.000000, mae: 1363.080566, mean_q: -166.009521
 2774/5000: episode: 2774, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1008.692, mean reward: -1008.692 [-1008.692, -1008.692], mean action: 1.000 [1.000, 1.000],  loss: 11330765.000000, mae: 1043.260742, mean_q: -167.230438
 2775/5000: episode: 2775, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1061.632, mean reward: -1061.632 [-1061.632, -1061.632], mean action: 1.000 [1.000, 1.000],  loss: 10068213.000000, mae: 1082.730713, mean_q: -167.147552
 2776/5000: episode: 2776, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1218.838, mean reward: -1218.838 [-1218.838, -1218.838], mean action: 1.000 [1.000, 1.000],  loss: 17668082.000000, mae: 1330.665771, mean_q: -167.274582
 2777/5000: episode: 2777, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -4586.476, mean reward: -4586.476 [-4586.476, -4586.476], mean action: 1.000 [1.000, 1.000],  loss: 17134648.000000, mae: 1308.572021, mean_q: -167.493622
 2778/5000: episode: 2778, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1701.046, mean reward: -1701.046 [-1701.046, -1701.046], mean action: 1.000 [1.000, 1.000],  loss: 9933876.000000, mae: 929.948486, mean_q: -167.714203
 2779/5000: episode: 2779, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8068.537, mean reward: -8068.537 [-8068.537, -8068.537], mean action: 1.000 [1.000, 1.000],  loss: 20969018.000000, mae: 1464.426514, mean_q: -167.831848
 2780/5000: episode: 2780, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7575.051, mean reward: -7575.051 [-7575.051, -7575.051], mean action: 1.000 [1.000, 1.000],  loss: 9088205.000000, mae: 993.062256, mean_q: -167.735809
 2781/5000: episode: 2781, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3158.760, mean reward: -3158.760 [-3158.760, -3158.760], mean action: 1.000 [1.000, 1.000],  loss: 15406538.000000, mae: 1157.384644, mean_q: -168.037384
 2782/5000: episode: 2782, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4259.121, mean reward: -4259.121 [-4259.121, -4259.121], mean action: 1.000 [1.000, 1.000],  loss: 15007875.000000, mae: 1301.378906, mean_q: -168.273132
 2783/5000: episode: 2783, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6679.487, mean reward: -6679.487 [-6679.487, -6679.487], mean action: 1.000 [1.000, 1.000],  loss: 10363955.000000, mae: 1037.780151, mean_q: -168.642700
 2784/5000: episode: 2784, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4146.574, mean reward: -4146.574 [-4146.574, -4146.574], mean action: 1.000 [1.000, 1.000],  loss: 11422970.000000, mae: 1090.468018, mean_q: -168.761017
 2785/5000: episode: 2785, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7275.693, mean reward: -7275.693 [-7275.693, -7275.693], mean action: 1.000 [1.000, 1.000],  loss: 9775474.000000, mae: 1057.196777, mean_q: -168.578857
 2786/5000: episode: 2786, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9193.578, mean reward: -9193.578 [-9193.578, -9193.578], mean action: 1.000 [1.000, 1.000],  loss: 10548566.000000, mae: 1034.234009, mean_q: -168.880035
 2787/5000: episode: 2787, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2255.216, mean reward: -2255.216 [-2255.216, -2255.216], mean action: 1.000 [1.000, 1.000],  loss: 9324762.000000, mae: 1011.081055, mean_q: -168.777283
 2788/5000: episode: 2788, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6624.869, mean reward: -6624.869 [-6624.869, -6624.869], mean action: 1.000 [1.000, 1.000],  loss: 7172829.500000, mae: 884.164551, mean_q: -169.292221
 2789/5000: episode: 2789, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -986.990, mean reward: -986.990 [-986.990, -986.990], mean action: 1.000 [1.000, 1.000],  loss: 6550348.500000, mae: 838.051392, mean_q: -170.127686
 2790/5000: episode: 2790, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1026.628, mean reward: -1026.628 [-1026.628, -1026.628], mean action: 1.000 [1.000, 1.000],  loss: 16366836.000000, mae: 1300.975464, mean_q: -169.599243
 2791/5000: episode: 2791, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2611.244, mean reward: -2611.244 [-2611.244, -2611.244], mean action: 1.000 [1.000, 1.000],  loss: 19265862.000000, mae: 1397.382690, mean_q: -168.403290
 2792/5000: episode: 2792, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3232.135, mean reward: -3232.135 [-3232.135, -3232.135], mean action: 1.000 [1.000, 1.000],  loss: 14156910.000000, mae: 1198.008789, mean_q: -169.604858
 2793/5000: episode: 2793, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -9398.684, mean reward: -9398.684 [-9398.684, -9398.684], mean action: 1.000 [1.000, 1.000],  loss: 6986925.000000, mae: 890.388184, mean_q: -170.254761
 2794/5000: episode: 2794, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9258.115, mean reward: -9258.115 [-9258.115, -9258.115], mean action: 1.000 [1.000, 1.000],  loss: 16077273.000000, mae: 1296.209473, mean_q: -170.382080
 2795/5000: episode: 2795, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -2226.074, mean reward: -2226.074 [-2226.074, -2226.074], mean action: 2.000 [2.000, 2.000],  loss: 10249018.000000, mae: 1010.761292, mean_q: -169.845383
 2796/5000: episode: 2796, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2830.781, mean reward: -2830.781 [-2830.781, -2830.781], mean action: 1.000 [1.000, 1.000],  loss: 16392856.000000, mae: 1369.862061, mean_q: -169.733734
 2797/5000: episode: 2797, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1274.806, mean reward: -1274.806 [-1274.806, -1274.806], mean action: 1.000 [1.000, 1.000],  loss: 13095451.000000, mae: 1165.443604, mean_q: -170.933044
 2798/5000: episode: 2798, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -4513.734, mean reward: -4513.734 [-4513.734, -4513.734], mean action: 1.000 [1.000, 1.000],  loss: 11942320.000000, mae: 1073.356567, mean_q: -170.478668
 2799/5000: episode: 2799, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3561.559, mean reward: -3561.559 [-3561.559, -3561.559], mean action: 1.000 [1.000, 1.000],  loss: 8098161.000000, mae: 943.932739, mean_q: -170.486832
 2800/5000: episode: 2800, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2796.845, mean reward: -2796.845 [-2796.845, -2796.845], mean action: 1.000 [1.000, 1.000],  loss: 15244416.000000, mae: 1276.551270, mean_q: -170.944275
 2801/5000: episode: 2801, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -865.505, mean reward: -865.505 [-865.505, -865.505], mean action: 1.000 [1.000, 1.000],  loss: 13280398.000000, mae: 1092.865601, mean_q: -170.226517
 2802/5000: episode: 2802, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2433.593, mean reward: -2433.593 [-2433.593, -2433.593], mean action: 1.000 [1.000, 1.000],  loss: 15844067.000000, mae: 1282.828979, mean_q: -170.874313
 2803/5000: episode: 2803, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -4651.486, mean reward: -4651.486 [-4651.486, -4651.486], mean action: 1.000 [1.000, 1.000],  loss: 11615619.000000, mae: 1124.146240, mean_q: -171.613556
 2804/5000: episode: 2804, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8971.069, mean reward: -8971.069 [-8971.069, -8971.069], mean action: 1.000 [1.000, 1.000],  loss: 12564388.000000, mae: 1118.000854, mean_q: -171.725754
 2805/5000: episode: 2805, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -662.310, mean reward: -662.310 [-662.310, -662.310], mean action: 1.000 [1.000, 1.000],  loss: 15840584.000000, mae: 1232.659058, mean_q: -171.293365
 2806/5000: episode: 2806, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4789.168, mean reward: -4789.168 [-4789.168, -4789.168], mean action: 3.000 [3.000, 3.000],  loss: 9919579.000000, mae: 1028.853149, mean_q: -171.387787
 2807/5000: episode: 2807, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3042.545, mean reward: -3042.545 [-3042.545, -3042.545], mean action: 1.000 [1.000, 1.000],  loss: 13713874.000000, mae: 1123.385376, mean_q: -171.640503
 2808/5000: episode: 2808, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3962.255, mean reward: -3962.255 [-3962.255, -3962.255], mean action: 1.000 [1.000, 1.000],  loss: 13895972.000000, mae: 1260.352173, mean_q: -171.849411
 2809/5000: episode: 2809, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4522.147, mean reward: -4522.147 [-4522.147, -4522.147], mean action: 1.000 [1.000, 1.000],  loss: 10721809.000000, mae: 1031.221680, mean_q: -172.974594
 2810/5000: episode: 2810, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8179.941, mean reward: -8179.941 [-8179.941, -8179.941], mean action: 3.000 [3.000, 3.000],  loss: 11562824.000000, mae: 1070.532471, mean_q: -171.997177
 2811/5000: episode: 2811, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7471.112, mean reward: -7471.112 [-7471.112, -7471.112], mean action: 2.000 [2.000, 2.000],  loss: 8979868.000000, mae: 986.596619, mean_q: -172.447617
 2812/5000: episode: 2812, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4016.323, mean reward: -4016.323 [-4016.323, -4016.323], mean action: 2.000 [2.000, 2.000],  loss: 12888204.000000, mae: 1185.771729, mean_q: -172.364578
 2813/5000: episode: 2813, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7317.285, mean reward: -7317.285 [-7317.285, -7317.285], mean action: 1.000 [1.000, 1.000],  loss: 12739576.000000, mae: 1140.054443, mean_q: -172.544083
 2814/5000: episode: 2814, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3951.508, mean reward: -3951.508 [-3951.508, -3951.508], mean action: 1.000 [1.000, 1.000],  loss: 12521880.000000, mae: 1094.416016, mean_q: -172.566177
 2815/5000: episode: 2815, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9724.905, mean reward: -9724.905 [-9724.905, -9724.905], mean action: 1.000 [1.000, 1.000],  loss: 11044312.000000, mae: 1119.223267, mean_q: -172.323822
 2816/5000: episode: 2816, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -6645.387, mean reward: -6645.387 [-6645.387, -6645.387], mean action: 1.000 [1.000, 1.000],  loss: 10718945.000000, mae: 1019.968323, mean_q: -173.927490
 2817/5000: episode: 2817, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -353.007, mean reward: -353.007 [-353.007, -353.007], mean action: 1.000 [1.000, 1.000],  loss: 13679347.000000, mae: 1109.663208, mean_q: -173.187592
 2818/5000: episode: 2818, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -897.514, mean reward: -897.514 [-897.514, -897.514], mean action: 2.000 [2.000, 2.000],  loss: 15600527.000000, mae: 1183.990479, mean_q: -173.250076
 2819/5000: episode: 2819, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7279.684, mean reward: -7279.684 [-7279.684, -7279.684], mean action: 0.000 [0.000, 0.000],  loss: 15746914.000000, mae: 1170.891113, mean_q: -173.075150
 2820/5000: episode: 2820, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3771.988, mean reward: -3771.988 [-3771.988, -3771.988], mean action: 1.000 [1.000, 1.000],  loss: 13769952.000000, mae: 1224.686035, mean_q: -173.690796
 2821/5000: episode: 2821, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3586.025, mean reward: -3586.025 [-3586.025, -3586.025], mean action: 2.000 [2.000, 2.000],  loss: 18114078.000000, mae: 1308.522949, mean_q: -173.924988
 2822/5000: episode: 2822, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5484.350, mean reward: -5484.350 [-5484.350, -5484.350], mean action: 1.000 [1.000, 1.000],  loss: 23001892.000000, mae: 1495.662231, mean_q: -173.798553
 2823/5000: episode: 2823, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4360.907, mean reward: -4360.907 [-4360.907, -4360.907], mean action: 1.000 [1.000, 1.000],  loss: 8875980.000000, mae: 1018.101929, mean_q: -174.461945
 2824/5000: episode: 2824, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2206.508, mean reward: -2206.508 [-2206.508, -2206.508], mean action: 1.000 [1.000, 1.000],  loss: 12850722.000000, mae: 1020.920288, mean_q: -174.511108
 2825/5000: episode: 2825, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1449.657, mean reward: -1449.657 [-1449.657, -1449.657], mean action: 1.000 [1.000, 1.000],  loss: 13581906.000000, mae: 1237.469604, mean_q: -174.690033
 2826/5000: episode: 2826, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -671.489, mean reward: -671.489 [-671.489, -671.489], mean action: 1.000 [1.000, 1.000],  loss: 14383396.000000, mae: 1203.388794, mean_q: -174.390106
 2827/5000: episode: 2827, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5999.632, mean reward: -5999.632 [-5999.632, -5999.632], mean action: 1.000 [1.000, 1.000],  loss: 17754276.000000, mae: 1372.773193, mean_q: -173.803223
 2828/5000: episode: 2828, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3268.057, mean reward: -3268.057 [-3268.057, -3268.057], mean action: 1.000 [1.000, 1.000],  loss: 17161732.000000, mae: 1316.752319, mean_q: -175.166168
 2829/5000: episode: 2829, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4536.114, mean reward: -4536.114 [-4536.114, -4536.114], mean action: 1.000 [1.000, 1.000],  loss: 12428956.000000, mae: 1084.245605, mean_q: -174.673035
 2830/5000: episode: 2830, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4007.540, mean reward: -4007.540 [-4007.540, -4007.540], mean action: 1.000 [1.000, 1.000],  loss: 10635784.000000, mae: 1063.137573, mean_q: -174.594360
 2831/5000: episode: 2831, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4855.764, mean reward: -4855.764 [-4855.764, -4855.764], mean action: 1.000 [1.000, 1.000],  loss: 12389175.000000, mae: 1120.955078, mean_q: -174.436432
 2832/5000: episode: 2832, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -879.647, mean reward: -879.647 [-879.647, -879.647], mean action: 1.000 [1.000, 1.000],  loss: 13254644.000000, mae: 1218.426270, mean_q: -175.388672
 2833/5000: episode: 2833, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5241.690, mean reward: -5241.690 [-5241.690, -5241.690], mean action: 1.000 [1.000, 1.000],  loss: 10334405.000000, mae: 1017.561401, mean_q: -174.987198
 2834/5000: episode: 2834, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5784.360, mean reward: -5784.360 [-5784.360, -5784.360], mean action: 1.000 [1.000, 1.000],  loss: 11428076.000000, mae: 967.968384, mean_q: -175.602615
 2835/5000: episode: 2835, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2661.118, mean reward: -2661.118 [-2661.118, -2661.118], mean action: 1.000 [1.000, 1.000],  loss: 13568647.000000, mae: 1172.605835, mean_q: -175.325912
 2836/5000: episode: 2836, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3404.973, mean reward: -3404.973 [-3404.973, -3404.973], mean action: 1.000 [1.000, 1.000],  loss: 13833092.000000, mae: 1171.520264, mean_q: -175.631866
 2837/5000: episode: 2837, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4494.470, mean reward: -4494.470 [-4494.470, -4494.470], mean action: 1.000 [1.000, 1.000],  loss: 14107939.000000, mae: 1191.459229, mean_q: -175.197830
 2838/5000: episode: 2838, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2484.144, mean reward: -2484.144 [-2484.144, -2484.144], mean action: 1.000 [1.000, 1.000],  loss: 16747953.000000, mae: 1133.680176, mean_q: -175.977798
 2839/5000: episode: 2839, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3112.420, mean reward: -3112.420 [-3112.420, -3112.420], mean action: 1.000 [1.000, 1.000],  loss: 9945082.000000, mae: 1036.397461, mean_q: -176.352020
 2840/5000: episode: 2840, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -13846.742, mean reward: -13846.742 [-13846.742, -13846.742], mean action: 1.000 [1.000, 1.000],  loss: 14667000.000000, mae: 1154.115723, mean_q: -176.342255
 2841/5000: episode: 2841, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2098.903, mean reward: -2098.903 [-2098.903, -2098.903], mean action: 1.000 [1.000, 1.000],  loss: 9022176.000000, mae: 1004.861145, mean_q: -176.670898
 2842/5000: episode: 2842, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4665.231, mean reward: -4665.231 [-4665.231, -4665.231], mean action: 1.000 [1.000, 1.000],  loss: 14298789.000000, mae: 1188.948730, mean_q: -176.311005
 2843/5000: episode: 2843, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6168.084, mean reward: -6168.084 [-6168.084, -6168.084], mean action: 1.000 [1.000, 1.000],  loss: 19809444.000000, mae: 1449.407593, mean_q: -175.821106
 2844/5000: episode: 2844, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1277.259, mean reward: -1277.259 [-1277.259, -1277.259], mean action: 1.000 [1.000, 1.000],  loss: 12907105.000000, mae: 1212.317139, mean_q: -177.501587
 2845/5000: episode: 2845, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6046.819, mean reward: -6046.819 [-6046.819, -6046.819], mean action: 1.000 [1.000, 1.000],  loss: 12526566.000000, mae: 1127.068970, mean_q: -176.779938
 2846/5000: episode: 2846, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4462.111, mean reward: -4462.111 [-4462.111, -4462.111], mean action: 2.000 [2.000, 2.000],  loss: 16285314.000000, mae: 1331.540527, mean_q: -176.705078
 2847/5000: episode: 2847, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4182.462, mean reward: -4182.462 [-4182.462, -4182.462], mean action: 1.000 [1.000, 1.000],  loss: 11135143.000000, mae: 1061.366943, mean_q: -177.736832
 2848/5000: episode: 2848, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7357.167, mean reward: -7357.167 [-7357.167, -7357.167], mean action: 1.000 [1.000, 1.000],  loss: 15110974.000000, mae: 1183.440186, mean_q: -177.055573
 2849/5000: episode: 2849, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2365.914, mean reward: -2365.914 [-2365.914, -2365.914], mean action: 1.000 [1.000, 1.000],  loss: 15468120.000000, mae: 1234.754395, mean_q: -177.508621
 2850/5000: episode: 2850, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9956.858, mean reward: -9956.858 [-9956.858, -9956.858], mean action: 1.000 [1.000, 1.000],  loss: 9636990.000000, mae: 1050.228760, mean_q: -178.242432
 2851/5000: episode: 2851, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3356.426, mean reward: -3356.426 [-3356.426, -3356.426], mean action: 1.000 [1.000, 1.000],  loss: 10824086.000000, mae: 1083.446777, mean_q: -177.240387
 2852/5000: episode: 2852, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -358.256, mean reward: -358.256 [-358.256, -358.256], mean action: 1.000 [1.000, 1.000],  loss: 13147885.000000, mae: 1153.664062, mean_q: -178.139221
 2853/5000: episode: 2853, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5272.481, mean reward: -5272.481 [-5272.481, -5272.481], mean action: 1.000 [1.000, 1.000],  loss: 14748360.000000, mae: 1283.093872, mean_q: -177.688965
 2854/5000: episode: 2854, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6996.514, mean reward: -6996.514 [-6996.514, -6996.514], mean action: 1.000 [1.000, 1.000],  loss: 12101142.000000, mae: 990.259094, mean_q: -179.150986
 2855/5000: episode: 2855, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -487.528, mean reward: -487.528 [-487.528, -487.528], mean action: 1.000 [1.000, 1.000],  loss: 12136364.000000, mae: 1148.466064, mean_q: -178.081604
 2856/5000: episode: 2856, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5085.327, mean reward: -5085.327 [-5085.327, -5085.327], mean action: 1.000 [1.000, 1.000],  loss: 13442936.000000, mae: 1210.056152, mean_q: -178.512299
 2857/5000: episode: 2857, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4714.652, mean reward: -4714.652 [-4714.652, -4714.652], mean action: 1.000 [1.000, 1.000],  loss: 8801930.000000, mae: 988.132446, mean_q: -178.356781
 2858/5000: episode: 2858, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -11830.368, mean reward: -11830.368 [-11830.368, -11830.368], mean action: 1.000 [1.000, 1.000],  loss: 15997556.000000, mae: 1177.474365, mean_q: -179.005844
 2859/5000: episode: 2859, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1705.135, mean reward: -1705.135 [-1705.135, -1705.135], mean action: 1.000 [1.000, 1.000],  loss: 15659441.000000, mae: 1307.864136, mean_q: -179.287170
 2860/5000: episode: 2860, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2923.525, mean reward: -2923.525 [-2923.525, -2923.525], mean action: 3.000 [3.000, 3.000],  loss: 10621428.000000, mae: 1032.927490, mean_q: -179.915558
 2861/5000: episode: 2861, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -434.473, mean reward: -434.473 [-434.473, -434.473], mean action: 1.000 [1.000, 1.000],  loss: 12196196.000000, mae: 1114.645874, mean_q: -179.393066
 2862/5000: episode: 2862, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -616.112, mean reward: -616.112 [-616.112, -616.112], mean action: 1.000 [1.000, 1.000],  loss: 15456511.000000, mae: 1183.881104, mean_q: -179.416870
 2863/5000: episode: 2863, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5119.822, mean reward: -5119.822 [-5119.822, -5119.822], mean action: 1.000 [1.000, 1.000],  loss: 10260900.000000, mae: 1082.510498, mean_q: -179.125061
 2864/5000: episode: 2864, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4134.248, mean reward: -4134.248 [-4134.248, -4134.248], mean action: 1.000 [1.000, 1.000],  loss: 21307724.000000, mae: 1353.860596, mean_q: -179.452332
 2865/5000: episode: 2865, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1019.998, mean reward: -1019.998 [-1019.998, -1019.998], mean action: 1.000 [1.000, 1.000],  loss: 13674128.000000, mae: 1255.982666, mean_q: -179.389587
 2866/5000: episode: 2866, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1143.161, mean reward: -1143.161 [-1143.161, -1143.161], mean action: 1.000 [1.000, 1.000],  loss: 12921673.000000, mae: 1166.374512, mean_q: -179.882690
 2867/5000: episode: 2867, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4284.872, mean reward: -4284.872 [-4284.872, -4284.872], mean action: 1.000 [1.000, 1.000],  loss: 15409864.000000, mae: 1275.245117, mean_q: -180.514511
 2868/5000: episode: 2868, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1642.434, mean reward: -1642.434 [-1642.434, -1642.434], mean action: 1.000 [1.000, 1.000],  loss: 17385024.000000, mae: 1298.967041, mean_q: -179.802521
 2869/5000: episode: 2869, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3436.562, mean reward: -3436.562 [-3436.562, -3436.562], mean action: 1.000 [1.000, 1.000],  loss: 20124684.000000, mae: 1339.416016, mean_q: -179.641388
 2870/5000: episode: 2870, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1988.142, mean reward: -1988.142 [-1988.142, -1988.142], mean action: 2.000 [2.000, 2.000],  loss: 15002374.000000, mae: 1212.780518, mean_q: -180.766891
 2871/5000: episode: 2871, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -73.144, mean reward: -73.144 [-73.144, -73.144], mean action: 2.000 [2.000, 2.000],  loss: 14050936.000000, mae: 1275.095947, mean_q: -180.668564
 2872/5000: episode: 2872, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2103.477, mean reward: -2103.477 [-2103.477, -2103.477], mean action: 2.000 [2.000, 2.000],  loss: 19153482.000000, mae: 1413.006592, mean_q: -180.176361
 2873/5000: episode: 2873, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -931.507, mean reward: -931.507 [-931.507, -931.507], mean action: 2.000 [2.000, 2.000],  loss: 12408337.000000, mae: 1236.552979, mean_q: -180.969803
 2874/5000: episode: 2874, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1314.867, mean reward: -1314.867 [-1314.867, -1314.867], mean action: 2.000 [2.000, 2.000],  loss: 11868694.000000, mae: 1110.906250, mean_q: -181.402344
 2875/5000: episode: 2875, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -9629.789, mean reward: -9629.789 [-9629.789, -9629.789], mean action: 0.000 [0.000, 0.000],  loss: 17689644.000000, mae: 1338.264893, mean_q: -181.609100
 2876/5000: episode: 2876, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2795.467, mean reward: -2795.467 [-2795.467, -2795.467], mean action: 2.000 [2.000, 2.000],  loss: 15495394.000000, mae: 1294.837158, mean_q: -181.671600
 2877/5000: episode: 2877, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5646.517, mean reward: -5646.517 [-5646.517, -5646.517], mean action: 0.000 [0.000, 0.000],  loss: 10276384.000000, mae: 1101.641968, mean_q: -181.476654
 2878/5000: episode: 2878, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -100.060, mean reward: -100.060 [-100.060, -100.060], mean action: 2.000 [2.000, 2.000],  loss: 10875760.000000, mae: 1093.549438, mean_q: -182.459351
 2879/5000: episode: 2879, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3678.756, mean reward: -3678.756 [-3678.756, -3678.756], mean action: 2.000 [2.000, 2.000],  loss: 14879344.000000, mae: 1237.267578, mean_q: -181.971619
 2880/5000: episode: 2880, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2517.281, mean reward: -2517.281 [-2517.281, -2517.281], mean action: 2.000 [2.000, 2.000],  loss: 8863014.000000, mae: 945.389038, mean_q: -182.199280
 2881/5000: episode: 2881, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -527.167, mean reward: -527.167 [-527.167, -527.167], mean action: 2.000 [2.000, 2.000],  loss: 12861608.000000, mae: 1228.157104, mean_q: -181.362305
 2882/5000: episode: 2882, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -982.912, mean reward: -982.912 [-982.912, -982.912], mean action: 2.000 [2.000, 2.000],  loss: 16417536.000000, mae: 1276.385498, mean_q: -182.731293
 2883/5000: episode: 2883, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1195.869, mean reward: -1195.869 [-1195.869, -1195.869], mean action: 2.000 [2.000, 2.000],  loss: 12458656.000000, mae: 1157.244507, mean_q: -182.383423
 2884/5000: episode: 2884, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3130.202, mean reward: -3130.202 [-3130.202, -3130.202], mean action: 2.000 [2.000, 2.000],  loss: 9134532.000000, mae: 919.624634, mean_q: -184.225403
 2885/5000: episode: 2885, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2789.241, mean reward: -2789.241 [-2789.241, -2789.241], mean action: 2.000 [2.000, 2.000],  loss: 13880576.000000, mae: 1205.730591, mean_q: -182.484558
 2886/5000: episode: 2886, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1499.306, mean reward: -1499.306 [-1499.306, -1499.306], mean action: 2.000 [2.000, 2.000],  loss: 7736970.000000, mae: 950.694458, mean_q: -183.380615
 2887/5000: episode: 2887, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3574.905, mean reward: -3574.905 [-3574.905, -3574.905], mean action: 2.000 [2.000, 2.000],  loss: 21097870.000000, mae: 1522.066162, mean_q: -181.920471
 2888/5000: episode: 2888, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4393.841, mean reward: -4393.841 [-4393.841, -4393.841], mean action: 0.000 [0.000, 0.000],  loss: 14753312.000000, mae: 1242.356934, mean_q: -183.427612
 2889/5000: episode: 2889, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4109.454, mean reward: -4109.454 [-4109.454, -4109.454], mean action: 2.000 [2.000, 2.000],  loss: 17021492.000000, mae: 1302.563965, mean_q: -183.493866
 2890/5000: episode: 2890, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1707.537, mean reward: -1707.537 [-1707.537, -1707.537], mean action: 2.000 [2.000, 2.000],  loss: 8776758.000000, mae: 956.988708, mean_q: -183.677246
 2891/5000: episode: 2891, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3298.308, mean reward: -3298.308 [-3298.308, -3298.308], mean action: 2.000 [2.000, 2.000],  loss: 11084975.000000, mae: 1072.784424, mean_q: -183.484802
 2892/5000: episode: 2892, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3451.620, mean reward: -3451.620 [-3451.620, -3451.620], mean action: 2.000 [2.000, 2.000],  loss: 19688922.000000, mae: 1340.079224, mean_q: -182.881317
 2893/5000: episode: 2893, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -13767.384, mean reward: -13767.384 [-13767.384, -13767.384], mean action: 0.000 [0.000, 0.000],  loss: 12920670.000000, mae: 1157.356934, mean_q: -184.462555
 2894/5000: episode: 2894, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5071.970, mean reward: -5071.970 [-5071.970, -5071.970], mean action: 2.000 [2.000, 2.000],  loss: 15012096.000000, mae: 1256.004028, mean_q: -183.959320
 2895/5000: episode: 2895, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1122.140, mean reward: -1122.140 [-1122.140, -1122.140], mean action: 2.000 [2.000, 2.000],  loss: 12826246.000000, mae: 1149.652100, mean_q: -183.948334
 2896/5000: episode: 2896, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2660.915, mean reward: -2660.915 [-2660.915, -2660.915], mean action: 2.000 [2.000, 2.000],  loss: 11518393.000000, mae: 1132.561646, mean_q: -183.851837
 2897/5000: episode: 2897, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1391.047, mean reward: -1391.047 [-1391.047, -1391.047], mean action: 2.000 [2.000, 2.000],  loss: 12954134.000000, mae: 1110.916016, mean_q: -184.442001
 2898/5000: episode: 2898, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -440.100, mean reward: -440.100 [-440.100, -440.100], mean action: 2.000 [2.000, 2.000],  loss: 13686590.000000, mae: 1144.448364, mean_q: -184.712860
 2899/5000: episode: 2899, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1645.805, mean reward: -1645.805 [-1645.805, -1645.805], mean action: 1.000 [1.000, 1.000],  loss: 11084118.000000, mae: 963.143799, mean_q: -185.316589
 2900/5000: episode: 2900, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -2787.082, mean reward: -2787.082 [-2787.082, -2787.082], mean action: 2.000 [2.000, 2.000],  loss: 15025000.000000, mae: 1190.290039, mean_q: -185.541885
 2901/5000: episode: 2901, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -736.794, mean reward: -736.794 [-736.794, -736.794], mean action: 2.000 [2.000, 2.000],  loss: 11756012.000000, mae: 1147.359741, mean_q: -185.339813
 2902/5000: episode: 2902, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5444.879, mean reward: -5444.879 [-5444.879, -5444.879], mean action: 2.000 [2.000, 2.000],  loss: 12195952.000000, mae: 1072.866455, mean_q: -185.058151
 2903/5000: episode: 2903, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4746.630, mean reward: -4746.630 [-4746.630, -4746.630], mean action: 2.000 [2.000, 2.000],  loss: 9631232.000000, mae: 991.042603, mean_q: -185.502060
 2904/5000: episode: 2904, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3001.005, mean reward: -3001.005 [-3001.005, -3001.005], mean action: 2.000 [2.000, 2.000],  loss: 11340136.000000, mae: 1056.435303, mean_q: -186.440857
 2905/5000: episode: 2905, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -773.628, mean reward: -773.628 [-773.628, -773.628], mean action: 2.000 [2.000, 2.000],  loss: 17857304.000000, mae: 1305.783447, mean_q: -185.435745
 2906/5000: episode: 2906, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2523.308, mean reward: -2523.308 [-2523.308, -2523.308], mean action: 2.000 [2.000, 2.000],  loss: 13418162.000000, mae: 1193.422241, mean_q: -185.747040
 2907/5000: episode: 2907, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1977.038, mean reward: -1977.038 [-1977.038, -1977.038], mean action: 2.000 [2.000, 2.000],  loss: 13241810.000000, mae: 1200.686768, mean_q: -185.755768
 2908/5000: episode: 2908, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8341.736, mean reward: -8341.736 [-8341.736, -8341.736], mean action: 0.000 [0.000, 0.000],  loss: 18365256.000000, mae: 1387.217529, mean_q: -185.261841
 2909/5000: episode: 2909, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1230.690, mean reward: -1230.690 [-1230.690, -1230.690], mean action: 2.000 [2.000, 2.000],  loss: 12977600.000000, mae: 1246.182739, mean_q: -185.425995
 2910/5000: episode: 2910, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2885.123, mean reward: -2885.123 [-2885.123, -2885.123], mean action: 2.000 [2.000, 2.000],  loss: 16855918.000000, mae: 1250.171021, mean_q: -186.331177
 2911/5000: episode: 2911, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -4610.172, mean reward: -4610.172 [-4610.172, -4610.172], mean action: 2.000 [2.000, 2.000],  loss: 12862060.000000, mae: 1209.171265, mean_q: -186.388947
 2912/5000: episode: 2912, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5393.948, mean reward: -5393.948 [-5393.948, -5393.948], mean action: 3.000 [3.000, 3.000],  loss: 10289236.000000, mae: 1013.431946, mean_q: -186.963654
 2913/5000: episode: 2913, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1012.834, mean reward: -1012.834 [-1012.834, -1012.834], mean action: 2.000 [2.000, 2.000],  loss: 18088844.000000, mae: 1377.068970, mean_q: -185.481018
 2914/5000: episode: 2914, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7991.302, mean reward: -7991.302 [-7991.302, -7991.302], mean action: 2.000 [2.000, 2.000],  loss: 8851120.000000, mae: 967.395813, mean_q: -186.589142
 2915/5000: episode: 2915, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2959.695, mean reward: -2959.695 [-2959.695, -2959.695], mean action: 2.000 [2.000, 2.000],  loss: 10256224.000000, mae: 1069.120239, mean_q: -186.996002
 2916/5000: episode: 2916, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3479.900, mean reward: -3479.900 [-3479.900, -3479.900], mean action: 2.000 [2.000, 2.000],  loss: 9018198.000000, mae: 974.084595, mean_q: -187.520981
 2917/5000: episode: 2917, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -344.475, mean reward: -344.475 [-344.475, -344.475], mean action: 2.000 [2.000, 2.000],  loss: 13980128.000000, mae: 1208.747925, mean_q: -186.290115
 2918/5000: episode: 2918, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2805.012, mean reward: -2805.012 [-2805.012, -2805.012], mean action: 2.000 [2.000, 2.000],  loss: 11676804.000000, mae: 1126.448486, mean_q: -186.737503
 2919/5000: episode: 2919, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5204.374, mean reward: -5204.374 [-5204.374, -5204.374], mean action: 2.000 [2.000, 2.000],  loss: 13353182.000000, mae: 1160.084473, mean_q: -187.647675
 2920/5000: episode: 2920, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -432.235, mean reward: -432.235 [-432.235, -432.235], mean action: 2.000 [2.000, 2.000],  loss: 16500536.000000, mae: 1286.647461, mean_q: -187.622238
 2921/5000: episode: 2921, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4400.754, mean reward: -4400.754 [-4400.754, -4400.754], mean action: 2.000 [2.000, 2.000],  loss: 14560080.000000, mae: 1239.806396, mean_q: -187.666656
 2922/5000: episode: 2922, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -819.097, mean reward: -819.097 [-819.097, -819.097], mean action: 2.000 [2.000, 2.000],  loss: 15348012.000000, mae: 1204.098633, mean_q: -187.760376
 2923/5000: episode: 2923, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -185.879, mean reward: -185.879 [-185.879, -185.879], mean action: 2.000 [2.000, 2.000],  loss: 10455012.000000, mae: 1028.398193, mean_q: -188.259674
 2924/5000: episode: 2924, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1732.385, mean reward: -1732.385 [-1732.385, -1732.385], mean action: 2.000 [2.000, 2.000],  loss: 14933882.000000, mae: 1233.910278, mean_q: -187.918854
 2925/5000: episode: 2925, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3049.546, mean reward: -3049.546 [-3049.546, -3049.546], mean action: 2.000 [2.000, 2.000],  loss: 12325742.000000, mae: 1142.022461, mean_q: -189.069702
 2926/5000: episode: 2926, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -595.816, mean reward: -595.816 [-595.816, -595.816], mean action: 2.000 [2.000, 2.000],  loss: 10812611.000000, mae: 1080.758911, mean_q: -188.931946
 2927/5000: episode: 2927, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3670.933, mean reward: -3670.933 [-3670.933, -3670.933], mean action: 2.000 [2.000, 2.000],  loss: 14665135.000000, mae: 1209.261230, mean_q: -188.699936
 2928/5000: episode: 2928, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2451.504, mean reward: -2451.504 [-2451.504, -2451.504], mean action: 2.000 [2.000, 2.000],  loss: 18942100.000000, mae: 1351.020508, mean_q: -188.780121
 2929/5000: episode: 2929, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3673.223, mean reward: -3673.223 [-3673.223, -3673.223], mean action: 2.000 [2.000, 2.000],  loss: 11424245.000000, mae: 1104.399658, mean_q: -188.821472
 2930/5000: episode: 2930, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3461.533, mean reward: -3461.533 [-3461.533, -3461.533], mean action: 1.000 [1.000, 1.000],  loss: 7190867.500000, mae: 903.816772, mean_q: -189.678375
 2931/5000: episode: 2931, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4030.934, mean reward: -4030.934 [-4030.934, -4030.934], mean action: 2.000 [2.000, 2.000],  loss: 11788835.000000, mae: 1099.726074, mean_q: -188.997543
 2932/5000: episode: 2932, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8997.857, mean reward: -8997.857 [-8997.857, -8997.857], mean action: 2.000 [2.000, 2.000],  loss: 12471514.000000, mae: 1132.147583, mean_q: -189.238861
 2933/5000: episode: 2933, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4006.808, mean reward: -4006.808 [-4006.808, -4006.808], mean action: 2.000 [2.000, 2.000],  loss: 10063698.000000, mae: 1045.305054, mean_q: -188.911957
 2934/5000: episode: 2934, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1728.027, mean reward: -1728.027 [-1728.027, -1728.027], mean action: 2.000 [2.000, 2.000],  loss: 11019436.000000, mae: 1041.074707, mean_q: -190.124756
 2935/5000: episode: 2935, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1838.686, mean reward: -1838.686 [-1838.686, -1838.686], mean action: 2.000 [2.000, 2.000],  loss: 11756468.000000, mae: 1112.930908, mean_q: -189.835327
 2936/5000: episode: 2936, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2681.898, mean reward: -2681.898 [-2681.898, -2681.898], mean action: 2.000 [2.000, 2.000],  loss: 13691350.000000, mae: 1189.282227, mean_q: -189.752029
 2937/5000: episode: 2937, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1114.104, mean reward: -1114.104 [-1114.104, -1114.104], mean action: 2.000 [2.000, 2.000],  loss: 11647910.000000, mae: 1067.378174, mean_q: -190.156052
 2938/5000: episode: 2938, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -630.409, mean reward: -630.409 [-630.409, -630.409], mean action: 2.000 [2.000, 2.000],  loss: 17871666.000000, mae: 1269.552612, mean_q: -189.747238
 2939/5000: episode: 2939, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -751.946, mean reward: -751.946 [-751.946, -751.946], mean action: 2.000 [2.000, 2.000],  loss: 11137932.000000, mae: 1003.492798, mean_q: -190.124954
 2940/5000: episode: 2940, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3081.711, mean reward: -3081.711 [-3081.711, -3081.711], mean action: 2.000 [2.000, 2.000],  loss: 17173952.000000, mae: 1380.040649, mean_q: -189.356964
 2941/5000: episode: 2941, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1793.231, mean reward: -1793.231 [-1793.231, -1793.231], mean action: 2.000 [2.000, 2.000],  loss: 15694986.000000, mae: 1253.952271, mean_q: -190.687500
 2942/5000: episode: 2942, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1838.647, mean reward: -1838.647 [-1838.647, -1838.647], mean action: 2.000 [2.000, 2.000],  loss: 11512574.000000, mae: 1084.387451, mean_q: -190.721130
 2943/5000: episode: 2943, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2495.227, mean reward: -2495.227 [-2495.227, -2495.227], mean action: 2.000 [2.000, 2.000],  loss: 16127038.000000, mae: 1264.786621, mean_q: -189.757614
 2944/5000: episode: 2944, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6378.610, mean reward: -6378.610 [-6378.610, -6378.610], mean action: 2.000 [2.000, 2.000],  loss: 14543879.000000, mae: 1145.230835, mean_q: -190.895432
 2945/5000: episode: 2945, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5373.443, mean reward: -5373.443 [-5373.443, -5373.443], mean action: 2.000 [2.000, 2.000],  loss: 6138680.500000, mae: 793.826904, mean_q: -191.389435
 2946/5000: episode: 2946, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6738.073, mean reward: -6738.073 [-6738.073, -6738.073], mean action: 2.000 [2.000, 2.000],  loss: 8520333.000000, mae: 986.413696, mean_q: -191.069458
 2947/5000: episode: 2947, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -941.786, mean reward: -941.786 [-941.786, -941.786], mean action: 2.000 [2.000, 2.000],  loss: 11538160.000000, mae: 1046.509155, mean_q: -191.318024
 2948/5000: episode: 2948, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2244.479, mean reward: -2244.479 [-2244.479, -2244.479], mean action: 2.000 [2.000, 2.000],  loss: 14864346.000000, mae: 1320.290405, mean_q: -191.205841
 2949/5000: episode: 2949, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5521.394, mean reward: -5521.394 [-5521.394, -5521.394], mean action: 2.000 [2.000, 2.000],  loss: 11335662.000000, mae: 1095.530273, mean_q: -191.494293
 2950/5000: episode: 2950, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1044.990, mean reward: -1044.990 [-1044.990, -1044.990], mean action: 2.000 [2.000, 2.000],  loss: 9738598.000000, mae: 1048.627563, mean_q: -191.818420
 2951/5000: episode: 2951, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4426.198, mean reward: -4426.198 [-4426.198, -4426.198], mean action: 2.000 [2.000, 2.000],  loss: 12238358.000000, mae: 1127.244141, mean_q: -192.333649
 2952/5000: episode: 2952, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2235.074, mean reward: -2235.074 [-2235.074, -2235.074], mean action: 2.000 [2.000, 2.000],  loss: 14870344.000000, mae: 1184.719116, mean_q: -193.069458
 2953/5000: episode: 2953, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -679.432, mean reward: -679.432 [-679.432, -679.432], mean action: 2.000 [2.000, 2.000],  loss: 13161079.000000, mae: 1275.128296, mean_q: -191.501038
 2954/5000: episode: 2954, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8676.129, mean reward: -8676.129 [-8676.129, -8676.129], mean action: 2.000 [2.000, 2.000],  loss: 13674600.000000, mae: 1208.583740, mean_q: -192.075806
 2955/5000: episode: 2955, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7649.739, mean reward: -7649.739 [-7649.739, -7649.739], mean action: 3.000 [3.000, 3.000],  loss: 19162314.000000, mae: 1328.065186, mean_q: -192.619476
 2956/5000: episode: 2956, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -797.927, mean reward: -797.927 [-797.927, -797.927], mean action: 2.000 [2.000, 2.000],  loss: 11132338.000000, mae: 1165.630859, mean_q: -192.061020
 2957/5000: episode: 2957, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2194.083, mean reward: -2194.083 [-2194.083, -2194.083], mean action: 2.000 [2.000, 2.000],  loss: 10355967.000000, mae: 994.145020, mean_q: -193.033218
 2958/5000: episode: 2958, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -242.889, mean reward: -242.889 [-242.889, -242.889], mean action: 2.000 [2.000, 2.000],  loss: 13645037.000000, mae: 1212.710693, mean_q: -192.874710
 2959/5000: episode: 2959, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -23.819, mean reward: -23.819 [-23.819, -23.819], mean action: 2.000 [2.000, 2.000],  loss: 9633618.000000, mae: 963.795166, mean_q: -193.005554
 2960/5000: episode: 2960, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4291.406, mean reward: -4291.406 [-4291.406, -4291.406], mean action: 2.000 [2.000, 2.000],  loss: 13822198.000000, mae: 1232.286499, mean_q: -193.050278
 2961/5000: episode: 2961, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1780.374, mean reward: -1780.374 [-1780.374, -1780.374], mean action: 2.000 [2.000, 2.000],  loss: 12583410.000000, mae: 1132.825684, mean_q: -193.407043
 2962/5000: episode: 2962, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7213.546, mean reward: -7213.546 [-7213.546, -7213.546], mean action: 2.000 [2.000, 2.000],  loss: 9356227.000000, mae: 1023.005676, mean_q: -193.337921
 2963/5000: episode: 2963, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5004.071, mean reward: -5004.071 [-5004.071, -5004.071], mean action: 2.000 [2.000, 2.000],  loss: 12314395.000000, mae: 1161.474365, mean_q: -193.263519
 2964/5000: episode: 2964, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3724.309, mean reward: -3724.309 [-3724.309, -3724.309], mean action: 2.000 [2.000, 2.000],  loss: 11281395.000000, mae: 1096.151611, mean_q: -193.511658
 2965/5000: episode: 2965, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8254.960, mean reward: -8254.960 [-8254.960, -8254.960], mean action: 2.000 [2.000, 2.000],  loss: 15183015.000000, mae: 1229.179688, mean_q: -194.035339
 2966/5000: episode: 2966, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -153.257, mean reward: -153.257 [-153.257, -153.257], mean action: 2.000 [2.000, 2.000],  loss: 15072418.000000, mae: 1193.731934, mean_q: -194.257797
 2967/5000: episode: 2967, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7887.465, mean reward: -7887.465 [-7887.465, -7887.465], mean action: 1.000 [1.000, 1.000],  loss: 16819840.000000, mae: 1373.875854, mean_q: -193.967484
 2968/5000: episode: 2968, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4097.531, mean reward: -4097.531 [-4097.531, -4097.531], mean action: 2.000 [2.000, 2.000],  loss: 8163462.000000, mae: 956.532715, mean_q: -194.920975
 2969/5000: episode: 2969, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2206.832, mean reward: -2206.832 [-2206.832, -2206.832], mean action: 2.000 [2.000, 2.000],  loss: 17367530.000000, mae: 1341.874023, mean_q: -194.047668
 2970/5000: episode: 2970, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4002.376, mean reward: -4002.376 [-4002.376, -4002.376], mean action: 2.000 [2.000, 2.000],  loss: 12213442.000000, mae: 1134.285034, mean_q: -194.236633
 2971/5000: episode: 2971, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -105.968, mean reward: -105.968 [-105.968, -105.968], mean action: 2.000 [2.000, 2.000],  loss: 10337485.000000, mae: 1010.944397, mean_q: -194.679581
 2972/5000: episode: 2972, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2527.837, mean reward: -2527.837 [-2527.837, -2527.837], mean action: 2.000 [2.000, 2.000],  loss: 14162170.000000, mae: 1178.838379, mean_q: -195.193954
 2973/5000: episode: 2973, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1993.190, mean reward: -1993.190 [-1993.190, -1993.190], mean action: 2.000 [2.000, 2.000],  loss: 10756327.000000, mae: 1075.048828, mean_q: -194.673431
 2974/5000: episode: 2974, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3018.966, mean reward: -3018.966 [-3018.966, -3018.966], mean action: 2.000 [2.000, 2.000],  loss: 16053512.000000, mae: 1371.918213, mean_q: -195.498901
 2975/5000: episode: 2975, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -840.139, mean reward: -840.139 [-840.139, -840.139], mean action: 2.000 [2.000, 2.000],  loss: 15643279.000000, mae: 1193.833008, mean_q: -195.090698
 2976/5000: episode: 2976, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3746.955, mean reward: -3746.955 [-3746.955, -3746.955], mean action: 2.000 [2.000, 2.000],  loss: 14201701.000000, mae: 1311.466675, mean_q: -195.217651
 2977/5000: episode: 2977, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -1508.987, mean reward: -1508.987 [-1508.987, -1508.987], mean action: 2.000 [2.000, 2.000],  loss: 16195248.000000, mae: 1284.075928, mean_q: -194.506851
 2978/5000: episode: 2978, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -723.895, mean reward: -723.895 [-723.895, -723.895], mean action: 2.000 [2.000, 2.000],  loss: 12406141.000000, mae: 1156.779175, mean_q: -195.857330
 2979/5000: episode: 2979, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -140.017, mean reward: -140.017 [-140.017, -140.017], mean action: 2.000 [2.000, 2.000],  loss: 9384590.000000, mae: 997.946655, mean_q: -195.750000
 2980/5000: episode: 2980, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -979.738, mean reward: -979.738 [-979.738, -979.738], mean action: 2.000 [2.000, 2.000],  loss: 14365648.000000, mae: 1232.506958, mean_q: -195.923660
 2981/5000: episode: 2981, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1287.312, mean reward: -1287.312 [-1287.312, -1287.312], mean action: 2.000 [2.000, 2.000],  loss: 13139700.000000, mae: 1248.350342, mean_q: -196.595795
 2982/5000: episode: 2982, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2668.702, mean reward: -2668.702 [-2668.702, -2668.702], mean action: 3.000 [3.000, 3.000],  loss: 14168773.000000, mae: 1257.035522, mean_q: -196.123474
 2983/5000: episode: 2983, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -803.848, mean reward: -803.848 [-803.848, -803.848], mean action: 2.000 [2.000, 2.000],  loss: 14327860.000000, mae: 1233.912109, mean_q: -197.003479
 2984/5000: episode: 2984, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3993.094, mean reward: -3993.094 [-3993.094, -3993.094], mean action: 2.000 [2.000, 2.000],  loss: 16165979.000000, mae: 1226.920532, mean_q: -196.022018
 2985/5000: episode: 2985, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -950.099, mean reward: -950.099 [-950.099, -950.099], mean action: 2.000 [2.000, 2.000],  loss: 14226331.000000, mae: 1271.442749, mean_q: -196.487564
 2986/5000: episode: 2986, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -570.790, mean reward: -570.790 [-570.790, -570.790], mean action: 2.000 [2.000, 2.000],  loss: 11448128.000000, mae: 1098.822510, mean_q: -196.410583
 2987/5000: episode: 2987, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3421.181, mean reward: -3421.181 [-3421.181, -3421.181], mean action: 2.000 [2.000, 2.000],  loss: 10143034.000000, mae: 1081.499634, mean_q: -197.639420
 2988/5000: episode: 2988, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -679.494, mean reward: -679.494 [-679.494, -679.494], mean action: 2.000 [2.000, 2.000],  loss: 13189460.000000, mae: 1189.544189, mean_q: -197.190887
 2989/5000: episode: 2989, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3084.049, mean reward: -3084.049 [-3084.049, -3084.049], mean action: 2.000 [2.000, 2.000],  loss: 12275420.000000, mae: 1178.023193, mean_q: -198.097473
 2990/5000: episode: 2990, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1911.317, mean reward: -1911.317 [-1911.317, -1911.317], mean action: 2.000 [2.000, 2.000],  loss: 14296898.000000, mae: 1149.430054, mean_q: -197.853271
 2991/5000: episode: 2991, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -386.247, mean reward: -386.247 [-386.247, -386.247], mean action: 2.000 [2.000, 2.000],  loss: 11673352.000000, mae: 1100.913818, mean_q: -197.314758
 2992/5000: episode: 2992, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6373.293, mean reward: -6373.293 [-6373.293, -6373.293], mean action: 2.000 [2.000, 2.000],  loss: 7738397.000000, mae: 869.613892, mean_q: -198.268158
 2993/5000: episode: 2993, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1182.757, mean reward: -1182.757 [-1182.757, -1182.757], mean action: 2.000 [2.000, 2.000],  loss: 10613097.000000, mae: 996.219482, mean_q: -197.898880
 2994/5000: episode: 2994, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -10856.246, mean reward: -10856.246 [-10856.246, -10856.246], mean action: 0.000 [0.000, 0.000],  loss: 7843364.500000, mae: 867.571899, mean_q: -198.506287
 2995/5000: episode: 2995, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -360.454, mean reward: -360.454 [-360.454, -360.454], mean action: 2.000 [2.000, 2.000],  loss: 10538730.000000, mae: 1081.317505, mean_q: -198.286835
 2996/5000: episode: 2996, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -1056.376, mean reward: -1056.376 [-1056.376, -1056.376], mean action: 2.000 [2.000, 2.000],  loss: 10602068.000000, mae: 1125.517578, mean_q: -198.745728
 2997/5000: episode: 2997, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -156.778, mean reward: -156.778 [-156.778, -156.778], mean action: 2.000 [2.000, 2.000],  loss: 9783264.000000, mae: 1045.870239, mean_q: -199.367279
 2998/5000: episode: 2998, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -818.254, mean reward: -818.254 [-818.254, -818.254], mean action: 2.000 [2.000, 2.000],  loss: 8367596.500000, mae: 964.914795, mean_q: -199.696198
 2999/5000: episode: 2999, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -2167.269, mean reward: -2167.269 [-2167.269, -2167.269], mean action: 2.000 [2.000, 2.000],  loss: 15394915.000000, mae: 1203.563232, mean_q: -199.438217
 3000/5000: episode: 3000, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2495.881, mean reward: -2495.881 [-2495.881, -2495.881], mean action: 2.000 [2.000, 2.000],  loss: 15598492.000000, mae: 1272.768921, mean_q: -199.504318
 3001/5000: episode: 3001, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -824.750, mean reward: -824.750 [-824.750, -824.750], mean action: 2.000 [2.000, 2.000],  loss: 7650538.000000, mae: 931.782104, mean_q: -199.819534
 3002/5000: episode: 3002, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -291.531, mean reward: -291.531 [-291.531, -291.531], mean action: 2.000 [2.000, 2.000],  loss: 15302370.000000, mae: 1325.322754, mean_q: -199.547714
 3003/5000: episode: 3003, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1089.552, mean reward: -1089.552 [-1089.552, -1089.552], mean action: 2.000 [2.000, 2.000],  loss: 12258799.000000, mae: 1067.311035, mean_q: -199.154724
 3004/5000: episode: 3004, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3415.264, mean reward: -3415.264 [-3415.264, -3415.264], mean action: 2.000 [2.000, 2.000],  loss: 11116000.000000, mae: 1116.440918, mean_q: -200.360779
 3005/5000: episode: 3005, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -7710.616, mean reward: -7710.616 [-7710.616, -7710.616], mean action: 2.000 [2.000, 2.000],  loss: 15590546.000000, mae: 1263.221558, mean_q: -199.624023
 3006/5000: episode: 3006, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3821.520, mean reward: -3821.520 [-3821.520, -3821.520], mean action: 1.000 [1.000, 1.000],  loss: 16608736.000000, mae: 1295.837891, mean_q: -200.582031
 3007/5000: episode: 3007, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6344.607, mean reward: -6344.607 [-6344.607, -6344.607], mean action: 2.000 [2.000, 2.000],  loss: 9241105.000000, mae: 1002.048218, mean_q: -200.800461
 3008/5000: episode: 3008, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -905.026, mean reward: -905.026 [-905.026, -905.026], mean action: 2.000 [2.000, 2.000],  loss: 6805283.000000, mae: 906.738647, mean_q: -201.048767
 3009/5000: episode: 3009, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2486.887, mean reward: -2486.887 [-2486.887, -2486.887], mean action: 2.000 [2.000, 2.000],  loss: 14872865.000000, mae: 1271.816406, mean_q: -201.074310
 3010/5000: episode: 3010, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8585.481, mean reward: -8585.481 [-8585.481, -8585.481], mean action: 2.000 [2.000, 2.000],  loss: 10479889.000000, mae: 1096.618408, mean_q: -200.976791
 3011/5000: episode: 3011, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2078.260, mean reward: -2078.260 [-2078.260, -2078.260], mean action: 2.000 [2.000, 2.000],  loss: 9326766.000000, mae: 955.707458, mean_q: -201.507217
 3012/5000: episode: 3012, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -242.434, mean reward: -242.434 [-242.434, -242.434], mean action: 2.000 [2.000, 2.000],  loss: 12243925.000000, mae: 1141.140137, mean_q: -201.919403
 3013/5000: episode: 3013, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -379.335, mean reward: -379.335 [-379.335, -379.335], mean action: 2.000 [2.000, 2.000],  loss: 9755650.000000, mae: 1018.702026, mean_q: -202.379333
 3014/5000: episode: 3014, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -9827.431, mean reward: -9827.431 [-9827.431, -9827.431], mean action: 2.000 [2.000, 2.000],  loss: 11256078.000000, mae: 1100.447144, mean_q: -201.812805
 3015/5000: episode: 3015, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9010.541, mean reward: -9010.541 [-9010.541, -9010.541], mean action: 2.000 [2.000, 2.000],  loss: 11838238.000000, mae: 1106.479370, mean_q: -201.581284
 3016/5000: episode: 3016, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -310.805, mean reward: -310.805 [-310.805, -310.805], mean action: 2.000 [2.000, 2.000],  loss: 9346612.000000, mae: 977.828613, mean_q: -202.048019
 3017/5000: episode: 3017, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1546.241, mean reward: -1546.241 [-1546.241, -1546.241], mean action: 2.000 [2.000, 2.000],  loss: 11642430.000000, mae: 1132.342285, mean_q: -201.189117
 3018/5000: episode: 3018, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -633.228, mean reward: -633.228 [-633.228, -633.228], mean action: 2.000 [2.000, 2.000],  loss: 13750753.000000, mae: 1301.125732, mean_q: -202.398254
 3019/5000: episode: 3019, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4357.540, mean reward: -4357.540 [-4357.540, -4357.540], mean action: 2.000 [2.000, 2.000],  loss: 13110320.000000, mae: 1138.665283, mean_q: -201.470657
 3020/5000: episode: 3020, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3924.994, mean reward: -3924.994 [-3924.994, -3924.994], mean action: 2.000 [2.000, 2.000],  loss: 10145024.000000, mae: 1051.708984, mean_q: -203.407928
 3021/5000: episode: 3021, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1114.088, mean reward: -1114.088 [-1114.088, -1114.088], mean action: 2.000 [2.000, 2.000],  loss: 14539688.000000, mae: 1154.465698, mean_q: -202.367538
 3022/5000: episode: 3022, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1762.613, mean reward: -1762.613 [-1762.613, -1762.613], mean action: 2.000 [2.000, 2.000],  loss: 17007800.000000, mae: 1242.863281, mean_q: -201.564453
 3023/5000: episode: 3023, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -7670.867, mean reward: -7670.867 [-7670.867, -7670.867], mean action: 2.000 [2.000, 2.000],  loss: 15433793.000000, mae: 1312.339355, mean_q: -202.686676
 3024/5000: episode: 3024, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1135.190, mean reward: -1135.190 [-1135.190, -1135.190], mean action: 2.000 [2.000, 2.000],  loss: 14298962.000000, mae: 1254.703003, mean_q: -203.813354
 3025/5000: episode: 3025, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -38.087, mean reward: -38.087 [-38.087, -38.087], mean action: 2.000 [2.000, 2.000],  loss: 10976892.000000, mae: 1098.085449, mean_q: -203.310898
 3026/5000: episode: 3026, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2178.717, mean reward: -2178.717 [-2178.717, -2178.717], mean action: 2.000 [2.000, 2.000],  loss: 11215809.000000, mae: 1113.415283, mean_q: -202.206482
 3027/5000: episode: 3027, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5.491, mean reward: -5.491 [-5.491, -5.491], mean action: 2.000 [2.000, 2.000],  loss: 19424242.000000, mae: 1347.268677, mean_q: -203.598938
 3028/5000: episode: 3028, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3525.441, mean reward: -3525.441 [-3525.441, -3525.441], mean action: 2.000 [2.000, 2.000],  loss: 8069372.000000, mae: 969.423584, mean_q: -203.559937
 3029/5000: episode: 3029, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1083.819, mean reward: -1083.819 [-1083.819, -1083.819], mean action: 2.000 [2.000, 2.000],  loss: 23053704.000000, mae: 1491.230225, mean_q: -203.191620
 3030/5000: episode: 3030, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2232.685, mean reward: -2232.685 [-2232.685, -2232.685], mean action: 2.000 [2.000, 2.000],  loss: 10784180.000000, mae: 1127.888428, mean_q: -203.367813
 3031/5000: episode: 3031, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -517.885, mean reward: -517.885 [-517.885, -517.885], mean action: 2.000 [2.000, 2.000],  loss: 12320607.000000, mae: 1067.560913, mean_q: -204.205017
 3032/5000: episode: 3032, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7546.170, mean reward: -7546.170 [-7546.170, -7546.170], mean action: 2.000 [2.000, 2.000],  loss: 7592023.000000, mae: 962.104614, mean_q: -204.131073
 3033/5000: episode: 3033, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1227.900, mean reward: -1227.900 [-1227.900, -1227.900], mean action: 2.000 [2.000, 2.000],  loss: 15509774.000000, mae: 1224.683105, mean_q: -204.471344
 3034/5000: episode: 3034, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2427.230, mean reward: -2427.230 [-2427.230, -2427.230], mean action: 2.000 [2.000, 2.000],  loss: 11384699.000000, mae: 990.583191, mean_q: -203.783936
 3035/5000: episode: 3035, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5852.314, mean reward: -5852.314 [-5852.314, -5852.314], mean action: 2.000 [2.000, 2.000],  loss: 13304804.000000, mae: 1201.873535, mean_q: -205.261017
 3036/5000: episode: 3036, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -990.073, mean reward: -990.073 [-990.073, -990.073], mean action: 2.000 [2.000, 2.000],  loss: 19567108.000000, mae: 1397.709106, mean_q: -204.621613
 3037/5000: episode: 3037, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3017.113, mean reward: -3017.113 [-3017.113, -3017.113], mean action: 2.000 [2.000, 2.000],  loss: 17045768.000000, mae: 1270.017822, mean_q: -205.190018
 3038/5000: episode: 3038, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9442.267, mean reward: -9442.267 [-9442.267, -9442.267], mean action: 2.000 [2.000, 2.000],  loss: 13062703.000000, mae: 1128.326538, mean_q: -204.811798
 3039/5000: episode: 3039, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -199.560, mean reward: -199.560 [-199.560, -199.560], mean action: 2.000 [2.000, 2.000],  loss: 11305782.000000, mae: 1075.037354, mean_q: -206.152740
 3040/5000: episode: 3040, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3949.330, mean reward: -3949.330 [-3949.330, -3949.330], mean action: 2.000 [2.000, 2.000],  loss: 20032472.000000, mae: 1441.010742, mean_q: -205.553970
 3041/5000: episode: 3041, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2376.467, mean reward: -2376.467 [-2376.467, -2376.467], mean action: 2.000 [2.000, 2.000],  loss: 6507239.000000, mae: 854.272461, mean_q: -206.304443
 3042/5000: episode: 3042, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7724.550, mean reward: -7724.550 [-7724.550, -7724.550], mean action: 2.000 [2.000, 2.000],  loss: 7494693.500000, mae: 893.385254, mean_q: -205.942062
 3043/5000: episode: 3043, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1252.821, mean reward: -1252.821 [-1252.821, -1252.821], mean action: 2.000 [2.000, 2.000],  loss: 13159907.000000, mae: 1108.688477, mean_q: -206.074127
 3044/5000: episode: 3044, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1518.342, mean reward: -1518.342 [-1518.342, -1518.342], mean action: 2.000 [2.000, 2.000],  loss: 16624460.000000, mae: 1301.194458, mean_q: -205.031494
 3045/5000: episode: 3045, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3209.976, mean reward: -3209.976 [-3209.976, -3209.976], mean action: 2.000 [2.000, 2.000],  loss: 16572317.000000, mae: 1250.238037, mean_q: -205.331177
 3046/5000: episode: 3046, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -69.839, mean reward: -69.839 [-69.839, -69.839], mean action: 2.000 [2.000, 2.000],  loss: 13346311.000000, mae: 1176.853760, mean_q: -207.245483
 3047/5000: episode: 3047, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -411.487, mean reward: -411.487 [-411.487, -411.487], mean action: 2.000 [2.000, 2.000],  loss: 11862084.000000, mae: 1144.112183, mean_q: -207.202789
 3048/5000: episode: 3048, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1939.693, mean reward: -1939.693 [-1939.693, -1939.693], mean action: 2.000 [2.000, 2.000],  loss: 15702400.000000, mae: 1376.361084, mean_q: -206.225464
 3049/5000: episode: 3049, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1807.082, mean reward: -1807.082 [-1807.082, -1807.082], mean action: 2.000 [2.000, 2.000],  loss: 10787732.000000, mae: 1079.981689, mean_q: -207.028839
 3050/5000: episode: 3050, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2104.603, mean reward: -2104.603 [-2104.603, -2104.603], mean action: 2.000 [2.000, 2.000],  loss: 5169114.000000, mae: 805.684692, mean_q: -208.166748
 3051/5000: episode: 3051, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5762.252, mean reward: -5762.252 [-5762.252, -5762.252], mean action: 0.000 [0.000, 0.000],  loss: 14823347.000000, mae: 1319.616943, mean_q: -207.657730
 3052/5000: episode: 3052, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -677.999, mean reward: -677.999 [-677.999, -677.999], mean action: 2.000 [2.000, 2.000],  loss: 15039662.000000, mae: 1284.761230, mean_q: -207.306976
 3053/5000: episode: 3053, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4271.550, mean reward: -4271.550 [-4271.550, -4271.550], mean action: 2.000 [2.000, 2.000],  loss: 9481410.000000, mae: 1027.716064, mean_q: -208.320038
 3054/5000: episode: 3054, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2217.084, mean reward: -2217.084 [-2217.084, -2217.084], mean action: 2.000 [2.000, 2.000],  loss: 9609246.000000, mae: 1018.843018, mean_q: -208.120514
 3055/5000: episode: 3055, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3241.209, mean reward: -3241.209 [-3241.209, -3241.209], mean action: 2.000 [2.000, 2.000],  loss: 9255050.000000, mae: 1041.736572, mean_q: -207.631744
 3056/5000: episode: 3056, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2883.267, mean reward: -2883.267 [-2883.267, -2883.267], mean action: 2.000 [2.000, 2.000],  loss: 12373762.000000, mae: 1108.385254, mean_q: -208.254623
 3057/5000: episode: 3057, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -3256.078, mean reward: -3256.078 [-3256.078, -3256.078], mean action: 2.000 [2.000, 2.000],  loss: 12528385.000000, mae: 1187.418091, mean_q: -208.793945
 3058/5000: episode: 3058, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4563.360, mean reward: -4563.360 [-4563.360, -4563.360], mean action: 0.000 [0.000, 0.000],  loss: 13827253.000000, mae: 1195.122070, mean_q: -208.750076
 3059/5000: episode: 3059, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3636.491, mean reward: -3636.491 [-3636.491, -3636.491], mean action: 1.000 [1.000, 1.000],  loss: 13415808.000000, mae: 1273.362549, mean_q: -208.904648
 3060/5000: episode: 3060, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -774.510, mean reward: -774.510 [-774.510, -774.510], mean action: 2.000 [2.000, 2.000],  loss: 12394854.000000, mae: 1084.359497, mean_q: -208.070511
 3061/5000: episode: 3061, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3716.043, mean reward: -3716.043 [-3716.043, -3716.043], mean action: 2.000 [2.000, 2.000],  loss: 8545712.000000, mae: 937.267029, mean_q: -209.185669
 3062/5000: episode: 3062, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -361.759, mean reward: -361.759 [-361.759, -361.759], mean action: 2.000 [2.000, 2.000],  loss: 11520554.000000, mae: 1086.460327, mean_q: -208.437988
 3063/5000: episode: 3063, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -998.627, mean reward: -998.627 [-998.627, -998.627], mean action: 2.000 [2.000, 2.000],  loss: 10844522.000000, mae: 1092.733521, mean_q: -209.215118
 3064/5000: episode: 3064, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -458.852, mean reward: -458.852 [-458.852, -458.852], mean action: 3.000 [3.000, 3.000],  loss: 10122751.000000, mae: 1055.199707, mean_q: -208.846710
 3065/5000: episode: 3065, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3056.946, mean reward: -3056.946 [-3056.946, -3056.946], mean action: 0.000 [0.000, 0.000],  loss: 10338161.000000, mae: 1080.596436, mean_q: -209.173080
 3066/5000: episode: 3066, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -571.007, mean reward: -571.007 [-571.007, -571.007], mean action: 2.000 [2.000, 2.000],  loss: 16550671.000000, mae: 1195.874023, mean_q: -209.145035
 3067/5000: episode: 3067, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -378.481, mean reward: -378.481 [-378.481, -378.481], mean action: 2.000 [2.000, 2.000],  loss: 14590100.000000, mae: 1148.953613, mean_q: -209.593750
 3068/5000: episode: 3068, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1993.530, mean reward: -1993.530 [-1993.530, -1993.530], mean action: 2.000 [2.000, 2.000],  loss: 17018894.000000, mae: 1426.373047, mean_q: -209.099945
 3069/5000: episode: 3069, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2771.924, mean reward: -2771.924 [-2771.924, -2771.924], mean action: 2.000 [2.000, 2.000],  loss: 8225387.000000, mae: 1010.518616, mean_q: -209.572571
 3070/5000: episode: 3070, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -104.658, mean reward: -104.658 [-104.658, -104.658], mean action: 2.000 [2.000, 2.000],  loss: 10717159.000000, mae: 1103.733887, mean_q: -209.907684
 3071/5000: episode: 3071, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1488.394, mean reward: -1488.394 [-1488.394, -1488.394], mean action: 2.000 [2.000, 2.000],  loss: 10102404.000000, mae: 1109.567627, mean_q: -210.844559
 3072/5000: episode: 3072, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1858.002, mean reward: -1858.002 [-1858.002, -1858.002], mean action: 2.000 [2.000, 2.000],  loss: 12219192.000000, mae: 1182.468018, mean_q: -209.968918
 3073/5000: episode: 3073, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2443.308, mean reward: -2443.308 [-2443.308, -2443.308], mean action: 2.000 [2.000, 2.000],  loss: 15909942.000000, mae: 1223.129883, mean_q: -211.348267
 3074/5000: episode: 3074, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -965.278, mean reward: -965.278 [-965.278, -965.278], mean action: 2.000 [2.000, 2.000],  loss: 15256016.000000, mae: 1340.069824, mean_q: -211.223862
 3075/5000: episode: 3075, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -435.079, mean reward: -435.079 [-435.079, -435.079], mean action: 2.000 [2.000, 2.000],  loss: 14317019.000000, mae: 1224.776367, mean_q: -211.511047
 3076/5000: episode: 3076, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3801.589, mean reward: -3801.589 [-3801.589, -3801.589], mean action: 2.000 [2.000, 2.000],  loss: 11067224.000000, mae: 1118.621704, mean_q: -211.255051
 3077/5000: episode: 3077, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7614.782, mean reward: -7614.782 [-7614.782, -7614.782], mean action: 2.000 [2.000, 2.000],  loss: 9271565.000000, mae: 1014.606140, mean_q: -211.024658
 3078/5000: episode: 3078, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5347.477, mean reward: -5347.477 [-5347.477, -5347.477], mean action: 2.000 [2.000, 2.000],  loss: 10681840.000000, mae: 1059.941528, mean_q: -211.793350
 3079/5000: episode: 3079, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5577.236, mean reward: -5577.236 [-5577.236, -5577.236], mean action: 1.000 [1.000, 1.000],  loss: 20367536.000000, mae: 1534.480469, mean_q: -211.017868
 3080/5000: episode: 3080, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2079.543, mean reward: -2079.543 [-2079.543, -2079.543], mean action: 2.000 [2.000, 2.000],  loss: 10307165.000000, mae: 1126.251709, mean_q: -212.256348
 3081/5000: episode: 3081, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -102.452, mean reward: -102.452 [-102.452, -102.452], mean action: 2.000 [2.000, 2.000],  loss: 11937294.000000, mae: 1185.657104, mean_q: -211.598267
 3082/5000: episode: 3082, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5974.044, mean reward: -5974.044 [-5974.044, -5974.044], mean action: 2.000 [2.000, 2.000],  loss: 9915855.000000, mae: 1069.203857, mean_q: -212.146591
 3083/5000: episode: 3083, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2624.450, mean reward: -2624.450 [-2624.450, -2624.450], mean action: 2.000 [2.000, 2.000],  loss: 14132146.000000, mae: 1149.425049, mean_q: -212.280945
 3084/5000: episode: 3084, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3442.856, mean reward: -3442.856 [-3442.856, -3442.856], mean action: 2.000 [2.000, 2.000],  loss: 9120590.000000, mae: 990.423767, mean_q: -212.118683
 3085/5000: episode: 3085, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -150.033, mean reward: -150.033 [-150.033, -150.033], mean action: 2.000 [2.000, 2.000],  loss: 10462697.000000, mae: 979.760376, mean_q: -212.901169
 3086/5000: episode: 3086, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -139.128, mean reward: -139.128 [-139.128, -139.128], mean action: 2.000 [2.000, 2.000],  loss: 11730217.000000, mae: 1156.473389, mean_q: -212.761215
 3087/5000: episode: 3087, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1722.474, mean reward: -1722.474 [-1722.474, -1722.474], mean action: 2.000 [2.000, 2.000],  loss: 10956780.000000, mae: 1120.481812, mean_q: -212.634140
 3088/5000: episode: 3088, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1951.635, mean reward: -1951.635 [-1951.635, -1951.635], mean action: 2.000 [2.000, 2.000],  loss: 9142778.000000, mae: 1002.519165, mean_q: -214.594818
 3089/5000: episode: 3089, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2290.307, mean reward: -2290.307 [-2290.307, -2290.307], mean action: 2.000 [2.000, 2.000],  loss: 11135232.000000, mae: 1108.579224, mean_q: -214.205597
 3090/5000: episode: 3090, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1196.163, mean reward: -1196.163 [-1196.163, -1196.163], mean action: 2.000 [2.000, 2.000],  loss: 10479918.000000, mae: 1023.158691, mean_q: -213.470337
 3091/5000: episode: 3091, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3420.108, mean reward: -3420.108 [-3420.108, -3420.108], mean action: 2.000 [2.000, 2.000],  loss: 11027706.000000, mae: 1105.796631, mean_q: -213.005280
 3092/5000: episode: 3092, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -7804.600, mean reward: -7804.600 [-7804.600, -7804.600], mean action: 2.000 [2.000, 2.000],  loss: 9043152.000000, mae: 1032.761963, mean_q: -213.858551
 3093/5000: episode: 3093, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3198.988, mean reward: -3198.988 [-3198.988, -3198.988], mean action: 2.000 [2.000, 2.000],  loss: 13472641.000000, mae: 1193.705078, mean_q: -213.160065
 3094/5000: episode: 3094, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2748.880, mean reward: -2748.880 [-2748.880, -2748.880], mean action: 2.000 [2.000, 2.000],  loss: 10377269.000000, mae: 1054.802246, mean_q: -214.190384
 3095/5000: episode: 3095, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4167.227, mean reward: -4167.227 [-4167.227, -4167.227], mean action: 2.000 [2.000, 2.000],  loss: 7211758.500000, mae: 906.430908, mean_q: -214.558731
 3096/5000: episode: 3096, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -412.153, mean reward: -412.153 [-412.153, -412.153], mean action: 2.000 [2.000, 2.000],  loss: 10870835.000000, mae: 1039.625732, mean_q: -214.266235
 3097/5000: episode: 3097, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1836.172, mean reward: -1836.172 [-1836.172, -1836.172], mean action: 2.000 [2.000, 2.000],  loss: 12663748.000000, mae: 1163.921753, mean_q: -213.809692
 3098/5000: episode: 3098, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4564.946, mean reward: -4564.946 [-4564.946, -4564.946], mean action: 2.000 [2.000, 2.000],  loss: 14270033.000000, mae: 1198.689453, mean_q: -214.982239
 3099/5000: episode: 3099, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2202.626, mean reward: -2202.626 [-2202.626, -2202.626], mean action: 2.000 [2.000, 2.000],  loss: 10509806.000000, mae: 1045.270996, mean_q: -215.126801
 3100/5000: episode: 3100, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3335.749, mean reward: -3335.749 [-3335.749, -3335.749], mean action: 2.000 [2.000, 2.000],  loss: 8518080.000000, mae: 1035.730957, mean_q: -215.950714
 3101/5000: episode: 3101, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5586.932, mean reward: -5586.932 [-5586.932, -5586.932], mean action: 2.000 [2.000, 2.000],  loss: 19360312.000000, mae: 1447.900635, mean_q: -214.625870
 3102/5000: episode: 3102, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6991.887, mean reward: -6991.887 [-6991.887, -6991.887], mean action: 2.000 [2.000, 2.000],  loss: 12014864.000000, mae: 1130.071045, mean_q: -215.739624
 3103/5000: episode: 3103, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4646.313, mean reward: -4646.313 [-4646.313, -4646.313], mean action: 2.000 [2.000, 2.000],  loss: 10463937.000000, mae: 1131.955566, mean_q: -215.370514
 3104/5000: episode: 3104, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3923.482, mean reward: -3923.482 [-3923.482, -3923.482], mean action: 2.000 [2.000, 2.000],  loss: 8374010.500000, mae: 991.422363, mean_q: -215.788177
 3105/5000: episode: 3105, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1323.023, mean reward: -1323.023 [-1323.023, -1323.023], mean action: 2.000 [2.000, 2.000],  loss: 12851722.000000, mae: 1183.923096, mean_q: -215.371765
 3106/5000: episode: 3106, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2098.488, mean reward: -2098.488 [-2098.488, -2098.488], mean action: 2.000 [2.000, 2.000],  loss: 16880198.000000, mae: 1240.733643, mean_q: -215.073700
 3107/5000: episode: 3107, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3625.489, mean reward: -3625.489 [-3625.489, -3625.489], mean action: 2.000 [2.000, 2.000],  loss: 8933242.000000, mae: 1000.730469, mean_q: -216.747589
 3108/5000: episode: 3108, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -446.348, mean reward: -446.348 [-446.348, -446.348], mean action: 2.000 [2.000, 2.000],  loss: 16857234.000000, mae: 1293.525635, mean_q: -215.119675
 3109/5000: episode: 3109, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1227.898, mean reward: -1227.898 [-1227.898, -1227.898], mean action: 2.000 [2.000, 2.000],  loss: 14892478.000000, mae: 1250.197510, mean_q: -215.722458
 3110/5000: episode: 3110, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -8975.843, mean reward: -8975.843 [-8975.843, -8975.843], mean action: 2.000 [2.000, 2.000],  loss: 9009561.000000, mae: 978.908325, mean_q: -216.343201
 3111/5000: episode: 3111, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3044.442, mean reward: -3044.442 [-3044.442, -3044.442], mean action: 2.000 [2.000, 2.000],  loss: 15259160.000000, mae: 1199.021606, mean_q: -216.177750
 3112/5000: episode: 3112, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3945.029, mean reward: -3945.029 [-3945.029, -3945.029], mean action: 2.000 [2.000, 2.000],  loss: 15712736.000000, mae: 1344.875122, mean_q: -216.371826
 3113/5000: episode: 3113, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3183.425, mean reward: -3183.425 [-3183.425, -3183.425], mean action: 2.000 [2.000, 2.000],  loss: 14954222.000000, mae: 1265.759521, mean_q: -216.397858
 3114/5000: episode: 3114, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5042.445, mean reward: -5042.445 [-5042.445, -5042.445], mean action: 2.000 [2.000, 2.000],  loss: 12594320.000000, mae: 1252.176147, mean_q: -217.061905
 3115/5000: episode: 3115, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3047.135, mean reward: -3047.135 [-3047.135, -3047.135], mean action: 2.000 [2.000, 2.000],  loss: 7989074.000000, mae: 981.527100, mean_q: -218.317963
 3116/5000: episode: 3116, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2490.915, mean reward: -2490.915 [-2490.915, -2490.915], mean action: 2.000 [2.000, 2.000],  loss: 14308431.000000, mae: 1286.738281, mean_q: -216.799835
 3117/5000: episode: 3117, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3478.694, mean reward: -3478.694 [-3478.694, -3478.694], mean action: 3.000 [3.000, 3.000],  loss: 16172023.000000, mae: 1185.384033, mean_q: -217.979523
 3118/5000: episode: 3118, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2269.762, mean reward: -2269.762 [-2269.762, -2269.762], mean action: 3.000 [3.000, 3.000],  loss: 8255435.500000, mae: 998.280273, mean_q: -217.585663
 3119/5000: episode: 3119, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -273.736, mean reward: -273.736 [-273.736, -273.736], mean action: 2.000 [2.000, 2.000],  loss: 14292442.000000, mae: 1255.576050, mean_q: -218.161835
 3120/5000: episode: 3120, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3122.708, mean reward: -3122.708 [-3122.708, -3122.708], mean action: 2.000 [2.000, 2.000],  loss: 10163606.000000, mae: 1005.320923, mean_q: -217.997238
 3121/5000: episode: 3121, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3353.051, mean reward: -3353.051 [-3353.051, -3353.051], mean action: 2.000 [2.000, 2.000],  loss: 15675477.000000, mae: 1294.167480, mean_q: -217.438095
 3122/5000: episode: 3122, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2775.367, mean reward: -2775.367 [-2775.367, -2775.367], mean action: 2.000 [2.000, 2.000],  loss: 10128786.000000, mae: 1091.602783, mean_q: -218.916580
 3123/5000: episode: 3123, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -934.681, mean reward: -934.681 [-934.681, -934.681], mean action: 2.000 [2.000, 2.000],  loss: 11952206.000000, mae: 1148.720093, mean_q: -218.663162
 3124/5000: episode: 3124, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4672.128, mean reward: -4672.128 [-4672.128, -4672.128], mean action: 2.000 [2.000, 2.000],  loss: 13287699.000000, mae: 1182.568726, mean_q: -218.836853
 3125/5000: episode: 3125, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1877.261, mean reward: -1877.261 [-1877.261, -1877.261], mean action: 2.000 [2.000, 2.000],  loss: 11399176.000000, mae: 1131.421387, mean_q: -218.434021
 3126/5000: episode: 3126, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -245.769, mean reward: -245.769 [-245.769, -245.769], mean action: 2.000 [2.000, 2.000],  loss: 12849456.000000, mae: 1143.995728, mean_q: -219.132446
 3127/5000: episode: 3127, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -721.499, mean reward: -721.499 [-721.499, -721.499], mean action: 2.000 [2.000, 2.000],  loss: 7280127.000000, mae: 932.223877, mean_q: -219.481964
 3128/5000: episode: 3128, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -379.727, mean reward: -379.727 [-379.727, -379.727], mean action: 2.000 [2.000, 2.000],  loss: 15534521.000000, mae: 1370.064453, mean_q: -218.839905
 3129/5000: episode: 3129, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -272.679, mean reward: -272.679 [-272.679, -272.679], mean action: 2.000 [2.000, 2.000],  loss: 6313197.500000, mae: 829.644165, mean_q: -219.588348
 3130/5000: episode: 3130, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -876.891, mean reward: -876.891 [-876.891, -876.891], mean action: 2.000 [2.000, 2.000],  loss: 13673282.000000, mae: 1164.666748, mean_q: -219.554276
 3131/5000: episode: 3131, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2484.739, mean reward: -2484.739 [-2484.739, -2484.739], mean action: 2.000 [2.000, 2.000],  loss: 13545526.000000, mae: 1213.668457, mean_q: -220.376373
 3132/5000: episode: 3132, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5463.033, mean reward: -5463.033 [-5463.033, -5463.033], mean action: 2.000 [2.000, 2.000],  loss: 18877204.000000, mae: 1384.469604, mean_q: -219.074860
 3133/5000: episode: 3133, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6148.959, mean reward: -6148.959 [-6148.959, -6148.959], mean action: 2.000 [2.000, 2.000],  loss: 10775590.000000, mae: 1007.878235, mean_q: -220.956879
 3134/5000: episode: 3134, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -2813.276, mean reward: -2813.276 [-2813.276, -2813.276], mean action: 2.000 [2.000, 2.000],  loss: 9829792.000000, mae: 1113.052734, mean_q: -220.426865
 3135/5000: episode: 3135, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4178.161, mean reward: -4178.161 [-4178.161, -4178.161], mean action: 2.000 [2.000, 2.000],  loss: 11536923.000000, mae: 1168.209717, mean_q: -220.562164
 3136/5000: episode: 3136, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -1351.209, mean reward: -1351.209 [-1351.209, -1351.209], mean action: 2.000 [2.000, 2.000],  loss: 15298225.000000, mae: 1268.003418, mean_q: -220.594849
 3137/5000: episode: 3137, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -276.055, mean reward: -276.055 [-276.055, -276.055], mean action: 2.000 [2.000, 2.000],  loss: 13423395.000000, mae: 1183.656494, mean_q: -220.267227
 3138/5000: episode: 3138, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -747.448, mean reward: -747.448 [-747.448, -747.448], mean action: 2.000 [2.000, 2.000],  loss: 14008540.000000, mae: 1205.543701, mean_q: -220.849792
 3139/5000: episode: 3139, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -3989.090, mean reward: -3989.090 [-3989.090, -3989.090], mean action: 2.000 [2.000, 2.000],  loss: 7353550.000000, mae: 916.475098, mean_q: -220.997589
 3140/5000: episode: 3140, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3073.779, mean reward: -3073.779 [-3073.779, -3073.779], mean action: 2.000 [2.000, 2.000],  loss: 11581034.000000, mae: 1169.339355, mean_q: -220.863983
 3141/5000: episode: 3141, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2687.260, mean reward: -2687.260 [-2687.260, -2687.260], mean action: 2.000 [2.000, 2.000],  loss: 19051904.000000, mae: 1474.120972, mean_q: -220.628082
 3142/5000: episode: 3142, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5680.206, mean reward: -5680.206 [-5680.206, -5680.206], mean action: 2.000 [2.000, 2.000],  loss: 11747482.000000, mae: 1095.776978, mean_q: -222.279495
 3143/5000: episode: 3143, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2310.161, mean reward: -2310.161 [-2310.161, -2310.161], mean action: 2.000 [2.000, 2.000],  loss: 13727522.000000, mae: 1128.625122, mean_q: -221.342026
 3144/5000: episode: 3144, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -4149.868, mean reward: -4149.868 [-4149.868, -4149.868], mean action: 2.000 [2.000, 2.000],  loss: 11287746.000000, mae: 1151.580811, mean_q: -221.796661
 3145/5000: episode: 3145, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1714.898, mean reward: -1714.898 [-1714.898, -1714.898], mean action: 2.000 [2.000, 2.000],  loss: 15087348.000000, mae: 1293.040894, mean_q: -222.235809
 3146/5000: episode: 3146, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -843.631, mean reward: -843.631 [-843.631, -843.631], mean action: 2.000 [2.000, 2.000],  loss: 10172704.000000, mae: 1071.608398, mean_q: -222.180130
 3147/5000: episode: 3147, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1267.079, mean reward: -1267.079 [-1267.079, -1267.079], mean action: 2.000 [2.000, 2.000],  loss: 13953216.000000, mae: 1277.332520, mean_q: -222.734467
 3148/5000: episode: 3148, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2993.425, mean reward: -2993.425 [-2993.425, -2993.425], mean action: 2.000 [2.000, 2.000],  loss: 12611298.000000, mae: 1163.218384, mean_q: -222.695023
 3149/5000: episode: 3149, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7810.700, mean reward: -7810.700 [-7810.700, -7810.700], mean action: 2.000 [2.000, 2.000],  loss: 10545308.000000, mae: 1083.127075, mean_q: -221.753204
 3150/5000: episode: 3150, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -26.077, mean reward: -26.077 [-26.077, -26.077], mean action: 2.000 [2.000, 2.000],  loss: 13095660.000000, mae: 1230.252930, mean_q: -223.025223
 3151/5000: episode: 3151, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10337.842, mean reward: -10337.842 [-10337.842, -10337.842], mean action: 2.000 [2.000, 2.000],  loss: 14174846.000000, mae: 1223.367188, mean_q: -222.475677
 3152/5000: episode: 3152, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7587.090, mean reward: -7587.090 [-7587.090, -7587.090], mean action: 0.000 [0.000, 0.000],  loss: 9739594.000000, mae: 1078.416626, mean_q: -224.395493
 3153/5000: episode: 3153, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5802.658, mean reward: -5802.658 [-5802.658, -5802.658], mean action: 2.000 [2.000, 2.000],  loss: 8862418.000000, mae: 987.970825, mean_q: -223.837250
 3154/5000: episode: 3154, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2567.654, mean reward: -2567.654 [-2567.654, -2567.654], mean action: 2.000 [2.000, 2.000],  loss: 7806132.500000, mae: 850.994751, mean_q: -224.043045
 3155/5000: episode: 3155, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -369.095, mean reward: -369.095 [-369.095, -369.095], mean action: 2.000 [2.000, 2.000],  loss: 13781161.000000, mae: 1281.090210, mean_q: -224.475632
 3156/5000: episode: 3156, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -331.684, mean reward: -331.684 [-331.684, -331.684], mean action: 2.000 [2.000, 2.000],  loss: 11572244.000000, mae: 1089.696045, mean_q: -224.714035
 3157/5000: episode: 3157, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4026.809, mean reward: -4026.809 [-4026.809, -4026.809], mean action: 2.000 [2.000, 2.000],  loss: 9865912.000000, mae: 1055.641357, mean_q: -223.344452
 3158/5000: episode: 3158, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1037.547, mean reward: -1037.547 [-1037.547, -1037.547], mean action: 2.000 [2.000, 2.000],  loss: 15296822.000000, mae: 1282.099854, mean_q: -224.959534
 3159/5000: episode: 3159, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -659.727, mean reward: -659.727 [-659.727, -659.727], mean action: 2.000 [2.000, 2.000],  loss: 10211322.000000, mae: 1020.338623, mean_q: -225.627716
 3160/5000: episode: 3160, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7220.689, mean reward: -7220.689 [-7220.689, -7220.689], mean action: 0.000 [0.000, 0.000],  loss: 7837121.000000, mae: 999.761536, mean_q: -224.570831
 3161/5000: episode: 3161, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4048.555, mean reward: -4048.555 [-4048.555, -4048.555], mean action: 2.000 [2.000, 2.000],  loss: 23285964.000000, mae: 1546.763672, mean_q: -223.929352
 3162/5000: episode: 3162, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -486.122, mean reward: -486.122 [-486.122, -486.122], mean action: 2.000 [2.000, 2.000],  loss: 16465762.000000, mae: 1202.551514, mean_q: -224.601196
 3163/5000: episode: 3163, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4510.824, mean reward: -4510.824 [-4510.824, -4510.824], mean action: 2.000 [2.000, 2.000],  loss: 12304484.000000, mae: 1182.488525, mean_q: -225.521912
 3164/5000: episode: 3164, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3575.541, mean reward: -3575.541 [-3575.541, -3575.541], mean action: 2.000 [2.000, 2.000],  loss: 13904020.000000, mae: 1106.701660, mean_q: -224.598282
 3165/5000: episode: 3165, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -9648.113, mean reward: -9648.113 [-9648.113, -9648.113], mean action: 0.000 [0.000, 0.000],  loss: 10926387.000000, mae: 1068.043945, mean_q: -225.568146
 3166/5000: episode: 3166, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4447.727, mean reward: -4447.727 [-4447.727, -4447.727], mean action: 2.000 [2.000, 2.000],  loss: 11167878.000000, mae: 1045.323120, mean_q: -225.211929
 3167/5000: episode: 3167, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9030.341, mean reward: -9030.341 [-9030.341, -9030.341], mean action: 2.000 [2.000, 2.000],  loss: 8842370.000000, mae: 1008.473389, mean_q: -225.374908
 3168/5000: episode: 3168, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3537.215, mean reward: -3537.215 [-3537.215, -3537.215], mean action: 2.000 [2.000, 2.000],  loss: 11510772.000000, mae: 1134.252686, mean_q: -225.493530
 3169/5000: episode: 3169, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3938.740, mean reward: -3938.740 [-3938.740, -3938.740], mean action: 2.000 [2.000, 2.000],  loss: 18580506.000000, mae: 1356.672241, mean_q: -225.969177
 3170/5000: episode: 3170, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4048.798, mean reward: -4048.798 [-4048.798, -4048.798], mean action: 2.000 [2.000, 2.000],  loss: 13323145.000000, mae: 1178.224121, mean_q: -225.414047
 3171/5000: episode: 3171, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -751.946, mean reward: -751.946 [-751.946, -751.946], mean action: 3.000 [3.000, 3.000],  loss: 19923240.000000, mae: 1417.067017, mean_q: -225.961319
 3172/5000: episode: 3172, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1015.098, mean reward: -1015.098 [-1015.098, -1015.098], mean action: 2.000 [2.000, 2.000],  loss: 9986658.000000, mae: 1043.805908, mean_q: -226.862854
 3173/5000: episode: 3173, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -636.849, mean reward: -636.849 [-636.849, -636.849], mean action: 2.000 [2.000, 2.000],  loss: 8148588.000000, mae: 1011.193481, mean_q: -227.793243
 3174/5000: episode: 3174, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1381.822, mean reward: -1381.822 [-1381.822, -1381.822], mean action: 2.000 [2.000, 2.000],  loss: 8951440.000000, mae: 977.557617, mean_q: -226.978973
 3175/5000: episode: 3175, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9083.419, mean reward: -9083.419 [-9083.419, -9083.419], mean action: 0.000 [0.000, 0.000],  loss: 11095766.000000, mae: 1079.940430, mean_q: -227.064056
 3176/5000: episode: 3176, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5412.780, mean reward: -5412.780 [-5412.780, -5412.780], mean action: 2.000 [2.000, 2.000],  loss: 9285189.000000, mae: 977.053040, mean_q: -228.765045
 3177/5000: episode: 3177, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1146.643, mean reward: -1146.643 [-1146.643, -1146.643], mean action: 1.000 [1.000, 1.000],  loss: 14436397.000000, mae: 1279.629639, mean_q: -226.455444
 3178/5000: episode: 3178, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3318.869, mean reward: -3318.869 [-3318.869, -3318.869], mean action: 2.000 [2.000, 2.000],  loss: 11839550.000000, mae: 1133.564453, mean_q: -227.782166
 3179/5000: episode: 3179, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2221.208, mean reward: -2221.208 [-2221.208, -2221.208], mean action: 2.000 [2.000, 2.000],  loss: 18845766.000000, mae: 1391.550781, mean_q: -227.513062
 3180/5000: episode: 3180, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4392.856, mean reward: -4392.856 [-4392.856, -4392.856], mean action: 2.000 [2.000, 2.000],  loss: 13471152.000000, mae: 1219.114502, mean_q: -228.510864
 3181/5000: episode: 3181, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1941.026, mean reward: -1941.026 [-1941.026, -1941.026], mean action: 2.000 [2.000, 2.000],  loss: 11061016.000000, mae: 1073.415527, mean_q: -228.157211
 3182/5000: episode: 3182, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1519.936, mean reward: -1519.936 [-1519.936, -1519.936], mean action: 2.000 [2.000, 2.000],  loss: 10542657.000000, mae: 1041.891846, mean_q: -229.094299
 3183/5000: episode: 3183, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2162.957, mean reward: -2162.957 [-2162.957, -2162.957], mean action: 2.000 [2.000, 2.000],  loss: 11990476.000000, mae: 1140.712036, mean_q: -228.394104
 3184/5000: episode: 3184, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2453.746, mean reward: -2453.746 [-2453.746, -2453.746], mean action: 2.000 [2.000, 2.000],  loss: 13304362.000000, mae: 1187.483887, mean_q: -229.141968
 3185/5000: episode: 3185, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3899.865, mean reward: -3899.865 [-3899.865, -3899.865], mean action: 2.000 [2.000, 2.000],  loss: 9754257.000000, mae: 1105.987061, mean_q: -228.983749
 3186/5000: episode: 3186, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1353.792, mean reward: -1353.792 [-1353.792, -1353.792], mean action: 2.000 [2.000, 2.000],  loss: 7605531.500000, mae: 929.286255, mean_q: -229.909348
 3187/5000: episode: 3187, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -13.029, mean reward: -13.029 [-13.029, -13.029], mean action: 2.000 [2.000, 2.000],  loss: 9550425.000000, mae: 991.693115, mean_q: -229.197235
 3188/5000: episode: 3188, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1428.706, mean reward: -1428.706 [-1428.706, -1428.706], mean action: 2.000 [2.000, 2.000],  loss: 12298924.000000, mae: 1150.374512, mean_q: -228.950439
 3189/5000: episode: 3189, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2389.994, mean reward: -2389.994 [-2389.994, -2389.994], mean action: 2.000 [2.000, 2.000],  loss: 6411063.500000, mae: 856.858643, mean_q: -229.961685
 3190/5000: episode: 3190, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1100.500, mean reward: -1100.500 [-1100.500, -1100.500], mean action: 2.000 [2.000, 2.000],  loss: 10490923.000000, mae: 1007.811951, mean_q: -230.030426
 3191/5000: episode: 3191, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2031.196, mean reward: -2031.196 [-2031.196, -2031.196], mean action: 2.000 [2.000, 2.000],  loss: 10133804.000000, mae: 1132.396484, mean_q: -229.584290
 3192/5000: episode: 3192, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1996.018, mean reward: -1996.018 [-1996.018, -1996.018], mean action: 2.000 [2.000, 2.000],  loss: 12933798.000000, mae: 1189.231323, mean_q: -229.121216
 3193/5000: episode: 3193, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6285.114, mean reward: -6285.114 [-6285.114, -6285.114], mean action: 2.000 [2.000, 2.000],  loss: 11061617.000000, mae: 1217.569336, mean_q: -230.080902
 3194/5000: episode: 3194, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2892.989, mean reward: -2892.989 [-2892.989, -2892.989], mean action: 2.000 [2.000, 2.000],  loss: 11812954.000000, mae: 1081.651855, mean_q: -231.139252
 3195/5000: episode: 3195, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4755.129, mean reward: -4755.129 [-4755.129, -4755.129], mean action: 2.000 [2.000, 2.000],  loss: 11648896.000000, mae: 1043.553833, mean_q: -230.604584
 3196/5000: episode: 3196, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -7603.204, mean reward: -7603.204 [-7603.204, -7603.204], mean action: 3.000 [3.000, 3.000],  loss: 11661808.000000, mae: 1180.133301, mean_q: -229.598358
 3197/5000: episode: 3197, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2129.469, mean reward: -2129.469 [-2129.469, -2129.469], mean action: 2.000 [2.000, 2.000],  loss: 12066185.000000, mae: 1201.092529, mean_q: -231.375443
 3198/5000: episode: 3198, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3528.019, mean reward: -3528.019 [-3528.019, -3528.019], mean action: 2.000 [2.000, 2.000],  loss: 11790442.000000, mae: 1129.720215, mean_q: -231.012207
 3199/5000: episode: 3199, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -849.958, mean reward: -849.958 [-849.958, -849.958], mean action: 2.000 [2.000, 2.000],  loss: 9228291.000000, mae: 1055.918213, mean_q: -230.768890
 3200/5000: episode: 3200, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1372.759, mean reward: -1372.759 [-1372.759, -1372.759], mean action: 2.000 [2.000, 2.000],  loss: 10408136.000000, mae: 1116.414307, mean_q: -231.395035
 3201/5000: episode: 3201, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -580.184, mean reward: -580.184 [-580.184, -580.184], mean action: 2.000 [2.000, 2.000],  loss: 10499076.000000, mae: 1073.922852, mean_q: -231.775330
 3202/5000: episode: 3202, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3055.414, mean reward: -3055.414 [-3055.414, -3055.414], mean action: 2.000 [2.000, 2.000],  loss: 11271815.000000, mae: 1077.758789, mean_q: -231.419846
 3203/5000: episode: 3203, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1615.721, mean reward: -1615.721 [-1615.721, -1615.721], mean action: 2.000 [2.000, 2.000],  loss: 14201272.000000, mae: 1191.739990, mean_q: -231.309326
 3204/5000: episode: 3204, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -471.898, mean reward: -471.898 [-471.898, -471.898], mean action: 2.000 [2.000, 2.000],  loss: 11459570.000000, mae: 1129.963989, mean_q: -232.037109
 3205/5000: episode: 3205, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -9118.892, mean reward: -9118.892 [-9118.892, -9118.892], mean action: 0.000 [0.000, 0.000],  loss: 10603673.000000, mae: 1111.206299, mean_q: -231.619751
 3206/5000: episode: 3206, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2439.552, mean reward: -2439.552 [-2439.552, -2439.552], mean action: 2.000 [2.000, 2.000],  loss: 14822864.000000, mae: 1231.473633, mean_q: -232.727142
 3207/5000: episode: 3207, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -805.934, mean reward: -805.934 [-805.934, -805.934], mean action: 2.000 [2.000, 2.000],  loss: 10139998.000000, mae: 1103.347412, mean_q: -233.034576
 3208/5000: episode: 3208, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3290.319, mean reward: -3290.319 [-3290.319, -3290.319], mean action: 2.000 [2.000, 2.000],  loss: 10562562.000000, mae: 1097.482910, mean_q: -233.097061
 3209/5000: episode: 3209, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6196.721, mean reward: -6196.721 [-6196.721, -6196.721], mean action: 2.000 [2.000, 2.000],  loss: 15389192.000000, mae: 1280.328613, mean_q: -232.418381
 3210/5000: episode: 3210, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3064.281, mean reward: -3064.281 [-3064.281, -3064.281], mean action: 2.000 [2.000, 2.000],  loss: 9925857.000000, mae: 995.089478, mean_q: -232.976837
 3211/5000: episode: 3211, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -468.460, mean reward: -468.460 [-468.460, -468.460], mean action: 2.000 [2.000, 2.000],  loss: 15205926.000000, mae: 1259.114380, mean_q: -233.294205
 3212/5000: episode: 3212, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2039.179, mean reward: -2039.179 [-2039.179, -2039.179], mean action: 2.000 [2.000, 2.000],  loss: 7069028.000000, mae: 942.941040, mean_q: -234.698730
 3213/5000: episode: 3213, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1252.213, mean reward: -1252.213 [-1252.213, -1252.213], mean action: 2.000 [2.000, 2.000],  loss: 13361193.000000, mae: 1219.261841, mean_q: -233.454163
 3214/5000: episode: 3214, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -580.435, mean reward: -580.435 [-580.435, -580.435], mean action: 2.000 [2.000, 2.000],  loss: 13538254.000000, mae: 1123.103394, mean_q: -233.054077
 3215/5000: episode: 3215, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -4386.304, mean reward: -4386.304 [-4386.304, -4386.304], mean action: 0.000 [0.000, 0.000],  loss: 18342118.000000, mae: 1365.918091, mean_q: -233.930740
 3216/5000: episode: 3216, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -94.457, mean reward: -94.457 [-94.457, -94.457], mean action: 2.000 [2.000, 2.000],  loss: 9622797.000000, mae: 1068.075928, mean_q: -234.052368
 3217/5000: episode: 3217, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1037.428, mean reward: -1037.428 [-1037.428, -1037.428], mean action: 2.000 [2.000, 2.000],  loss: 10451092.000000, mae: 1068.551270, mean_q: -233.657043
 3218/5000: episode: 3218, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2149.810, mean reward: -2149.810 [-2149.810, -2149.810], mean action: 2.000 [2.000, 2.000],  loss: 10383276.000000, mae: 1174.808594, mean_q: -234.500885
 3219/5000: episode: 3219, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -953.061, mean reward: -953.061 [-953.061, -953.061], mean action: 3.000 [3.000, 3.000],  loss: 13477978.000000, mae: 1192.181396, mean_q: -235.136017
 3220/5000: episode: 3220, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1346.343, mean reward: -1346.343 [-1346.343, -1346.343], mean action: 2.000 [2.000, 2.000],  loss: 13430878.000000, mae: 1233.584717, mean_q: -234.566162
 3221/5000: episode: 3221, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5016.525, mean reward: -5016.525 [-5016.525, -5016.525], mean action: 2.000 [2.000, 2.000],  loss: 10449419.000000, mae: 1017.894836, mean_q: -235.570221
 3222/5000: episode: 3222, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2773.936, mean reward: -2773.936 [-2773.936, -2773.936], mean action: 2.000 [2.000, 2.000],  loss: 7558527.000000, mae: 976.221619, mean_q: -235.666748
 3223/5000: episode: 3223, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3747.352, mean reward: -3747.352 [-3747.352, -3747.352], mean action: 2.000 [2.000, 2.000],  loss: 11036020.000000, mae: 1115.523926, mean_q: -235.788177
 3224/5000: episode: 3224, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1714.999, mean reward: -1714.999 [-1714.999, -1714.999], mean action: 2.000 [2.000, 2.000],  loss: 13538290.000000, mae: 1188.839111, mean_q: -235.639221
 3225/5000: episode: 3225, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -894.253, mean reward: -894.253 [-894.253, -894.253], mean action: 2.000 [2.000, 2.000],  loss: 13551162.000000, mae: 1189.584473, mean_q: -235.391998
 3226/5000: episode: 3226, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2601.381, mean reward: -2601.381 [-2601.381, -2601.381], mean action: 2.000 [2.000, 2.000],  loss: 16224490.000000, mae: 1248.441406, mean_q: -235.848419
 3227/5000: episode: 3227, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -280.031, mean reward: -280.031 [-280.031, -280.031], mean action: 2.000 [2.000, 2.000],  loss: 9467173.000000, mae: 1108.997681, mean_q: -236.728577
 3228/5000: episode: 3228, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1560.809, mean reward: -1560.809 [-1560.809, -1560.809], mean action: 2.000 [2.000, 2.000],  loss: 7129917.000000, mae: 982.532593, mean_q: -236.041962
 3229/5000: episode: 3229, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3491.030, mean reward: -3491.030 [-3491.030, -3491.030], mean action: 3.000 [3.000, 3.000],  loss: 16489064.000000, mae: 1297.731079, mean_q: -235.456680
 3230/5000: episode: 3230, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -170.321, mean reward: -170.321 [-170.321, -170.321], mean action: 2.000 [2.000, 2.000],  loss: 15762029.000000, mae: 1223.182007, mean_q: -237.053833
 3231/5000: episode: 3231, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1825.649, mean reward: -1825.649 [-1825.649, -1825.649], mean action: 3.000 [3.000, 3.000],  loss: 12441158.000000, mae: 1113.083618, mean_q: -236.897369
 3232/5000: episode: 3232, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5077.461, mean reward: -5077.461 [-5077.461, -5077.461], mean action: 2.000 [2.000, 2.000],  loss: 6494868.000000, mae: 835.470459, mean_q: -237.988739
 3233/5000: episode: 3233, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4008.265, mean reward: -4008.265 [-4008.265, -4008.265], mean action: 2.000 [2.000, 2.000],  loss: 11597596.000000, mae: 1102.767334, mean_q: -236.721207
 3234/5000: episode: 3234, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -131.910, mean reward: -131.910 [-131.910, -131.910], mean action: 2.000 [2.000, 2.000],  loss: 13249694.000000, mae: 1228.190918, mean_q: -236.428818
 3235/5000: episode: 3235, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1795.956, mean reward: -1795.956 [-1795.956, -1795.956], mean action: 2.000 [2.000, 2.000],  loss: 14333310.000000, mae: 1241.304932, mean_q: -237.365967
 3236/5000: episode: 3236, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1599.897, mean reward: -1599.897 [-1599.897, -1599.897], mean action: 2.000 [2.000, 2.000],  loss: 8328564.000000, mae: 968.116882, mean_q: -237.186691
 3237/5000: episode: 3237, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -593.422, mean reward: -593.422 [-593.422, -593.422], mean action: 2.000 [2.000, 2.000],  loss: 10768338.000000, mae: 1141.522217, mean_q: -237.894424
 3238/5000: episode: 3238, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3825.514, mean reward: -3825.514 [-3825.514, -3825.514], mean action: 2.000 [2.000, 2.000],  loss: 12845125.000000, mae: 1196.244141, mean_q: -238.532837
 3239/5000: episode: 3239, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2377.171, mean reward: -2377.171 [-2377.171, -2377.171], mean action: 2.000 [2.000, 2.000],  loss: 12671294.000000, mae: 1197.783691, mean_q: -237.440643
 3240/5000: episode: 3240, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1114.347, mean reward: -1114.347 [-1114.347, -1114.347], mean action: 2.000 [2.000, 2.000],  loss: 10840434.000000, mae: 1074.051025, mean_q: -238.574432
 3241/5000: episode: 3241, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2429.887, mean reward: -2429.887 [-2429.887, -2429.887], mean action: 2.000 [2.000, 2.000],  loss: 17454432.000000, mae: 1338.266357, mean_q: -237.702637
 3242/5000: episode: 3242, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1913.262, mean reward: -1913.262 [-1913.262, -1913.262], mean action: 2.000 [2.000, 2.000],  loss: 12190314.000000, mae: 1116.353149, mean_q: -239.034103
 3243/5000: episode: 3243, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2157.319, mean reward: -2157.319 [-2157.319, -2157.319], mean action: 2.000 [2.000, 2.000],  loss: 11722626.000000, mae: 1105.291260, mean_q: -239.157623
 3244/5000: episode: 3244, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6562.120, mean reward: -6562.120 [-6562.120, -6562.120], mean action: 2.000 [2.000, 2.000],  loss: 11785818.000000, mae: 1148.383179, mean_q: -239.308868
 3245/5000: episode: 3245, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2941.452, mean reward: -2941.452 [-2941.452, -2941.452], mean action: 2.000 [2.000, 2.000],  loss: 16803042.000000, mae: 1200.954468, mean_q: -238.894104
 3246/5000: episode: 3246, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3693.979, mean reward: -3693.979 [-3693.979, -3693.979], mean action: 2.000 [2.000, 2.000],  loss: 10536568.000000, mae: 1032.742432, mean_q: -240.311127
 3247/5000: episode: 3247, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6903.634, mean reward: -6903.634 [-6903.634, -6903.634], mean action: 2.000 [2.000, 2.000],  loss: 8841342.000000, mae: 1042.697266, mean_q: -240.419449
 3248/5000: episode: 3248, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -955.305, mean reward: -955.305 [-955.305, -955.305], mean action: 2.000 [2.000, 2.000],  loss: 9271563.000000, mae: 1002.685852, mean_q: -240.192841
 3249/5000: episode: 3249, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -601.154, mean reward: -601.154 [-601.154, -601.154], mean action: 2.000 [2.000, 2.000],  loss: 8765361.000000, mae: 975.603516, mean_q: -240.260757
 3250/5000: episode: 3250, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -667.395, mean reward: -667.395 [-667.395, -667.395], mean action: 2.000 [2.000, 2.000],  loss: 18096050.000000, mae: 1393.139771, mean_q: -239.326843
 3251/5000: episode: 3251, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -793.824, mean reward: -793.824 [-793.824, -793.824], mean action: 2.000 [2.000, 2.000],  loss: 8479617.000000, mae: 903.755493, mean_q: -241.261780
 3252/5000: episode: 3252, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -314.273, mean reward: -314.273 [-314.273, -314.273], mean action: 2.000 [2.000, 2.000],  loss: 9908544.000000, mae: 1044.495361, mean_q: -241.276230
 3253/5000: episode: 3253, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1506.615, mean reward: -1506.615 [-1506.615, -1506.615], mean action: 1.000 [1.000, 1.000],  loss: 11616500.000000, mae: 1105.822876, mean_q: -240.673889
 3254/5000: episode: 3254, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1213.294, mean reward: -1213.294 [-1213.294, -1213.294], mean action: 2.000 [2.000, 2.000],  loss: 8733701.000000, mae: 942.216187, mean_q: -239.915924
 3255/5000: episode: 3255, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1396.309, mean reward: -1396.309 [-1396.309, -1396.309], mean action: 2.000 [2.000, 2.000],  loss: 16570862.000000, mae: 1286.594238, mean_q: -240.556717
 3256/5000: episode: 3256, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -83.813, mean reward: -83.813 [-83.813, -83.813], mean action: 2.000 [2.000, 2.000],  loss: 13082696.000000, mae: 1260.321289, mean_q: -241.855347
 3257/5000: episode: 3257, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3308.979, mean reward: -3308.979 [-3308.979, -3308.979], mean action: 2.000 [2.000, 2.000],  loss: 13355288.000000, mae: 1173.255249, mean_q: -240.574127
 3258/5000: episode: 3258, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -4099.777, mean reward: -4099.777 [-4099.777, -4099.777], mean action: 1.000 [1.000, 1.000],  loss: 11793206.000000, mae: 1129.256958, mean_q: -242.006439
 3259/5000: episode: 3259, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -748.391, mean reward: -748.391 [-748.391, -748.391], mean action: 2.000 [2.000, 2.000],  loss: 9644374.000000, mae: 1068.848511, mean_q: -241.766815
 3260/5000: episode: 3260, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -314.587, mean reward: -314.587 [-314.587, -314.587], mean action: 2.000 [2.000, 2.000],  loss: 10544278.000000, mae: 1117.182617, mean_q: -241.772110
 3261/5000: episode: 3261, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -680.966, mean reward: -680.966 [-680.966, -680.966], mean action: 2.000 [2.000, 2.000],  loss: 15879538.000000, mae: 1297.341064, mean_q: -241.278458
 3262/5000: episode: 3262, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2941.242, mean reward: -2941.242 [-2941.242, -2941.242], mean action: 2.000 [2.000, 2.000],  loss: 11641894.000000, mae: 1193.029785, mean_q: -241.736526
 3263/5000: episode: 3263, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4217.139, mean reward: -4217.139 [-4217.139, -4217.139], mean action: 2.000 [2.000, 2.000],  loss: 10989279.000000, mae: 1158.041260, mean_q: -242.015320
 3264/5000: episode: 3264, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -458.408, mean reward: -458.408 [-458.408, -458.408], mean action: 2.000 [2.000, 2.000],  loss: 10720040.000000, mae: 1132.507202, mean_q: -242.106384
 3265/5000: episode: 3265, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4137.731, mean reward: -4137.731 [-4137.731, -4137.731], mean action: 3.000 [3.000, 3.000],  loss: 12256063.000000, mae: 1135.643799, mean_q: -241.848007
 3266/5000: episode: 3266, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4780.129, mean reward: -4780.129 [-4780.129, -4780.129], mean action: 2.000 [2.000, 2.000],  loss: 10737812.000000, mae: 1153.839111, mean_q: -241.878571
 3267/5000: episode: 3267, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4758.298, mean reward: -4758.298 [-4758.298, -4758.298], mean action: 2.000 [2.000, 2.000],  loss: 7593455.000000, mae: 939.416138, mean_q: -242.950775
 3268/5000: episode: 3268, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -2676.354, mean reward: -2676.354 [-2676.354, -2676.354], mean action: 2.000 [2.000, 2.000],  loss: 10329615.000000, mae: 1099.910522, mean_q: -242.859406
 3269/5000: episode: 3269, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5571.723, mean reward: -5571.723 [-5571.723, -5571.723], mean action: 2.000 [2.000, 2.000],  loss: 9916204.000000, mae: 1004.463623, mean_q: -244.962616
 3270/5000: episode: 3270, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -862.417, mean reward: -862.417 [-862.417, -862.417], mean action: 1.000 [1.000, 1.000],  loss: 15115437.000000, mae: 1264.998657, mean_q: -243.870880
 3271/5000: episode: 3271, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4564.385, mean reward: -4564.385 [-4564.385, -4564.385], mean action: 2.000 [2.000, 2.000],  loss: 13879214.000000, mae: 1307.561279, mean_q: -243.450531
 3272/5000: episode: 3272, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3627.267, mean reward: -3627.267 [-3627.267, -3627.267], mean action: 2.000 [2.000, 2.000],  loss: 8039776.000000, mae: 908.575439, mean_q: -243.309128
 3273/5000: episode: 3273, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2282.975, mean reward: -2282.975 [-2282.975, -2282.975], mean action: 2.000 [2.000, 2.000],  loss: 5515895.000000, mae: 845.611816, mean_q: -243.364410
 3274/5000: episode: 3274, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1604.289, mean reward: -1604.289 [-1604.289, -1604.289], mean action: 2.000 [2.000, 2.000],  loss: 10831919.000000, mae: 1187.991089, mean_q: -243.283142
 3275/5000: episode: 3275, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7727.914, mean reward: -7727.914 [-7727.914, -7727.914], mean action: 0.000 [0.000, 0.000],  loss: 9480696.000000, mae: 1038.640259, mean_q: -244.933868
 3276/5000: episode: 3276, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2846.691, mean reward: -2846.691 [-2846.691, -2846.691], mean action: 2.000 [2.000, 2.000],  loss: 11887574.000000, mae: 1200.318848, mean_q: -244.397308
 3277/5000: episode: 3277, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2954.921, mean reward: -2954.921 [-2954.921, -2954.921], mean action: 2.000 [2.000, 2.000],  loss: 12273746.000000, mae: 1180.567383, mean_q: -244.329605
 3278/5000: episode: 3278, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2323.358, mean reward: -2323.358 [-2323.358, -2323.358], mean action: 2.000 [2.000, 2.000],  loss: 13462742.000000, mae: 1273.500977, mean_q: -244.530365
 3279/5000: episode: 3279, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6573.986, mean reward: -6573.986 [-6573.986, -6573.986], mean action: 2.000 [2.000, 2.000],  loss: 20351104.000000, mae: 1541.930420, mean_q: -244.251068
 3280/5000: episode: 3280, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -884.614, mean reward: -884.614 [-884.614, -884.614], mean action: 2.000 [2.000, 2.000],  loss: 9760620.000000, mae: 1131.548218, mean_q: -245.009308
 3281/5000: episode: 3281, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1336.019, mean reward: -1336.019 [-1336.019, -1336.019], mean action: 2.000 [2.000, 2.000],  loss: 10452020.000000, mae: 1051.924194, mean_q: -246.011993
 3282/5000: episode: 3282, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3155.291, mean reward: -3155.291 [-3155.291, -3155.291], mean action: 2.000 [2.000, 2.000],  loss: 7292011.000000, mae: 979.388733, mean_q: -245.183624
 3283/5000: episode: 3283, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -6240.650, mean reward: -6240.650 [-6240.650, -6240.650], mean action: 3.000 [3.000, 3.000],  loss: 16042611.000000, mae: 1263.807617, mean_q: -245.351105
 3284/5000: episode: 3284, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3573.278, mean reward: -3573.278 [-3573.278, -3573.278], mean action: 2.000 [2.000, 2.000],  loss: 11156209.000000, mae: 1140.903076, mean_q: -246.494293
 3285/5000: episode: 3285, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6615.462, mean reward: -6615.462 [-6615.462, -6615.462], mean action: 3.000 [3.000, 3.000],  loss: 9480976.000000, mae: 1053.059570, mean_q: -247.467712
 3286/5000: episode: 3286, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -210.926, mean reward: -210.926 [-210.926, -210.926], mean action: 2.000 [2.000, 2.000],  loss: 14514259.000000, mae: 1271.242432, mean_q: -245.620529
 3287/5000: episode: 3287, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -39.666, mean reward: -39.666 [-39.666, -39.666], mean action: 3.000 [3.000, 3.000],  loss: 12227366.000000, mae: 1181.370361, mean_q: -245.938766
 3288/5000: episode: 3288, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1977.712, mean reward: -1977.712 [-1977.712, -1977.712], mean action: 3.000 [3.000, 3.000],  loss: 10723674.000000, mae: 1134.551758, mean_q: -246.461334
 3289/5000: episode: 3289, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -901.803, mean reward: -901.803 [-901.803, -901.803], mean action: 3.000 [3.000, 3.000],  loss: 4634003.000000, mae: 813.022705, mean_q: -247.991180
 3290/5000: episode: 3290, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -7345.503, mean reward: -7345.503 [-7345.503, -7345.503], mean action: 3.000 [3.000, 3.000],  loss: 11503776.000000, mae: 1158.525513, mean_q: -247.722717
 3291/5000: episode: 3291, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3498.758, mean reward: -3498.758 [-3498.758, -3498.758], mean action: 3.000 [3.000, 3.000],  loss: 13603562.000000, mae: 1157.764893, mean_q: -247.710571
 3292/5000: episode: 3292, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4884.176, mean reward: -4884.176 [-4884.176, -4884.176], mean action: 3.000 [3.000, 3.000],  loss: 10748576.000000, mae: 1136.912354, mean_q: -246.999161
 3293/5000: episode: 3293, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5620.626, mean reward: -5620.626 [-5620.626, -5620.626], mean action: 3.000 [3.000, 3.000],  loss: 10393958.000000, mae: 1117.472534, mean_q: -248.243713
 3294/5000: episode: 3294, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1679.041, mean reward: -1679.041 [-1679.041, -1679.041], mean action: 2.000 [2.000, 2.000],  loss: 14627952.000000, mae: 1281.554932, mean_q: -247.301285
 3295/5000: episode: 3295, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5656.903, mean reward: -5656.903 [-5656.903, -5656.903], mean action: 3.000 [3.000, 3.000],  loss: 10353650.000000, mae: 1070.996460, mean_q: -247.874130
 3296/5000: episode: 3296, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7203.616, mean reward: -7203.616 [-7203.616, -7203.616], mean action: 3.000 [3.000, 3.000],  loss: 12585843.000000, mae: 1206.669189, mean_q: -247.444336
 3297/5000: episode: 3297, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3158.704, mean reward: -3158.704 [-3158.704, -3158.704], mean action: 3.000 [3.000, 3.000],  loss: 11335377.000000, mae: 1144.998047, mean_q: -248.111374
 3298/5000: episode: 3298, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1113.973, mean reward: -1113.973 [-1113.973, -1113.973], mean action: 3.000 [3.000, 3.000],  loss: 8698572.000000, mae: 999.593201, mean_q: -247.880005
 3299/5000: episode: 3299, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -19.245, mean reward: -19.245 [-19.245, -19.245], mean action: 3.000 [3.000, 3.000],  loss: 10084630.000000, mae: 1068.767700, mean_q: -248.449127
 3300/5000: episode: 3300, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2741.210, mean reward: -2741.210 [-2741.210, -2741.210], mean action: 3.000 [3.000, 3.000],  loss: 13824707.000000, mae: 1213.474365, mean_q: -248.308411
 3301/5000: episode: 3301, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6700.919, mean reward: -6700.919 [-6700.919, -6700.919], mean action: 3.000 [3.000, 3.000],  loss: 18319548.000000, mae: 1377.693604, mean_q: -248.314789
 3302/5000: episode: 3302, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2406.918, mean reward: -2406.918 [-2406.918, -2406.918], mean action: 3.000 [3.000, 3.000],  loss: 9618044.000000, mae: 1085.401611, mean_q: -250.071381
 3303/5000: episode: 3303, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2760.415, mean reward: -2760.415 [-2760.415, -2760.415], mean action: 3.000 [3.000, 3.000],  loss: 13449891.000000, mae: 1223.976318, mean_q: -248.423126
 3304/5000: episode: 3304, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -6850.703, mean reward: -6850.703 [-6850.703, -6850.703], mean action: 3.000 [3.000, 3.000],  loss: 14878144.000000, mae: 1312.260864, mean_q: -249.033951
 3305/5000: episode: 3305, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2603.244, mean reward: -2603.244 [-2603.244, -2603.244], mean action: 3.000 [3.000, 3.000],  loss: 12837668.000000, mae: 1199.048340, mean_q: -248.902374
 3306/5000: episode: 3306, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -6391.232, mean reward: -6391.232 [-6391.232, -6391.232], mean action: 3.000 [3.000, 3.000],  loss: 16687210.000000, mae: 1257.492188, mean_q: -249.066559
 3307/5000: episode: 3307, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -2739.350, mean reward: -2739.350 [-2739.350, -2739.350], mean action: 3.000 [3.000, 3.000],  loss: 15125490.000000, mae: 1178.873047, mean_q: -248.887604
 3308/5000: episode: 3308, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3637.642, mean reward: -3637.642 [-3637.642, -3637.642], mean action: 3.000 [3.000, 3.000],  loss: 11850646.000000, mae: 1184.136963, mean_q: -250.048370
 3309/5000: episode: 3309, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -6946.215, mean reward: -6946.215 [-6946.215, -6946.215], mean action: 3.000 [3.000, 3.000],  loss: 11062730.000000, mae: 1129.847290, mean_q: -250.532318
 3310/5000: episode: 3310, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3268.944, mean reward: -3268.944 [-3268.944, -3268.944], mean action: 3.000 [3.000, 3.000],  loss: 19299780.000000, mae: 1336.211670, mean_q: -249.214172
 3311/5000: episode: 3311, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -534.374, mean reward: -534.374 [-534.374, -534.374], mean action: 3.000 [3.000, 3.000],  loss: 13947658.000000, mae: 1206.187500, mean_q: -249.987030
 3312/5000: episode: 3312, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -4547.630, mean reward: -4547.630 [-4547.630, -4547.630], mean action: 3.000 [3.000, 3.000],  loss: 24642422.000000, mae: 1566.933105, mean_q: -248.802063
 3313/5000: episode: 3313, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -6580.532, mean reward: -6580.532 [-6580.532, -6580.532], mean action: 3.000 [3.000, 3.000],  loss: 10920826.000000, mae: 1129.148560, mean_q: -250.626709
 3314/5000: episode: 3314, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -1083.941, mean reward: -1083.941 [-1083.941, -1083.941], mean action: 2.000 [2.000, 2.000],  loss: 9075755.000000, mae: 1098.394287, mean_q: -252.013702
 3315/5000: episode: 3315, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -3595.246, mean reward: -3595.246 [-3595.246, -3595.246], mean action: 3.000 [3.000, 3.000],  loss: 9509132.000000, mae: 1042.601074, mean_q: -251.103989
 3316/5000: episode: 3316, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -9965.625, mean reward: -9965.625 [-9965.625, -9965.625], mean action: 3.000 [3.000, 3.000],  loss: 11961578.000000, mae: 1082.457153, mean_q: -250.961609
 3317/5000: episode: 3317, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -3345.649, mean reward: -3345.649 [-3345.649, -3345.649], mean action: 3.000 [3.000, 3.000],  loss: 9462632.000000, mae: 1008.916626, mean_q: -251.680359
 3318/5000: episode: 3318, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2059.775, mean reward: -2059.775 [-2059.775, -2059.775], mean action: 3.000 [3.000, 3.000],  loss: 15497491.000000, mae: 1273.364990, mean_q: -251.453339
 3319/5000: episode: 3319, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1402.656, mean reward: -1402.656 [-1402.656, -1402.656], mean action: 3.000 [3.000, 3.000],  loss: 11596714.000000, mae: 1114.429810, mean_q: -252.417496
 3320/5000: episode: 3320, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -2274.863, mean reward: -2274.863 [-2274.863, -2274.863], mean action: 3.000 [3.000, 3.000],  loss: 11199204.000000, mae: 1057.679932, mean_q: -249.943146
 3321/5000: episode: 3321, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -11275.272, mean reward: -11275.272 [-11275.272, -11275.272], mean action: 3.000 [3.000, 3.000],  loss: 13186880.000000, mae: 1202.151611, mean_q: -251.555557
 3322/5000: episode: 3322, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -6079.526, mean reward: -6079.526 [-6079.526, -6079.526], mean action: 3.000 [3.000, 3.000],  loss: 10652648.000000, mae: 1082.911743, mean_q: -252.652908
 3323/5000: episode: 3323, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -787.380, mean reward: -787.380 [-787.380, -787.380], mean action: 3.000 [3.000, 3.000],  loss: 9103258.000000, mae: 1083.617554, mean_q: -252.944916
 3324/5000: episode: 3324, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -522.016, mean reward: -522.016 [-522.016, -522.016], mean action: 3.000 [3.000, 3.000],  loss: 7480278.000000, mae: 996.025635, mean_q: -253.491882
 3325/5000: episode: 3325, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4036.748, mean reward: -4036.748 [-4036.748, -4036.748], mean action: 3.000 [3.000, 3.000],  loss: 10195892.000000, mae: 1095.474609, mean_q: -254.030045
 3326/5000: episode: 3326, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -834.435, mean reward: -834.435 [-834.435, -834.435], mean action: 3.000 [3.000, 3.000],  loss: 9975186.000000, mae: 1030.629150, mean_q: -252.885071
 3327/5000: episode: 3327, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -4701.961, mean reward: -4701.961 [-4701.961, -4701.961], mean action: 3.000 [3.000, 3.000],  loss: 6464464.000000, mae: 883.884399, mean_q: -254.825760
 3328/5000: episode: 3328, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -6785.102, mean reward: -6785.102 [-6785.102, -6785.102], mean action: 3.000 [3.000, 3.000],  loss: 12357004.000000, mae: 1188.102295, mean_q: -252.512543
 3329/5000: episode: 3329, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3038.582, mean reward: -3038.582 [-3038.582, -3038.582], mean action: 0.000 [0.000, 0.000],  loss: 5507383.500000, mae: 877.427490, mean_q: -254.724197
 3330/5000: episode: 3330, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9953.273, mean reward: -9953.273 [-9953.273, -9953.273], mean action: 3.000 [3.000, 3.000],  loss: 14295670.000000, mae: 1221.768433, mean_q: -254.657669
 3331/5000: episode: 3331, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -602.886, mean reward: -602.886 [-602.886, -602.886], mean action: 3.000 [3.000, 3.000],  loss: 11297620.000000, mae: 1070.339722, mean_q: -254.001236
 3332/5000: episode: 3332, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5616.373, mean reward: -5616.373 [-5616.373, -5616.373], mean action: 3.000 [3.000, 3.000],  loss: 12053110.000000, mae: 1146.896484, mean_q: -255.049255
 3333/5000: episode: 3333, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -215.113, mean reward: -215.113 [-215.113, -215.113], mean action: 3.000 [3.000, 3.000],  loss: 17681806.000000, mae: 1396.534912, mean_q: -254.095215
 3334/5000: episode: 3334, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -657.718, mean reward: -657.718 [-657.718, -657.718], mean action: 3.000 [3.000, 3.000],  loss: 12994326.000000, mae: 1150.676025, mean_q: -254.052933
 3335/5000: episode: 3335, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -409.186, mean reward: -409.186 [-409.186, -409.186], mean action: 3.000 [3.000, 3.000],  loss: 7762799.000000, mae: 971.109131, mean_q: -255.788284
 3336/5000: episode: 3336, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3773.319, mean reward: -3773.319 [-3773.319, -3773.319], mean action: 3.000 [3.000, 3.000],  loss: 11247226.000000, mae: 1089.471069, mean_q: -255.310272
 3337/5000: episode: 3337, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -8.383, mean reward: -8.383 [-8.383, -8.383], mean action: 3.000 [3.000, 3.000],  loss: 13620968.000000, mae: 1287.961914, mean_q: -255.336700
 3338/5000: episode: 3338, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1940.257, mean reward: -1940.257 [-1940.257, -1940.257], mean action: 3.000 [3.000, 3.000],  loss: 11624816.000000, mae: 1113.463745, mean_q: -255.411896
 3339/5000: episode: 3339, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2092.996, mean reward: -2092.996 [-2092.996, -2092.996], mean action: 3.000 [3.000, 3.000],  loss: 10283232.000000, mae: 1141.370117, mean_q: -256.469482
 3340/5000: episode: 3340, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -780.509, mean reward: -780.509 [-780.509, -780.509], mean action: 3.000 [3.000, 3.000],  loss: 12215634.000000, mae: 1226.078613, mean_q: -255.611801
 3341/5000: episode: 3341, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -6903.616, mean reward: -6903.616 [-6903.616, -6903.616], mean action: 3.000 [3.000, 3.000],  loss: 12767602.000000, mae: 1150.012939, mean_q: -256.275360
 3342/5000: episode: 3342, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -949.800, mean reward: -949.800 [-949.800, -949.800], mean action: 3.000 [3.000, 3.000],  loss: 9280571.000000, mae: 1041.648193, mean_q: -256.181946
 3343/5000: episode: 3343, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -111.224, mean reward: -111.224 [-111.224, -111.224], mean action: 3.000 [3.000, 3.000],  loss: 13895368.000000, mae: 1241.763672, mean_q: -256.541565
 3344/5000: episode: 3344, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2166.010, mean reward: -2166.010 [-2166.010, -2166.010], mean action: 3.000 [3.000, 3.000],  loss: 8169561.000000, mae: 1039.506104, mean_q: -256.431458
 3345/5000: episode: 3345, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4465.557, mean reward: -4465.557 [-4465.557, -4465.557], mean action: 3.000 [3.000, 3.000],  loss: 13084137.000000, mae: 1056.765015, mean_q: -256.493256
 3346/5000: episode: 3346, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7062.425, mean reward: -7062.425 [-7062.425, -7062.425], mean action: 3.000 [3.000, 3.000],  loss: 9888171.000000, mae: 1119.445557, mean_q: -257.639984
 3347/5000: episode: 3347, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5887.196, mean reward: -5887.196 [-5887.196, -5887.196], mean action: 3.000 [3.000, 3.000],  loss: 7345706.500000, mae: 990.336914, mean_q: -257.099426
 3348/5000: episode: 3348, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5811.199, mean reward: -5811.199 [-5811.199, -5811.199], mean action: 3.000 [3.000, 3.000],  loss: 7838126.000000, mae: 937.174133, mean_q: -257.917542
 3349/5000: episode: 3349, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -952.987, mean reward: -952.987 [-952.987, -952.987], mean action: 3.000 [3.000, 3.000],  loss: 11443249.000000, mae: 1116.694580, mean_q: -257.056030
 3350/5000: episode: 3350, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5403.071, mean reward: -5403.071 [-5403.071, -5403.071], mean action: 3.000 [3.000, 3.000],  loss: 11663123.000000, mae: 1160.552002, mean_q: -258.009460
 3351/5000: episode: 3351, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3372.197, mean reward: -3372.197 [-3372.197, -3372.197], mean action: 3.000 [3.000, 3.000],  loss: 7437333.000000, mae: 973.770081, mean_q: -259.073425
 3352/5000: episode: 3352, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1241.845, mean reward: -1241.845 [-1241.845, -1241.845], mean action: 2.000 [2.000, 2.000],  loss: 22273562.000000, mae: 1636.808594, mean_q: -257.039612
 3353/5000: episode: 3353, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2368.052, mean reward: -2368.052 [-2368.052, -2368.052], mean action: 2.000 [2.000, 2.000],  loss: 12391860.000000, mae: 1153.195068, mean_q: -258.645660
 3354/5000: episode: 3354, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5.688, mean reward: -5.688 [-5.688, -5.688], mean action: 2.000 [2.000, 2.000],  loss: 14848884.000000, mae: 1251.906738, mean_q: -258.772919
 3355/5000: episode: 3355, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5251.365, mean reward: -5251.365 [-5251.365, -5251.365], mean action: 3.000 [3.000, 3.000],  loss: 14930662.000000, mae: 1361.431519, mean_q: -257.658966
 3356/5000: episode: 3356, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2106.789, mean reward: -2106.789 [-2106.789, -2106.789], mean action: 3.000 [3.000, 3.000],  loss: 11179720.000000, mae: 1138.534912, mean_q: -257.728302
 3357/5000: episode: 3357, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2017.811, mean reward: -2017.811 [-2017.811, -2017.811], mean action: 3.000 [3.000, 3.000],  loss: 9862549.000000, mae: 1102.127197, mean_q: -259.173767
 3358/5000: episode: 3358, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6527.108, mean reward: -6527.108 [-6527.108, -6527.108], mean action: 3.000 [3.000, 3.000],  loss: 7898804.500000, mae: 1027.467529, mean_q: -259.363586
 3359/5000: episode: 3359, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1032.825, mean reward: -1032.825 [-1032.825, -1032.825], mean action: 2.000 [2.000, 2.000],  loss: 12839920.000000, mae: 1166.532715, mean_q: -258.895752
 3360/5000: episode: 3360, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9830.647, mean reward: -9830.647 [-9830.647, -9830.647], mean action: 1.000 [1.000, 1.000],  loss: 11683964.000000, mae: 1186.427612, mean_q: -259.535828
 3361/5000: episode: 3361, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2791.849, mean reward: -2791.849 [-2791.849, -2791.849], mean action: 2.000 [2.000, 2.000],  loss: 7929028.000000, mae: 921.129028, mean_q: -260.323669
 3362/5000: episode: 3362, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -618.307, mean reward: -618.307 [-618.307, -618.307], mean action: 2.000 [2.000, 2.000],  loss: 18233724.000000, mae: 1436.659424, mean_q: -260.227966
 3363/5000: episode: 3363, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2865.693, mean reward: -2865.693 [-2865.693, -2865.693], mean action: 2.000 [2.000, 2.000],  loss: 8850210.000000, mae: 1035.923828, mean_q: -259.912598
 3364/5000: episode: 3364, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1719.485, mean reward: -1719.485 [-1719.485, -1719.485], mean action: 2.000 [2.000, 2.000],  loss: 12084832.000000, mae: 1161.850830, mean_q: -261.071136
 3365/5000: episode: 3365, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1424.108, mean reward: -1424.108 [-1424.108, -1424.108], mean action: 2.000 [2.000, 2.000],  loss: 8424666.000000, mae: 1006.242188, mean_q: -260.839661
 3366/5000: episode: 3366, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1928.023, mean reward: -1928.023 [-1928.023, -1928.023], mean action: 2.000 [2.000, 2.000],  loss: 11044014.000000, mae: 1136.589844, mean_q: -260.909973
 3367/5000: episode: 3367, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -3028.229, mean reward: -3028.229 [-3028.229, -3028.229], mean action: 2.000 [2.000, 2.000],  loss: 9583292.000000, mae: 1080.920044, mean_q: -260.749634
 3368/5000: episode: 3368, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3331.132, mean reward: -3331.132 [-3331.132, -3331.132], mean action: 2.000 [2.000, 2.000],  loss: 11164692.000000, mae: 1188.910400, mean_q: -260.424194
 3369/5000: episode: 3369, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1512.281, mean reward: -1512.281 [-1512.281, -1512.281], mean action: 2.000 [2.000, 2.000],  loss: 9999230.000000, mae: 1073.477417, mean_q: -260.542236
 3370/5000: episode: 3370, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -797.815, mean reward: -797.815 [-797.815, -797.815], mean action: 2.000 [2.000, 2.000],  loss: 15557107.000000, mae: 1350.710815, mean_q: -260.982819
 3371/5000: episode: 3371, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2499.887, mean reward: -2499.887 [-2499.887, -2499.887], mean action: 2.000 [2.000, 2.000],  loss: 12833912.000000, mae: 1259.243530, mean_q: -260.098755
 3372/5000: episode: 3372, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6615.422, mean reward: -6615.422 [-6615.422, -6615.422], mean action: 2.000 [2.000, 2.000],  loss: 14359078.000000, mae: 1272.524292, mean_q: -260.736786
 3373/5000: episode: 3373, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2821.364, mean reward: -2821.364 [-2821.364, -2821.364], mean action: 2.000 [2.000, 2.000],  loss: 12813459.000000, mae: 1167.996338, mean_q: -260.153259
 3374/5000: episode: 3374, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1537.875, mean reward: -1537.875 [-1537.875, -1537.875], mean action: 2.000 [2.000, 2.000],  loss: 9528410.000000, mae: 1003.927734, mean_q: -261.212280
 3375/5000: episode: 3375, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1179.110, mean reward: -1179.110 [-1179.110, -1179.110], mean action: 2.000 [2.000, 2.000],  loss: 11314352.000000, mae: 1162.384521, mean_q: -261.522278
 3376/5000: episode: 3376, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2471.830, mean reward: -2471.830 [-2471.830, -2471.830], mean action: 2.000 [2.000, 2.000],  loss: 12381228.000000, mae: 1143.759277, mean_q: -261.685486
 3377/5000: episode: 3377, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1619.462, mean reward: -1619.462 [-1619.462, -1619.462], mean action: 3.000 [3.000, 3.000],  loss: 13449562.000000, mae: 1203.439087, mean_q: -262.277069
 3378/5000: episode: 3378, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1975.373, mean reward: -1975.373 [-1975.373, -1975.373], mean action: 2.000 [2.000, 2.000],  loss: 9583064.000000, mae: 1059.332275, mean_q: -262.137207
 3379/5000: episode: 3379, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9177.082, mean reward: -9177.082 [-9177.082, -9177.082], mean action: 2.000 [2.000, 2.000],  loss: 12768636.000000, mae: 1156.676392, mean_q: -263.268188
 3380/5000: episode: 3380, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3648.522, mean reward: -3648.522 [-3648.522, -3648.522], mean action: 2.000 [2.000, 2.000],  loss: 7329350.000000, mae: 974.328125, mean_q: -262.268494
 3381/5000: episode: 3381, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2414.893, mean reward: -2414.893 [-2414.893, -2414.893], mean action: 2.000 [2.000, 2.000],  loss: 10589144.000000, mae: 1113.266357, mean_q: -262.532166
 3382/5000: episode: 3382, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6287.984, mean reward: -6287.984 [-6287.984, -6287.984], mean action: 1.000 [1.000, 1.000],  loss: 15092720.000000, mae: 1178.780396, mean_q: -263.041260
 3383/5000: episode: 3383, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -218.369, mean reward: -218.369 [-218.369, -218.369], mean action: 2.000 [2.000, 2.000],  loss: 16402129.000000, mae: 1293.203613, mean_q: -263.955902
 3384/5000: episode: 3384, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2935.677, mean reward: -2935.677 [-2935.677, -2935.677], mean action: 0.000 [0.000, 0.000],  loss: 14873840.000000, mae: 1260.980957, mean_q: -263.715332
 3385/5000: episode: 3385, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -1535.014, mean reward: -1535.014 [-1535.014, -1535.014], mean action: 2.000 [2.000, 2.000],  loss: 11348467.000000, mae: 1058.875854, mean_q: -264.175476
 3386/5000: episode: 3386, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1286.419, mean reward: -1286.419 [-1286.419, -1286.419], mean action: 2.000 [2.000, 2.000],  loss: 11232545.000000, mae: 1082.143555, mean_q: -263.987488
 3387/5000: episode: 3387, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3069.131, mean reward: -3069.131 [-3069.131, -3069.131], mean action: 2.000 [2.000, 2.000],  loss: 14804420.000000, mae: 1229.293701, mean_q: -264.780457
 3388/5000: episode: 3388, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -408.663, mean reward: -408.663 [-408.663, -408.663], mean action: 2.000 [2.000, 2.000],  loss: 14778645.000000, mae: 1275.697876, mean_q: -264.003845
 3389/5000: episode: 3389, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1437.150, mean reward: -1437.150 [-1437.150, -1437.150], mean action: 2.000 [2.000, 2.000],  loss: 14863910.000000, mae: 1314.215454, mean_q: -263.150696
 3390/5000: episode: 3390, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2574.066, mean reward: -2574.066 [-2574.066, -2574.066], mean action: 2.000 [2.000, 2.000],  loss: 8282129.000000, mae: 943.597839, mean_q: -264.913208
 3391/5000: episode: 3391, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1953.520, mean reward: -1953.520 [-1953.520, -1953.520], mean action: 2.000 [2.000, 2.000],  loss: 11956276.000000, mae: 1164.794556, mean_q: -264.492493
 3392/5000: episode: 3392, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5930.397, mean reward: -5930.397 [-5930.397, -5930.397], mean action: 3.000 [3.000, 3.000],  loss: 11441406.000000, mae: 1215.695068, mean_q: -265.314819
 3393/5000: episode: 3393, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4084.749, mean reward: -4084.749 [-4084.749, -4084.749], mean action: 2.000 [2.000, 2.000],  loss: 11870034.000000, mae: 1211.461182, mean_q: -265.014374
 3394/5000: episode: 3394, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5.677, mean reward: -5.677 [-5.677, -5.677], mean action: 2.000 [2.000, 2.000],  loss: 9421915.000000, mae: 1030.681885, mean_q: -266.229614
 3395/5000: episode: 3395, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1265.519, mean reward: -1265.519 [-1265.519, -1265.519], mean action: 2.000 [2.000, 2.000],  loss: 14474451.000000, mae: 1230.739502, mean_q: -264.897766
 3396/5000: episode: 3396, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -296.539, mean reward: -296.539 [-296.539, -296.539], mean action: 2.000 [2.000, 2.000],  loss: 9336588.000000, mae: 1084.312988, mean_q: -265.409363
 3397/5000: episode: 3397, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5335.261, mean reward: -5335.261 [-5335.261, -5335.261], mean action: 3.000 [3.000, 3.000],  loss: 9531710.000000, mae: 1084.177612, mean_q: -265.616760
 3398/5000: episode: 3398, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1639.296, mean reward: -1639.296 [-1639.296, -1639.296], mean action: 3.000 [3.000, 3.000],  loss: 16293538.000000, mae: 1323.708252, mean_q: -266.439362
 3399/5000: episode: 3399, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2469.375, mean reward: -2469.375 [-2469.375, -2469.375], mean action: 3.000 [3.000, 3.000],  loss: 11729002.000000, mae: 1188.255371, mean_q: -265.759796
 3400/5000: episode: 3400, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5853.831, mean reward: -5853.831 [-5853.831, -5853.831], mean action: 3.000 [3.000, 3.000],  loss: 14353657.000000, mae: 1272.395752, mean_q: -265.223907
 3401/5000: episode: 3401, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3190.829, mean reward: -3190.829 [-3190.829, -3190.829], mean action: 3.000 [3.000, 3.000],  loss: 16961150.000000, mae: 1394.057007, mean_q: -266.353088
 3402/5000: episode: 3402, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1998.834, mean reward: -1998.834 [-1998.834, -1998.834], mean action: 3.000 [3.000, 3.000],  loss: 8912594.000000, mae: 978.788940, mean_q: -267.654053
 3403/5000: episode: 3403, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -860.995, mean reward: -860.995 [-860.995, -860.995], mean action: 3.000 [3.000, 3.000],  loss: 17024470.000000, mae: 1456.341553, mean_q: -266.626617
 3404/5000: episode: 3404, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -1205.414, mean reward: -1205.414 [-1205.414, -1205.414], mean action: 3.000 [3.000, 3.000],  loss: 6398961.000000, mae: 894.995544, mean_q: -268.256714
 3405/5000: episode: 3405, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1810.670, mean reward: -1810.670 [-1810.670, -1810.670], mean action: 3.000 [3.000, 3.000],  loss: 12143416.000000, mae: 1173.891602, mean_q: -268.271790
 3406/5000: episode: 3406, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -9445.750, mean reward: -9445.750 [-9445.750, -9445.750], mean action: 0.000 [0.000, 0.000],  loss: 7146140.500000, mae: 938.952271, mean_q: -266.792053
 3407/5000: episode: 3407, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3771.512, mean reward: -3771.512 [-3771.512, -3771.512], mean action: 3.000 [3.000, 3.000],  loss: 18697096.000000, mae: 1526.550171, mean_q: -266.625244
 3408/5000: episode: 3408, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -1619.726, mean reward: -1619.726 [-1619.726, -1619.726], mean action: 3.000 [3.000, 3.000],  loss: 12284442.000000, mae: 1129.145752, mean_q: -266.983551
 3409/5000: episode: 3409, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3127.120, mean reward: -3127.120 [-3127.120, -3127.120], mean action: 3.000 [3.000, 3.000],  loss: 16811280.000000, mae: 1311.278931, mean_q: -267.324127
 3410/5000: episode: 3410, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -2358.357, mean reward: -2358.357 [-2358.357, -2358.357], mean action: 2.000 [2.000, 2.000],  loss: 16204602.000000, mae: 1189.865723, mean_q: -267.864258
 3411/5000: episode: 3411, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -486.174, mean reward: -486.174 [-486.174, -486.174], mean action: 3.000 [3.000, 3.000],  loss: 5695929.000000, mae: 862.824951, mean_q: -268.230103
 3412/5000: episode: 3412, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -204.368, mean reward: -204.368 [-204.368, -204.368], mean action: 2.000 [2.000, 2.000],  loss: 9962062.000000, mae: 1102.488770, mean_q: -268.447540
 3413/5000: episode: 3413, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3885.657, mean reward: -3885.657 [-3885.657, -3885.657], mean action: 2.000 [2.000, 2.000],  loss: 7071815.500000, mae: 903.483032, mean_q: -269.107544
 3414/5000: episode: 3414, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -753.459, mean reward: -753.459 [-753.459, -753.459], mean action: 2.000 [2.000, 2.000],  loss: 10677343.000000, mae: 1090.344238, mean_q: -268.489380
 3415/5000: episode: 3415, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1536.067, mean reward: -1536.067 [-1536.067, -1536.067], mean action: 2.000 [2.000, 2.000],  loss: 12214676.000000, mae: 1125.327148, mean_q: -269.879395
 3416/5000: episode: 3416, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -43.763, mean reward: -43.763 [-43.763, -43.763], mean action: 2.000 [2.000, 2.000],  loss: 12207426.000000, mae: 1166.104248, mean_q: -269.709808
 3417/5000: episode: 3417, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2689.839, mean reward: -2689.839 [-2689.839, -2689.839], mean action: 3.000 [3.000, 3.000],  loss: 13352770.000000, mae: 1157.016968, mean_q: -269.941589
 3418/5000: episode: 3418, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2740.679, mean reward: -2740.679 [-2740.679, -2740.679], mean action: 2.000 [2.000, 2.000],  loss: 8003973.000000, mae: 937.736694, mean_q: -269.265137
 3419/5000: episode: 3419, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7691.800, mean reward: -7691.800 [-7691.800, -7691.800], mean action: 3.000 [3.000, 3.000],  loss: 11148770.000000, mae: 1134.491821, mean_q: -269.972351
 3420/5000: episode: 3420, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1874.078, mean reward: -1874.078 [-1874.078, -1874.078], mean action: 1.000 [1.000, 1.000],  loss: 13059608.000000, mae: 1117.602173, mean_q: -268.996185
 3421/5000: episode: 3421, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -783.871, mean reward: -783.871 [-783.871, -783.871], mean action: 2.000 [2.000, 2.000],  loss: 14063217.000000, mae: 1272.428223, mean_q: -269.822479
 3422/5000: episode: 3422, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1456.114, mean reward: -1456.114 [-1456.114, -1456.114], mean action: 2.000 [2.000, 2.000],  loss: 8316455.500000, mae: 1007.160461, mean_q: -269.205688
 3423/5000: episode: 3423, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3541.609, mean reward: -3541.609 [-3541.609, -3541.609], mean action: 2.000 [2.000, 2.000],  loss: 17184836.000000, mae: 1406.918701, mean_q: -270.516846
 3424/5000: episode: 3424, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1124.598, mean reward: -1124.598 [-1124.598, -1124.598], mean action: 2.000 [2.000, 2.000],  loss: 11291800.000000, mae: 1139.297485, mean_q: -270.327087
 3425/5000: episode: 3425, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7067.209, mean reward: -7067.209 [-7067.209, -7067.209], mean action: 1.000 [1.000, 1.000],  loss: 9071784.000000, mae: 1064.881348, mean_q: -271.250305
 3426/5000: episode: 3426, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -570.718, mean reward: -570.718 [-570.718, -570.718], mean action: 2.000 [2.000, 2.000],  loss: 9896652.000000, mae: 1065.037354, mean_q: -271.159790
 3427/5000: episode: 3427, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -776.673, mean reward: -776.673 [-776.673, -776.673], mean action: 2.000 [2.000, 2.000],  loss: 11307980.000000, mae: 1164.961914, mean_q: -270.714966
 3428/5000: episode: 3428, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2751.453, mean reward: -2751.453 [-2751.453, -2751.453], mean action: 2.000 [2.000, 2.000],  loss: 12886575.000000, mae: 1271.681885, mean_q: -271.760864
 3429/5000: episode: 3429, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1136.141, mean reward: -1136.141 [-1136.141, -1136.141], mean action: 2.000 [2.000, 2.000],  loss: 6857729.500000, mae: 940.779053, mean_q: -272.633240
 3430/5000: episode: 3430, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2602.264, mean reward: -2602.264 [-2602.264, -2602.264], mean action: 2.000 [2.000, 2.000],  loss: 10621295.000000, mae: 986.937500, mean_q: -272.511963
 3431/5000: episode: 3431, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -636.844, mean reward: -636.844 [-636.844, -636.844], mean action: 2.000 [2.000, 2.000],  loss: 9586736.000000, mae: 1109.150757, mean_q: -271.257660
 3432/5000: episode: 3432, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2524.238, mean reward: -2524.238 [-2524.238, -2524.238], mean action: 2.000 [2.000, 2.000],  loss: 8615528.000000, mae: 993.081177, mean_q: -271.830383
 3433/5000: episode: 3433, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2017.999, mean reward: -2017.999 [-2017.999, -2017.999], mean action: 2.000 [2.000, 2.000],  loss: 12577582.000000, mae: 1207.278320, mean_q: -271.899323
 3434/5000: episode: 3434, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3016.920, mean reward: -3016.920 [-3016.920, -3016.920], mean action: 2.000 [2.000, 2.000],  loss: 9703425.000000, mae: 1091.606201, mean_q: -271.511017
 3435/5000: episode: 3435, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -508.651, mean reward: -508.651 [-508.651, -508.651], mean action: 2.000 [2.000, 2.000],  loss: 12836952.000000, mae: 1237.890747, mean_q: -272.124969
 3436/5000: episode: 3436, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1720.067, mean reward: -1720.067 [-1720.067, -1720.067], mean action: 2.000 [2.000, 2.000],  loss: 14931723.000000, mae: 1283.539795, mean_q: -272.196472
 3437/5000: episode: 3437, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -393.604, mean reward: -393.604 [-393.604, -393.604], mean action: 2.000 [2.000, 2.000],  loss: 10323850.000000, mae: 1124.124634, mean_q: -273.241943
 3438/5000: episode: 3438, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2644.475, mean reward: -2644.475 [-2644.475, -2644.475], mean action: 3.000 [3.000, 3.000],  loss: 7901086.500000, mae: 971.512817, mean_q: -272.643158
 3439/5000: episode: 3439, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9614.528, mean reward: -9614.528 [-9614.528, -9614.528], mean action: 2.000 [2.000, 2.000],  loss: 10518203.000000, mae: 1041.273193, mean_q: -271.565613
 3440/5000: episode: 3440, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2563.229, mean reward: -2563.229 [-2563.229, -2563.229], mean action: 2.000 [2.000, 2.000],  loss: 11351888.000000, mae: 1081.381348, mean_q: -272.700623
 3441/5000: episode: 3441, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6128.908, mean reward: -6128.908 [-6128.908, -6128.908], mean action: 2.000 [2.000, 2.000],  loss: 9097476.000000, mae: 1056.724487, mean_q: -273.300873
 3442/5000: episode: 3442, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6088.822, mean reward: -6088.822 [-6088.822, -6088.822], mean action: 2.000 [2.000, 2.000],  loss: 10318136.000000, mae: 1100.943848, mean_q: -274.335205
 3443/5000: episode: 3443, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2926.740, mean reward: -2926.740 [-2926.740, -2926.740], mean action: 2.000 [2.000, 2.000],  loss: 12830992.000000, mae: 1098.829468, mean_q: -273.850464
 3444/5000: episode: 3444, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -979.736, mean reward: -979.736 [-979.736, -979.736], mean action: 2.000 [2.000, 2.000],  loss: 9335780.000000, mae: 1107.875977, mean_q: -274.329163
 3445/5000: episode: 3445, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -396.179, mean reward: -396.179 [-396.179, -396.179], mean action: 2.000 [2.000, 2.000],  loss: 13126176.000000, mae: 1193.840210, mean_q: -274.670654
 3446/5000: episode: 3446, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -675.366, mean reward: -675.366 [-675.366, -675.366], mean action: 2.000 [2.000, 2.000],  loss: 7603135.000000, mae: 984.417664, mean_q: -274.019958
 3447/5000: episode: 3447, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4419.575, mean reward: -4419.575 [-4419.575, -4419.575], mean action: 2.000 [2.000, 2.000],  loss: 9959749.000000, mae: 1056.520874, mean_q: -274.919556
 3448/5000: episode: 3448, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -327.274, mean reward: -327.274 [-327.274, -327.274], mean action: 2.000 [2.000, 2.000],  loss: 6406191.500000, mae: 916.236511, mean_q: -275.507599
 3449/5000: episode: 3449, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1291.215, mean reward: -1291.215 [-1291.215, -1291.215], mean action: 3.000 [3.000, 3.000],  loss: 15067536.000000, mae: 1285.156250, mean_q: -274.737488
 3450/5000: episode: 3450, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3193.163, mean reward: -3193.163 [-3193.163, -3193.163], mean action: 3.000 [3.000, 3.000],  loss: 9674730.000000, mae: 1104.141724, mean_q: -274.951447
 3451/5000: episode: 3451, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4364.187, mean reward: -4364.187 [-4364.187, -4364.187], mean action: 3.000 [3.000, 3.000],  loss: 19857610.000000, mae: 1376.929077, mean_q: -274.829926
 3452/5000: episode: 3452, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4077.367, mean reward: -4077.367 [-4077.367, -4077.367], mean action: 3.000 [3.000, 3.000],  loss: 16540178.000000, mae: 1430.148438, mean_q: -275.060547
 3453/5000: episode: 3453, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -965.486, mean reward: -965.486 [-965.486, -965.486], mean action: 3.000 [3.000, 3.000],  loss: 11254118.000000, mae: 1106.199951, mean_q: -276.268860
 3454/5000: episode: 3454, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -3092.240, mean reward: -3092.240 [-3092.240, -3092.240], mean action: 3.000 [3.000, 3.000],  loss: 18934768.000000, mae: 1439.552734, mean_q: -274.841675
 3455/5000: episode: 3455, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3436.031, mean reward: -3436.031 [-3436.031, -3436.031], mean action: 3.000 [3.000, 3.000],  loss: 18565836.000000, mae: 1382.308472, mean_q: -275.925171
 3456/5000: episode: 3456, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1366.313, mean reward: -1366.313 [-1366.313, -1366.313], mean action: 3.000 [3.000, 3.000],  loss: 16847894.000000, mae: 1280.019409, mean_q: -275.494507
 3457/5000: episode: 3457, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5136.625, mean reward: -5136.625 [-5136.625, -5136.625], mean action: 3.000 [3.000, 3.000],  loss: 10415944.000000, mae: 1132.287720, mean_q: -276.219360
 3458/5000: episode: 3458, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -965.364, mean reward: -965.364 [-965.364, -965.364], mean action: 3.000 [3.000, 3.000],  loss: 8015458.000000, mae: 1011.913574, mean_q: -276.922363
 3459/5000: episode: 3459, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2816.036, mean reward: -2816.036 [-2816.036, -2816.036], mean action: 3.000 [3.000, 3.000],  loss: 9218428.000000, mae: 1074.945312, mean_q: -276.934448
 3460/5000: episode: 3460, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -749.163, mean reward: -749.163 [-749.163, -749.163], mean action: 3.000 [3.000, 3.000],  loss: 13853898.000000, mae: 1232.507080, mean_q: -276.806366
 3461/5000: episode: 3461, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1379.353, mean reward: -1379.353 [-1379.353, -1379.353], mean action: 3.000 [3.000, 3.000],  loss: 15734326.000000, mae: 1311.942871, mean_q: -276.969086
 3462/5000: episode: 3462, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5368.942, mean reward: -5368.942 [-5368.942, -5368.942], mean action: 3.000 [3.000, 3.000],  loss: 15521326.000000, mae: 1303.357056, mean_q: -276.806519
 3463/5000: episode: 3463, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6657.692, mean reward: -6657.692 [-6657.692, -6657.692], mean action: 3.000 [3.000, 3.000],  loss: 15400374.000000, mae: 1311.291260, mean_q: -277.849579
 3464/5000: episode: 3464, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -196.317, mean reward: -196.317 [-196.317, -196.317], mean action: 3.000 [3.000, 3.000],  loss: 12031802.000000, mae: 1203.381836, mean_q: -275.959473
 3465/5000: episode: 3465, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1198.708, mean reward: -1198.708 [-1198.708, -1198.708], mean action: 3.000 [3.000, 3.000],  loss: 12203133.000000, mae: 1204.882324, mean_q: -277.779297
 3466/5000: episode: 3466, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1071.493, mean reward: -1071.493 [-1071.493, -1071.493], mean action: 3.000 [3.000, 3.000],  loss: 9018324.000000, mae: 1064.010132, mean_q: -279.509521
 3467/5000: episode: 3467, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -835.491, mean reward: -835.491 [-835.491, -835.491], mean action: 3.000 [3.000, 3.000],  loss: 11466806.000000, mae: 1214.371826, mean_q: -278.743744
 3468/5000: episode: 3468, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4403.294, mean reward: -4403.294 [-4403.294, -4403.294], mean action: 3.000 [3.000, 3.000],  loss: 11280498.000000, mae: 1146.526123, mean_q: -277.637512
 3469/5000: episode: 3469, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6014.497, mean reward: -6014.497 [-6014.497, -6014.497], mean action: 3.000 [3.000, 3.000],  loss: 17561452.000000, mae: 1376.891113, mean_q: -277.696716
 3470/5000: episode: 3470, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1036.146, mean reward: -1036.146 [-1036.146, -1036.146], mean action: 3.000 [3.000, 3.000],  loss: 9467863.000000, mae: 1128.739502, mean_q: -279.328796
 3471/5000: episode: 3471, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8464.707, mean reward: -8464.707 [-8464.707, -8464.707], mean action: 3.000 [3.000, 3.000],  loss: 14876530.000000, mae: 1251.952637, mean_q: -279.558075
 3472/5000: episode: 3472, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -667.249, mean reward: -667.249 [-667.249, -667.249], mean action: 3.000 [3.000, 3.000],  loss: 9417888.000000, mae: 1083.202148, mean_q: -278.957855
 3473/5000: episode: 3473, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -846.747, mean reward: -846.747 [-846.747, -846.747], mean action: 3.000 [3.000, 3.000],  loss: 11401990.000000, mae: 1176.542236, mean_q: -278.519379
 3474/5000: episode: 3474, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4259.601, mean reward: -4259.601 [-4259.601, -4259.601], mean action: 3.000 [3.000, 3.000],  loss: 17310002.000000, mae: 1414.195557, mean_q: -280.243042
 3475/5000: episode: 3475, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -273.673, mean reward: -273.673 [-273.673, -273.673], mean action: 3.000 [3.000, 3.000],  loss: 7545585.500000, mae: 990.198486, mean_q: -278.776062
 3476/5000: episode: 3476, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1569.829, mean reward: -1569.829 [-1569.829, -1569.829], mean action: 3.000 [3.000, 3.000],  loss: 15016322.000000, mae: 1361.737305, mean_q: -279.624084
 3477/5000: episode: 3477, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5400.239, mean reward: -5400.239 [-5400.239, -5400.239], mean action: 3.000 [3.000, 3.000],  loss: 11691065.000000, mae: 1184.106445, mean_q: -278.961334
 3478/5000: episode: 3478, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3743.043, mean reward: -3743.043 [-3743.043, -3743.043], mean action: 3.000 [3.000, 3.000],  loss: 9484808.000000, mae: 1020.731995, mean_q: -281.646301
 3479/5000: episode: 3479, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5529.145, mean reward: -5529.145 [-5529.145, -5529.145], mean action: 3.000 [3.000, 3.000],  loss: 15300343.000000, mae: 1273.907715, mean_q: -280.501709
 3480/5000: episode: 3480, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4386.149, mean reward: -4386.149 [-4386.149, -4386.149], mean action: 3.000 [3.000, 3.000],  loss: 9335169.000000, mae: 1051.769531, mean_q: -279.658508
 3481/5000: episode: 3481, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6591.485, mean reward: -6591.485 [-6591.485, -6591.485], mean action: 3.000 [3.000, 3.000],  loss: 11375906.000000, mae: 1129.308105, mean_q: -280.082916
 3482/5000: episode: 3482, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5075.898, mean reward: -5075.898 [-5075.898, -5075.898], mean action: 3.000 [3.000, 3.000],  loss: 8998678.000000, mae: 1045.025269, mean_q: -281.750031
 3483/5000: episode: 3483, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1334.245, mean reward: -1334.245 [-1334.245, -1334.245], mean action: 2.000 [2.000, 2.000],  loss: 13022336.000000, mae: 1150.388428, mean_q: -280.337524
 3484/5000: episode: 3484, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -657.387, mean reward: -657.387 [-657.387, -657.387], mean action: 3.000 [3.000, 3.000],  loss: 8948112.000000, mae: 1046.900146, mean_q: -282.097412
 3485/5000: episode: 3485, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4287.874, mean reward: -4287.874 [-4287.874, -4287.874], mean action: 3.000 [3.000, 3.000],  loss: 9466298.000000, mae: 1073.026489, mean_q: -281.866241
 3486/5000: episode: 3486, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -8384.296, mean reward: -8384.296 [-8384.296, -8384.296], mean action: 3.000 [3.000, 3.000],  loss: 16339837.000000, mae: 1308.254517, mean_q: -282.016998
 3487/5000: episode: 3487, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1167.673, mean reward: -1167.673 [-1167.673, -1167.673], mean action: 3.000 [3.000, 3.000],  loss: 14902027.000000, mae: 1354.397583, mean_q: -281.679504
 3488/5000: episode: 3488, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2838.804, mean reward: -2838.804 [-2838.804, -2838.804], mean action: 3.000 [3.000, 3.000],  loss: 13405954.000000, mae: 1255.541138, mean_q: -282.068512
 3489/5000: episode: 3489, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -3816.142, mean reward: -3816.142 [-3816.142, -3816.142], mean action: 3.000 [3.000, 3.000],  loss: 11908801.000000, mae: 1170.401123, mean_q: -281.435699
 3490/5000: episode: 3490, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2922.147, mean reward: -2922.147 [-2922.147, -2922.147], mean action: 2.000 [2.000, 2.000],  loss: 10680785.000000, mae: 1186.195068, mean_q: -282.164917
 3491/5000: episode: 3491, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -1869.075, mean reward: -1869.075 [-1869.075, -1869.075], mean action: 2.000 [2.000, 2.000],  loss: 12997070.000000, mae: 1164.022705, mean_q: -283.729309
 3492/5000: episode: 3492, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5150.618, mean reward: -5150.618 [-5150.618, -5150.618], mean action: 2.000 [2.000, 2.000],  loss: 9269362.000000, mae: 1065.455811, mean_q: -282.462097
 3493/5000: episode: 3493, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1656.217, mean reward: -1656.217 [-1656.217, -1656.217], mean action: 2.000 [2.000, 2.000],  loss: 16169831.000000, mae: 1387.372192, mean_q: -282.315186
 3494/5000: episode: 3494, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1908.758, mean reward: -1908.758 [-1908.758, -1908.758], mean action: 2.000 [2.000, 2.000],  loss: 12774948.000000, mae: 1200.301514, mean_q: -283.214844
 3495/5000: episode: 3495, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1168.845, mean reward: -1168.845 [-1168.845, -1168.845], mean action: 2.000 [2.000, 2.000],  loss: 8550942.000000, mae: 991.905762, mean_q: -282.889221
 3496/5000: episode: 3496, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2895.264, mean reward: -2895.264 [-2895.264, -2895.264], mean action: 2.000 [2.000, 2.000],  loss: 13621226.000000, mae: 1202.917236, mean_q: -283.742615
 3497/5000: episode: 3497, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -79.579, mean reward: -79.579 [-79.579, -79.579], mean action: 2.000 [2.000, 2.000],  loss: 16288346.000000, mae: 1307.441528, mean_q: -283.963928
 3498/5000: episode: 3498, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3299.659, mean reward: -3299.659 [-3299.659, -3299.659], mean action: 2.000 [2.000, 2.000],  loss: 14507038.000000, mae: 1292.766968, mean_q: -282.869812
 3499/5000: episode: 3499, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -59.459, mean reward: -59.459 [-59.459, -59.459], mean action: 2.000 [2.000, 2.000],  loss: 6584306.500000, mae: 952.623840, mean_q: -284.582458
 3500/5000: episode: 3500, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1575.013, mean reward: -1575.013 [-1575.013, -1575.013], mean action: 2.000 [2.000, 2.000],  loss: 12824953.000000, mae: 1147.085938, mean_q: -285.093140
 3501/5000: episode: 3501, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6128.750, mean reward: -6128.750 [-6128.750, -6128.750], mean action: 3.000 [3.000, 3.000],  loss: 14684874.000000, mae: 1182.221680, mean_q: -284.358276
 3502/5000: episode: 3502, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1394.392, mean reward: -1394.392 [-1394.392, -1394.392], mean action: 2.000 [2.000, 2.000],  loss: 16579152.000000, mae: 1345.549927, mean_q: -284.463867
 3503/5000: episode: 3503, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2156.527, mean reward: -2156.527 [-2156.527, -2156.527], mean action: 2.000 [2.000, 2.000],  loss: 13137076.000000, mae: 1148.437744, mean_q: -284.031921
 3504/5000: episode: 3504, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -616.396, mean reward: -616.396 [-616.396, -616.396], mean action: 2.000 [2.000, 2.000],  loss: 15131248.000000, mae: 1394.452881, mean_q: -285.109375
 3505/5000: episode: 3505, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -9542.867, mean reward: -9542.867 [-9542.867, -9542.867], mean action: 3.000 [3.000, 3.000],  loss: 7849919.500000, mae: 1034.201294, mean_q: -285.764679
 3506/5000: episode: 3506, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -475.438, mean reward: -475.438 [-475.438, -475.438], mean action: 2.000 [2.000, 2.000],  loss: 17111792.000000, mae: 1314.634399, mean_q: -284.637024
 3507/5000: episode: 3507, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6094.281, mean reward: -6094.281 [-6094.281, -6094.281], mean action: 3.000 [3.000, 3.000],  loss: 10094078.000000, mae: 1152.165771, mean_q: -286.257721
 3508/5000: episode: 3508, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -1532.739, mean reward: -1532.739 [-1532.739, -1532.739], mean action: 2.000 [2.000, 2.000],  loss: 12491804.000000, mae: 1261.775024, mean_q: -284.690247
 3509/5000: episode: 3509, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -42.617, mean reward: -42.617 [-42.617, -42.617], mean action: 2.000 [2.000, 2.000],  loss: 13918500.000000, mae: 1231.815918, mean_q: -286.073730
 3510/5000: episode: 3510, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1342.077, mean reward: -1342.077 [-1342.077, -1342.077], mean action: 2.000 [2.000, 2.000],  loss: 11567910.000000, mae: 1166.569824, mean_q: -286.375092
 3511/5000: episode: 3511, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3848.076, mean reward: -3848.076 [-3848.076, -3848.076], mean action: 2.000 [2.000, 2.000],  loss: 7227383.000000, mae: 961.572021, mean_q: -286.644562
 3512/5000: episode: 3512, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -455.338, mean reward: -455.338 [-455.338, -455.338], mean action: 2.000 [2.000, 2.000],  loss: 8981714.000000, mae: 1049.030029, mean_q: -285.605530
 3513/5000: episode: 3513, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2602.676, mean reward: -2602.676 [-2602.676, -2602.676], mean action: 2.000 [2.000, 2.000],  loss: 14453276.000000, mae: 1274.177490, mean_q: -286.648834
 3514/5000: episode: 3514, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1586.036, mean reward: -1586.036 [-1586.036, -1586.036], mean action: 2.000 [2.000, 2.000],  loss: 15017103.000000, mae: 1323.230713, mean_q: -286.489746
 3515/5000: episode: 3515, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3395.327, mean reward: -3395.327 [-3395.327, -3395.327], mean action: 2.000 [2.000, 2.000],  loss: 7591152.000000, mae: 1000.435669, mean_q: -286.406372
 3516/5000: episode: 3516, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6468.176, mean reward: -6468.176 [-6468.176, -6468.176], mean action: 3.000 [3.000, 3.000],  loss: 10553691.000000, mae: 1032.504639, mean_q: -287.257324
 3517/5000: episode: 3517, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -941.990, mean reward: -941.990 [-941.990, -941.990], mean action: 2.000 [2.000, 2.000],  loss: 9414005.000000, mae: 1076.285400, mean_q: -288.603882
 3518/5000: episode: 3518, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -305.907, mean reward: -305.907 [-305.907, -305.907], mean action: 2.000 [2.000, 2.000],  loss: 9421011.000000, mae: 982.919189, mean_q: -288.070007
 3519/5000: episode: 3519, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1905.500, mean reward: -1905.500 [-1905.500, -1905.500], mean action: 2.000 [2.000, 2.000],  loss: 16538270.000000, mae: 1436.807983, mean_q: -286.836884
 3520/5000: episode: 3520, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3897.990, mean reward: -3897.990 [-3897.990, -3897.990], mean action: 2.000 [2.000, 2.000],  loss: 12333802.000000, mae: 1290.562256, mean_q: -286.794739
 3521/5000: episode: 3521, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -679.558, mean reward: -679.558 [-679.558, -679.558], mean action: 3.000 [3.000, 3.000],  loss: 14038234.000000, mae: 1188.779541, mean_q: -288.117432
 3522/5000: episode: 3522, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3079.678, mean reward: -3079.678 [-3079.678, -3079.678], mean action: 2.000 [2.000, 2.000],  loss: 15438862.000000, mae: 1282.410156, mean_q: -287.319641
 3523/5000: episode: 3523, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2405.643, mean reward: -2405.643 [-2405.643, -2405.643], mean action: 2.000 [2.000, 2.000],  loss: 12608068.000000, mae: 1239.930420, mean_q: -287.804413
 3524/5000: episode: 3524, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -534.096, mean reward: -534.096 [-534.096, -534.096], mean action: 2.000 [2.000, 2.000],  loss: 10987136.000000, mae: 1171.629517, mean_q: -287.774139
 3525/5000: episode: 3525, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -968.015, mean reward: -968.015 [-968.015, -968.015], mean action: 2.000 [2.000, 2.000],  loss: 5431353.000000, mae: 917.535034, mean_q: -291.186066
 3526/5000: episode: 3526, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -206.336, mean reward: -206.336 [-206.336, -206.336], mean action: 2.000 [2.000, 2.000],  loss: 11613088.000000, mae: 1183.963013, mean_q: -290.653992
 3527/5000: episode: 3527, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2122.636, mean reward: -2122.636 [-2122.636, -2122.636], mean action: 2.000 [2.000, 2.000],  loss: 13827790.000000, mae: 1258.510986, mean_q: -289.323425
 3528/5000: episode: 3528, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2569.734, mean reward: -2569.734 [-2569.734, -2569.734], mean action: 3.000 [3.000, 3.000],  loss: 11632200.000000, mae: 1134.270264, mean_q: -289.933533
 3529/5000: episode: 3529, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -602.532, mean reward: -602.532 [-602.532, -602.532], mean action: 2.000 [2.000, 2.000],  loss: 11871319.000000, mae: 1127.853760, mean_q: -289.464813
 3530/5000: episode: 3530, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -991.579, mean reward: -991.579 [-991.579, -991.579], mean action: 2.000 [2.000, 2.000],  loss: 6383886.000000, mae: 946.591370, mean_q: -290.327332
 3531/5000: episode: 3531, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1118.951, mean reward: -1118.951 [-1118.951, -1118.951], mean action: 2.000 [2.000, 2.000],  loss: 13760135.000000, mae: 1181.177734, mean_q: -289.625366
 3532/5000: episode: 3532, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3017.858, mean reward: -3017.858 [-3017.858, -3017.858], mean action: 2.000 [2.000, 2.000],  loss: 5340060.500000, mae: 862.140137, mean_q: -291.406677
 3533/5000: episode: 3533, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2509.594, mean reward: -2509.594 [-2509.594, -2509.594], mean action: 1.000 [1.000, 1.000],  loss: 12136582.000000, mae: 1254.137939, mean_q: -290.337036
 3534/5000: episode: 3534, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -900.965, mean reward: -900.965 [-900.965, -900.965], mean action: 2.000 [2.000, 2.000],  loss: 15176460.000000, mae: 1257.060425, mean_q: -289.961914
 3535/5000: episode: 3535, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1371.682, mean reward: -1371.682 [-1371.682, -1371.682], mean action: 2.000 [2.000, 2.000],  loss: 16052638.000000, mae: 1298.129395, mean_q: -289.633209
 3536/5000: episode: 3536, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2390.196, mean reward: -2390.196 [-2390.196, -2390.196], mean action: 2.000 [2.000, 2.000],  loss: 14196437.000000, mae: 1186.194702, mean_q: -290.570740
 3537/5000: episode: 3537, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -653.807, mean reward: -653.807 [-653.807, -653.807], mean action: 2.000 [2.000, 2.000],  loss: 12953848.000000, mae: 1239.383789, mean_q: -290.549011
 3538/5000: episode: 3538, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -448.873, mean reward: -448.873 [-448.873, -448.873], mean action: 2.000 [2.000, 2.000],  loss: 12598539.000000, mae: 1271.100098, mean_q: -290.120483
 3539/5000: episode: 3539, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3370.577, mean reward: -3370.577 [-3370.577, -3370.577], mean action: 2.000 [2.000, 2.000],  loss: 13300514.000000, mae: 1211.178467, mean_q: -290.852020
 3540/5000: episode: 3540, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -931.288, mean reward: -931.288 [-931.288, -931.288], mean action: 2.000 [2.000, 2.000],  loss: 7518981.000000, mae: 1018.546753, mean_q: -292.081696
 3541/5000: episode: 3541, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -3080.703, mean reward: -3080.703 [-3080.703, -3080.703], mean action: 2.000 [2.000, 2.000],  loss: 9538232.000000, mae: 1098.992920, mean_q: -292.170410
 3542/5000: episode: 3542, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -291.731, mean reward: -291.731 [-291.731, -291.731], mean action: 2.000 [2.000, 2.000],  loss: 13733357.000000, mae: 1251.715820, mean_q: -292.769745
 3543/5000: episode: 3543, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1809.415, mean reward: -1809.415 [-1809.415, -1809.415], mean action: 2.000 [2.000, 2.000],  loss: 5660776.000000, mae: 892.504700, mean_q: -292.594177
 3544/5000: episode: 3544, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3189.302, mean reward: -3189.302 [-3189.302, -3189.302], mean action: 2.000 [2.000, 2.000],  loss: 8771514.000000, mae: 1167.509033, mean_q: -293.986237
 3545/5000: episode: 3545, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -985.904, mean reward: -985.904 [-985.904, -985.904], mean action: 2.000 [2.000, 2.000],  loss: 11699234.000000, mae: 1200.831787, mean_q: -290.909210
 3546/5000: episode: 3546, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3866.505, mean reward: -3866.505 [-3866.505, -3866.505], mean action: 2.000 [2.000, 2.000],  loss: 13608159.000000, mae: 1295.912354, mean_q: -292.753235
 3547/5000: episode: 3547, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3036.129, mean reward: -3036.129 [-3036.129, -3036.129], mean action: 2.000 [2.000, 2.000],  loss: 12070320.000000, mae: 1189.203857, mean_q: -293.831604
 3548/5000: episode: 3548, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1306.911, mean reward: -1306.911 [-1306.911, -1306.911], mean action: 2.000 [2.000, 2.000],  loss: 12528765.000000, mae: 1111.504150, mean_q: -292.723755
 3549/5000: episode: 3549, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3587.138, mean reward: -3587.138 [-3587.138, -3587.138], mean action: 2.000 [2.000, 2.000],  loss: 8169931.500000, mae: 926.908569, mean_q: -292.977448
 3550/5000: episode: 3550, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2160.231, mean reward: -2160.231 [-2160.231, -2160.231], mean action: 2.000 [2.000, 2.000],  loss: 6796793.500000, mae: 983.518311, mean_q: -294.275818
 3551/5000: episode: 3551, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -730.859, mean reward: -730.859 [-730.859, -730.859], mean action: 2.000 [2.000, 2.000],  loss: 8829514.000000, mae: 1024.789795, mean_q: -294.337341
 3552/5000: episode: 3552, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -745.334, mean reward: -745.334 [-745.334, -745.334], mean action: 2.000 [2.000, 2.000],  loss: 13906104.000000, mae: 1275.121582, mean_q: -293.077881
 3553/5000: episode: 3553, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -825.549, mean reward: -825.549 [-825.549, -825.549], mean action: 2.000 [2.000, 2.000],  loss: 13841660.000000, mae: 1308.663086, mean_q: -294.407867
 3554/5000: episode: 3554, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8007.667, mean reward: -8007.667 [-8007.667, -8007.667], mean action: 2.000 [2.000, 2.000],  loss: 14460072.000000, mae: 1280.312256, mean_q: -292.539246
 3555/5000: episode: 3555, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -815.017, mean reward: -815.017 [-815.017, -815.017], mean action: 2.000 [2.000, 2.000],  loss: 13566864.000000, mae: 1135.015381, mean_q: -293.190308
 3556/5000: episode: 3556, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4140.561, mean reward: -4140.561 [-4140.561, -4140.561], mean action: 2.000 [2.000, 2.000],  loss: 10946011.000000, mae: 1069.328491, mean_q: -294.842438
 3557/5000: episode: 3557, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1344.154, mean reward: -1344.154 [-1344.154, -1344.154], mean action: 2.000 [2.000, 2.000],  loss: 18235960.000000, mae: 1383.417969, mean_q: -294.024078
 3558/5000: episode: 3558, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8259.806, mean reward: -8259.806 [-8259.806, -8259.806], mean action: 2.000 [2.000, 2.000],  loss: 7174480.500000, mae: 889.252319, mean_q: -295.383118
 3559/5000: episode: 3559, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -1911.557, mean reward: -1911.557 [-1911.557, -1911.557], mean action: 2.000 [2.000, 2.000],  loss: 16209715.000000, mae: 1289.316650, mean_q: -295.359955
 3560/5000: episode: 3560, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7010.852, mean reward: -7010.852 [-7010.852, -7010.852], mean action: 2.000 [2.000, 2.000],  loss: 11745755.000000, mae: 1155.060059, mean_q: -294.526154
 3561/5000: episode: 3561, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4531.809, mean reward: -4531.809 [-4531.809, -4531.809], mean action: 2.000 [2.000, 2.000],  loss: 11023228.000000, mae: 1005.215393, mean_q: -295.582092
 3562/5000: episode: 3562, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5865.119, mean reward: -5865.119 [-5865.119, -5865.119], mean action: 2.000 [2.000, 2.000],  loss: 9746942.000000, mae: 1139.548584, mean_q: -295.774841
 3563/5000: episode: 3563, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4625.700, mean reward: -4625.700 [-4625.700, -4625.700], mean action: 2.000 [2.000, 2.000],  loss: 16739555.000000, mae: 1423.928467, mean_q: -296.184631
 3564/5000: episode: 3564, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2159.297, mean reward: -2159.297 [-2159.297, -2159.297], mean action: 2.000 [2.000, 2.000],  loss: 14654595.000000, mae: 1251.301758, mean_q: -295.868073
 3565/5000: episode: 3565, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -749.683, mean reward: -749.683 [-749.683, -749.683], mean action: 2.000 [2.000, 2.000],  loss: 10838508.000000, mae: 1137.761841, mean_q: -296.055420
 3566/5000: episode: 3566, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6021.116, mean reward: -6021.116 [-6021.116, -6021.116], mean action: 2.000 [2.000, 2.000],  loss: 13324818.000000, mae: 1206.657104, mean_q: -295.519653
 3567/5000: episode: 3567, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -1994.092, mean reward: -1994.092 [-1994.092, -1994.092], mean action: 2.000 [2.000, 2.000],  loss: 9417311.000000, mae: 989.190491, mean_q: -297.592896
 3568/5000: episode: 3568, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1074.138, mean reward: -1074.138 [-1074.138, -1074.138], mean action: 2.000 [2.000, 2.000],  loss: 8927168.000000, mae: 1085.862549, mean_q: -296.767944
 3569/5000: episode: 3569, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1288.637, mean reward: -1288.637 [-1288.637, -1288.637], mean action: 2.000 [2.000, 2.000],  loss: 12186004.000000, mae: 1201.912720, mean_q: -296.126862
 3570/5000: episode: 3570, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1406.667, mean reward: -1406.667 [-1406.667, -1406.667], mean action: 2.000 [2.000, 2.000],  loss: 12959934.000000, mae: 1302.046631, mean_q: -296.393921
 3571/5000: episode: 3571, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1197.624, mean reward: -1197.624 [-1197.624, -1197.624], mean action: 2.000 [2.000, 2.000],  loss: 13853442.000000, mae: 1201.104614, mean_q: -298.345184
 3572/5000: episode: 3572, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4478.193, mean reward: -4478.193 [-4478.193, -4478.193], mean action: 2.000 [2.000, 2.000],  loss: 9704692.000000, mae: 1118.025024, mean_q: -297.889771
 3573/5000: episode: 3573, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -75.045, mean reward: -75.045 [-75.045, -75.045], mean action: 2.000 [2.000, 2.000],  loss: 11530157.000000, mae: 1190.015259, mean_q: -297.977905
 3574/5000: episode: 3574, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5193.284, mean reward: -5193.284 [-5193.284, -5193.284], mean action: 2.000 [2.000, 2.000],  loss: 9945151.000000, mae: 1129.996826, mean_q: -298.505463
 3575/5000: episode: 3575, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2570.330, mean reward: -2570.330 [-2570.330, -2570.330], mean action: 2.000 [2.000, 2.000],  loss: 11325086.000000, mae: 1140.353638, mean_q: -298.605072
 3576/5000: episode: 3576, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -37.774, mean reward: -37.774 [-37.774, -37.774], mean action: 2.000 [2.000, 2.000],  loss: 10698552.000000, mae: 1115.881348, mean_q: -299.600922
 3577/5000: episode: 3577, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1147.462, mean reward: -1147.462 [-1147.462, -1147.462], mean action: 2.000 [2.000, 2.000],  loss: 7228805.500000, mae: 983.050781, mean_q: -299.583130
 3578/5000: episode: 3578, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6825.539, mean reward: -6825.539 [-6825.539, -6825.539], mean action: 2.000 [2.000, 2.000],  loss: 9147375.000000, mae: 1029.465820, mean_q: -298.504700
 3579/5000: episode: 3579, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2241.091, mean reward: -2241.091 [-2241.091, -2241.091], mean action: 2.000 [2.000, 2.000],  loss: 14457183.000000, mae: 1266.857666, mean_q: -297.462585
 3580/5000: episode: 3580, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6317.284, mean reward: -6317.284 [-6317.284, -6317.284], mean action: 1.000 [1.000, 1.000],  loss: 10113084.000000, mae: 1129.609131, mean_q: -298.951294
 3581/5000: episode: 3581, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -600.946, mean reward: -600.946 [-600.946, -600.946], mean action: 2.000 [2.000, 2.000],  loss: 9324825.000000, mae: 1080.470947, mean_q: -298.072449
 3582/5000: episode: 3582, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1024.236, mean reward: -1024.236 [-1024.236, -1024.236], mean action: 2.000 [2.000, 2.000],  loss: 8059609.000000, mae: 956.215576, mean_q: -299.973328
 3583/5000: episode: 3583, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5797.138, mean reward: -5797.138 [-5797.138, -5797.138], mean action: 1.000 [1.000, 1.000],  loss: 8075670.000000, mae: 1039.001465, mean_q: -299.611694
 3584/5000: episode: 3584, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1855.391, mean reward: -1855.391 [-1855.391, -1855.391], mean action: 2.000 [2.000, 2.000],  loss: 6960786.000000, mae: 943.904480, mean_q: -298.996857
 3585/5000: episode: 3585, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6.926, mean reward: -6.926 [-6.926, -6.926], mean action: 2.000 [2.000, 2.000],  loss: 11700280.000000, mae: 1140.984375, mean_q: -299.894653
 3586/5000: episode: 3586, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1044.266, mean reward: -1044.266 [-1044.266, -1044.266], mean action: 2.000 [2.000, 2.000],  loss: 10429479.000000, mae: 1121.592651, mean_q: -299.307343
 3587/5000: episode: 3587, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4551.174, mean reward: -4551.174 [-4551.174, -4551.174], mean action: 0.000 [0.000, 0.000],  loss: 7836834.500000, mae: 978.898193, mean_q: -299.565308
 3588/5000: episode: 3588, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1479.907, mean reward: -1479.907 [-1479.907, -1479.907], mean action: 2.000 [2.000, 2.000],  loss: 15050720.000000, mae: 1277.587646, mean_q: -299.882629
 3589/5000: episode: 3589, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -551.379, mean reward: -551.379 [-551.379, -551.379], mean action: 2.000 [2.000, 2.000],  loss: 10945431.000000, mae: 1119.551880, mean_q: -299.974609
 3590/5000: episode: 3590, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -768.434, mean reward: -768.434 [-768.434, -768.434], mean action: 2.000 [2.000, 2.000],  loss: 9418599.000000, mae: 1093.054932, mean_q: -300.756104
 3591/5000: episode: 3591, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -337.376, mean reward: -337.376 [-337.376, -337.376], mean action: 2.000 [2.000, 2.000],  loss: 11389150.000000, mae: 1203.401367, mean_q: -300.920593
 3592/5000: episode: 3592, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4552.935, mean reward: -4552.935 [-4552.935, -4552.935], mean action: 2.000 [2.000, 2.000],  loss: 9178986.000000, mae: 1038.525146, mean_q: -302.360168
 3593/5000: episode: 3593, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2054.948, mean reward: -2054.948 [-2054.948, -2054.948], mean action: 2.000 [2.000, 2.000],  loss: 7357276.500000, mae: 982.880554, mean_q: -301.124146
 3594/5000: episode: 3594, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2649.460, mean reward: -2649.460 [-2649.460, -2649.460], mean action: 2.000 [2.000, 2.000],  loss: 7614194.000000, mae: 981.209045, mean_q: -301.596985
 3595/5000: episode: 3595, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3114.600, mean reward: -3114.600 [-3114.600, -3114.600], mean action: 2.000 [2.000, 2.000],  loss: 10138513.000000, mae: 1143.557861, mean_q: -301.683838
 3596/5000: episode: 3596, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4823.177, mean reward: -4823.177 [-4823.177, -4823.177], mean action: 2.000 [2.000, 2.000],  loss: 10601610.000000, mae: 1165.615234, mean_q: -301.162994
 3597/5000: episode: 3597, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -923.640, mean reward: -923.640 [-923.640, -923.640], mean action: 2.000 [2.000, 2.000],  loss: 12068751.000000, mae: 1211.958374, mean_q: -302.675232
 3598/5000: episode: 3598, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -10943.534, mean reward: -10943.534 [-10943.534, -10943.534], mean action: 3.000 [3.000, 3.000],  loss: 15112912.000000, mae: 1257.790649, mean_q: -301.209747
 3599/5000: episode: 3599, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4226.327, mean reward: -4226.327 [-4226.327, -4226.327], mean action: 3.000 [3.000, 3.000],  loss: 11583228.000000, mae: 1138.720825, mean_q: -302.039673
 3600/5000: episode: 3600, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -10173.009, mean reward: -10173.009 [-10173.009, -10173.009], mean action: 3.000 [3.000, 3.000],  loss: 10504313.000000, mae: 1074.857178, mean_q: -302.639618
 3601/5000: episode: 3601, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1366.093, mean reward: -1366.093 [-1366.093, -1366.093], mean action: 2.000 [2.000, 2.000],  loss: 6169403.000000, mae: 929.374634, mean_q: -302.901337
 3602/5000: episode: 3602, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -922.713, mean reward: -922.713 [-922.713, -922.713], mean action: 3.000 [3.000, 3.000],  loss: 7834324.500000, mae: 1036.455078, mean_q: -302.782837
 3603/5000: episode: 3603, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -170.160, mean reward: -170.160 [-170.160, -170.160], mean action: 2.000 [2.000, 2.000],  loss: 15326754.000000, mae: 1343.201904, mean_q: -303.174377
 3604/5000: episode: 3604, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2814.371, mean reward: -2814.371 [-2814.371, -2814.371], mean action: 3.000 [3.000, 3.000],  loss: 8800482.000000, mae: 1068.810059, mean_q: -303.738892
 3605/5000: episode: 3605, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2204.355, mean reward: -2204.355 [-2204.355, -2204.355], mean action: 2.000 [2.000, 2.000],  loss: 16850668.000000, mae: 1270.213135, mean_q: -302.915649
 3606/5000: episode: 3606, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -690.640, mean reward: -690.640 [-690.640, -690.640], mean action: 2.000 [2.000, 2.000],  loss: 10243949.000000, mae: 1165.021484, mean_q: -303.776093
 3607/5000: episode: 3607, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5196.168, mean reward: -5196.168 [-5196.168, -5196.168], mean action: 3.000 [3.000, 3.000],  loss: 9430810.000000, mae: 1036.388184, mean_q: -304.362061
 3608/5000: episode: 3608, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1534.218, mean reward: -1534.218 [-1534.218, -1534.218], mean action: 3.000 [3.000, 3.000],  loss: 10415175.000000, mae: 1061.931519, mean_q: -304.999207
 3609/5000: episode: 3609, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5002.677, mean reward: -5002.677 [-5002.677, -5002.677], mean action: 2.000 [2.000, 2.000],  loss: 11125452.000000, mae: 1127.951538, mean_q: -305.512085
 3610/5000: episode: 3610, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1925.182, mean reward: -1925.182 [-1925.182, -1925.182], mean action: 2.000 [2.000, 2.000],  loss: 9269683.000000, mae: 1083.008301, mean_q: -303.521484
 3611/5000: episode: 3611, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5716.322, mean reward: -5716.322 [-5716.322, -5716.322], mean action: 3.000 [3.000, 3.000],  loss: 8773774.000000, mae: 1031.879395, mean_q: -304.824402
 3612/5000: episode: 3612, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -16.492, mean reward: -16.492 [-16.492, -16.492], mean action: 2.000 [2.000, 2.000],  loss: 8890541.000000, mae: 1055.781494, mean_q: -304.102875
 3613/5000: episode: 3613, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -12787.692, mean reward: -12787.692 [-12787.692, -12787.692], mean action: 3.000 [3.000, 3.000],  loss: 11975532.000000, mae: 1235.280518, mean_q: -304.677917
 3614/5000: episode: 3614, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2448.212, mean reward: -2448.212 [-2448.212, -2448.212], mean action: 2.000 [2.000, 2.000],  loss: 11566785.000000, mae: 1163.625977, mean_q: -304.252991
 3615/5000: episode: 3615, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8597.510, mean reward: -8597.510 [-8597.510, -8597.510], mean action: 2.000 [2.000, 2.000],  loss: 12891486.000000, mae: 1284.274780, mean_q: -305.162811
 3616/5000: episode: 3616, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8616.508, mean reward: -8616.508 [-8616.508, -8616.508], mean action: 0.000 [0.000, 0.000],  loss: 13826189.000000, mae: 1300.765747, mean_q: -305.663391
 3617/5000: episode: 3617, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -2009.190, mean reward: -2009.190 [-2009.190, -2009.190], mean action: 2.000 [2.000, 2.000],  loss: 11560662.000000, mae: 1107.177734, mean_q: -305.831116
 3618/5000: episode: 3618, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1173.164, mean reward: -1173.164 [-1173.164, -1173.164], mean action: 2.000 [2.000, 2.000],  loss: 13668038.000000, mae: 1254.419678, mean_q: -304.795654
 3619/5000: episode: 3619, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4204.972, mean reward: -4204.972 [-4204.972, -4204.972], mean action: 2.000 [2.000, 2.000],  loss: 10003019.000000, mae: 1059.466309, mean_q: -307.007111
 3620/5000: episode: 3620, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -931.188, mean reward: -931.188 [-931.188, -931.188], mean action: 2.000 [2.000, 2.000],  loss: 11659867.000000, mae: 1160.847778, mean_q: -305.936310
 3621/5000: episode: 3621, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4935.358, mean reward: -4935.358 [-4935.358, -4935.358], mean action: 2.000 [2.000, 2.000],  loss: 9824448.000000, mae: 1141.046875, mean_q: -307.192810
 3622/5000: episode: 3622, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5627.140, mean reward: -5627.140 [-5627.140, -5627.140], mean action: 2.000 [2.000, 2.000],  loss: 6417834.000000, mae: 898.621216, mean_q: -306.907654
 3623/5000: episode: 3623, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2825.701, mean reward: -2825.701 [-2825.701, -2825.701], mean action: 2.000 [2.000, 2.000],  loss: 9365340.000000, mae: 1101.303711, mean_q: -306.057068
 3624/5000: episode: 3624, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5698.925, mean reward: -5698.925 [-5698.925, -5698.925], mean action: 2.000 [2.000, 2.000],  loss: 10745858.000000, mae: 1124.965942, mean_q: -306.567993
 3625/5000: episode: 3625, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -26.466, mean reward: -26.466 [-26.466, -26.466], mean action: 2.000 [2.000, 2.000],  loss: 12359736.000000, mae: 1221.474487, mean_q: -307.447449
 3626/5000: episode: 3626, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9696.493, mean reward: -9696.493 [-9696.493, -9696.493], mean action: 2.000 [2.000, 2.000],  loss: 9769353.000000, mae: 1121.309082, mean_q: -305.922180
 3627/5000: episode: 3627, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3115.402, mean reward: -3115.402 [-3115.402, -3115.402], mean action: 2.000 [2.000, 2.000],  loss: 15407236.000000, mae: 1365.877319, mean_q: -306.714325
 3628/5000: episode: 3628, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4925.856, mean reward: -4925.856 [-4925.856, -4925.856], mean action: 0.000 [0.000, 0.000],  loss: 10727562.000000, mae: 1126.055908, mean_q: -307.376160
 3629/5000: episode: 3629, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -881.695, mean reward: -881.695 [-881.695, -881.695], mean action: 2.000 [2.000, 2.000],  loss: 13353588.000000, mae: 1280.745972, mean_q: -307.208069
 3630/5000: episode: 3630, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1407.445, mean reward: -1407.445 [-1407.445, -1407.445], mean action: 2.000 [2.000, 2.000],  loss: 16597674.000000, mae: 1348.929688, mean_q: -307.651367
 3631/5000: episode: 3631, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -60.916, mean reward: -60.916 [-60.916, -60.916], mean action: 2.000 [2.000, 2.000],  loss: 10502401.000000, mae: 1179.064087, mean_q: -308.191010
 3632/5000: episode: 3632, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6526.543, mean reward: -6526.543 [-6526.543, -6526.543], mean action: 2.000 [2.000, 2.000],  loss: 16408452.000000, mae: 1313.899414, mean_q: -308.348755
 3633/5000: episode: 3633, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -242.699, mean reward: -242.699 [-242.699, -242.699], mean action: 1.000 [1.000, 1.000],  loss: 14408998.000000, mae: 1310.519043, mean_q: -308.451294
 3634/5000: episode: 3634, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2022.530, mean reward: -2022.530 [-2022.530, -2022.530], mean action: 2.000 [2.000, 2.000],  loss: 14775894.000000, mae: 1324.774902, mean_q: -308.613159
 3635/5000: episode: 3635, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1719.742, mean reward: -1719.742 [-1719.742, -1719.742], mean action: 2.000 [2.000, 2.000],  loss: 14123926.000000, mae: 1230.881836, mean_q: -308.874329
 3636/5000: episode: 3636, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -13415.640, mean reward: -13415.640 [-13415.640, -13415.640], mean action: 0.000 [0.000, 0.000],  loss: 9853177.000000, mae: 1142.708740, mean_q: -309.871826
 3637/5000: episode: 3637, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3007.118, mean reward: -3007.118 [-3007.118, -3007.118], mean action: 3.000 [3.000, 3.000],  loss: 13246770.000000, mae: 1256.050415, mean_q: -309.446930
 3638/5000: episode: 3638, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -333.417, mean reward: -333.417 [-333.417, -333.417], mean action: 2.000 [2.000, 2.000],  loss: 11369806.000000, mae: 1219.671631, mean_q: -309.164246
 3639/5000: episode: 3639, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1370.216, mean reward: -1370.216 [-1370.216, -1370.216], mean action: 3.000 [3.000, 3.000],  loss: 17299228.000000, mae: 1469.363281, mean_q: -308.719360
 3640/5000: episode: 3640, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -296.455, mean reward: -296.455 [-296.455, -296.455], mean action: 3.000 [3.000, 3.000],  loss: 10207221.000000, mae: 1101.527832, mean_q: -308.711609
 3641/5000: episode: 3641, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -7025.607, mean reward: -7025.607 [-7025.607, -7025.607], mean action: 3.000 [3.000, 3.000],  loss: 14715617.000000, mae: 1323.210449, mean_q: -310.873901
 3642/5000: episode: 3642, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4437.229, mean reward: -4437.229 [-4437.229, -4437.229], mean action: 0.000 [0.000, 0.000],  loss: 10536924.000000, mae: 1137.035400, mean_q: -309.822571
 3643/5000: episode: 3643, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -9716.342, mean reward: -9716.342 [-9716.342, -9716.342], mean action: 1.000 [1.000, 1.000],  loss: 9214600.000000, mae: 1119.775269, mean_q: -310.361511
 3644/5000: episode: 3644, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5610.598, mean reward: -5610.598 [-5610.598, -5610.598], mean action: 3.000 [3.000, 3.000],  loss: 9525200.000000, mae: 1051.195801, mean_q: -310.610748
 3645/5000: episode: 3645, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7177.333, mean reward: -7177.333 [-7177.333, -7177.333], mean action: 3.000 [3.000, 3.000],  loss: 11833856.000000, mae: 1234.732178, mean_q: -310.851074
 3646/5000: episode: 3646, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5202.745, mean reward: -5202.745 [-5202.745, -5202.745], mean action: 3.000 [3.000, 3.000],  loss: 9042221.000000, mae: 1115.912842, mean_q: -311.914551
 3647/5000: episode: 3647, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1229.179, mean reward: -1229.179 [-1229.179, -1229.179], mean action: 2.000 [2.000, 2.000],  loss: 15136051.000000, mae: 1265.021240, mean_q: -309.943787
 3648/5000: episode: 3648, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3453.244, mean reward: -3453.244 [-3453.244, -3453.244], mean action: 3.000 [3.000, 3.000],  loss: 14994268.000000, mae: 1281.032959, mean_q: -310.758728
 3649/5000: episode: 3649, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1517.232, mean reward: -1517.232 [-1517.232, -1517.232], mean action: 3.000 [3.000, 3.000],  loss: 14112100.000000, mae: 1307.836060, mean_q: -310.522186
 3650/5000: episode: 3650, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1824.033, mean reward: -1824.033 [-1824.033, -1824.033], mean action: 3.000 [3.000, 3.000],  loss: 7768881.500000, mae: 958.814331, mean_q: -311.867249
 3651/5000: episode: 3651, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -11087.837, mean reward: -11087.837 [-11087.837, -11087.837], mean action: 3.000 [3.000, 3.000],  loss: 12226358.000000, mae: 1186.937256, mean_q: -311.347290
 3652/5000: episode: 3652, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2370.622, mean reward: -2370.622 [-2370.622, -2370.622], mean action: 3.000 [3.000, 3.000],  loss: 15273563.000000, mae: 1235.193481, mean_q: -311.511536
 3653/5000: episode: 3653, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -49.023, mean reward: -49.023 [-49.023, -49.023], mean action: 3.000 [3.000, 3.000],  loss: 12461321.000000, mae: 1157.744873, mean_q: -311.369171
 3654/5000: episode: 3654, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7882.099, mean reward: -7882.099 [-7882.099, -7882.099], mean action: 3.000 [3.000, 3.000],  loss: 17668186.000000, mae: 1435.968994, mean_q: -310.410217
 3655/5000: episode: 3655, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7236.614, mean reward: -7236.614 [-7236.614, -7236.614], mean action: 3.000 [3.000, 3.000],  loss: 7274289.500000, mae: 937.545593, mean_q: -313.173920
 3656/5000: episode: 3656, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1384.368, mean reward: -1384.368 [-1384.368, -1384.368], mean action: 3.000 [3.000, 3.000],  loss: 14342660.000000, mae: 1314.412964, mean_q: -312.682251
 3657/5000: episode: 3657, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -808.585, mean reward: -808.585 [-808.585, -808.585], mean action: 3.000 [3.000, 3.000],  loss: 10418272.000000, mae: 1027.591553, mean_q: -312.695465
 3658/5000: episode: 3658, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2793.132, mean reward: -2793.132 [-2793.132, -2793.132], mean action: 3.000 [3.000, 3.000],  loss: 10159042.000000, mae: 1053.510498, mean_q: -312.480164
 3659/5000: episode: 3659, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -336.351, mean reward: -336.351 [-336.351, -336.351], mean action: 3.000 [3.000, 3.000],  loss: 10827419.000000, mae: 1205.646729, mean_q: -313.286865
 3660/5000: episode: 3660, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2852.766, mean reward: -2852.766 [-2852.766, -2852.766], mean action: 3.000 [3.000, 3.000],  loss: 11062192.000000, mae: 1181.305908, mean_q: -313.902435
 3661/5000: episode: 3661, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1952.432, mean reward: -1952.432 [-1952.432, -1952.432], mean action: 3.000 [3.000, 3.000],  loss: 8384550.000000, mae: 975.152954, mean_q: -314.165009
 3662/5000: episode: 3662, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3211.774, mean reward: -3211.774 [-3211.774, -3211.774], mean action: 3.000 [3.000, 3.000],  loss: 16218470.000000, mae: 1400.159424, mean_q: -313.067322
 3663/5000: episode: 3663, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4542.432, mean reward: -4542.432 [-4542.432, -4542.432], mean action: 3.000 [3.000, 3.000],  loss: 11624640.000000, mae: 1121.921143, mean_q: -315.559448
 3664/5000: episode: 3664, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -49.042, mean reward: -49.042 [-49.042, -49.042], mean action: 3.000 [3.000, 3.000],  loss: 12438455.000000, mae: 1218.264893, mean_q: -315.056580
 3665/5000: episode: 3665, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2514.408, mean reward: -2514.408 [-2514.408, -2514.408], mean action: 3.000 [3.000, 3.000],  loss: 5909222.500000, mae: 837.754578, mean_q: -315.884064
 3666/5000: episode: 3666, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2997.565, mean reward: -2997.565 [-2997.565, -2997.565], mean action: 3.000 [3.000, 3.000],  loss: 10711991.000000, mae: 1140.694580, mean_q: -313.812988
 3667/5000: episode: 3667, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8610.407, mean reward: -8610.407 [-8610.407, -8610.407], mean action: 0.000 [0.000, 0.000],  loss: 14209189.000000, mae: 1267.793945, mean_q: -314.746460
 3668/5000: episode: 3668, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2031.532, mean reward: -2031.532 [-2031.532, -2031.532], mean action: 3.000 [3.000, 3.000],  loss: 12007660.000000, mae: 1198.838013, mean_q: -315.257996
 3669/5000: episode: 3669, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1325.374, mean reward: -1325.374 [-1325.374, -1325.374], mean action: 3.000 [3.000, 3.000],  loss: 9593856.000000, mae: 1024.341187, mean_q: -314.457214
 3670/5000: episode: 3670, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -9306.255, mean reward: -9306.255 [-9306.255, -9306.255], mean action: 3.000 [3.000, 3.000],  loss: 8041252.500000, mae: 958.965515, mean_q: -315.448456
 3671/5000: episode: 3671, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -52.768, mean reward: -52.768 [-52.768, -52.768], mean action: 3.000 [3.000, 3.000],  loss: 8683142.000000, mae: 1002.394653, mean_q: -314.297821
 3672/5000: episode: 3672, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6654.430, mean reward: -6654.430 [-6654.430, -6654.430], mean action: 1.000 [1.000, 1.000],  loss: 9134268.000000, mae: 1132.713257, mean_q: -315.235962
 3673/5000: episode: 3673, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -12501.017, mean reward: -12501.017 [-12501.017, -12501.017], mean action: 3.000 [3.000, 3.000],  loss: 14982754.000000, mae: 1256.963135, mean_q: -314.929504
 3674/5000: episode: 3674, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7705.162, mean reward: -7705.162 [-7705.162, -7705.162], mean action: 1.000 [1.000, 1.000],  loss: 18186896.000000, mae: 1361.568359, mean_q: -314.136230
 3675/5000: episode: 3675, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5678.772, mean reward: -5678.772 [-5678.772, -5678.772], mean action: 3.000 [3.000, 3.000],  loss: 7687510.500000, mae: 1039.654541, mean_q: -316.875793
 3676/5000: episode: 3676, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -2537.297, mean reward: -2537.297 [-2537.297, -2537.297], mean action: 3.000 [3.000, 3.000],  loss: 10503193.000000, mae: 1155.586792, mean_q: -317.617981
 3677/5000: episode: 3677, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -7799.589, mean reward: -7799.589 [-7799.589, -7799.589], mean action: 3.000 [3.000, 3.000],  loss: 7005172.000000, mae: 998.880859, mean_q: -316.680664
 3678/5000: episode: 3678, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -813.036, mean reward: -813.036 [-813.036, -813.036], mean action: 3.000 [3.000, 3.000],  loss: 10212058.000000, mae: 1087.034668, mean_q: -317.165985
 3679/5000: episode: 3679, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2016.286, mean reward: -2016.286 [-2016.286, -2016.286], mean action: 3.000 [3.000, 3.000],  loss: 19196380.000000, mae: 1493.832031, mean_q: -315.597717
 3680/5000: episode: 3680, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -757.711, mean reward: -757.711 [-757.711, -757.711], mean action: 3.000 [3.000, 3.000],  loss: 13540197.000000, mae: 1195.570068, mean_q: -315.546112
 3681/5000: episode: 3681, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1978.162, mean reward: -1978.162 [-1978.162, -1978.162], mean action: 3.000 [3.000, 3.000],  loss: 10000500.000000, mae: 1157.846313, mean_q: -316.479919
 3682/5000: episode: 3682, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2110.446, mean reward: -2110.446 [-2110.446, -2110.446], mean action: 3.000 [3.000, 3.000],  loss: 9802378.000000, mae: 1164.254395, mean_q: -317.781036
 3683/5000: episode: 3683, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3018.569, mean reward: -3018.569 [-3018.569, -3018.569], mean action: 3.000 [3.000, 3.000],  loss: 12472552.000000, mae: 1246.606445, mean_q: -319.545929
 3684/5000: episode: 3684, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5.652, mean reward: -5.652 [-5.652, -5.652], mean action: 3.000 [3.000, 3.000],  loss: 6598011.000000, mae: 963.839966, mean_q: -318.633423
 3685/5000: episode: 3685, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -463.775, mean reward: -463.775 [-463.775, -463.775], mean action: 3.000 [3.000, 3.000],  loss: 9718970.000000, mae: 1137.807739, mean_q: -318.198120
 3686/5000: episode: 3686, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -1517.751, mean reward: -1517.751 [-1517.751, -1517.751], mean action: 2.000 [2.000, 2.000],  loss: 12066082.000000, mae: 1186.456543, mean_q: -317.191498
 3687/5000: episode: 3687, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -8994.848, mean reward: -8994.848 [-8994.848, -8994.848], mean action: 3.000 [3.000, 3.000],  loss: 12179772.000000, mae: 1261.758301, mean_q: -316.946777
 3688/5000: episode: 3688, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -17.260, mean reward: -17.260 [-17.260, -17.260], mean action: 3.000 [3.000, 3.000],  loss: 13590426.000000, mae: 1338.457642, mean_q: -318.025513
 3689/5000: episode: 3689, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -154.031, mean reward: -154.031 [-154.031, -154.031], mean action: 3.000 [3.000, 3.000],  loss: 8185892.000000, mae: 985.384949, mean_q: -320.190948
 3690/5000: episode: 3690, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -222.671, mean reward: -222.671 [-222.671, -222.671], mean action: 3.000 [3.000, 3.000],  loss: 7030743.500000, mae: 942.402954, mean_q: -319.632294
 3691/5000: episode: 3691, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2603.756, mean reward: -2603.756 [-2603.756, -2603.756], mean action: 3.000 [3.000, 3.000],  loss: 9547717.000000, mae: 1111.473511, mean_q: -319.201416
 3692/5000: episode: 3692, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3011.629, mean reward: -3011.629 [-3011.629, -3011.629], mean action: 3.000 [3.000, 3.000],  loss: 10068138.000000, mae: 1080.221680, mean_q: -320.826996
 3693/5000: episode: 3693, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4841.743, mean reward: -4841.743 [-4841.743, -4841.743], mean action: 3.000 [3.000, 3.000],  loss: 9904355.000000, mae: 1169.158813, mean_q: -319.301086
 3694/5000: episode: 3694, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -9.674, mean reward: -9.674 [-9.674, -9.674], mean action: 3.000 [3.000, 3.000],  loss: 8035842.000000, mae: 1013.935669, mean_q: -320.261536
 3695/5000: episode: 3695, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1452.955, mean reward: -1452.955 [-1452.955, -1452.955], mean action: 3.000 [3.000, 3.000],  loss: 8729945.000000, mae: 1096.551392, mean_q: -319.165375
 3696/5000: episode: 3696, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4804.185, mean reward: -4804.185 [-4804.185, -4804.185], mean action: 3.000 [3.000, 3.000],  loss: 10942618.000000, mae: 1149.649902, mean_q: -319.556458
 3697/5000: episode: 3697, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2470.269, mean reward: -2470.269 [-2470.269, -2470.269], mean action: 3.000 [3.000, 3.000],  loss: 11328373.000000, mae: 1181.955444, mean_q: -320.709778
 3698/5000: episode: 3698, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2578.305, mean reward: -2578.305 [-2578.305, -2578.305], mean action: 3.000 [3.000, 3.000],  loss: 13994188.000000, mae: 1274.263428, mean_q: -319.664429
 3699/5000: episode: 3699, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3711.755, mean reward: -3711.755 [-3711.755, -3711.755], mean action: 3.000 [3.000, 3.000],  loss: 7613935.000000, mae: 985.041016, mean_q: -321.513519
 3700/5000: episode: 3700, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -652.127, mean reward: -652.127 [-652.127, -652.127], mean action: 3.000 [3.000, 3.000],  loss: 14680392.000000, mae: 1381.098877, mean_q: -320.166260
 3701/5000: episode: 3701, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3999.586, mean reward: -3999.586 [-3999.586, -3999.586], mean action: 3.000 [3.000, 3.000],  loss: 12624397.000000, mae: 1245.983398, mean_q: -321.165710
 3702/5000: episode: 3702, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4025.008, mean reward: -4025.008 [-4025.008, -4025.008], mean action: 3.000 [3.000, 3.000],  loss: 5317328.000000, mae: 889.131226, mean_q: -320.698120
 3703/5000: episode: 3703, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1387.903, mean reward: -1387.903 [-1387.903, -1387.903], mean action: 3.000 [3.000, 3.000],  loss: 14885104.000000, mae: 1358.709473, mean_q: -322.049255
 3704/5000: episode: 3704, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5060.164, mean reward: -5060.164 [-5060.164, -5060.164], mean action: 3.000 [3.000, 3.000],  loss: 11543147.000000, mae: 1235.182861, mean_q: -322.051239
 3705/5000: episode: 3705, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2168.070, mean reward: -2168.070 [-2168.070, -2168.070], mean action: 3.000 [3.000, 3.000],  loss: 10863116.000000, mae: 1127.178955, mean_q: -322.272949
 3706/5000: episode: 3706, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5974.280, mean reward: -5974.280 [-5974.280, -5974.280], mean action: 3.000 [3.000, 3.000],  loss: 16529196.000000, mae: 1327.363281, mean_q: -320.786255
 3707/5000: episode: 3707, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -698.070, mean reward: -698.070 [-698.070, -698.070], mean action: 3.000 [3.000, 3.000],  loss: 7425641.000000, mae: 1027.516479, mean_q: -323.577820
 3708/5000: episode: 3708, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -1978.746, mean reward: -1978.746 [-1978.746, -1978.746], mean action: 3.000 [3.000, 3.000],  loss: 14291668.000000, mae: 1238.566650, mean_q: -322.120361
 3709/5000: episode: 3709, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -566.722, mean reward: -566.722 [-566.722, -566.722], mean action: 3.000 [3.000, 3.000],  loss: 15964817.000000, mae: 1302.940918, mean_q: -323.671570
 3710/5000: episode: 3710, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4257.736, mean reward: -4257.736 [-4257.736, -4257.736], mean action: 3.000 [3.000, 3.000],  loss: 12993777.000000, mae: 1233.531250, mean_q: -324.490601
 3711/5000: episode: 3711, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -574.336, mean reward: -574.336 [-574.336, -574.336], mean action: 3.000 [3.000, 3.000],  loss: 12269605.000000, mae: 1195.747070, mean_q: -322.906555
 3712/5000: episode: 3712, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -4414.870, mean reward: -4414.870 [-4414.870, -4414.870], mean action: 3.000 [3.000, 3.000],  loss: 14368407.000000, mae: 1362.963135, mean_q: -322.773132
 3713/5000: episode: 3713, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2307.152, mean reward: -2307.152 [-2307.152, -2307.152], mean action: 3.000 [3.000, 3.000],  loss: 7795170.500000, mae: 996.657410, mean_q: -323.805908
 3714/5000: episode: 3714, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -11258.775, mean reward: -11258.775 [-11258.775, -11258.775], mean action: 3.000 [3.000, 3.000],  loss: 13154536.000000, mae: 1158.194336, mean_q: -323.836670
 3715/5000: episode: 3715, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6935.617, mean reward: -6935.617 [-6935.617, -6935.617], mean action: 3.000 [3.000, 3.000],  loss: 9723186.000000, mae: 1099.744385, mean_q: -325.654724
 3716/5000: episode: 3716, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1889.292, mean reward: -1889.292 [-1889.292, -1889.292], mean action: 3.000 [3.000, 3.000],  loss: 12846383.000000, mae: 1234.438721, mean_q: -323.849335
 3717/5000: episode: 3717, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3053.146, mean reward: -3053.146 [-3053.146, -3053.146], mean action: 3.000 [3.000, 3.000],  loss: 12115280.000000, mae: 1212.702148, mean_q: -324.357239
 3718/5000: episode: 3718, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2277.631, mean reward: -2277.631 [-2277.631, -2277.631], mean action: 3.000 [3.000, 3.000],  loss: 7324018.500000, mae: 1011.609436, mean_q: -325.822662
 3719/5000: episode: 3719, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -342.902, mean reward: -342.902 [-342.902, -342.902], mean action: 1.000 [1.000, 1.000],  loss: 13457692.000000, mae: 1260.732666, mean_q: -324.762024
 3720/5000: episode: 3720, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -793.658, mean reward: -793.658 [-793.658, -793.658], mean action: 3.000 [3.000, 3.000],  loss: 13086598.000000, mae: 1132.358887, mean_q: -326.001953
 3721/5000: episode: 3721, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2494.833, mean reward: -2494.833 [-2494.833, -2494.833], mean action: 1.000 [1.000, 1.000],  loss: 12788136.000000, mae: 1259.217896, mean_q: -324.080292
 3722/5000: episode: 3722, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -189.714, mean reward: -189.714 [-189.714, -189.714], mean action: 3.000 [3.000, 3.000],  loss: 14547929.000000, mae: 1351.619141, mean_q: -324.931946
 3723/5000: episode: 3723, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1549.752, mean reward: -1549.752 [-1549.752, -1549.752], mean action: 1.000 [1.000, 1.000],  loss: 9757330.000000, mae: 1123.875488, mean_q: -325.970886
 3724/5000: episode: 3724, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1611.580, mean reward: -1611.580 [-1611.580, -1611.580], mean action: 1.000 [1.000, 1.000],  loss: 6237037.000000, mae: 874.724060, mean_q: -326.282776
 3725/5000: episode: 3725, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4631.374, mean reward: -4631.374 [-4631.374, -4631.374], mean action: 3.000 [3.000, 3.000],  loss: 9973642.000000, mae: 1097.978271, mean_q: -325.039429
 3726/5000: episode: 3726, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3707.513, mean reward: -3707.513 [-3707.513, -3707.513], mean action: 1.000 [1.000, 1.000],  loss: 12293581.000000, mae: 1166.419189, mean_q: -326.774139
 3727/5000: episode: 3727, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8549.106, mean reward: -8549.106 [-8549.106, -8549.106], mean action: 1.000 [1.000, 1.000],  loss: 13703220.000000, mae: 1263.486206, mean_q: -325.207123
 3728/5000: episode: 3728, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1119.394, mean reward: -1119.394 [-1119.394, -1119.394], mean action: 1.000 [1.000, 1.000],  loss: 9226620.000000, mae: 1066.738770, mean_q: -326.734497
 3729/5000: episode: 3729, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4479.260, mean reward: -4479.260 [-4479.260, -4479.260], mean action: 1.000 [1.000, 1.000],  loss: 7163207.500000, mae: 1001.531616, mean_q: -327.015991
 3730/5000: episode: 3730, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -844.148, mean reward: -844.148 [-844.148, -844.148], mean action: 1.000 [1.000, 1.000],  loss: 10397148.000000, mae: 1154.846436, mean_q: -326.694702
 3731/5000: episode: 3731, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3858.444, mean reward: -3858.444 [-3858.444, -3858.444], mean action: 1.000 [1.000, 1.000],  loss: 14305420.000000, mae: 1384.585449, mean_q: -326.002289
 3732/5000: episode: 3732, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4984.399, mean reward: -4984.399 [-4984.399, -4984.399], mean action: 1.000 [1.000, 1.000],  loss: 22931194.000000, mae: 1573.844238, mean_q: -324.874817
 3733/5000: episode: 3733, duration: 0.047s, episode steps:   1, steps per second:  22, episode reward: -4250.380, mean reward: -4250.380 [-4250.380, -4250.380], mean action: 1.000 [1.000, 1.000],  loss: 12647281.000000, mae: 1260.889893, mean_q: -327.862549
 3734/5000: episode: 3734, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4411.254, mean reward: -4411.254 [-4411.254, -4411.254], mean action: 1.000 [1.000, 1.000],  loss: 8678163.000000, mae: 1015.581299, mean_q: -327.033264
 3735/5000: episode: 3735, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2062.564, mean reward: -2062.564 [-2062.564, -2062.564], mean action: 1.000 [1.000, 1.000],  loss: 8768559.000000, mae: 1102.460083, mean_q: -327.901764
 3736/5000: episode: 3736, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -6303.422, mean reward: -6303.422 [-6303.422, -6303.422], mean action: 1.000 [1.000, 1.000],  loss: 6880532.500000, mae: 993.027222, mean_q: -329.131470
 3737/5000: episode: 3737, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -9766.695, mean reward: -9766.695 [-9766.695, -9766.695], mean action: 1.000 [1.000, 1.000],  loss: 12842098.000000, mae: 1272.382324, mean_q: -327.548096
 3738/5000: episode: 3738, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1377.303, mean reward: -1377.303 [-1377.303, -1377.303], mean action: 1.000 [1.000, 1.000],  loss: 8565076.000000, mae: 1090.658447, mean_q: -327.136688
 3739/5000: episode: 3739, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -2928.470, mean reward: -2928.470 [-2928.470, -2928.470], mean action: 1.000 [1.000, 1.000],  loss: 12819466.000000, mae: 1229.771484, mean_q: -328.923492
 3740/5000: episode: 3740, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -475.439, mean reward: -475.439 [-475.439, -475.439], mean action: 1.000 [1.000, 1.000],  loss: 10248000.000000, mae: 1089.465942, mean_q: -330.649445
 3741/5000: episode: 3741, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3703.234, mean reward: -3703.234 [-3703.234, -3703.234], mean action: 1.000 [1.000, 1.000],  loss: 14125043.000000, mae: 1274.868530, mean_q: -328.590271
 3742/5000: episode: 3742, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5874.118, mean reward: -5874.118 [-5874.118, -5874.118], mean action: 1.000 [1.000, 1.000],  loss: 10709185.000000, mae: 1192.219971, mean_q: -328.362549
 3743/5000: episode: 3743, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2409.719, mean reward: -2409.719 [-2409.719, -2409.719], mean action: 1.000 [1.000, 1.000],  loss: 11248753.000000, mae: 1160.972412, mean_q: -329.170898
 3744/5000: episode: 3744, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9620.753, mean reward: -9620.753 [-9620.753, -9620.753], mean action: 1.000 [1.000, 1.000],  loss: 11300196.000000, mae: 1135.656006, mean_q: -328.087921
 3745/5000: episode: 3745, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1848.381, mean reward: -1848.381 [-1848.381, -1848.381], mean action: 3.000 [3.000, 3.000],  loss: 9052570.000000, mae: 1087.154297, mean_q: -329.842133
 3746/5000: episode: 3746, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5926.090, mean reward: -5926.090 [-5926.090, -5926.090], mean action: 1.000 [1.000, 1.000],  loss: 11460130.000000, mae: 1128.807983, mean_q: -330.592834
 3747/5000: episode: 3747, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2442.964, mean reward: -2442.964 [-2442.964, -2442.964], mean action: 1.000 [1.000, 1.000],  loss: 11453138.000000, mae: 1204.357788, mean_q: -330.723938
 3748/5000: episode: 3748, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -758.936, mean reward: -758.936 [-758.936, -758.936], mean action: 1.000 [1.000, 1.000],  loss: 6363221.000000, mae: 989.206909, mean_q: -330.747192
 3749/5000: episode: 3749, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2888.461, mean reward: -2888.461 [-2888.461, -2888.461], mean action: 1.000 [1.000, 1.000],  loss: 10759426.000000, mae: 1112.211426, mean_q: -330.173126
 3750/5000: episode: 3750, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4097.212, mean reward: -4097.212 [-4097.212, -4097.212], mean action: 1.000 [1.000, 1.000],  loss: 19787558.000000, mae: 1556.465820, mean_q: -329.367126
 3751/5000: episode: 3751, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7460.860, mean reward: -7460.860 [-7460.860, -7460.860], mean action: 1.000 [1.000, 1.000],  loss: 10165819.000000, mae: 1081.662231, mean_q: -330.241058
 3752/5000: episode: 3752, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2301.022, mean reward: -2301.022 [-2301.022, -2301.022], mean action: 1.000 [1.000, 1.000],  loss: 12471394.000000, mae: 1235.068848, mean_q: -330.035889
 3753/5000: episode: 3753, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3532.375, mean reward: -3532.375 [-3532.375, -3532.375], mean action: 1.000 [1.000, 1.000],  loss: 12164290.000000, mae: 1175.402710, mean_q: -331.453796
 3754/5000: episode: 3754, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6994.258, mean reward: -6994.258 [-6994.258, -6994.258], mean action: 0.000 [0.000, 0.000],  loss: 5977699.500000, mae: 820.133728, mean_q: -332.030701
 3755/5000: episode: 3755, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1971.493, mean reward: -1971.493 [-1971.493, -1971.493], mean action: 1.000 [1.000, 1.000],  loss: 12598466.000000, mae: 1301.735474, mean_q: -330.501740
 3756/5000: episode: 3756, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2909.481, mean reward: -2909.481 [-2909.481, -2909.481], mean action: 1.000 [1.000, 1.000],  loss: 16412689.000000, mae: 1346.363647, mean_q: -330.969727
 3757/5000: episode: 3757, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9973.077, mean reward: -9973.077 [-9973.077, -9973.077], mean action: 1.000 [1.000, 1.000],  loss: 9580266.000000, mae: 1084.891602, mean_q: -333.096588
 3758/5000: episode: 3758, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -600.280, mean reward: -600.280 [-600.280, -600.280], mean action: 1.000 [1.000, 1.000],  loss: 6306329.000000, mae: 925.536865, mean_q: -332.489899
 3759/5000: episode: 3759, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2801.913, mean reward: -2801.913 [-2801.913, -2801.913], mean action: 3.000 [3.000, 3.000],  loss: 10817541.000000, mae: 1131.995605, mean_q: -332.497406
 3760/5000: episode: 3760, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5145.904, mean reward: -5145.904 [-5145.904, -5145.904], mean action: 1.000 [1.000, 1.000],  loss: 8062120.500000, mae: 1008.727844, mean_q: -331.775757
 3761/5000: episode: 3761, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4453.969, mean reward: -4453.969 [-4453.969, -4453.969], mean action: 0.000 [0.000, 0.000],  loss: 10215604.000000, mae: 1115.960449, mean_q: -330.956665
 3762/5000: episode: 3762, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7158.402, mean reward: -7158.402 [-7158.402, -7158.402], mean action: 1.000 [1.000, 1.000],  loss: 8322302.000000, mae: 1053.258301, mean_q: -333.231659
 3763/5000: episode: 3763, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2196.206, mean reward: -2196.206 [-2196.206, -2196.206], mean action: 1.000 [1.000, 1.000],  loss: 16027875.000000, mae: 1278.471802, mean_q: -332.259979
 3764/5000: episode: 3764, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4028.166, mean reward: -4028.166 [-4028.166, -4028.166], mean action: 1.000 [1.000, 1.000],  loss: 16210960.000000, mae: 1301.336670, mean_q: -331.830566
 3765/5000: episode: 3765, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5117.094, mean reward: -5117.094 [-5117.094, -5117.094], mean action: 1.000 [1.000, 1.000],  loss: 14062460.000000, mae: 1254.300903, mean_q: -333.421875
 3766/5000: episode: 3766, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4002.386, mean reward: -4002.386 [-4002.386, -4002.386], mean action: 1.000 [1.000, 1.000],  loss: 5698152.500000, mae: 797.869995, mean_q: -333.120148
 3767/5000: episode: 3767, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5783.761, mean reward: -5783.761 [-5783.761, -5783.761], mean action: 1.000 [1.000, 1.000],  loss: 13763136.000000, mae: 1249.721924, mean_q: -333.105225
 3768/5000: episode: 3768, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9690.427, mean reward: -9690.427 [-9690.427, -9690.427], mean action: 1.000 [1.000, 1.000],  loss: 13381688.000000, mae: 1299.356567, mean_q: -334.130096
 3769/5000: episode: 3769, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2395.890, mean reward: -2395.890 [-2395.890, -2395.890], mean action: 1.000 [1.000, 1.000],  loss: 8240537.000000, mae: 1006.155212, mean_q: -334.782440
 3770/5000: episode: 3770, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4346.245, mean reward: -4346.245 [-4346.245, -4346.245], mean action: 1.000 [1.000, 1.000],  loss: 11631930.000000, mae: 1215.670288, mean_q: -334.587219
 3771/5000: episode: 3771, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4738.313, mean reward: -4738.313 [-4738.313, -4738.313], mean action: 1.000 [1.000, 1.000],  loss: 9470189.000000, mae: 1031.017944, mean_q: -333.788330
 3772/5000: episode: 3772, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5145.493, mean reward: -5145.493 [-5145.493, -5145.493], mean action: 1.000 [1.000, 1.000],  loss: 8160521.000000, mae: 986.872864, mean_q: -334.280090
 3773/5000: episode: 3773, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -985.898, mean reward: -985.898 [-985.898, -985.898], mean action: 1.000 [1.000, 1.000],  loss: 9448789.000000, mae: 1060.270874, mean_q: -333.027466
 3774/5000: episode: 3774, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6432.393, mean reward: -6432.393 [-6432.393, -6432.393], mean action: 1.000 [1.000, 1.000],  loss: 7769599.500000, mae: 1003.211792, mean_q: -334.802795
 3775/5000: episode: 3775, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5700.862, mean reward: -5700.862 [-5700.862, -5700.862], mean action: 1.000 [1.000, 1.000],  loss: 10817360.000000, mae: 1159.340698, mean_q: -335.713135
 3776/5000: episode: 3776, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3753.941, mean reward: -3753.941 [-3753.941, -3753.941], mean action: 1.000 [1.000, 1.000],  loss: 12416516.000000, mae: 1218.260010, mean_q: -334.861084
 3777/5000: episode: 3777, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6314.420, mean reward: -6314.420 [-6314.420, -6314.420], mean action: 1.000 [1.000, 1.000],  loss: 9836332.000000, mae: 1100.622559, mean_q: -335.967957
 3778/5000: episode: 3778, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -716.634, mean reward: -716.634 [-716.634, -716.634], mean action: 1.000 [1.000, 1.000],  loss: 10573034.000000, mae: 1111.947266, mean_q: -336.284668
 3779/5000: episode: 3779, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -6376.295, mean reward: -6376.295 [-6376.295, -6376.295], mean action: 1.000 [1.000, 1.000],  loss: 5101469.000000, mae: 901.502563, mean_q: -336.427002
 3780/5000: episode: 3780, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -2655.574, mean reward: -2655.574 [-2655.574, -2655.574], mean action: 1.000 [1.000, 1.000],  loss: 10601116.000000, mae: 1199.560791, mean_q: -336.246735
 3781/5000: episode: 3781, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8176.629, mean reward: -8176.629 [-8176.629, -8176.629], mean action: 0.000 [0.000, 0.000],  loss: 5747883.000000, mae: 883.292542, mean_q: -337.670837
 3782/5000: episode: 3782, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4556.502, mean reward: -4556.502 [-4556.502, -4556.502], mean action: 1.000 [1.000, 1.000],  loss: 10100470.000000, mae: 1111.873901, mean_q: -335.787537
 3783/5000: episode: 3783, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -920.717, mean reward: -920.717 [-920.717, -920.717], mean action: 1.000 [1.000, 1.000],  loss: 8415068.000000, mae: 1050.991699, mean_q: -336.777771
 3784/5000: episode: 3784, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4450.454, mean reward: -4450.454 [-4450.454, -4450.454], mean action: 1.000 [1.000, 1.000],  loss: 17004290.000000, mae: 1401.703735, mean_q: -335.996765
 3785/5000: episode: 3785, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1914.574, mean reward: -1914.574 [-1914.574, -1914.574], mean action: 1.000 [1.000, 1.000],  loss: 7470102.000000, mae: 959.679688, mean_q: -338.546753
 3786/5000: episode: 3786, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1868.097, mean reward: -1868.097 [-1868.097, -1868.097], mean action: 1.000 [1.000, 1.000],  loss: 10162521.000000, mae: 1152.990723, mean_q: -337.714539
 3787/5000: episode: 3787, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4212.802, mean reward: -4212.802 [-4212.802, -4212.802], mean action: 1.000 [1.000, 1.000],  loss: 10205699.000000, mae: 1108.082275, mean_q: -337.520020
 3788/5000: episode: 3788, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1472.012, mean reward: -1472.012 [-1472.012, -1472.012], mean action: 1.000 [1.000, 1.000],  loss: 7643623.000000, mae: 1025.852661, mean_q: -338.243469
 3789/5000: episode: 3789, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4607.540, mean reward: -4607.540 [-4607.540, -4607.540], mean action: 1.000 [1.000, 1.000],  loss: 8765984.000000, mae: 1107.106445, mean_q: -337.080750
 3790/5000: episode: 3790, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2357.244, mean reward: -2357.244 [-2357.244, -2357.244], mean action: 1.000 [1.000, 1.000],  loss: 12426679.000000, mae: 1259.759521, mean_q: -337.322235
 3791/5000: episode: 3791, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10217.910, mean reward: -10217.910 [-10217.910, -10217.910], mean action: 1.000 [1.000, 1.000],  loss: 12880252.000000, mae: 1242.719482, mean_q: -337.470703
 3792/5000: episode: 3792, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5530.260, mean reward: -5530.260 [-5530.260, -5530.260], mean action: 1.000 [1.000, 1.000],  loss: 9710476.000000, mae: 1078.182861, mean_q: -337.477661
 3793/5000: episode: 3793, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3599.615, mean reward: -3599.615 [-3599.615, -3599.615], mean action: 1.000 [1.000, 1.000],  loss: 11356915.000000, mae: 1189.762573, mean_q: -337.700958
 3794/5000: episode: 3794, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1881.908, mean reward: -1881.908 [-1881.908, -1881.908], mean action: 2.000 [2.000, 2.000],  loss: 15663316.000000, mae: 1216.031006, mean_q: -338.621216
 3795/5000: episode: 3795, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -779.956, mean reward: -779.956 [-779.956, -779.956], mean action: 1.000 [1.000, 1.000],  loss: 15294188.000000, mae: 1291.655762, mean_q: -338.476562
 3796/5000: episode: 3796, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1754.393, mean reward: -1754.393 [-1754.393, -1754.393], mean action: 1.000 [1.000, 1.000],  loss: 7193564.000000, mae: 1021.706360, mean_q: -336.839172
 3797/5000: episode: 3797, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8003.681, mean reward: -8003.681 [-8003.681, -8003.681], mean action: 1.000 [1.000, 1.000],  loss: 14061055.000000, mae: 1253.420288, mean_q: -336.803284
 3798/5000: episode: 3798, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4638.216, mean reward: -4638.216 [-4638.216, -4638.216], mean action: 1.000 [1.000, 1.000],  loss: 11031354.000000, mae: 1173.042480, mean_q: -339.391266
 3799/5000: episode: 3799, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -124.609, mean reward: -124.609 [-124.609, -124.609], mean action: 1.000 [1.000, 1.000],  loss: 11832420.000000, mae: 1097.722168, mean_q: -340.647430
 3800/5000: episode: 3800, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1918.838, mean reward: -1918.838 [-1918.838, -1918.838], mean action: 1.000 [1.000, 1.000],  loss: 17586616.000000, mae: 1443.560059, mean_q: -339.073181
 3801/5000: episode: 3801, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2607.541, mean reward: -2607.541 [-2607.541, -2607.541], mean action: 1.000 [1.000, 1.000],  loss: 13313764.000000, mae: 1291.029297, mean_q: -339.562073
 3802/5000: episode: 3802, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4112.129, mean reward: -4112.129 [-4112.129, -4112.129], mean action: 1.000 [1.000, 1.000],  loss: 13967410.000000, mae: 1276.568115, mean_q: -339.533020
 3803/5000: episode: 3803, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4953.003, mean reward: -4953.003 [-4953.003, -4953.003], mean action: 1.000 [1.000, 1.000],  loss: 13563618.000000, mae: 1219.440552, mean_q: -339.146729
 3804/5000: episode: 3804, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -11637.443, mean reward: -11637.443 [-11637.443, -11637.443], mean action: 1.000 [1.000, 1.000],  loss: 12261545.000000, mae: 1161.946655, mean_q: -339.354065
 3805/5000: episode: 3805, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2584.904, mean reward: -2584.904 [-2584.904, -2584.904], mean action: 1.000 [1.000, 1.000],  loss: 9023035.000000, mae: 1020.743713, mean_q: -341.443451
 3806/5000: episode: 3806, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4140.478, mean reward: -4140.478 [-4140.478, -4140.478], mean action: 1.000 [1.000, 1.000],  loss: 14526582.000000, mae: 1262.052856, mean_q: -341.680908
 3807/5000: episode: 3807, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2544.257, mean reward: -2544.257 [-2544.257, -2544.257], mean action: 1.000 [1.000, 1.000],  loss: 6503521.500000, mae: 964.306641, mean_q: -341.046387
 3808/5000: episode: 3808, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3213.380, mean reward: -3213.380 [-3213.380, -3213.380], mean action: 1.000 [1.000, 1.000],  loss: 11986756.000000, mae: 1266.874634, mean_q: -340.540314
 3809/5000: episode: 3809, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4072.120, mean reward: -4072.120 [-4072.120, -4072.120], mean action: 1.000 [1.000, 1.000],  loss: 14037922.000000, mae: 1212.146973, mean_q: -341.931671
 3810/5000: episode: 3810, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3749.442, mean reward: -3749.442 [-3749.442, -3749.442], mean action: 1.000 [1.000, 1.000],  loss: 8254077.500000, mae: 1082.822876, mean_q: -342.194366
 3811/5000: episode: 3811, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2772.953, mean reward: -2772.953 [-2772.953, -2772.953], mean action: 1.000 [1.000, 1.000],  loss: 13393425.000000, mae: 1270.214233, mean_q: -340.468750
 3812/5000: episode: 3812, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4127.998, mean reward: -4127.998 [-4127.998, -4127.998], mean action: 1.000 [1.000, 1.000],  loss: 9239626.000000, mae: 1072.094727, mean_q: -342.609100
 3813/5000: episode: 3813, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5663.165, mean reward: -5663.165 [-5663.165, -5663.165], mean action: 1.000 [1.000, 1.000],  loss: 9483014.000000, mae: 1121.181641, mean_q: -341.623047
 3814/5000: episode: 3814, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2345.603, mean reward: -2345.603 [-2345.603, -2345.603], mean action: 1.000 [1.000, 1.000],  loss: 8127269.000000, mae: 1028.351562, mean_q: -342.505432
 3815/5000: episode: 3815, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1336.130, mean reward: -1336.130 [-1336.130, -1336.130], mean action: 1.000 [1.000, 1.000],  loss: 8414524.000000, mae: 1092.596680, mean_q: -343.182434
 3816/5000: episode: 3816, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5964.750, mean reward: -5964.750 [-5964.750, -5964.750], mean action: 1.000 [1.000, 1.000],  loss: 15070868.000000, mae: 1332.141602, mean_q: -343.232422
 3817/5000: episode: 3817, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1345.278, mean reward: -1345.278 [-1345.278, -1345.278], mean action: 3.000 [3.000, 3.000],  loss: 9402278.000000, mae: 1078.462891, mean_q: -343.057648
 3818/5000: episode: 3818, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3923.264, mean reward: -3923.264 [-3923.264, -3923.264], mean action: 1.000 [1.000, 1.000],  loss: 9761515.000000, mae: 1025.574585, mean_q: -343.822693
 3819/5000: episode: 3819, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6390.821, mean reward: -6390.821 [-6390.821, -6390.821], mean action: 1.000 [1.000, 1.000],  loss: 8399565.000000, mae: 976.104797, mean_q: -343.403320
 3820/5000: episode: 3820, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2154.998, mean reward: -2154.998 [-2154.998, -2154.998], mean action: 2.000 [2.000, 2.000],  loss: 12164726.000000, mae: 1152.950195, mean_q: -342.552979
 3821/5000: episode: 3821, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4320.359, mean reward: -4320.359 [-4320.359, -4320.359], mean action: 1.000 [1.000, 1.000],  loss: 13729551.000000, mae: 1138.917725, mean_q: -344.681335
 3822/5000: episode: 3822, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3910.866, mean reward: -3910.866 [-3910.866, -3910.866], mean action: 1.000 [1.000, 1.000],  loss: 11264884.000000, mae: 1124.395264, mean_q: -344.436951
 3823/5000: episode: 3823, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2609.018, mean reward: -2609.018 [-2609.018, -2609.018], mean action: 1.000 [1.000, 1.000],  loss: 4123973.500000, mae: 827.794128, mean_q: -345.015411
 3824/5000: episode: 3824, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1548.700, mean reward: -1548.700 [-1548.700, -1548.700], mean action: 1.000 [1.000, 1.000],  loss: 15398157.000000, mae: 1363.472656, mean_q: -344.262634
 3825/5000: episode: 3825, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1010.066, mean reward: -1010.066 [-1010.066, -1010.066], mean action: 1.000 [1.000, 1.000],  loss: 14479970.000000, mae: 1308.625610, mean_q: -343.670410
 3826/5000: episode: 3826, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -832.984, mean reward: -832.984 [-832.984, -832.984], mean action: 1.000 [1.000, 1.000],  loss: 12571982.000000, mae: 1309.848877, mean_q: -345.102356
 3827/5000: episode: 3827, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4845.728, mean reward: -4845.728 [-4845.728, -4845.728], mean action: 1.000 [1.000, 1.000],  loss: 7773812.000000, mae: 982.332520, mean_q: -344.487427
 3828/5000: episode: 3828, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3507.206, mean reward: -3507.206 [-3507.206, -3507.206], mean action: 1.000 [1.000, 1.000],  loss: 6357882.000000, mae: 1029.171875, mean_q: -346.726440
 3829/5000: episode: 3829, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3437.685, mean reward: -3437.685 [-3437.685, -3437.685], mean action: 1.000 [1.000, 1.000],  loss: 15630957.000000, mae: 1261.098633, mean_q: -345.505371
 3830/5000: episode: 3830, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3763.474, mean reward: -3763.474 [-3763.474, -3763.474], mean action: 1.000 [1.000, 1.000],  loss: 10326540.000000, mae: 1198.880005, mean_q: -345.117737
 3831/5000: episode: 3831, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8224.084, mean reward: -8224.084 [-8224.084, -8224.084], mean action: 1.000 [1.000, 1.000],  loss: 12627759.000000, mae: 1246.150879, mean_q: -345.416595
 3832/5000: episode: 3832, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6730.059, mean reward: -6730.059 [-6730.059, -6730.059], mean action: 1.000 [1.000, 1.000],  loss: 7847818.500000, mae: 1013.348877, mean_q: -345.734222
 3833/5000: episode: 3833, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1332.807, mean reward: -1332.807 [-1332.807, -1332.807], mean action: 1.000 [1.000, 1.000],  loss: 11618890.000000, mae: 1227.607910, mean_q: -346.803223
 3834/5000: episode: 3834, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4782.218, mean reward: -4782.218 [-4782.218, -4782.218], mean action: 1.000 [1.000, 1.000],  loss: 12552664.000000, mae: 1214.464844, mean_q: -345.398407
 3835/5000: episode: 3835, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2656.781, mean reward: -2656.781 [-2656.781, -2656.781], mean action: 1.000 [1.000, 1.000],  loss: 12830372.000000, mae: 1150.248535, mean_q: -347.080444
 3836/5000: episode: 3836, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5197.565, mean reward: -5197.565 [-5197.565, -5197.565], mean action: 1.000 [1.000, 1.000],  loss: 11255813.000000, mae: 1233.035645, mean_q: -345.769653
 3837/5000: episode: 3837, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2480.786, mean reward: -2480.786 [-2480.786, -2480.786], mean action: 1.000 [1.000, 1.000],  loss: 8103617.000000, mae: 1042.316772, mean_q: -347.680847
 3838/5000: episode: 3838, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -10354.694, mean reward: -10354.694 [-10354.694, -10354.694], mean action: 1.000 [1.000, 1.000],  loss: 10782267.000000, mae: 1186.460938, mean_q: -346.617859
 3839/5000: episode: 3839, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -834.874, mean reward: -834.874 [-834.874, -834.874], mean action: 1.000 [1.000, 1.000],  loss: 11161118.000000, mae: 1155.910889, mean_q: -347.838318
 3840/5000: episode: 3840, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -7628.754, mean reward: -7628.754 [-7628.754, -7628.754], mean action: 1.000 [1.000, 1.000],  loss: 15968209.000000, mae: 1312.556763, mean_q: -346.567200
 3841/5000: episode: 3841, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2668.836, mean reward: -2668.836 [-2668.836, -2668.836], mean action: 1.000 [1.000, 1.000],  loss: 11589272.000000, mae: 1212.704590, mean_q: -346.864197
 3842/5000: episode: 3842, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2310.533, mean reward: -2310.533 [-2310.533, -2310.533], mean action: 1.000 [1.000, 1.000],  loss: 6780076.000000, mae: 1040.310059, mean_q: -348.266785
 3843/5000: episode: 3843, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -2611.571, mean reward: -2611.571 [-2611.571, -2611.571], mean action: 1.000 [1.000, 1.000],  loss: 10914160.000000, mae: 1072.150879, mean_q: -347.006958
 3844/5000: episode: 3844, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4009.561, mean reward: -4009.561 [-4009.561, -4009.561], mean action: 1.000 [1.000, 1.000],  loss: 9791182.000000, mae: 1112.304443, mean_q: -349.304962
 3845/5000: episode: 3845, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3808.562, mean reward: -3808.562 [-3808.562, -3808.562], mean action: 1.000 [1.000, 1.000],  loss: 12151806.000000, mae: 1248.336548, mean_q: -348.405090
 3846/5000: episode: 3846, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1898.852, mean reward: -1898.852 [-1898.852, -1898.852], mean action: 1.000 [1.000, 1.000],  loss: 10660740.000000, mae: 1105.360352, mean_q: -348.967773
 3847/5000: episode: 3847, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4702.619, mean reward: -4702.619 [-4702.619, -4702.619], mean action: 1.000 [1.000, 1.000],  loss: 14578532.000000, mae: 1297.784912, mean_q: -346.715942
 3848/5000: episode: 3848, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2638.498, mean reward: -2638.498 [-2638.498, -2638.498], mean action: 1.000 [1.000, 1.000],  loss: 9928282.000000, mae: 1130.437134, mean_q: -348.461273
 3849/5000: episode: 3849, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -743.279, mean reward: -743.279 [-743.279, -743.279], mean action: 1.000 [1.000, 1.000],  loss: 17095632.000000, mae: 1357.758789, mean_q: -348.879272
 3850/5000: episode: 3850, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9505.539, mean reward: -9505.539 [-9505.539, -9505.539], mean action: 1.000 [1.000, 1.000],  loss: 10533990.000000, mae: 1213.000000, mean_q: -350.237976
 3851/5000: episode: 3851, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3017.105, mean reward: -3017.105 [-3017.105, -3017.105], mean action: 1.000 [1.000, 1.000],  loss: 7017702.000000, mae: 1032.306152, mean_q: -349.331268
 3852/5000: episode: 3852, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4142.912, mean reward: -4142.912 [-4142.912, -4142.912], mean action: 1.000 [1.000, 1.000],  loss: 7572402.500000, mae: 1070.124634, mean_q: -349.676666
 3853/5000: episode: 3853, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -684.590, mean reward: -684.590 [-684.590, -684.590], mean action: 1.000 [1.000, 1.000],  loss: 14328953.000000, mae: 1320.240601, mean_q: -349.281769
 3854/5000: episode: 3854, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8897.191, mean reward: -8897.191 [-8897.191, -8897.191], mean action: 1.000 [1.000, 1.000],  loss: 8992036.000000, mae: 1056.933350, mean_q: -350.761902
 3855/5000: episode: 3855, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6925.008, mean reward: -6925.008 [-6925.008, -6925.008], mean action: 1.000 [1.000, 1.000],  loss: 8010795.000000, mae: 966.073364, mean_q: -348.895752
 3856/5000: episode: 3856, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -947.506, mean reward: -947.506 [-947.506, -947.506], mean action: 1.000 [1.000, 1.000],  loss: 16082352.000000, mae: 1424.713379, mean_q: -350.169830
 3857/5000: episode: 3857, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2408.298, mean reward: -2408.298 [-2408.298, -2408.298], mean action: 1.000 [1.000, 1.000],  loss: 9550685.000000, mae: 1145.304321, mean_q: -352.119385
 3858/5000: episode: 3858, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3856.109, mean reward: -3856.109 [-3856.109, -3856.109], mean action: 1.000 [1.000, 1.000],  loss: 8564417.000000, mae: 1026.258179, mean_q: -351.339996
 3859/5000: episode: 3859, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -4875.347, mean reward: -4875.347 [-4875.347, -4875.347], mean action: 1.000 [1.000, 1.000],  loss: 9132840.000000, mae: 1141.375122, mean_q: -351.027130
 3860/5000: episode: 3860, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1894.409, mean reward: -1894.409 [-1894.409, -1894.409], mean action: 1.000 [1.000, 1.000],  loss: 13541338.000000, mae: 1218.203003, mean_q: -351.324951
 3861/5000: episode: 3861, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6445.218, mean reward: -6445.218 [-6445.218, -6445.218], mean action: 1.000 [1.000, 1.000],  loss: 12315231.000000, mae: 1160.991211, mean_q: -351.574341
 3862/5000: episode: 3862, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3010.869, mean reward: -3010.869 [-3010.869, -3010.869], mean action: 1.000 [1.000, 1.000],  loss: 8846110.000000, mae: 1073.321411, mean_q: -351.628448
 3863/5000: episode: 3863, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -13856.253, mean reward: -13856.253 [-13856.253, -13856.253], mean action: 0.000 [0.000, 0.000],  loss: 15130308.000000, mae: 1385.496094, mean_q: -351.144409
 3864/5000: episode: 3864, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7548.066, mean reward: -7548.066 [-7548.066, -7548.066], mean action: 1.000 [1.000, 1.000],  loss: 5774512.500000, mae: 955.532715, mean_q: -353.142853
 3865/5000: episode: 3865, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2610.380, mean reward: -2610.380 [-2610.380, -2610.380], mean action: 1.000 [1.000, 1.000],  loss: 8115219.500000, mae: 1062.119019, mean_q: -352.067932
 3866/5000: episode: 3866, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -728.040, mean reward: -728.040 [-728.040, -728.040], mean action: 1.000 [1.000, 1.000],  loss: 7882171.000000, mae: 1069.889893, mean_q: -352.830688
 3867/5000: episode: 3867, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3972.991, mean reward: -3972.991 [-3972.991, -3972.991], mean action: 1.000 [1.000, 1.000],  loss: 5865871.500000, mae: 926.707397, mean_q: -353.213379
 3868/5000: episode: 3868, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2441.584, mean reward: -2441.584 [-2441.584, -2441.584], mean action: 1.000 [1.000, 1.000],  loss: 6575714.000000, mae: 961.060303, mean_q: -353.773682
 3869/5000: episode: 3869, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8873.351, mean reward: -8873.351 [-8873.351, -8873.351], mean action: 1.000 [1.000, 1.000],  loss: 14248868.000000, mae: 1300.518066, mean_q: -354.003906
 3870/5000: episode: 3870, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -7014.613, mean reward: -7014.613 [-7014.613, -7014.613], mean action: 1.000 [1.000, 1.000],  loss: 9487462.000000, mae: 1047.287354, mean_q: -355.032227
 3871/5000: episode: 3871, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2343.360, mean reward: -2343.360 [-2343.360, -2343.360], mean action: 2.000 [2.000, 2.000],  loss: 10355644.000000, mae: 1165.460938, mean_q: -353.983917
 3872/5000: episode: 3872, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1218.192, mean reward: -1218.192 [-1218.192, -1218.192], mean action: 1.000 [1.000, 1.000],  loss: 7085541.000000, mae: 1051.460205, mean_q: -353.591736
 3873/5000: episode: 3873, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6222.828, mean reward: -6222.828 [-6222.828, -6222.828], mean action: 1.000 [1.000, 1.000],  loss: 18868636.000000, mae: 1499.187500, mean_q: -353.831421
 3874/5000: episode: 3874, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7053.282, mean reward: -7053.282 [-7053.282, -7053.282], mean action: 3.000 [3.000, 3.000],  loss: 8096092.000000, mae: 1051.203857, mean_q: -353.176849
 3875/5000: episode: 3875, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -9657.096, mean reward: -9657.096 [-9657.096, -9657.096], mean action: 1.000 [1.000, 1.000],  loss: 10722717.000000, mae: 1150.434814, mean_q: -353.019531
 3876/5000: episode: 3876, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -1010.641, mean reward: -1010.641 [-1010.641, -1010.641], mean action: 1.000 [1.000, 1.000],  loss: 11718064.000000, mae: 1149.247070, mean_q: -355.378723
 3877/5000: episode: 3877, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5201.701, mean reward: -5201.701 [-5201.701, -5201.701], mean action: 1.000 [1.000, 1.000],  loss: 10678948.000000, mae: 1198.787842, mean_q: -355.420135
 3878/5000: episode: 3878, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -233.834, mean reward: -233.834 [-233.834, -233.834], mean action: 3.000 [3.000, 3.000],  loss: 11065813.000000, mae: 1185.833008, mean_q: -353.680267
 3879/5000: episode: 3879, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3007.280, mean reward: -3007.280 [-3007.280, -3007.280], mean action: 1.000 [1.000, 1.000],  loss: 15207350.000000, mae: 1305.596802, mean_q: -355.792206
 3880/5000: episode: 3880, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5360.419, mean reward: -5360.419 [-5360.419, -5360.419], mean action: 1.000 [1.000, 1.000],  loss: 10453569.000000, mae: 1169.215332, mean_q: -353.709045
 3881/5000: episode: 3881, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5691.289, mean reward: -5691.289 [-5691.289, -5691.289], mean action: 1.000 [1.000, 1.000],  loss: 7786021.000000, mae: 949.054810, mean_q: -355.394104
 3882/5000: episode: 3882, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -894.972, mean reward: -894.972 [-894.972, -894.972], mean action: 1.000 [1.000, 1.000],  loss: 13926868.000000, mae: 1159.014404, mean_q: -354.298279
 3883/5000: episode: 3883, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4954.616, mean reward: -4954.616 [-4954.616, -4954.616], mean action: 1.000 [1.000, 1.000],  loss: 5638391.500000, mae: 985.003418, mean_q: -356.931335
 3884/5000: episode: 3884, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9266.229, mean reward: -9266.229 [-9266.229, -9266.229], mean action: 1.000 [1.000, 1.000],  loss: 8449080.000000, mae: 1003.747681, mean_q: -357.198975
 3885/5000: episode: 3885, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -6072.449, mean reward: -6072.449 [-6072.449, -6072.449], mean action: 1.000 [1.000, 1.000],  loss: 8634964.000000, mae: 1074.858887, mean_q: -356.977905
 3886/5000: episode: 3886, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2312.076, mean reward: -2312.076 [-2312.076, -2312.076], mean action: 1.000 [1.000, 1.000],  loss: 16419690.000000, mae: 1232.163940, mean_q: -355.094513
 3887/5000: episode: 3887, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1013.052, mean reward: -1013.052 [-1013.052, -1013.052], mean action: 1.000 [1.000, 1.000],  loss: 5880010.500000, mae: 938.701904, mean_q: -356.059784
 3888/5000: episode: 3888, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2469.589, mean reward: -2469.589 [-2469.589, -2469.589], mean action: 1.000 [1.000, 1.000],  loss: 14034824.000000, mae: 1346.486572, mean_q: -355.850555
 3889/5000: episode: 3889, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3832.302, mean reward: -3832.302 [-3832.302, -3832.302], mean action: 1.000 [1.000, 1.000],  loss: 13263250.000000, mae: 1247.095703, mean_q: -356.650543
 3890/5000: episode: 3890, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10054.723, mean reward: -10054.723 [-10054.723, -10054.723], mean action: 1.000 [1.000, 1.000],  loss: 12183515.000000, mae: 1221.967041, mean_q: -358.539215
 3891/5000: episode: 3891, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8075.977, mean reward: -8075.977 [-8075.977, -8075.977], mean action: 1.000 [1.000, 1.000],  loss: 10827188.000000, mae: 1100.827759, mean_q: -357.758118
 3892/5000: episode: 3892, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -7682.774, mean reward: -7682.774 [-7682.774, -7682.774], mean action: 1.000 [1.000, 1.000],  loss: 7942967.000000, mae: 1011.639160, mean_q: -357.809448
 3893/5000: episode: 3893, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5087.338, mean reward: -5087.338 [-5087.338, -5087.338], mean action: 1.000 [1.000, 1.000],  loss: 8110477.000000, mae: 1103.835938, mean_q: -358.400635
 3894/5000: episode: 3894, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -7520.487, mean reward: -7520.487 [-7520.487, -7520.487], mean action: 1.000 [1.000, 1.000],  loss: 8466930.000000, mae: 1057.429321, mean_q: -359.047607
 3895/5000: episode: 3895, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3986.171, mean reward: -3986.171 [-3986.171, -3986.171], mean action: 1.000 [1.000, 1.000],  loss: 5584819.000000, mae: 897.961792, mean_q: -358.613342
 3896/5000: episode: 3896, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -1577.579, mean reward: -1577.579 [-1577.579, -1577.579], mean action: 1.000 [1.000, 1.000],  loss: 11286616.000000, mae: 1237.754272, mean_q: -358.354736
 3897/5000: episode: 3897, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -606.822, mean reward: -606.822 [-606.822, -606.822], mean action: 1.000 [1.000, 1.000],  loss: 5479369.000000, mae: 893.338928, mean_q: -360.093750
 3898/5000: episode: 3898, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3794.870, mean reward: -3794.870 [-3794.870, -3794.870], mean action: 1.000 [1.000, 1.000],  loss: 9133933.000000, mae: 1076.692505, mean_q: -358.162750
 3899/5000: episode: 3899, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -591.479, mean reward: -591.479 [-591.479, -591.479], mean action: 1.000 [1.000, 1.000],  loss: 10338708.000000, mae: 1128.050781, mean_q: -358.789642
 3900/5000: episode: 3900, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5766.162, mean reward: -5766.162 [-5766.162, -5766.162], mean action: 1.000 [1.000, 1.000],  loss: 11212538.000000, mae: 1276.112549, mean_q: -358.648132
 3901/5000: episode: 3901, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5750.466, mean reward: -5750.466 [-5750.466, -5750.466], mean action: 1.000 [1.000, 1.000],  loss: 11037296.000000, mae: 1103.686279, mean_q: -358.871796
 3902/5000: episode: 3902, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2652.429, mean reward: -2652.429 [-2652.429, -2652.429], mean action: 1.000 [1.000, 1.000],  loss: 9498789.000000, mae: 1077.296021, mean_q: -359.998749
 3903/5000: episode: 3903, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1036.870, mean reward: -1036.870 [-1036.870, -1036.870], mean action: 1.000 [1.000, 1.000],  loss: 13776152.000000, mae: 1236.303101, mean_q: -358.137573
 3904/5000: episode: 3904, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1300.241, mean reward: -1300.241 [-1300.241, -1300.241], mean action: 1.000 [1.000, 1.000],  loss: 12112867.000000, mae: 1273.485840, mean_q: -359.482269
 3905/5000: episode: 3905, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1118.451, mean reward: -1118.451 [-1118.451, -1118.451], mean action: 1.000 [1.000, 1.000],  loss: 12026559.000000, mae: 1188.095825, mean_q: -358.828918
 3906/5000: episode: 3906, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1843.604, mean reward: -1843.604 [-1843.604, -1843.604], mean action: 2.000 [2.000, 2.000],  loss: 11275575.000000, mae: 1172.464355, mean_q: -360.440460
 3907/5000: episode: 3907, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4132.045, mean reward: -4132.045 [-4132.045, -4132.045], mean action: 1.000 [1.000, 1.000],  loss: 12293190.000000, mae: 1214.036621, mean_q: -360.074341
 3908/5000: episode: 3908, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -4229.953, mean reward: -4229.953 [-4229.953, -4229.953], mean action: 1.000 [1.000, 1.000],  loss: 14157054.000000, mae: 1306.438477, mean_q: -359.859253
 3909/5000: episode: 3909, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4391.917, mean reward: -4391.917 [-4391.917, -4391.917], mean action: 1.000 [1.000, 1.000],  loss: 12595889.000000, mae: 1288.084961, mean_q: -361.075256
 3910/5000: episode: 3910, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6012.411, mean reward: -6012.411 [-6012.411, -6012.411], mean action: 1.000 [1.000, 1.000],  loss: 12840152.000000, mae: 1335.659180, mean_q: -359.721924
 3911/5000: episode: 3911, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -587.077, mean reward: -587.077 [-587.077, -587.077], mean action: 1.000 [1.000, 1.000],  loss: 12830306.000000, mae: 1274.127075, mean_q: -360.093536
 3912/5000: episode: 3912, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -7428.750, mean reward: -7428.750 [-7428.750, -7428.750], mean action: 1.000 [1.000, 1.000],  loss: 9375328.000000, mae: 1089.526123, mean_q: -361.739349
 3913/5000: episode: 3913, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7698.591, mean reward: -7698.591 [-7698.591, -7698.591], mean action: 1.000 [1.000, 1.000],  loss: 14628493.000000, mae: 1369.382568, mean_q: -358.719238
 3914/5000: episode: 3914, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5736.806, mean reward: -5736.806 [-5736.806, -5736.806], mean action: 1.000 [1.000, 1.000],  loss: 10106324.000000, mae: 1160.064209, mean_q: -362.025757
 3915/5000: episode: 3915, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -998.189, mean reward: -998.189 [-998.189, -998.189], mean action: 1.000 [1.000, 1.000],  loss: 10049504.000000, mae: 1140.309814, mean_q: -361.667419
 3916/5000: episode: 3916, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2744.363, mean reward: -2744.363 [-2744.363, -2744.363], mean action: 1.000 [1.000, 1.000],  loss: 8518530.000000, mae: 1037.450317, mean_q: -362.722198
 3917/5000: episode: 3917, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2428.486, mean reward: -2428.486 [-2428.486, -2428.486], mean action: 1.000 [1.000, 1.000],  loss: 13927781.000000, mae: 1282.851318, mean_q: -361.833496
 3918/5000: episode: 3918, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2344.944, mean reward: -2344.944 [-2344.944, -2344.944], mean action: 1.000 [1.000, 1.000],  loss: 9796021.000000, mae: 1097.319946, mean_q: -363.483154
 3919/5000: episode: 3919, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8496.865, mean reward: -8496.865 [-8496.865, -8496.865], mean action: 1.000 [1.000, 1.000],  loss: 15526599.000000, mae: 1417.661377, mean_q: -362.350861
 3920/5000: episode: 3920, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2675.256, mean reward: -2675.256 [-2675.256, -2675.256], mean action: 1.000 [1.000, 1.000],  loss: 8267682.500000, mae: 1048.784790, mean_q: -364.555847
 3921/5000: episode: 3921, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3691.694, mean reward: -3691.694 [-3691.694, -3691.694], mean action: 1.000 [1.000, 1.000],  loss: 13031117.000000, mae: 1251.054443, mean_q: -364.051971
 3922/5000: episode: 3922, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3819.687, mean reward: -3819.687 [-3819.687, -3819.687], mean action: 1.000 [1.000, 1.000],  loss: 7132658.000000, mae: 1052.841797, mean_q: -362.636353
 3923/5000: episode: 3923, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7554.915, mean reward: -7554.915 [-7554.915, -7554.915], mean action: 1.000 [1.000, 1.000],  loss: 10122481.000000, mae: 1116.975342, mean_q: -363.011719
 3924/5000: episode: 3924, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7336.988, mean reward: -7336.988 [-7336.988, -7336.988], mean action: 1.000 [1.000, 1.000],  loss: 16222318.000000, mae: 1360.767578, mean_q: -362.534546
 3925/5000: episode: 3925, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -10201.050, mean reward: -10201.050 [-10201.050, -10201.050], mean action: 1.000 [1.000, 1.000],  loss: 9872656.000000, mae: 1022.921692, mean_q: -365.103577
 3926/5000: episode: 3926, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3345.016, mean reward: -3345.016 [-3345.016, -3345.016], mean action: 1.000 [1.000, 1.000],  loss: 6669246.000000, mae: 987.898254, mean_q: -364.771973
 3927/5000: episode: 3927, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2497.860, mean reward: -2497.860 [-2497.860, -2497.860], mean action: 1.000 [1.000, 1.000],  loss: 11194970.000000, mae: 1177.084839, mean_q: -364.146484
 3928/5000: episode: 3928, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8734.860, mean reward: -8734.860 [-8734.860, -8734.860], mean action: 1.000 [1.000, 1.000],  loss: 9044789.000000, mae: 1011.605835, mean_q: -365.450378
 3929/5000: episode: 3929, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8337.415, mean reward: -8337.415 [-8337.415, -8337.415], mean action: 1.000 [1.000, 1.000],  loss: 12460712.000000, mae: 1266.690918, mean_q: -364.412415
 3930/5000: episode: 3930, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6098.223, mean reward: -6098.223 [-6098.223, -6098.223], mean action: 1.000 [1.000, 1.000],  loss: 16878670.000000, mae: 1333.459595, mean_q: -364.238831
 3931/5000: episode: 3931, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1671.527, mean reward: -1671.527 [-1671.527, -1671.527], mean action: 1.000 [1.000, 1.000],  loss: 8496894.000000, mae: 1102.932373, mean_q: -364.864624
 3932/5000: episode: 3932, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1576.653, mean reward: -1576.653 [-1576.653, -1576.653], mean action: 1.000 [1.000, 1.000],  loss: 13523229.000000, mae: 1265.221924, mean_q: -365.829376
 3933/5000: episode: 3933, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3650.718, mean reward: -3650.718 [-3650.718, -3650.718], mean action: 1.000 [1.000, 1.000],  loss: 6910569.000000, mae: 943.420227, mean_q: -366.104065
 3934/5000: episode: 3934, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7978.203, mean reward: -7978.203 [-7978.203, -7978.203], mean action: 1.000 [1.000, 1.000],  loss: 9596657.000000, mae: 1119.079590, mean_q: -366.153900
 3935/5000: episode: 3935, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -738.410, mean reward: -738.410 [-738.410, -738.410], mean action: 1.000 [1.000, 1.000],  loss: 9643698.000000, mae: 1151.216553, mean_q: -365.865479
 3936/5000: episode: 3936, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4505.916, mean reward: -4505.916 [-4505.916, -4505.916], mean action: 0.000 [0.000, 0.000],  loss: 15032985.000000, mae: 1375.555420, mean_q: -366.699890
 3937/5000: episode: 3937, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5995.929, mean reward: -5995.929 [-5995.929, -5995.929], mean action: 0.000 [0.000, 0.000],  loss: 9676710.000000, mae: 1126.925049, mean_q: -367.247314
 3938/5000: episode: 3938, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5974.517, mean reward: -5974.517 [-5974.517, -5974.517], mean action: 0.000 [0.000, 0.000],  loss: 8329500.000000, mae: 1054.296997, mean_q: -367.424683
 3939/5000: episode: 3939, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -10243.803, mean reward: -10243.803 [-10243.803, -10243.803], mean action: 0.000 [0.000, 0.000],  loss: 12093994.000000, mae: 1257.072388, mean_q: -367.735596
 3940/5000: episode: 3940, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8027.293, mean reward: -8027.293 [-8027.293, -8027.293], mean action: 0.000 [0.000, 0.000],  loss: 9985836.000000, mae: 1189.189331, mean_q: -367.340271
 3941/5000: episode: 3941, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -3335.099, mean reward: -3335.099 [-3335.099, -3335.099], mean action: 0.000 [0.000, 0.000],  loss: 12413466.000000, mae: 1197.200684, mean_q: -366.328400
 3942/5000: episode: 3942, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -9538.291, mean reward: -9538.291 [-9538.291, -9538.291], mean action: 0.000 [0.000, 0.000],  loss: 14291289.000000, mae: 1415.517212, mean_q: -366.801453
 3943/5000: episode: 3943, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8013.976, mean reward: -8013.976 [-8013.976, -8013.976], mean action: 0.000 [0.000, 0.000],  loss: 8657081.000000, mae: 1024.337280, mean_q: -368.608765
 3944/5000: episode: 3944, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3730.790, mean reward: -3730.790 [-3730.790, -3730.790], mean action: 0.000 [0.000, 0.000],  loss: 15913042.000000, mae: 1306.675049, mean_q: -365.982697
 3945/5000: episode: 3945, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5576.259, mean reward: -5576.259 [-5576.259, -5576.259], mean action: 0.000 [0.000, 0.000],  loss: 11109464.000000, mae: 1189.333984, mean_q: -368.423706
 3946/5000: episode: 3946, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3075.784, mean reward: -3075.784 [-3075.784, -3075.784], mean action: 0.000 [0.000, 0.000],  loss: 12398866.000000, mae: 1286.941040, mean_q: -367.410889
 3947/5000: episode: 3947, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3327.287, mean reward: -3327.287 [-3327.287, -3327.287], mean action: 0.000 [0.000, 0.000],  loss: 16831160.000000, mae: 1371.852783, mean_q: -367.499603
 3948/5000: episode: 3948, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4474.448, mean reward: -4474.448 [-4474.448, -4474.448], mean action: 0.000 [0.000, 0.000],  loss: 9848415.000000, mae: 1125.897095, mean_q: -368.467987
 3949/5000: episode: 3949, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5708.816, mean reward: -5708.816 [-5708.816, -5708.816], mean action: 0.000 [0.000, 0.000],  loss: 5447305.000000, mae: 1015.723633, mean_q: -368.140076
 3950/5000: episode: 3950, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8190.231, mean reward: -8190.231 [-8190.231, -8190.231], mean action: 0.000 [0.000, 0.000],  loss: 4494729.000000, mae: 857.158325, mean_q: -369.649750
 3951/5000: episode: 3951, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3638.397, mean reward: -3638.397 [-3638.397, -3638.397], mean action: 0.000 [0.000, 0.000],  loss: 12792011.000000, mae: 1289.509033, mean_q: -368.965271
 3952/5000: episode: 3952, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -15212.107, mean reward: -15212.107 [-15212.107, -15212.107], mean action: 0.000 [0.000, 0.000],  loss: 8178020.500000, mae: 1120.278564, mean_q: -368.505371
 3953/5000: episode: 3953, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4646.702, mean reward: -4646.702 [-4646.702, -4646.702], mean action: 0.000 [0.000, 0.000],  loss: 10229474.000000, mae: 1088.420166, mean_q: -370.618805
 3954/5000: episode: 3954, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5617.906, mean reward: -5617.906 [-5617.906, -5617.906], mean action: 0.000 [0.000, 0.000],  loss: 13799414.000000, mae: 1308.646362, mean_q: -369.274628
 3955/5000: episode: 3955, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6975.709, mean reward: -6975.709 [-6975.709, -6975.709], mean action: 0.000 [0.000, 0.000],  loss: 15905934.000000, mae: 1430.632690, mean_q: -370.138458
 3956/5000: episode: 3956, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -7875.739, mean reward: -7875.739 [-7875.739, -7875.739], mean action: 0.000 [0.000, 0.000],  loss: 13953822.000000, mae: 1367.072144, mean_q: -368.114563
 3957/5000: episode: 3957, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4737.169, mean reward: -4737.169 [-4737.169, -4737.169], mean action: 0.000 [0.000, 0.000],  loss: 7744937.000000, mae: 1052.688965, mean_q: -370.148621
 3958/5000: episode: 3958, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8346.046, mean reward: -8346.046 [-8346.046, -8346.046], mean action: 0.000 [0.000, 0.000],  loss: 8502644.000000, mae: 1022.563599, mean_q: -371.716125
 3959/5000: episode: 3959, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5603.856, mean reward: -5603.856 [-5603.856, -5603.856], mean action: 0.000 [0.000, 0.000],  loss: 12099854.000000, mae: 1149.796387, mean_q: -370.355713
 3960/5000: episode: 3960, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8566.872, mean reward: -8566.872 [-8566.872, -8566.872], mean action: 0.000 [0.000, 0.000],  loss: 9993448.000000, mae: 1091.287842, mean_q: -370.028015
 3961/5000: episode: 3961, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12500.584, mean reward: -12500.584 [-12500.584, -12500.584], mean action: 0.000 [0.000, 0.000],  loss: 6725419.000000, mae: 921.468689, mean_q: -372.694519
 3962/5000: episode: 3962, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12824.749, mean reward: -12824.749 [-12824.749, -12824.749], mean action: 0.000 [0.000, 0.000],  loss: 16878444.000000, mae: 1477.333374, mean_q: -371.374634
 3963/5000: episode: 3963, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6161.435, mean reward: -6161.435 [-6161.435, -6161.435], mean action: 0.000 [0.000, 0.000],  loss: 8840348.000000, mae: 1124.096802, mean_q: -371.963837
 3964/5000: episode: 3964, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -8578.284, mean reward: -8578.284 [-8578.284, -8578.284], mean action: 0.000 [0.000, 0.000],  loss: 12444301.000000, mae: 1332.721436, mean_q: -372.076172
 3965/5000: episode: 3965, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5770.862, mean reward: -5770.862 [-5770.862, -5770.862], mean action: 1.000 [1.000, 1.000],  loss: 13771272.000000, mae: 1391.909912, mean_q: -369.280579
 3966/5000: episode: 3966, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -8633.899, mean reward: -8633.899 [-8633.899, -8633.899], mean action: 0.000 [0.000, 0.000],  loss: 12368037.000000, mae: 1207.799805, mean_q: -372.387115
 3967/5000: episode: 3967, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -8765.719, mean reward: -8765.719 [-8765.719, -8765.719], mean action: 0.000 [0.000, 0.000],  loss: 8731762.000000, mae: 1173.373291, mean_q: -373.598602
 3968/5000: episode: 3968, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3701.090, mean reward: -3701.090 [-3701.090, -3701.090], mean action: 0.000 [0.000, 0.000],  loss: 24653116.000000, mae: 1543.735840, mean_q: -370.289459
 3969/5000: episode: 3969, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -14505.525, mean reward: -14505.525 [-14505.525, -14505.525], mean action: 0.000 [0.000, 0.000],  loss: 11128050.000000, mae: 1173.795654, mean_q: -373.314941
 3970/5000: episode: 3970, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6456.302, mean reward: -6456.302 [-6456.302, -6456.302], mean action: 0.000 [0.000, 0.000],  loss: 9473782.000000, mae: 1100.218262, mean_q: -373.520020
 3971/5000: episode: 3971, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5600.257, mean reward: -5600.257 [-5600.257, -5600.257], mean action: 2.000 [2.000, 2.000],  loss: 9774437.000000, mae: 1061.378418, mean_q: -373.039032
 3972/5000: episode: 3972, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -11875.357, mean reward: -11875.357 [-11875.357, -11875.357], mean action: 0.000 [0.000, 0.000],  loss: 6587042.500000, mae: 977.347656, mean_q: -372.330872
 3973/5000: episode: 3973, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10603.381, mean reward: -10603.381 [-10603.381, -10603.381], mean action: 0.000 [0.000, 0.000],  loss: 11072457.000000, mae: 1148.596680, mean_q: -374.847778
 3974/5000: episode: 3974, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2036.627, mean reward: -2036.627 [-2036.627, -2036.627], mean action: 0.000 [0.000, 0.000],  loss: 10926722.000000, mae: 1199.702271, mean_q: -374.009430
 3975/5000: episode: 3975, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5014.752, mean reward: -5014.752 [-5014.752, -5014.752], mean action: 3.000 [3.000, 3.000],  loss: 9782249.000000, mae: 1115.810913, mean_q: -374.454102
 3976/5000: episode: 3976, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4513.539, mean reward: -4513.539 [-4513.539, -4513.539], mean action: 0.000 [0.000, 0.000],  loss: 13554192.000000, mae: 1290.975464, mean_q: -372.904114
 3977/5000: episode: 3977, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5103.270, mean reward: -5103.270 [-5103.270, -5103.270], mean action: 0.000 [0.000, 0.000],  loss: 12754364.000000, mae: 1368.139648, mean_q: -374.829102
 3978/5000: episode: 3978, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9841.928, mean reward: -9841.928 [-9841.928, -9841.928], mean action: 0.000 [0.000, 0.000],  loss: 18144584.000000, mae: 1427.041992, mean_q: -373.479492
 3979/5000: episode: 3979, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3217.266, mean reward: -3217.266 [-3217.266, -3217.266], mean action: 0.000 [0.000, 0.000],  loss: 11455322.000000, mae: 1250.227539, mean_q: -375.329590
 3980/5000: episode: 3980, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8700.528, mean reward: -8700.528 [-8700.528, -8700.528], mean action: 0.000 [0.000, 0.000],  loss: 16934018.000000, mae: 1510.505249, mean_q: -374.455841
 3981/5000: episode: 3981, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -8361.943, mean reward: -8361.943 [-8361.943, -8361.943], mean action: 0.000 [0.000, 0.000],  loss: 11971794.000000, mae: 1249.610352, mean_q: -376.093811
 3982/5000: episode: 3982, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1554.235, mean reward: -1554.235 [-1554.235, -1554.235], mean action: 0.000 [0.000, 0.000],  loss: 12324970.000000, mae: 1306.444336, mean_q: -373.607788
 3983/5000: episode: 3983, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -556.876, mean reward: -556.876 [-556.876, -556.876], mean action: 3.000 [3.000, 3.000],  loss: 11312336.000000, mae: 1138.013062, mean_q: -374.357452
 3984/5000: episode: 3984, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4469.887, mean reward: -4469.887 [-4469.887, -4469.887], mean action: 0.000 [0.000, 0.000],  loss: 7157569.500000, mae: 1030.855347, mean_q: -374.954193
 3985/5000: episode: 3985, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10903.976, mean reward: -10903.976 [-10903.976, -10903.976], mean action: 0.000 [0.000, 0.000],  loss: 8477191.000000, mae: 1077.140137, mean_q: -375.256714
 3986/5000: episode: 3986, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -6416.440, mean reward: -6416.440 [-6416.440, -6416.440], mean action: 0.000 [0.000, 0.000],  loss: 8736725.000000, mae: 1158.406006, mean_q: -377.233521
 3987/5000: episode: 3987, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8958.776, mean reward: -8958.776 [-8958.776, -8958.776], mean action: 0.000 [0.000, 0.000],  loss: 7438488.500000, mae: 1089.999512, mean_q: -376.218872
 3988/5000: episode: 3988, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7174.307, mean reward: -7174.307 [-7174.307, -7174.307], mean action: 0.000 [0.000, 0.000],  loss: 11957383.000000, mae: 1140.210571, mean_q: -376.955597
 3989/5000: episode: 3989, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5732.944, mean reward: -5732.944 [-5732.944, -5732.944], mean action: 0.000 [0.000, 0.000],  loss: 14175626.000000, mae: 1240.992065, mean_q: -376.150543
 3990/5000: episode: 3990, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -12225.391, mean reward: -12225.391 [-12225.391, -12225.391], mean action: 0.000 [0.000, 0.000],  loss: 10820552.000000, mae: 1199.563965, mean_q: -376.297241
 3991/5000: episode: 3991, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7239.033, mean reward: -7239.033 [-7239.033, -7239.033], mean action: 0.000 [0.000, 0.000],  loss: 8883447.000000, mae: 1092.671387, mean_q: -376.437164
 3992/5000: episode: 3992, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1763.296, mean reward: -1763.296 [-1763.296, -1763.296], mean action: 0.000 [0.000, 0.000],  loss: 15852832.000000, mae: 1384.291016, mean_q: -376.879272
 3993/5000: episode: 3993, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10887.027, mean reward: -10887.027 [-10887.027, -10887.027], mean action: 0.000 [0.000, 0.000],  loss: 11827173.000000, mae: 1164.456055, mean_q: -376.748108
 3994/5000: episode: 3994, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -10606.837, mean reward: -10606.837 [-10606.837, -10606.837], mean action: 0.000 [0.000, 0.000],  loss: 7647575.500000, mae: 1033.732422, mean_q: -377.285858
 3995/5000: episode: 3995, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6661.448, mean reward: -6661.448 [-6661.448, -6661.448], mean action: 0.000 [0.000, 0.000],  loss: 10119941.000000, mae: 1133.028564, mean_q: -377.563538
 3996/5000: episode: 3996, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4721.941, mean reward: -4721.941 [-4721.941, -4721.941], mean action: 0.000 [0.000, 0.000],  loss: 8529002.000000, mae: 1054.383057, mean_q: -378.368500
 3997/5000: episode: 3997, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1308.466, mean reward: -1308.466 [-1308.466, -1308.466], mean action: 3.000 [3.000, 3.000],  loss: 8685192.000000, mae: 1117.431030, mean_q: -377.847412
 3998/5000: episode: 3998, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5798.615, mean reward: -5798.615 [-5798.615, -5798.615], mean action: 1.000 [1.000, 1.000],  loss: 11038008.000000, mae: 1237.498047, mean_q: -378.383484
 3999/5000: episode: 3999, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -10159.883, mean reward: -10159.883 [-10159.883, -10159.883], mean action: 0.000 [0.000, 0.000],  loss: 12662660.000000, mae: 1277.039795, mean_q: -377.104736
 4000/5000: episode: 4000, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3644.366, mean reward: -3644.366 [-3644.366, -3644.366], mean action: 1.000 [1.000, 1.000],  loss: 15216244.000000, mae: 1343.490479, mean_q: -379.219666
 4001/5000: episode: 4001, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3100.440, mean reward: -3100.440 [-3100.440, -3100.440], mean action: 0.000 [0.000, 0.000],  loss: 7108674.000000, mae: 937.453735, mean_q: -379.600647
 4002/5000: episode: 4002, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -522.210, mean reward: -522.210 [-522.210, -522.210], mean action: 2.000 [2.000, 2.000],  loss: 12992805.000000, mae: 1292.833252, mean_q: -377.829712
 4003/5000: episode: 4003, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7019.317, mean reward: -7019.317 [-7019.317, -7019.317], mean action: 0.000 [0.000, 0.000],  loss: 12857311.000000, mae: 1244.301392, mean_q: -380.543396
 4004/5000: episode: 4004, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3164.681, mean reward: -3164.681 [-3164.681, -3164.681], mean action: 0.000 [0.000, 0.000],  loss: 9691776.000000, mae: 1110.115601, mean_q: -378.631836
 4005/5000: episode: 4005, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5349.599, mean reward: -5349.599 [-5349.599, -5349.599], mean action: 0.000 [0.000, 0.000],  loss: 9395313.000000, mae: 1111.123047, mean_q: -379.617981
 4006/5000: episode: 4006, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12476.519, mean reward: -12476.519 [-12476.519, -12476.519], mean action: 0.000 [0.000, 0.000],  loss: 12089518.000000, mae: 1152.809082, mean_q: -381.278931
 4007/5000: episode: 4007, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2238.142, mean reward: -2238.142 [-2238.142, -2238.142], mean action: 0.000 [0.000, 0.000],  loss: 13212625.000000, mae: 1363.513184, mean_q: -379.029480
 4008/5000: episode: 4008, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4748.386, mean reward: -4748.386 [-4748.386, -4748.386], mean action: 0.000 [0.000, 0.000],  loss: 7859350.000000, mae: 1111.012451, mean_q: -380.087341
 4009/5000: episode: 4009, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6858.203, mean reward: -6858.203 [-6858.203, -6858.203], mean action: 0.000 [0.000, 0.000],  loss: 8007850.000000, mae: 1073.934570, mean_q: -382.535339
 4010/5000: episode: 4010, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6845.574, mean reward: -6845.574 [-6845.574, -6845.574], mean action: 0.000 [0.000, 0.000],  loss: 10962351.000000, mae: 1240.763550, mean_q: -381.603088
 4011/5000: episode: 4011, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -6892.119, mean reward: -6892.119 [-6892.119, -6892.119], mean action: 0.000 [0.000, 0.000],  loss: 11340591.000000, mae: 1176.184814, mean_q: -382.532104
 4012/5000: episode: 4012, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10343.688, mean reward: -10343.688 [-10343.688, -10343.688], mean action: 0.000 [0.000, 0.000],  loss: 11715622.000000, mae: 1282.247559, mean_q: -380.153961
 4013/5000: episode: 4013, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -12126.979, mean reward: -12126.979 [-12126.979, -12126.979], mean action: 0.000 [0.000, 0.000],  loss: 15205372.000000, mae: 1340.537354, mean_q: -379.908478
 4014/5000: episode: 4014, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -11582.082, mean reward: -11582.082 [-11582.082, -11582.082], mean action: 0.000 [0.000, 0.000],  loss: 10966827.000000, mae: 1235.882324, mean_q: -380.507660
 4015/5000: episode: 4015, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6938.661, mean reward: -6938.661 [-6938.661, -6938.661], mean action: 0.000 [0.000, 0.000],  loss: 9976141.000000, mae: 1154.640625, mean_q: -380.452087
 4016/5000: episode: 4016, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4183.132, mean reward: -4183.132 [-4183.132, -4183.132], mean action: 0.000 [0.000, 0.000],  loss: 8559696.000000, mae: 1150.468872, mean_q: -384.882874
 4017/5000: episode: 4017, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5831.808, mean reward: -5831.808 [-5831.808, -5831.808], mean action: 0.000 [0.000, 0.000],  loss: 8863476.000000, mae: 1031.394775, mean_q: -381.270935
 4018/5000: episode: 4018, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12049.347, mean reward: -12049.347 [-12049.347, -12049.347], mean action: 0.000 [0.000, 0.000],  loss: 10292098.000000, mae: 1132.810425, mean_q: -382.045288
 4019/5000: episode: 4019, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4329.312, mean reward: -4329.312 [-4329.312, -4329.312], mean action: 0.000 [0.000, 0.000],  loss: 10556391.000000, mae: 1126.546265, mean_q: -382.967255
 4020/5000: episode: 4020, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2978.274, mean reward: -2978.274 [-2978.274, -2978.274], mean action: 0.000 [0.000, 0.000],  loss: 10341202.000000, mae: 1168.781006, mean_q: -384.108429
 4021/5000: episode: 4021, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -12465.145, mean reward: -12465.145 [-12465.145, -12465.145], mean action: 0.000 [0.000, 0.000],  loss: 11968456.000000, mae: 1217.674316, mean_q: -382.430511
 4022/5000: episode: 4022, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2776.575, mean reward: -2776.575 [-2776.575, -2776.575], mean action: 2.000 [2.000, 2.000],  loss: 9146410.000000, mae: 1088.153076, mean_q: -382.582367
 4023/5000: episode: 4023, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -10293.865, mean reward: -10293.865 [-10293.865, -10293.865], mean action: 0.000 [0.000, 0.000],  loss: 9229818.000000, mae: 1158.264893, mean_q: -383.736938
 4024/5000: episode: 4024, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1642.422, mean reward: -1642.422 [-1642.422, -1642.422], mean action: 0.000 [0.000, 0.000],  loss: 11063891.000000, mae: 1236.020752, mean_q: -383.506500
 4025/5000: episode: 4025, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7416.098, mean reward: -7416.098 [-7416.098, -7416.098], mean action: 0.000 [0.000, 0.000],  loss: 6934169.000000, mae: 1050.279297, mean_q: -384.837585
 4026/5000: episode: 4026, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -13126.854, mean reward: -13126.854 [-13126.854, -13126.854], mean action: 0.000 [0.000, 0.000],  loss: 8734634.000000, mae: 1095.015259, mean_q: -382.887268
 4027/5000: episode: 4027, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2364.411, mean reward: -2364.411 [-2364.411, -2364.411], mean action: 0.000 [0.000, 0.000],  loss: 9297546.000000, mae: 1127.478516, mean_q: -383.551178
 4028/5000: episode: 4028, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6429.537, mean reward: -6429.537 [-6429.537, -6429.537], mean action: 0.000 [0.000, 0.000],  loss: 11872369.000000, mae: 1166.999756, mean_q: -384.031586
 4029/5000: episode: 4029, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -12117.989, mean reward: -12117.989 [-12117.989, -12117.989], mean action: 0.000 [0.000, 0.000],  loss: 13464541.000000, mae: 1220.351562, mean_q: -384.634827
 4030/5000: episode: 4030, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -11527.922, mean reward: -11527.922 [-11527.922, -11527.922], mean action: 0.000 [0.000, 0.000],  loss: 11646476.000000, mae: 1303.896973, mean_q: -384.347229
 4031/5000: episode: 4031, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4155.031, mean reward: -4155.031 [-4155.031, -4155.031], mean action: 0.000 [0.000, 0.000],  loss: 13055600.000000, mae: 1280.156982, mean_q: -384.206787
 4032/5000: episode: 4032, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8339.351, mean reward: -8339.351 [-8339.351, -8339.351], mean action: 0.000 [0.000, 0.000],  loss: 7039438.000000, mae: 1044.982910, mean_q: -385.494080
 4033/5000: episode: 4033, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10348.673, mean reward: -10348.673 [-10348.673, -10348.673], mean action: 0.000 [0.000, 0.000],  loss: 9331686.000000, mae: 1099.585083, mean_q: -386.859741
 4034/5000: episode: 4034, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6798.308, mean reward: -6798.308 [-6798.308, -6798.308], mean action: 0.000 [0.000, 0.000],  loss: 9888035.000000, mae: 1141.833008, mean_q: -384.653687
 4035/5000: episode: 4035, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8290.666, mean reward: -8290.666 [-8290.666, -8290.666], mean action: 0.000 [0.000, 0.000],  loss: 12891754.000000, mae: 1259.955688, mean_q: -384.534088
 4036/5000: episode: 4036, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4561.123, mean reward: -4561.123 [-4561.123, -4561.123], mean action: 0.000 [0.000, 0.000],  loss: 9649366.000000, mae: 1090.295776, mean_q: -385.924835
 4037/5000: episode: 4037, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7388.638, mean reward: -7388.638 [-7388.638, -7388.638], mean action: 0.000 [0.000, 0.000],  loss: 10537225.000000, mae: 1200.817383, mean_q: -385.602112
 4038/5000: episode: 4038, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6233.857, mean reward: -6233.857 [-6233.857, -6233.857], mean action: 0.000 [0.000, 0.000],  loss: 11068688.000000, mae: 1261.195557, mean_q: -385.735229
 4039/5000: episode: 4039, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6272.658, mean reward: -6272.658 [-6272.658, -6272.658], mean action: 0.000 [0.000, 0.000],  loss: 5770875.000000, mae: 890.101929, mean_q: -386.487915
 4040/5000: episode: 4040, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -7425.646, mean reward: -7425.646 [-7425.646, -7425.646], mean action: 0.000 [0.000, 0.000],  loss: 10075487.000000, mae: 1196.678955, mean_q: -388.496124
 4041/5000: episode: 4041, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12276.478, mean reward: -12276.478 [-12276.478, -12276.478], mean action: 0.000 [0.000, 0.000],  loss: 10951200.000000, mae: 1178.954102, mean_q: -387.421631
 4042/5000: episode: 4042, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3641.010, mean reward: -3641.010 [-3641.010, -3641.010], mean action: 0.000 [0.000, 0.000],  loss: 11710621.000000, mae: 1254.256104, mean_q: -387.063416
 4043/5000: episode: 4043, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10070.211, mean reward: -10070.211 [-10070.211, -10070.211], mean action: 0.000 [0.000, 0.000],  loss: 13731164.000000, mae: 1270.907471, mean_q: -386.525146
 4044/5000: episode: 4044, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7112.220, mean reward: -7112.220 [-7112.220, -7112.220], mean action: 0.000 [0.000, 0.000],  loss: 6329775.000000, mae: 1016.938354, mean_q: -387.916016
 4045/5000: episode: 4045, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4991.015, mean reward: -4991.015 [-4991.015, -4991.015], mean action: 0.000 [0.000, 0.000],  loss: 18400052.000000, mae: 1501.283447, mean_q: -386.344818
 4046/5000: episode: 4046, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -9791.375, mean reward: -9791.375 [-9791.375, -9791.375], mean action: 0.000 [0.000, 0.000],  loss: 17682182.000000, mae: 1522.582764, mean_q: -387.088409
 4047/5000: episode: 4047, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8870.769, mean reward: -8870.769 [-8870.769, -8870.769], mean action: 0.000 [0.000, 0.000],  loss: 15113834.000000, mae: 1350.464233, mean_q: -385.601013
 4048/5000: episode: 4048, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9919.982, mean reward: -9919.982 [-9919.982, -9919.982], mean action: 0.000 [0.000, 0.000],  loss: 17894426.000000, mae: 1562.612305, mean_q: -386.789429
 4049/5000: episode: 4049, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8194.675, mean reward: -8194.675 [-8194.675, -8194.675], mean action: 0.000 [0.000, 0.000],  loss: 8875552.000000, mae: 1134.229004, mean_q: -388.942810
 4050/5000: episode: 4050, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4029.840, mean reward: -4029.840 [-4029.840, -4029.840], mean action: 0.000 [0.000, 0.000],  loss: 17018078.000000, mae: 1502.757324, mean_q: -387.271301
 4051/5000: episode: 4051, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3388.592, mean reward: -3388.592 [-3388.592, -3388.592], mean action: 0.000 [0.000, 0.000],  loss: 16302307.000000, mae: 1341.094971, mean_q: -388.744995
 4052/5000: episode: 4052, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6972.919, mean reward: -6972.919 [-6972.919, -6972.919], mean action: 0.000 [0.000, 0.000],  loss: 12190568.000000, mae: 1183.837769, mean_q: -388.602783
 4053/5000: episode: 4053, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5963.435, mean reward: -5963.435 [-5963.435, -5963.435], mean action: 0.000 [0.000, 0.000],  loss: 12325494.000000, mae: 1276.381592, mean_q: -388.694916
 4054/5000: episode: 4054, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10210.994, mean reward: -10210.994 [-10210.994, -10210.994], mean action: 0.000 [0.000, 0.000],  loss: 10903920.000000, mae: 1177.129639, mean_q: -388.830078
 4055/5000: episode: 4055, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6798.144, mean reward: -6798.144 [-6798.144, -6798.144], mean action: 0.000 [0.000, 0.000],  loss: 14589784.000000, mae: 1324.959229, mean_q: -390.279297
 4056/5000: episode: 4056, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4201.128, mean reward: -4201.128 [-4201.128, -4201.128], mean action: 0.000 [0.000, 0.000],  loss: 7519405.000000, mae: 1017.539612, mean_q: -389.829041
 4057/5000: episode: 4057, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -12237.766, mean reward: -12237.766 [-12237.766, -12237.766], mean action: 0.000 [0.000, 0.000],  loss: 13545250.000000, mae: 1316.488770, mean_q: -388.584351
 4058/5000: episode: 4058, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3830.358, mean reward: -3830.358 [-3830.358, -3830.358], mean action: 0.000 [0.000, 0.000],  loss: 18186258.000000, mae: 1510.625732, mean_q: -388.553345
 4059/5000: episode: 4059, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5749.907, mean reward: -5749.907 [-5749.907, -5749.907], mean action: 0.000 [0.000, 0.000],  loss: 17274740.000000, mae: 1402.632812, mean_q: -390.335999
 4060/5000: episode: 4060, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -10048.687, mean reward: -10048.687 [-10048.687, -10048.687], mean action: 2.000 [2.000, 2.000],  loss: 9283973.000000, mae: 1181.104736, mean_q: -392.942749
 4061/5000: episode: 4061, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1941.610, mean reward: -1941.610 [-1941.610, -1941.610], mean action: 2.000 [2.000, 2.000],  loss: 14324238.000000, mae: 1369.609619, mean_q: -391.470001
 4062/5000: episode: 4062, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6142.035, mean reward: -6142.035 [-6142.035, -6142.035], mean action: 2.000 [2.000, 2.000],  loss: 9638534.000000, mae: 1191.096313, mean_q: -390.293152
 4063/5000: episode: 4063, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2778.442, mean reward: -2778.442 [-2778.442, -2778.442], mean action: 2.000 [2.000, 2.000],  loss: 12881453.000000, mae: 1223.009399, mean_q: -390.851685
 4064/5000: episode: 4064, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -11797.049, mean reward: -11797.049 [-11797.049, -11797.049], mean action: 0.000 [0.000, 0.000],  loss: 12762794.000000, mae: 1258.000122, mean_q: -390.822449
 4065/5000: episode: 4065, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -458.584, mean reward: -458.584 [-458.584, -458.584], mean action: 2.000 [2.000, 2.000],  loss: 10937656.000000, mae: 1250.277222, mean_q: -392.273132
 4066/5000: episode: 4066, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -507.019, mean reward: -507.019 [-507.019, -507.019], mean action: 2.000 [2.000, 2.000],  loss: 14875589.000000, mae: 1386.092285, mean_q: -392.236328
 4067/5000: episode: 4067, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1382.760, mean reward: -1382.760 [-1382.760, -1382.760], mean action: 2.000 [2.000, 2.000],  loss: 5960031.500000, mae: 925.981445, mean_q: -394.004242
 4068/5000: episode: 4068, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2085.975, mean reward: -2085.975 [-2085.975, -2085.975], mean action: 2.000 [2.000, 2.000],  loss: 14801890.000000, mae: 1380.256592, mean_q: -392.492920
 4069/5000: episode: 4069, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9341.220, mean reward: -9341.220 [-9341.220, -9341.220], mean action: 0.000 [0.000, 0.000],  loss: 12686777.000000, mae: 1301.503906, mean_q: -392.420105
 4070/5000: episode: 4070, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -112.619, mean reward: -112.619 [-112.619, -112.619], mean action: 2.000 [2.000, 2.000],  loss: 12188395.000000, mae: 1301.965942, mean_q: -391.504852
 4071/5000: episode: 4071, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -155.182, mean reward: -155.182 [-155.182, -155.182], mean action: 2.000 [2.000, 2.000],  loss: 7589414.000000, mae: 1096.191650, mean_q: -394.140930
 4072/5000: episode: 4072, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1406.146, mean reward: -1406.146 [-1406.146, -1406.146], mean action: 2.000 [2.000, 2.000],  loss: 12075904.000000, mae: 1227.057251, mean_q: -393.268463
 4073/5000: episode: 4073, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1460.287, mean reward: -1460.287 [-1460.287, -1460.287], mean action: 2.000 [2.000, 2.000],  loss: 11546293.000000, mae: 1106.549316, mean_q: -393.453979
 4074/5000: episode: 4074, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3733.814, mean reward: -3733.814 [-3733.814, -3733.814], mean action: 2.000 [2.000, 2.000],  loss: 8602642.000000, mae: 1098.311890, mean_q: -393.581451
 4075/5000: episode: 4075, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -467.634, mean reward: -467.634 [-467.634, -467.634], mean action: 2.000 [2.000, 2.000],  loss: 4973202.500000, mae: 917.557190, mean_q: -393.038452
 4076/5000: episode: 4076, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1906.864, mean reward: -1906.864 [-1906.864, -1906.864], mean action: 1.000 [1.000, 1.000],  loss: 14161157.000000, mae: 1354.969360, mean_q: -394.403320
 4077/5000: episode: 4077, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3504.858, mean reward: -3504.858 [-3504.858, -3504.858], mean action: 2.000 [2.000, 2.000],  loss: 13043759.000000, mae: 1276.515137, mean_q: -394.306763
 4078/5000: episode: 4078, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2234.056, mean reward: -2234.056 [-2234.056, -2234.056], mean action: 2.000 [2.000, 2.000],  loss: 8544057.000000, mae: 1060.144531, mean_q: -397.175659
 4079/5000: episode: 4079, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2953.279, mean reward: -2953.279 [-2953.279, -2953.279], mean action: 2.000 [2.000, 2.000],  loss: 10033941.000000, mae: 1181.163818, mean_q: -395.885254
 4080/5000: episode: 4080, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -437.604, mean reward: -437.604 [-437.604, -437.604], mean action: 2.000 [2.000, 2.000],  loss: 11313376.000000, mae: 1215.369141, mean_q: -393.237732
 4081/5000: episode: 4081, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2434.627, mean reward: -2434.627 [-2434.627, -2434.627], mean action: 2.000 [2.000, 2.000],  loss: 9107082.000000, mae: 1096.625488, mean_q: -394.565918
 4082/5000: episode: 4082, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -217.147, mean reward: -217.147 [-217.147, -217.147], mean action: 2.000 [2.000, 2.000],  loss: 11301940.000000, mae: 1216.050781, mean_q: -395.358246
 4083/5000: episode: 4083, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3969.464, mean reward: -3969.464 [-3969.464, -3969.464], mean action: 2.000 [2.000, 2.000],  loss: 17652632.000000, mae: 1444.000977, mean_q: -395.618317
 4084/5000: episode: 4084, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3804.230, mean reward: -3804.230 [-3804.230, -3804.230], mean action: 2.000 [2.000, 2.000],  loss: 10480329.000000, mae: 1204.550781, mean_q: -395.549072
 4085/5000: episode: 4085, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1678.675, mean reward: -1678.675 [-1678.675, -1678.675], mean action: 2.000 [2.000, 2.000],  loss: 10676377.000000, mae: 1153.045898, mean_q: -395.238403
 4086/5000: episode: 4086, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -324.504, mean reward: -324.504 [-324.504, -324.504], mean action: 2.000 [2.000, 2.000],  loss: 10694078.000000, mae: 1293.237915, mean_q: -397.406158
 4087/5000: episode: 4087, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -262.546, mean reward: -262.546 [-262.546, -262.546], mean action: 2.000 [2.000, 2.000],  loss: 13530816.000000, mae: 1354.854980, mean_q: -394.880005
 4088/5000: episode: 4088, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1254.046, mean reward: -1254.046 [-1254.046, -1254.046], mean action: 2.000 [2.000, 2.000],  loss: 7405189.000000, mae: 1046.938843, mean_q: -396.625763
 4089/5000: episode: 4089, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1590.988, mean reward: -1590.988 [-1590.988, -1590.988], mean action: 2.000 [2.000, 2.000],  loss: 14791386.000000, mae: 1356.754517, mean_q: -396.433136
 4090/5000: episode: 4090, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2125.860, mean reward: -2125.860 [-2125.860, -2125.860], mean action: 2.000 [2.000, 2.000],  loss: 9039046.000000, mae: 1152.429688, mean_q: -399.251709
 4091/5000: episode: 4091, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -869.888, mean reward: -869.888 [-869.888, -869.888], mean action: 2.000 [2.000, 2.000],  loss: 11483195.000000, mae: 1308.572266, mean_q: -398.200623
 4092/5000: episode: 4092, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2758.268, mean reward: -2758.268 [-2758.268, -2758.268], mean action: 2.000 [2.000, 2.000],  loss: 8447252.000000, mae: 1106.045654, mean_q: -397.786835
 4093/5000: episode: 4093, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3347.591, mean reward: -3347.591 [-3347.591, -3347.591], mean action: 0.000 [0.000, 0.000],  loss: 10469804.000000, mae: 1158.257935, mean_q: -398.872528
 4094/5000: episode: 4094, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8560.338, mean reward: -8560.338 [-8560.338, -8560.338], mean action: 0.000 [0.000, 0.000],  loss: 8905709.000000, mae: 1163.741455, mean_q: -398.819489
 4095/5000: episode: 4095, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -2783.853, mean reward: -2783.853 [-2783.853, -2783.853], mean action: 0.000 [0.000, 0.000],  loss: 7265447.000000, mae: 1008.163574, mean_q: -398.608978
 4096/5000: episode: 4096, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6802.843, mean reward: -6802.843 [-6802.843, -6802.843], mean action: 0.000 [0.000, 0.000],  loss: 10393550.000000, mae: 1217.714355, mean_q: -397.765167
 4097/5000: episode: 4097, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5698.724, mean reward: -5698.724 [-5698.724, -5698.724], mean action: 0.000 [0.000, 0.000],  loss: 10652741.000000, mae: 1251.726074, mean_q: -399.507721
 4098/5000: episode: 4098, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8549.695, mean reward: -8549.695 [-8549.695, -8549.695], mean action: 0.000 [0.000, 0.000],  loss: 7274068.000000, mae: 1051.493408, mean_q: -401.212830
 4099/5000: episode: 4099, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4874.587, mean reward: -4874.587 [-4874.587, -4874.587], mean action: 0.000 [0.000, 0.000],  loss: 15229262.000000, mae: 1427.205322, mean_q: -399.543213
 4100/5000: episode: 4100, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -12209.701, mean reward: -12209.701 [-12209.701, -12209.701], mean action: 0.000 [0.000, 0.000],  loss: 11469516.000000, mae: 1288.273682, mean_q: -398.039307
 4101/5000: episode: 4101, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5597.293, mean reward: -5597.293 [-5597.293, -5597.293], mean action: 1.000 [1.000, 1.000],  loss: 13445460.000000, mae: 1314.481567, mean_q: -399.979980
 4102/5000: episode: 4102, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -11230.253, mean reward: -11230.253 [-11230.253, -11230.253], mean action: 0.000 [0.000, 0.000],  loss: 14073509.000000, mae: 1392.765381, mean_q: -400.228088
 4103/5000: episode: 4103, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7604.173, mean reward: -7604.173 [-7604.173, -7604.173], mean action: 0.000 [0.000, 0.000],  loss: 6954146.500000, mae: 987.209778, mean_q: -399.976135
 4104/5000: episode: 4104, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7948.918, mean reward: -7948.918 [-7948.918, -7948.918], mean action: 0.000 [0.000, 0.000],  loss: 14236606.000000, mae: 1335.947998, mean_q: -399.217987
 4105/5000: episode: 4105, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7618.461, mean reward: -7618.461 [-7618.461, -7618.461], mean action: 0.000 [0.000, 0.000],  loss: 13023826.000000, mae: 1244.635498, mean_q: -400.725037
 4106/5000: episode: 4106, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -12998.309, mean reward: -12998.309 [-12998.309, -12998.309], mean action: 0.000 [0.000, 0.000],  loss: 8511088.000000, mae: 1109.892578, mean_q: -400.372955
 4107/5000: episode: 4107, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4397.435, mean reward: -4397.435 [-4397.435, -4397.435], mean action: 0.000 [0.000, 0.000],  loss: 10005648.000000, mae: 1141.484497, mean_q: -401.024628
 4108/5000: episode: 4108, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5242.381, mean reward: -5242.381 [-5242.381, -5242.381], mean action: 2.000 [2.000, 2.000],  loss: 14739542.000000, mae: 1316.610840, mean_q: -399.598877
 4109/5000: episode: 4109, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7348.176, mean reward: -7348.176 [-7348.176, -7348.176], mean action: 0.000 [0.000, 0.000],  loss: 9419279.000000, mae: 1185.035522, mean_q: -400.480408
 4110/5000: episode: 4110, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2656.252, mean reward: -2656.252 [-2656.252, -2656.252], mean action: 2.000 [2.000, 2.000],  loss: 10784040.000000, mae: 1165.409912, mean_q: -401.567108
 4111/5000: episode: 4111, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5033.007, mean reward: -5033.007 [-5033.007, -5033.007], mean action: 2.000 [2.000, 2.000],  loss: 10820293.000000, mae: 1193.602051, mean_q: -400.150940
 4112/5000: episode: 4112, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -24.421, mean reward: -24.421 [-24.421, -24.421], mean action: 2.000 [2.000, 2.000],  loss: 7886691.000000, mae: 1013.993835, mean_q: -404.757782
 4113/5000: episode: 4113, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3140.361, mean reward: -3140.361 [-3140.361, -3140.361], mean action: 2.000 [2.000, 2.000],  loss: 10862252.000000, mae: 1193.902832, mean_q: -402.158691
 4114/5000: episode: 4114, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -377.378, mean reward: -377.378 [-377.378, -377.378], mean action: 2.000 [2.000, 2.000],  loss: 9443406.000000, mae: 1145.677979, mean_q: -403.990906
 4115/5000: episode: 4115, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1504.724, mean reward: -1504.724 [-1504.724, -1504.724], mean action: 2.000 [2.000, 2.000],  loss: 11274607.000000, mae: 1133.478760, mean_q: -401.539185
 4116/5000: episode: 4116, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2936.867, mean reward: -2936.867 [-2936.867, -2936.867], mean action: 2.000 [2.000, 2.000],  loss: 6440071.500000, mae: 1041.214722, mean_q: -403.837036
 4117/5000: episode: 4117, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -11311.948, mean reward: -11311.948 [-11311.948, -11311.948], mean action: 0.000 [0.000, 0.000],  loss: 14297887.000000, mae: 1357.794434, mean_q: -402.406769
 4118/5000: episode: 4118, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2224.321, mean reward: -2224.321 [-2224.321, -2224.321], mean action: 2.000 [2.000, 2.000],  loss: 13073906.000000, mae: 1389.196777, mean_q: -402.844788
 4119/5000: episode: 4119, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -488.234, mean reward: -488.234 [-488.234, -488.234], mean action: 2.000 [2.000, 2.000],  loss: 9207160.000000, mae: 1156.124756, mean_q: -403.456543
 4120/5000: episode: 4120, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -950.392, mean reward: -950.392 [-950.392, -950.392], mean action: 2.000 [2.000, 2.000],  loss: 18386008.000000, mae: 1442.489502, mean_q: -402.364685
 4121/5000: episode: 4121, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -688.868, mean reward: -688.868 [-688.868, -688.868], mean action: 2.000 [2.000, 2.000],  loss: 10367119.000000, mae: 1157.335693, mean_q: -403.259979
 4122/5000: episode: 4122, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4003.003, mean reward: -4003.003 [-4003.003, -4003.003], mean action: 2.000 [2.000, 2.000],  loss: 11111856.000000, mae: 1221.249268, mean_q: -404.353638
 4123/5000: episode: 4123, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2382.562, mean reward: -2382.562 [-2382.562, -2382.562], mean action: 2.000 [2.000, 2.000],  loss: 10134016.000000, mae: 1109.080566, mean_q: -403.067810
 4124/5000: episode: 4124, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5158.802, mean reward: -5158.802 [-5158.802, -5158.802], mean action: 2.000 [2.000, 2.000],  loss: 13203224.000000, mae: 1268.456787, mean_q: -403.421295
 4125/5000: episode: 4125, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3601.660, mean reward: -3601.660 [-3601.660, -3601.660], mean action: 2.000 [2.000, 2.000],  loss: 12499249.000000, mae: 1205.755859, mean_q: -404.162598
 4126/5000: episode: 4126, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4881.850, mean reward: -4881.850 [-4881.850, -4881.850], mean action: 2.000 [2.000, 2.000],  loss: 11471183.000000, mae: 1253.457275, mean_q: -405.719910
 4127/5000: episode: 4127, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1192.233, mean reward: -1192.233 [-1192.233, -1192.233], mean action: 2.000 [2.000, 2.000],  loss: 11007026.000000, mae: 1287.153076, mean_q: -404.397705
 4128/5000: episode: 4128, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1002.047, mean reward: -1002.047 [-1002.047, -1002.047], mean action: 2.000 [2.000, 2.000],  loss: 5302424.500000, mae: 940.969849, mean_q: -405.320953
 4129/5000: episode: 4129, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -225.847, mean reward: -225.847 [-225.847, -225.847], mean action: 2.000 [2.000, 2.000],  loss: 14609810.000000, mae: 1322.406250, mean_q: -404.916504
 4130/5000: episode: 4130, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1931.794, mean reward: -1931.794 [-1931.794, -1931.794], mean action: 2.000 [2.000, 2.000],  loss: 10274358.000000, mae: 1261.192627, mean_q: -405.256653
 4131/5000: episode: 4131, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6980.450, mean reward: -6980.450 [-6980.450, -6980.450], mean action: 2.000 [2.000, 2.000],  loss: 13044875.000000, mae: 1255.430176, mean_q: -405.934601
 4132/5000: episode: 4132, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -7.563, mean reward: -7.563 [-7.563, -7.563], mean action: 2.000 [2.000, 2.000],  loss: 7773178.500000, mae: 1052.144531, mean_q: -405.973541
 4133/5000: episode: 4133, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1474.482, mean reward: -1474.482 [-1474.482, -1474.482], mean action: 2.000 [2.000, 2.000],  loss: 9711482.000000, mae: 1210.428345, mean_q: -406.501953
 4134/5000: episode: 4134, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -8896.527, mean reward: -8896.527 [-8896.527, -8896.527], mean action: 2.000 [2.000, 2.000],  loss: 13815238.000000, mae: 1339.899170, mean_q: -405.029266
 4135/5000: episode: 4135, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3868.636, mean reward: -3868.636 [-3868.636, -3868.636], mean action: 2.000 [2.000, 2.000],  loss: 8895483.000000, mae: 1111.130859, mean_q: -406.965698
 4136/5000: episode: 4136, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1640.175, mean reward: -1640.175 [-1640.175, -1640.175], mean action: 2.000 [2.000, 2.000],  loss: 12557302.000000, mae: 1271.542725, mean_q: -406.621918
 4137/5000: episode: 4137, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -386.632, mean reward: -386.632 [-386.632, -386.632], mean action: 2.000 [2.000, 2.000],  loss: 20192686.000000, mae: 1600.801758, mean_q: -406.724670
 4138/5000: episode: 4138, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2484.535, mean reward: -2484.535 [-2484.535, -2484.535], mean action: 2.000 [2.000, 2.000],  loss: 12084698.000000, mae: 1352.321045, mean_q: -406.669983
 4139/5000: episode: 4139, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1942.904, mean reward: -1942.904 [-1942.904, -1942.904], mean action: 2.000 [2.000, 2.000],  loss: 10253094.000000, mae: 1169.431396, mean_q: -407.142365
 4140/5000: episode: 4140, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -934.916, mean reward: -934.916 [-934.916, -934.916], mean action: 2.000 [2.000, 2.000],  loss: 20461448.000000, mae: 1639.543213, mean_q: -405.224640
 4141/5000: episode: 4141, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3315.817, mean reward: -3315.817 [-3315.817, -3315.817], mean action: 2.000 [2.000, 2.000],  loss: 7247222.500000, mae: 1005.489380, mean_q: -409.606445
 4142/5000: episode: 4142, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -392.501, mean reward: -392.501 [-392.501, -392.501], mean action: 2.000 [2.000, 2.000],  loss: 8956669.000000, mae: 1088.220703, mean_q: -408.666687
 4143/5000: episode: 4143, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -776.496, mean reward: -776.496 [-776.496, -776.496], mean action: 2.000 [2.000, 2.000],  loss: 7385711.500000, mae: 1016.476624, mean_q: -408.988037
 4144/5000: episode: 4144, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3593.100, mean reward: -3593.100 [-3593.100, -3593.100], mean action: 2.000 [2.000, 2.000],  loss: 13019374.000000, mae: 1203.808350, mean_q: -409.008972
 4145/5000: episode: 4145, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1330.089, mean reward: -1330.089 [-1330.089, -1330.089], mean action: 2.000 [2.000, 2.000],  loss: 10574667.000000, mae: 1207.579590, mean_q: -408.104675
 4146/5000: episode: 4146, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2453.604, mean reward: -2453.604 [-2453.604, -2453.604], mean action: 2.000 [2.000, 2.000],  loss: 12285856.000000, mae: 1256.518799, mean_q: -409.208771
 4147/5000: episode: 4147, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4586.769, mean reward: -4586.769 [-4586.769, -4586.769], mean action: 2.000 [2.000, 2.000],  loss: 14796147.000000, mae: 1353.677002, mean_q: -409.026978
 4148/5000: episode: 4148, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -451.366, mean reward: -451.366 [-451.366, -451.366], mean action: 2.000 [2.000, 2.000],  loss: 13559684.000000, mae: 1340.982666, mean_q: -407.872314
 4149/5000: episode: 4149, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1054.397, mean reward: -1054.397 [-1054.397, -1054.397], mean action: 2.000 [2.000, 2.000],  loss: 6764731.500000, mae: 1033.182373, mean_q: -409.992310
 4150/5000: episode: 4150, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3211.701, mean reward: -3211.701 [-3211.701, -3211.701], mean action: 2.000 [2.000, 2.000],  loss: 9028235.000000, mae: 1145.066162, mean_q: -409.729492
 4151/5000: episode: 4151, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5528.108, mean reward: -5528.108 [-5528.108, -5528.108], mean action: 2.000 [2.000, 2.000],  loss: 16553715.000000, mae: 1508.147949, mean_q: -408.426331
 4152/5000: episode: 4152, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1872.749, mean reward: -1872.749 [-1872.749, -1872.749], mean action: 2.000 [2.000, 2.000],  loss: 11717275.000000, mae: 1186.811035, mean_q: -410.490662
 4153/5000: episode: 4153, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1853.977, mean reward: -1853.977 [-1853.977, -1853.977], mean action: 2.000 [2.000, 2.000],  loss: 11387780.000000, mae: 1244.365967, mean_q: -410.496826
 4154/5000: episode: 4154, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1451.717, mean reward: -1451.717 [-1451.717, -1451.717], mean action: 3.000 [3.000, 3.000],  loss: 11798760.000000, mae: 1218.186890, mean_q: -409.566833
 4155/5000: episode: 4155, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -206.344, mean reward: -206.344 [-206.344, -206.344], mean action: 2.000 [2.000, 2.000],  loss: 7202835.000000, mae: 965.108276, mean_q: -412.301575
 4156/5000: episode: 4156, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -77.368, mean reward: -77.368 [-77.368, -77.368], mean action: 2.000 [2.000, 2.000],  loss: 13378424.000000, mae: 1365.545898, mean_q: -411.062805
 4157/5000: episode: 4157, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1009.999, mean reward: -1009.999 [-1009.999, -1009.999], mean action: 2.000 [2.000, 2.000],  loss: 9210516.000000, mae: 1120.682861, mean_q: -411.398804
 4158/5000: episode: 4158, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8024.500, mean reward: -8024.500 [-8024.500, -8024.500], mean action: 2.000 [2.000, 2.000],  loss: 15042563.000000, mae: 1383.178711, mean_q: -411.031738
 4159/5000: episode: 4159, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -220.986, mean reward: -220.986 [-220.986, -220.986], mean action: 1.000 [1.000, 1.000],  loss: 12597220.000000, mae: 1309.926147, mean_q: -411.798767
 4160/5000: episode: 4160, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2233.774, mean reward: -2233.774 [-2233.774, -2233.774], mean action: 1.000 [1.000, 1.000],  loss: 7859763.000000, mae: 1141.812256, mean_q: -411.979156
 4161/5000: episode: 4161, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2284.680, mean reward: -2284.680 [-2284.680, -2284.680], mean action: 2.000 [2.000, 2.000],  loss: 8893406.000000, mae: 1172.635864, mean_q: -410.851349
 4162/5000: episode: 4162, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6197.800, mean reward: -6197.800 [-6197.800, -6197.800], mean action: 2.000 [2.000, 2.000],  loss: 13801546.000000, mae: 1390.272461, mean_q: -411.172485
 4163/5000: episode: 4163, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2555.126, mean reward: -2555.126 [-2555.126, -2555.126], mean action: 2.000 [2.000, 2.000],  loss: 9129732.000000, mae: 1107.781250, mean_q: -412.028748
 4164/5000: episode: 4164, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3749.322, mean reward: -3749.322 [-3749.322, -3749.322], mean action: 2.000 [2.000, 2.000],  loss: 10890550.000000, mae: 1249.218506, mean_q: -411.597504
 4165/5000: episode: 4165, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -8431.024, mean reward: -8431.024 [-8431.024, -8431.024], mean action: 2.000 [2.000, 2.000],  loss: 5680668.000000, mae: 943.836609, mean_q: -411.524811
 4166/5000: episode: 4166, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1672.303, mean reward: -1672.303 [-1672.303, -1672.303], mean action: 2.000 [2.000, 2.000],  loss: 11420167.000000, mae: 1191.668457, mean_q: -412.122955
 4167/5000: episode: 4167, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -2542.278, mean reward: -2542.278 [-2542.278, -2542.278], mean action: 2.000 [2.000, 2.000],  loss: 14835519.000000, mae: 1361.260010, mean_q: -412.396606
 4168/5000: episode: 4168, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4981.700, mean reward: -4981.700 [-4981.700, -4981.700], mean action: 2.000 [2.000, 2.000],  loss: 12928189.000000, mae: 1295.007812, mean_q: -411.969849
 4169/5000: episode: 4169, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2910.709, mean reward: -2910.709 [-2910.709, -2910.709], mean action: 2.000 [2.000, 2.000],  loss: 11881934.000000, mae: 1260.941772, mean_q: -414.107971
 4170/5000: episode: 4170, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2871.868, mean reward: -2871.868 [-2871.868, -2871.868], mean action: 2.000 [2.000, 2.000],  loss: 6639329.500000, mae: 969.045105, mean_q: -414.336639
 4171/5000: episode: 4171, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1860.779, mean reward: -1860.779 [-1860.779, -1860.779], mean action: 2.000 [2.000, 2.000],  loss: 15440820.000000, mae: 1372.878662, mean_q: -414.510956
 4172/5000: episode: 4172, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5141.385, mean reward: -5141.385 [-5141.385, -5141.385], mean action: 2.000 [2.000, 2.000],  loss: 12864969.000000, mae: 1322.444702, mean_q: -413.542419
 4173/5000: episode: 4173, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2957.529, mean reward: -2957.529 [-2957.529, -2957.529], mean action: 2.000 [2.000, 2.000],  loss: 10652957.000000, mae: 1171.628540, mean_q: -414.166443
 4174/5000: episode: 4174, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -259.007, mean reward: -259.007 [-259.007, -259.007], mean action: 2.000 [2.000, 2.000],  loss: 4140528.500000, mae: 850.052490, mean_q: -415.912720
 4175/5000: episode: 4175, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -60.347, mean reward: -60.347 [-60.347, -60.347], mean action: 2.000 [2.000, 2.000],  loss: 9616923.000000, mae: 1161.744751, mean_q: -413.572906
 4176/5000: episode: 4176, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1446.902, mean reward: -1446.902 [-1446.902, -1446.902], mean action: 2.000 [2.000, 2.000],  loss: 13410213.000000, mae: 1305.802124, mean_q: -412.693298
 4177/5000: episode: 4177, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1390.398, mean reward: -1390.398 [-1390.398, -1390.398], mean action: 2.000 [2.000, 2.000],  loss: 12271579.000000, mae: 1323.999268, mean_q: -413.764618
 4178/5000: episode: 4178, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1394.905, mean reward: -1394.905 [-1394.905, -1394.905], mean action: 2.000 [2.000, 2.000],  loss: 10162559.000000, mae: 1159.006348, mean_q: -415.092072
 4179/5000: episode: 4179, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3015.257, mean reward: -3015.257 [-3015.257, -3015.257], mean action: 3.000 [3.000, 3.000],  loss: 14502003.000000, mae: 1354.358398, mean_q: -415.216614
 4180/5000: episode: 4180, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3223.087, mean reward: -3223.087 [-3223.087, -3223.087], mean action: 2.000 [2.000, 2.000],  loss: 7271396.000000, mae: 1009.886780, mean_q: -416.431305
 4181/5000: episode: 4181, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6706.130, mean reward: -6706.130 [-6706.130, -6706.130], mean action: 2.000 [2.000, 2.000],  loss: 17276604.000000, mae: 1283.781494, mean_q: -415.754547
 4182/5000: episode: 4182, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3666.347, mean reward: -3666.347 [-3666.347, -3666.347], mean action: 2.000 [2.000, 2.000],  loss: 10672998.000000, mae: 1145.532715, mean_q: -417.287964
 4183/5000: episode: 4183, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -8577.091, mean reward: -8577.091 [-8577.091, -8577.091], mean action: 2.000 [2.000, 2.000],  loss: 10051938.000000, mae: 1210.534790, mean_q: -415.414948
 4184/5000: episode: 4184, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2822.492, mean reward: -2822.492 [-2822.492, -2822.492], mean action: 2.000 [2.000, 2.000],  loss: 9673919.000000, mae: 1087.055176, mean_q: -417.150818
 4185/5000: episode: 4185, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2760.027, mean reward: -2760.027 [-2760.027, -2760.027], mean action: 2.000 [2.000, 2.000],  loss: 7197237.000000, mae: 1045.556396, mean_q: -417.514038
 4186/5000: episode: 4186, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1250.325, mean reward: -1250.325 [-1250.325, -1250.325], mean action: 2.000 [2.000, 2.000],  loss: 10412466.000000, mae: 1197.966675, mean_q: -417.014221
 4187/5000: episode: 4187, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2480.527, mean reward: -2480.527 [-2480.527, -2480.527], mean action: 2.000 [2.000, 2.000],  loss: 15231660.000000, mae: 1375.927734, mean_q: -416.225647
 4188/5000: episode: 4188, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2010.113, mean reward: -2010.113 [-2010.113, -2010.113], mean action: 2.000 [2.000, 2.000],  loss: 7530747.000000, mae: 1037.017090, mean_q: -416.862976
 4189/5000: episode: 4189, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1948.995, mean reward: -1948.995 [-1948.995, -1948.995], mean action: 2.000 [2.000, 2.000],  loss: 11140639.000000, mae: 1240.430420, mean_q: -416.632385
 4190/5000: episode: 4190, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2311.312, mean reward: -2311.312 [-2311.312, -2311.312], mean action: 2.000 [2.000, 2.000],  loss: 8543142.000000, mae: 1020.318848, mean_q: -417.947021
 4191/5000: episode: 4191, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1830.371, mean reward: -1830.371 [-1830.371, -1830.371], mean action: 2.000 [2.000, 2.000],  loss: 10038846.000000, mae: 1213.042236, mean_q: -417.390381
 4192/5000: episode: 4192, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6369.637, mean reward: -6369.637 [-6369.637, -6369.637], mean action: 2.000 [2.000, 2.000],  loss: 11760806.000000, mae: 1196.863770, mean_q: -418.231842
 4193/5000: episode: 4193, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9533.891, mean reward: -9533.891 [-9533.891, -9533.891], mean action: 2.000 [2.000, 2.000],  loss: 12191864.000000, mae: 1239.985840, mean_q: -417.835083
 4194/5000: episode: 4194, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2484.765, mean reward: -2484.765 [-2484.765, -2484.765], mean action: 2.000 [2.000, 2.000],  loss: 11079184.000000, mae: 1163.772461, mean_q: -417.869385
 4195/5000: episode: 4195, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2199.796, mean reward: -2199.796 [-2199.796, -2199.796], mean action: 2.000 [2.000, 2.000],  loss: 12403280.000000, mae: 1351.442505, mean_q: -418.735107
 4196/5000: episode: 4196, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -315.771, mean reward: -315.771 [-315.771, -315.771], mean action: 2.000 [2.000, 2.000],  loss: 11035698.000000, mae: 1203.301147, mean_q: -419.424408
 4197/5000: episode: 4197, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6031.661, mean reward: -6031.661 [-6031.661, -6031.661], mean action: 2.000 [2.000, 2.000],  loss: 8931530.000000, mae: 1116.975586, mean_q: -419.589905
 4198/5000: episode: 4198, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2647.121, mean reward: -2647.121 [-2647.121, -2647.121], mean action: 2.000 [2.000, 2.000],  loss: 11838325.000000, mae: 1234.915771, mean_q: -416.839722
 4199/5000: episode: 4199, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -59.531, mean reward: -59.531 [-59.531, -59.531], mean action: 2.000 [2.000, 2.000],  loss: 9845242.000000, mae: 1177.685547, mean_q: -419.540833
 4200/5000: episode: 4200, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3974.264, mean reward: -3974.264 [-3974.264, -3974.264], mean action: 2.000 [2.000, 2.000],  loss: 10329340.000000, mae: 1203.585205, mean_q: -420.613708
 4201/5000: episode: 4201, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1766.635, mean reward: -1766.635 [-1766.635, -1766.635], mean action: 2.000 [2.000, 2.000],  loss: 9940970.000000, mae: 1187.288086, mean_q: -419.193939
 4202/5000: episode: 4202, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -3590.790, mean reward: -3590.790 [-3590.790, -3590.790], mean action: 2.000 [2.000, 2.000],  loss: 12829318.000000, mae: 1110.405029, mean_q: -421.088409
 4203/5000: episode: 4203, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -11172.130, mean reward: -11172.130 [-11172.130, -11172.130], mean action: 0.000 [0.000, 0.000],  loss: 12605024.000000, mae: 1254.282227, mean_q: -419.928101
 4204/5000: episode: 4204, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3426.426, mean reward: -3426.426 [-3426.426, -3426.426], mean action: 2.000 [2.000, 2.000],  loss: 10152720.000000, mae: 1264.830566, mean_q: -421.174805
 4205/5000: episode: 4205, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1307.154, mean reward: -1307.154 [-1307.154, -1307.154], mean action: 2.000 [2.000, 2.000],  loss: 16733352.000000, mae: 1512.117188, mean_q: -420.667053
 4206/5000: episode: 4206, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4876.637, mean reward: -4876.637 [-4876.637, -4876.637], mean action: 2.000 [2.000, 2.000],  loss: 11643560.000000, mae: 1242.663086, mean_q: -420.207916
 4207/5000: episode: 4207, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -269.540, mean reward: -269.540 [-269.540, -269.540], mean action: 2.000 [2.000, 2.000],  loss: 11435468.000000, mae: 1197.749756, mean_q: -420.345490
 4208/5000: episode: 4208, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3397.223, mean reward: -3397.223 [-3397.223, -3397.223], mean action: 2.000 [2.000, 2.000],  loss: 6777011.500000, mae: 1005.953796, mean_q: -420.810822
 4209/5000: episode: 4209, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2914.283, mean reward: -2914.283 [-2914.283, -2914.283], mean action: 2.000 [2.000, 2.000],  loss: 9753784.000000, mae: 1147.909668, mean_q: -422.511963
 4210/5000: episode: 4210, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3135.944, mean reward: -3135.944 [-3135.944, -3135.944], mean action: 2.000 [2.000, 2.000],  loss: 9297492.000000, mae: 1138.979248, mean_q: -420.847168
 4211/5000: episode: 4211, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -214.611, mean reward: -214.611 [-214.611, -214.611], mean action: 2.000 [2.000, 2.000],  loss: 11633908.000000, mae: 1308.651245, mean_q: -421.942932
 4212/5000: episode: 4212, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2432.407, mean reward: -2432.407 [-2432.407, -2432.407], mean action: 2.000 [2.000, 2.000],  loss: 10856706.000000, mae: 1296.535400, mean_q: -421.989227
 4213/5000: episode: 4213, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -148.649, mean reward: -148.649 [-148.649, -148.649], mean action: 2.000 [2.000, 2.000],  loss: 10358292.000000, mae: 1179.017578, mean_q: -422.264709
 4214/5000: episode: 4214, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6104.458, mean reward: -6104.458 [-6104.458, -6104.458], mean action: 2.000 [2.000, 2.000],  loss: 9256254.000000, mae: 1170.902832, mean_q: -422.392212
 4215/5000: episode: 4215, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2338.236, mean reward: -2338.236 [-2338.236, -2338.236], mean action: 2.000 [2.000, 2.000],  loss: 19918652.000000, mae: 1603.814209, mean_q: -421.660156
 4216/5000: episode: 4216, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -398.620, mean reward: -398.620 [-398.620, -398.620], mean action: 2.000 [2.000, 2.000],  loss: 7691105.000000, mae: 1093.097900, mean_q: -423.263763
 4217/5000: episode: 4217, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2505.901, mean reward: -2505.901 [-2505.901, -2505.901], mean action: 2.000 [2.000, 2.000],  loss: 7362010.500000, mae: 984.724243, mean_q: -426.086182
 4218/5000: episode: 4218, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3874.777, mean reward: -3874.777 [-3874.777, -3874.777], mean action: 2.000 [2.000, 2.000],  loss: 8475155.000000, mae: 1121.348633, mean_q: -422.890594
 4219/5000: episode: 4219, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1465.173, mean reward: -1465.173 [-1465.173, -1465.173], mean action: 2.000 [2.000, 2.000],  loss: 6763718.000000, mae: 995.511597, mean_q: -423.730591
 4220/5000: episode: 4220, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -564.945, mean reward: -564.945 [-564.945, -564.945], mean action: 2.000 [2.000, 2.000],  loss: 10945738.000000, mae: 1253.211914, mean_q: -423.639496
 4221/5000: episode: 4221, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1683.496, mean reward: -1683.496 [-1683.496, -1683.496], mean action: 2.000 [2.000, 2.000],  loss: 11692235.000000, mae: 1241.244629, mean_q: -424.756287
 4222/5000: episode: 4222, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1741.778, mean reward: -1741.778 [-1741.778, -1741.778], mean action: 2.000 [2.000, 2.000],  loss: 6859850.000000, mae: 1076.312988, mean_q: -425.509216
 4223/5000: episode: 4223, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -363.493, mean reward: -363.493 [-363.493, -363.493], mean action: 2.000 [2.000, 2.000],  loss: 10186694.000000, mae: 1134.579712, mean_q: -425.797150
 4224/5000: episode: 4224, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1780.592, mean reward: -1780.592 [-1780.592, -1780.592], mean action: 2.000 [2.000, 2.000],  loss: 11718480.000000, mae: 1247.300293, mean_q: -426.004364
 4225/5000: episode: 4225, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2368.969, mean reward: -2368.969 [-2368.969, -2368.969], mean action: 2.000 [2.000, 2.000],  loss: 11639550.000000, mae: 1269.810059, mean_q: -425.716309
 4226/5000: episode: 4226, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1448.455, mean reward: -1448.455 [-1448.455, -1448.455], mean action: 2.000 [2.000, 2.000],  loss: 14446025.000000, mae: 1382.857300, mean_q: -425.171417
 4227/5000: episode: 4227, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2295.215, mean reward: -2295.215 [-2295.215, -2295.215], mean action: 2.000 [2.000, 2.000],  loss: 8711593.000000, mae: 1115.195557, mean_q: -425.796387
 4228/5000: episode: 4228, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -832.282, mean reward: -832.282 [-832.282, -832.282], mean action: 2.000 [2.000, 2.000],  loss: 16412005.000000, mae: 1375.726807, mean_q: -426.205383
 4229/5000: episode: 4229, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2204.671, mean reward: -2204.671 [-2204.671, -2204.671], mean action: 2.000 [2.000, 2.000],  loss: 14319509.000000, mae: 1356.388916, mean_q: -427.350006
 4230/5000: episode: 4230, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4739.954, mean reward: -4739.954 [-4739.954, -4739.954], mean action: 2.000 [2.000, 2.000],  loss: 11473854.000000, mae: 1224.064941, mean_q: -424.098816
 4231/5000: episode: 4231, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1079.178, mean reward: -1079.178 [-1079.178, -1079.178], mean action: 2.000 [2.000, 2.000],  loss: 8075405.000000, mae: 1092.498291, mean_q: -426.274841
 4232/5000: episode: 4232, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6291.320, mean reward: -6291.320 [-6291.320, -6291.320], mean action: 2.000 [2.000, 2.000],  loss: 13563995.000000, mae: 1288.259766, mean_q: -427.037292
 4233/5000: episode: 4233, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -10295.143, mean reward: -10295.143 [-10295.143, -10295.143], mean action: 0.000 [0.000, 0.000],  loss: 15011444.000000, mae: 1450.926025, mean_q: -426.680969
 4234/5000: episode: 4234, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1277.070, mean reward: -1277.070 [-1277.070, -1277.070], mean action: 2.000 [2.000, 2.000],  loss: 9340274.000000, mae: 1150.939209, mean_q: -426.281738
 4235/5000: episode: 4235, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -519.207, mean reward: -519.207 [-519.207, -519.207], mean action: 2.000 [2.000, 2.000],  loss: 5711668.000000, mae: 1016.710083, mean_q: -428.871399
 4236/5000: episode: 4236, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -894.933, mean reward: -894.933 [-894.933, -894.933], mean action: 2.000 [2.000, 2.000],  loss: 10840416.000000, mae: 1155.485352, mean_q: -427.321533
 4237/5000: episode: 4237, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1220.948, mean reward: -1220.948 [-1220.948, -1220.948], mean action: 2.000 [2.000, 2.000],  loss: 13602208.000000, mae: 1285.405762, mean_q: -427.889954
 4238/5000: episode: 4238, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2474.001, mean reward: -2474.001 [-2474.001, -2474.001], mean action: 2.000 [2.000, 2.000],  loss: 10642175.000000, mae: 1228.989502, mean_q: -426.895508
 4239/5000: episode: 4239, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -869.062, mean reward: -869.062 [-869.062, -869.062], mean action: 2.000 [2.000, 2.000],  loss: 7152405.000000, mae: 1048.719482, mean_q: -429.368500
 4240/5000: episode: 4240, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -314.600, mean reward: -314.600 [-314.600, -314.600], mean action: 2.000 [2.000, 2.000],  loss: 15180082.000000, mae: 1363.918457, mean_q: -428.324646
 4241/5000: episode: 4241, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3866.529, mean reward: -3866.529 [-3866.529, -3866.529], mean action: 2.000 [2.000, 2.000],  loss: 14344991.000000, mae: 1366.092896, mean_q: -428.828491
 4242/5000: episode: 4242, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1898.964, mean reward: -1898.964 [-1898.964, -1898.964], mean action: 2.000 [2.000, 2.000],  loss: 10652572.000000, mae: 1153.474365, mean_q: -429.750000
 4243/5000: episode: 4243, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4929.264, mean reward: -4929.264 [-4929.264, -4929.264], mean action: 2.000 [2.000, 2.000],  loss: 11465255.000000, mae: 1187.243408, mean_q: -430.551483
 4244/5000: episode: 4244, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4209.839, mean reward: -4209.839 [-4209.839, -4209.839], mean action: 2.000 [2.000, 2.000],  loss: 11202308.000000, mae: 1306.790283, mean_q: -429.343536
 4245/5000: episode: 4245, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1711.153, mean reward: -1711.153 [-1711.153, -1711.153], mean action: 2.000 [2.000, 2.000],  loss: 10382632.000000, mae: 1232.325806, mean_q: -428.673492
 4246/5000: episode: 4246, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -13.936, mean reward: -13.936 [-13.936, -13.936], mean action: 2.000 [2.000, 2.000],  loss: 11127430.000000, mae: 1235.364258, mean_q: -430.985016
 4247/5000: episode: 4247, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1699.411, mean reward: -1699.411 [-1699.411, -1699.411], mean action: 2.000 [2.000, 2.000],  loss: 9981284.000000, mae: 1165.096436, mean_q: -432.488312
 4248/5000: episode: 4248, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -903.697, mean reward: -903.697 [-903.697, -903.697], mean action: 2.000 [2.000, 2.000],  loss: 8324261.500000, mae: 1133.208618, mean_q: -428.988251
 4249/5000: episode: 4249, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2324.367, mean reward: -2324.367 [-2324.367, -2324.367], mean action: 2.000 [2.000, 2.000],  loss: 9206251.000000, mae: 1182.100342, mean_q: -429.647644
 4250/5000: episode: 4250, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5848.267, mean reward: -5848.267 [-5848.267, -5848.267], mean action: 2.000 [2.000, 2.000],  loss: 12406406.000000, mae: 1317.304077, mean_q: -431.770660
 4251/5000: episode: 4251, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2885.486, mean reward: -2885.486 [-2885.486, -2885.486], mean action: 2.000 [2.000, 2.000],  loss: 9035156.000000, mae: 1143.322021, mean_q: -431.641785
 4252/5000: episode: 4252, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3751.228, mean reward: -3751.228 [-3751.228, -3751.228], mean action: 2.000 [2.000, 2.000],  loss: 8435053.000000, mae: 1164.523193, mean_q: -432.081726
 4253/5000: episode: 4253, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2531.441, mean reward: -2531.441 [-2531.441, -2531.441], mean action: 2.000 [2.000, 2.000],  loss: 11448019.000000, mae: 1241.962646, mean_q: -431.833099
 4254/5000: episode: 4254, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2886.201, mean reward: -2886.201 [-2886.201, -2886.201], mean action: 2.000 [2.000, 2.000],  loss: 10349472.000000, mae: 1195.739136, mean_q: -430.187012
 4255/5000: episode: 4255, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -1961.248, mean reward: -1961.248 [-1961.248, -1961.248], mean action: 2.000 [2.000, 2.000],  loss: 12032374.000000, mae: 1236.776367, mean_q: -431.265228
 4256/5000: episode: 4256, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2999.263, mean reward: -2999.263 [-2999.263, -2999.263], mean action: 2.000 [2.000, 2.000],  loss: 6818185.000000, mae: 1026.619385, mean_q: -432.170227
 4257/5000: episode: 4257, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4165.340, mean reward: -4165.340 [-4165.340, -4165.340], mean action: 2.000 [2.000, 2.000],  loss: 8043574.500000, mae: 1094.826538, mean_q: -432.269104
 4258/5000: episode: 4258, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2421.996, mean reward: -2421.996 [-2421.996, -2421.996], mean action: 2.000 [2.000, 2.000],  loss: 8818125.000000, mae: 1133.773926, mean_q: -432.511169
 4259/5000: episode: 4259, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3567.867, mean reward: -3567.867 [-3567.867, -3567.867], mean action: 3.000 [3.000, 3.000],  loss: 8740160.000000, mae: 1134.143799, mean_q: -432.840027
 4260/5000: episode: 4260, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -8765.802, mean reward: -8765.802 [-8765.802, -8765.802], mean action: 0.000 [0.000, 0.000],  loss: 13128208.000000, mae: 1372.749390, mean_q: -433.033264
 4261/5000: episode: 4261, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4662.471, mean reward: -4662.471 [-4662.471, -4662.471], mean action: 2.000 [2.000, 2.000],  loss: 11013049.000000, mae: 1237.545166, mean_q: -432.543732
 4262/5000: episode: 4262, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -336.577, mean reward: -336.577 [-336.577, -336.577], mean action: 2.000 [2.000, 2.000],  loss: 17820116.000000, mae: 1445.550537, mean_q: -431.236389
 4263/5000: episode: 4263, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1874.686, mean reward: -1874.686 [-1874.686, -1874.686], mean action: 2.000 [2.000, 2.000],  loss: 11533680.000000, mae: 1223.840576, mean_q: -432.336426
 4264/5000: episode: 4264, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -568.040, mean reward: -568.040 [-568.040, -568.040], mean action: 2.000 [2.000, 2.000],  loss: 16156950.000000, mae: 1451.886353, mean_q: -431.731415
 4265/5000: episode: 4265, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9248.854, mean reward: -9248.854 [-9248.854, -9248.854], mean action: 2.000 [2.000, 2.000],  loss: 11551778.000000, mae: 1296.909912, mean_q: -433.143250
 4266/5000: episode: 4266, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -361.171, mean reward: -361.171 [-361.171, -361.171], mean action: 2.000 [2.000, 2.000],  loss: 12352130.000000, mae: 1331.175415, mean_q: -433.397339
 4267/5000: episode: 4267, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -92.907, mean reward: -92.907 [-92.907, -92.907], mean action: 2.000 [2.000, 2.000],  loss: 8104306.000000, mae: 1186.992065, mean_q: -434.403748
 4268/5000: episode: 4268, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -430.511, mean reward: -430.511 [-430.511, -430.511], mean action: 2.000 [2.000, 2.000],  loss: 8795521.000000, mae: 1113.278076, mean_q: -433.893005
 4269/5000: episode: 4269, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -687.143, mean reward: -687.143 [-687.143, -687.143], mean action: 3.000 [3.000, 3.000],  loss: 10535240.000000, mae: 1279.461182, mean_q: -435.559509
 4270/5000: episode: 4270, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8049.516, mean reward: -8049.516 [-8049.516, -8049.516], mean action: 3.000 [3.000, 3.000],  loss: 18240850.000000, mae: 1457.153809, mean_q: -433.263245
 4271/5000: episode: 4271, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -903.167, mean reward: -903.167 [-903.167, -903.167], mean action: 3.000 [3.000, 3.000],  loss: 9663134.000000, mae: 1181.806152, mean_q: -434.781830
 4272/5000: episode: 4272, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3216.084, mean reward: -3216.084 [-3216.084, -3216.084], mean action: 3.000 [3.000, 3.000],  loss: 13743329.000000, mae: 1433.001709, mean_q: -435.046753
 4273/5000: episode: 4273, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3021.896, mean reward: -3021.896 [-3021.896, -3021.896], mean action: 3.000 [3.000, 3.000],  loss: 9558340.000000, mae: 1209.802979, mean_q: -438.871948
 4274/5000: episode: 4274, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5601.439, mean reward: -5601.439 [-5601.439, -5601.439], mean action: 3.000 [3.000, 3.000],  loss: 11887857.000000, mae: 1223.071045, mean_q: -435.392792
 4275/5000: episode: 4275, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1715.640, mean reward: -1715.640 [-1715.640, -1715.640], mean action: 3.000 [3.000, 3.000],  loss: 7591784.000000, mae: 1082.609497, mean_q: -436.163361
 4276/5000: episode: 4276, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5567.635, mean reward: -5567.635 [-5567.635, -5567.635], mean action: 3.000 [3.000, 3.000],  loss: 7059449.000000, mae: 995.933350, mean_q: -437.802979
 4277/5000: episode: 4277, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3489.732, mean reward: -3489.732 [-3489.732, -3489.732], mean action: 3.000 [3.000, 3.000],  loss: 8640813.000000, mae: 1079.324829, mean_q: -439.221375
 4278/5000: episode: 4278, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3477.968, mean reward: -3477.968 [-3477.968, -3477.968], mean action: 3.000 [3.000, 3.000],  loss: 12510938.000000, mae: 1383.013550, mean_q: -436.179291
 4279/5000: episode: 4279, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3222.095, mean reward: -3222.095 [-3222.095, -3222.095], mean action: 3.000 [3.000, 3.000],  loss: 9413048.000000, mae: 1235.036255, mean_q: -436.500000
 4280/5000: episode: 4280, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5116.400, mean reward: -5116.400 [-5116.400, -5116.400], mean action: 3.000 [3.000, 3.000],  loss: 12024053.000000, mae: 1160.068115, mean_q: -438.108887
 4281/5000: episode: 4281, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5734.028, mean reward: -5734.028 [-5734.028, -5734.028], mean action: 3.000 [3.000, 3.000],  loss: 12455032.000000, mae: 1279.382202, mean_q: -437.166809
 4282/5000: episode: 4282, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4674.168, mean reward: -4674.168 [-4674.168, -4674.168], mean action: 3.000 [3.000, 3.000],  loss: 8287163.000000, mae: 1099.739380, mean_q: -437.416199
 4283/5000: episode: 4283, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4906.546, mean reward: -4906.546 [-4906.546, -4906.546], mean action: 3.000 [3.000, 3.000],  loss: 9788767.000000, mae: 1229.492676, mean_q: -436.983948
 4284/5000: episode: 4284, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5946.721, mean reward: -5946.721 [-5946.721, -5946.721], mean action: 1.000 [1.000, 1.000],  loss: 8619655.000000, mae: 1227.210205, mean_q: -437.390259
 4285/5000: episode: 4285, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6915.881, mean reward: -6915.881 [-6915.881, -6915.881], mean action: 3.000 [3.000, 3.000],  loss: 9120636.000000, mae: 1107.566528, mean_q: -438.692871
 4286/5000: episode: 4286, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7065.145, mean reward: -7065.145 [-7065.145, -7065.145], mean action: 3.000 [3.000, 3.000],  loss: 16312268.000000, mae: 1505.908569, mean_q: -438.196838
 4287/5000: episode: 4287, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2933.250, mean reward: -2933.250 [-2933.250, -2933.250], mean action: 3.000 [3.000, 3.000],  loss: 10518006.000000, mae: 1260.338135, mean_q: -438.152649
 4288/5000: episode: 4288, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -276.872, mean reward: -276.872 [-276.872, -276.872], mean action: 3.000 [3.000, 3.000],  loss: 5862204.500000, mae: 993.910217, mean_q: -439.501770
 4289/5000: episode: 4289, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8372.744, mean reward: -8372.744 [-8372.744, -8372.744], mean action: 3.000 [3.000, 3.000],  loss: 14979988.000000, mae: 1402.654785, mean_q: -438.500366
 4290/5000: episode: 4290, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2232.719, mean reward: -2232.719 [-2232.719, -2232.719], mean action: 3.000 [3.000, 3.000],  loss: 18360464.000000, mae: 1558.810059, mean_q: -437.453979
 4291/5000: episode: 4291, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -190.635, mean reward: -190.635 [-190.635, -190.635], mean action: 3.000 [3.000, 3.000],  loss: 12047094.000000, mae: 1175.354492, mean_q: -438.890869
 4292/5000: episode: 4292, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -8175.312, mean reward: -8175.312 [-8175.312, -8175.312], mean action: 3.000 [3.000, 3.000],  loss: 12812346.000000, mae: 1243.271729, mean_q: -438.670410
 4293/5000: episode: 4293, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -21.954, mean reward: -21.954 [-21.954, -21.954], mean action: 3.000 [3.000, 3.000],  loss: 10286909.000000, mae: 1191.024536, mean_q: -440.945160
 4294/5000: episode: 4294, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1699.059, mean reward: -1699.059 [-1699.059, -1699.059], mean action: 3.000 [3.000, 3.000],  loss: 12581754.000000, mae: 1281.523682, mean_q: -440.610046
 4295/5000: episode: 4295, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2750.857, mean reward: -2750.857 [-2750.857, -2750.857], mean action: 3.000 [3.000, 3.000],  loss: 5634742.000000, mae: 1026.400879, mean_q: -440.654663
 4296/5000: episode: 4296, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6780.342, mean reward: -6780.342 [-6780.342, -6780.342], mean action: 3.000 [3.000, 3.000],  loss: 8572108.000000, mae: 1082.110840, mean_q: -440.047760
 4297/5000: episode: 4297, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -729.235, mean reward: -729.235 [-729.235, -729.235], mean action: 3.000 [3.000, 3.000],  loss: 10883323.000000, mae: 1272.324829, mean_q: -440.561707
 4298/5000: episode: 4298, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7245.749, mean reward: -7245.749 [-7245.749, -7245.749], mean action: 3.000 [3.000, 3.000],  loss: 11177632.000000, mae: 1237.314453, mean_q: -440.629578
 4299/5000: episode: 4299, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -495.080, mean reward: -495.080 [-495.080, -495.080], mean action: 3.000 [3.000, 3.000],  loss: 11085752.000000, mae: 1142.925537, mean_q: -440.497559
 4300/5000: episode: 4300, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1284.745, mean reward: -1284.745 [-1284.745, -1284.745], mean action: 2.000 [2.000, 2.000],  loss: 14441604.000000, mae: 1370.333252, mean_q: -440.119385
 4301/5000: episode: 4301, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7692.039, mean reward: -7692.039 [-7692.039, -7692.039], mean action: 3.000 [3.000, 3.000],  loss: 9746446.000000, mae: 1154.069092, mean_q: -442.095795
 4302/5000: episode: 4302, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4040.354, mean reward: -4040.354 [-4040.354, -4040.354], mean action: 3.000 [3.000, 3.000],  loss: 12766922.000000, mae: 1333.966797, mean_q: -440.488159
 4303/5000: episode: 4303, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -11235.360, mean reward: -11235.360 [-11235.360, -11235.360], mean action: 3.000 [3.000, 3.000],  loss: 19489790.000000, mae: 1493.683838, mean_q: -440.986908
 4304/5000: episode: 4304, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2210.322, mean reward: -2210.322 [-2210.322, -2210.322], mean action: 3.000 [3.000, 3.000],  loss: 15048338.000000, mae: 1406.416138, mean_q: -440.824158
 4305/5000: episode: 4305, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8380.913, mean reward: -8380.913 [-8380.913, -8380.913], mean action: 3.000 [3.000, 3.000],  loss: 8611824.000000, mae: 1094.894409, mean_q: -443.221741
 4306/5000: episode: 4306, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7859.594, mean reward: -7859.594 [-7859.594, -7859.594], mean action: 3.000 [3.000, 3.000],  loss: 10887210.000000, mae: 1293.785156, mean_q: -442.629242
 4307/5000: episode: 4307, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -6254.968, mean reward: -6254.968 [-6254.968, -6254.968], mean action: 3.000 [3.000, 3.000],  loss: 5669900.000000, mae: 973.646606, mean_q: -443.383911
 4308/5000: episode: 4308, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1366.396, mean reward: -1366.396 [-1366.396, -1366.396], mean action: 3.000 [3.000, 3.000],  loss: 13256648.000000, mae: 1292.054443, mean_q: -442.503815
 4309/5000: episode: 4309, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -32.760, mean reward: -32.760 [-32.760, -32.760], mean action: 3.000 [3.000, 3.000],  loss: 9709242.000000, mae: 1201.098389, mean_q: -444.910400
 4310/5000: episode: 4310, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4033.253, mean reward: -4033.253 [-4033.253, -4033.253], mean action: 3.000 [3.000, 3.000],  loss: 8395922.000000, mae: 1107.386963, mean_q: -442.412384
 4311/5000: episode: 4311, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2534.432, mean reward: -2534.432 [-2534.432, -2534.432], mean action: 3.000 [3.000, 3.000],  loss: 13491440.000000, mae: 1244.330933, mean_q: -444.234833
 4312/5000: episode: 4312, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -361.843, mean reward: -361.843 [-361.843, -361.843], mean action: 3.000 [3.000, 3.000],  loss: 14654021.000000, mae: 1427.287598, mean_q: -443.485168
 4313/5000: episode: 4313, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1358.484, mean reward: -1358.484 [-1358.484, -1358.484], mean action: 3.000 [3.000, 3.000],  loss: 5652129.000000, mae: 1068.830811, mean_q: -444.784851
 4314/5000: episode: 4314, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -798.547, mean reward: -798.547 [-798.547, -798.547], mean action: 3.000 [3.000, 3.000],  loss: 10657186.000000, mae: 1204.141357, mean_q: -444.602264
 4315/5000: episode: 4315, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2260.136, mean reward: -2260.136 [-2260.136, -2260.136], mean action: 3.000 [3.000, 3.000],  loss: 15194355.000000, mae: 1345.739990, mean_q: -444.653259
 4316/5000: episode: 4316, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6962.418, mean reward: -6962.418 [-6962.418, -6962.418], mean action: 3.000 [3.000, 3.000],  loss: 11659452.000000, mae: 1255.380005, mean_q: -444.660461
 4317/5000: episode: 4317, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4902.989, mean reward: -4902.989 [-4902.989, -4902.989], mean action: 3.000 [3.000, 3.000],  loss: 7895088.000000, mae: 1061.801758, mean_q: -446.249939
 4318/5000: episode: 4318, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -330.683, mean reward: -330.683 [-330.683, -330.683], mean action: 3.000 [3.000, 3.000],  loss: 10954580.000000, mae: 1262.231934, mean_q: -446.482239
 4319/5000: episode: 4319, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6918.153, mean reward: -6918.153 [-6918.153, -6918.153], mean action: 3.000 [3.000, 3.000],  loss: 14253929.000000, mae: 1363.153076, mean_q: -444.887054
 4320/5000: episode: 4320, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2699.332, mean reward: -2699.332 [-2699.332, -2699.332], mean action: 3.000 [3.000, 3.000],  loss: 8366214.000000, mae: 1060.577148, mean_q: -446.525574
 4321/5000: episode: 4321, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2295.656, mean reward: -2295.656 [-2295.656, -2295.656], mean action: 3.000 [3.000, 3.000],  loss: 12235454.000000, mae: 1195.889282, mean_q: -446.728088
 4322/5000: episode: 4322, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5532.738, mean reward: -5532.738 [-5532.738, -5532.738], mean action: 3.000 [3.000, 3.000],  loss: 13750488.000000, mae: 1338.624512, mean_q: -447.194794
 4323/5000: episode: 4323, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2399.086, mean reward: -2399.086 [-2399.086, -2399.086], mean action: 3.000 [3.000, 3.000],  loss: 7056706.000000, mae: 1010.369507, mean_q: -445.291931
 4324/5000: episode: 4324, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -492.558, mean reward: -492.558 [-492.558, -492.558], mean action: 3.000 [3.000, 3.000],  loss: 11009086.000000, mae: 1227.031982, mean_q: -446.197632
 4325/5000: episode: 4325, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1347.220, mean reward: -1347.220 [-1347.220, -1347.220], mean action: 3.000 [3.000, 3.000],  loss: 8981896.000000, mae: 1149.644165, mean_q: -446.351746
 4326/5000: episode: 4326, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9047.508, mean reward: -9047.508 [-9047.508, -9047.508], mean action: 3.000 [3.000, 3.000],  loss: 13125954.000000, mae: 1421.760742, mean_q: -444.490845
 4327/5000: episode: 4327, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6278.759, mean reward: -6278.759 [-6278.759, -6278.759], mean action: 3.000 [3.000, 3.000],  loss: 7217574.000000, mae: 1094.043701, mean_q: -450.238159
 4328/5000: episode: 4328, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8475.672, mean reward: -8475.672 [-8475.672, -8475.672], mean action: 3.000 [3.000, 3.000],  loss: 8726995.000000, mae: 1086.020264, mean_q: -447.572754
 4329/5000: episode: 4329, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2805.773, mean reward: -2805.773 [-2805.773, -2805.773], mean action: 3.000 [3.000, 3.000],  loss: 9591152.000000, mae: 1104.657715, mean_q: -448.370605
 4330/5000: episode: 4330, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -134.897, mean reward: -134.897 [-134.897, -134.897], mean action: 3.000 [3.000, 3.000],  loss: 9663567.000000, mae: 1194.321045, mean_q: -446.419983
 4331/5000: episode: 4331, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1024.046, mean reward: -1024.046 [-1024.046, -1024.046], mean action: 1.000 [1.000, 1.000],  loss: 12374891.000000, mae: 1243.114624, mean_q: -449.144226
 4332/5000: episode: 4332, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -386.758, mean reward: -386.758 [-386.758, -386.758], mean action: 3.000 [3.000, 3.000],  loss: 9894446.000000, mae: 1172.835449, mean_q: -448.502167
 4333/5000: episode: 4333, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -668.316, mean reward: -668.316 [-668.316, -668.316], mean action: 3.000 [3.000, 3.000],  loss: 9811539.000000, mae: 1199.761719, mean_q: -449.345398
 4334/5000: episode: 4334, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4110.376, mean reward: -4110.376 [-4110.376, -4110.376], mean action: 3.000 [3.000, 3.000],  loss: 11636256.000000, mae: 1222.281250, mean_q: -447.020813
 4335/5000: episode: 4335, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1704.047, mean reward: -1704.047 [-1704.047, -1704.047], mean action: 1.000 [1.000, 1.000],  loss: 9914388.000000, mae: 1177.354370, mean_q: -448.477112
 4336/5000: episode: 4336, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1116.880, mean reward: -1116.880 [-1116.880, -1116.880], mean action: 3.000 [3.000, 3.000],  loss: 9134184.000000, mae: 1169.388062, mean_q: -448.739136
 4337/5000: episode: 4337, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2271.749, mean reward: -2271.749 [-2271.749, -2271.749], mean action: 3.000 [3.000, 3.000],  loss: 15762624.000000, mae: 1488.326294, mean_q: -448.653442
 4338/5000: episode: 4338, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -926.527, mean reward: -926.527 [-926.527, -926.527], mean action: 3.000 [3.000, 3.000],  loss: 8994354.000000, mae: 1064.586548, mean_q: -450.968323
 4339/5000: episode: 4339, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -71.973, mean reward: -71.973 [-71.973, -71.973], mean action: 3.000 [3.000, 3.000],  loss: 8756854.000000, mae: 1184.015869, mean_q: -450.228577
 4340/5000: episode: 4340, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -126.751, mean reward: -126.751 [-126.751, -126.751], mean action: 3.000 [3.000, 3.000],  loss: 8773362.000000, mae: 1116.784668, mean_q: -450.123352
 4341/5000: episode: 4341, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -530.538, mean reward: -530.538 [-530.538, -530.538], mean action: 3.000 [3.000, 3.000],  loss: 6103704.000000, mae: 1029.242676, mean_q: -449.715820
 4342/5000: episode: 4342, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2369.068, mean reward: -2369.068 [-2369.068, -2369.068], mean action: 3.000 [3.000, 3.000],  loss: 12033108.000000, mae: 1221.174194, mean_q: -450.736694
 4343/5000: episode: 4343, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9.105, mean reward: -9.105 [-9.105, -9.105], mean action: 3.000 [3.000, 3.000],  loss: 11847170.000000, mae: 1238.856445, mean_q: -449.547058
 4344/5000: episode: 4344, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -998.895, mean reward: -998.895 [-998.895, -998.895], mean action: 3.000 [3.000, 3.000],  loss: 11209023.000000, mae: 1233.196899, mean_q: -450.032959
 4345/5000: episode: 4345, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -813.242, mean reward: -813.242 [-813.242, -813.242], mean action: 3.000 [3.000, 3.000],  loss: 10754002.000000, mae: 1219.609985, mean_q: -450.091797
 4346/5000: episode: 4346, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -217.390, mean reward: -217.390 [-217.390, -217.390], mean action: 3.000 [3.000, 3.000],  loss: 10021392.000000, mae: 1218.073364, mean_q: -453.310333
 4347/5000: episode: 4347, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -695.085, mean reward: -695.085 [-695.085, -695.085], mean action: 3.000 [3.000, 3.000],  loss: 9445206.000000, mae: 1030.421143, mean_q: -452.280945
 4348/5000: episode: 4348, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2569.869, mean reward: -2569.869 [-2569.869, -2569.869], mean action: 3.000 [3.000, 3.000],  loss: 7291018.000000, mae: 1087.537354, mean_q: -452.126892
 4349/5000: episode: 4349, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5651.980, mean reward: -5651.980 [-5651.980, -5651.980], mean action: 3.000 [3.000, 3.000],  loss: 15858176.000000, mae: 1511.140381, mean_q: -451.495178
 4350/5000: episode: 4350, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9700.542, mean reward: -9700.542 [-9700.542, -9700.542], mean action: 3.000 [3.000, 3.000],  loss: 11520314.000000, mae: 1256.248535, mean_q: -451.223846
 4351/5000: episode: 4351, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -6640.343, mean reward: -6640.343 [-6640.343, -6640.343], mean action: 3.000 [3.000, 3.000],  loss: 18054896.000000, mae: 1414.190918, mean_q: -451.876160
 4352/5000: episode: 4352, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3101.433, mean reward: -3101.433 [-3101.433, -3101.433], mean action: 3.000 [3.000, 3.000],  loss: 10186790.000000, mae: 1134.938110, mean_q: -452.841278
 4353/5000: episode: 4353, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -118.961, mean reward: -118.961 [-118.961, -118.961], mean action: 3.000 [3.000, 3.000],  loss: 14015274.000000, mae: 1311.771973, mean_q: -452.690979
 4354/5000: episode: 4354, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -100.323, mean reward: -100.323 [-100.323, -100.323], mean action: 3.000 [3.000, 3.000],  loss: 8617179.000000, mae: 1164.979126, mean_q: -453.728943
 4355/5000: episode: 4355, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3716.887, mean reward: -3716.887 [-3716.887, -3716.887], mean action: 1.000 [1.000, 1.000],  loss: 9405878.000000, mae: 1184.031494, mean_q: -454.710693
 4356/5000: episode: 4356, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -112.192, mean reward: -112.192 [-112.192, -112.192], mean action: 3.000 [3.000, 3.000],  loss: 9717954.000000, mae: 1261.068604, mean_q: -454.222015
 4357/5000: episode: 4357, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -552.759, mean reward: -552.759 [-552.759, -552.759], mean action: 3.000 [3.000, 3.000],  loss: 16603871.000000, mae: 1545.440796, mean_q: -451.623047
 4358/5000: episode: 4358, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5474.342, mean reward: -5474.342 [-5474.342, -5474.342], mean action: 3.000 [3.000, 3.000],  loss: 12339766.000000, mae: 1223.099365, mean_q: -454.092010
 4359/5000: episode: 4359, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9270.141, mean reward: -9270.141 [-9270.141, -9270.141], mean action: 3.000 [3.000, 3.000],  loss: 7606826.000000, mae: 1113.999023, mean_q: -455.064301
 4360/5000: episode: 4360, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1986.424, mean reward: -1986.424 [-1986.424, -1986.424], mean action: 3.000 [3.000, 3.000],  loss: 17395498.000000, mae: 1396.282349, mean_q: -452.306946
 4361/5000: episode: 4361, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2009.724, mean reward: -2009.724 [-2009.724, -2009.724], mean action: 3.000 [3.000, 3.000],  loss: 11751935.000000, mae: 1304.319336, mean_q: -454.477051
 4362/5000: episode: 4362, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1187.638, mean reward: -1187.638 [-1187.638, -1187.638], mean action: 3.000 [3.000, 3.000],  loss: 16197445.000000, mae: 1437.037354, mean_q: -452.851349
 4363/5000: episode: 4363, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -818.364, mean reward: -818.364 [-818.364, -818.364], mean action: 3.000 [3.000, 3.000],  loss: 13984324.000000, mae: 1350.690186, mean_q: -453.245239
 4364/5000: episode: 4364, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4936.041, mean reward: -4936.041 [-4936.041, -4936.041], mean action: 3.000 [3.000, 3.000],  loss: 12879871.000000, mae: 1332.849609, mean_q: -455.562286
 4365/5000: episode: 4365, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2802.836, mean reward: -2802.836 [-2802.836, -2802.836], mean action: 3.000 [3.000, 3.000],  loss: 6788523.000000, mae: 1045.319336, mean_q: -454.897156
 4366/5000: episode: 4366, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -174.399, mean reward: -174.399 [-174.399, -174.399], mean action: 3.000 [3.000, 3.000],  loss: 11318266.000000, mae: 1285.365479, mean_q: -455.591583
 4367/5000: episode: 4367, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7774.953, mean reward: -7774.953 [-7774.953, -7774.953], mean action: 3.000 [3.000, 3.000],  loss: 13973558.000000, mae: 1355.770020, mean_q: -455.164673
 4368/5000: episode: 4368, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3182.290, mean reward: -3182.290 [-3182.290, -3182.290], mean action: 2.000 [2.000, 2.000],  loss: 8988914.000000, mae: 1241.837891, mean_q: -456.794250
 4369/5000: episode: 4369, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4531.789, mean reward: -4531.789 [-4531.789, -4531.789], mean action: 3.000 [3.000, 3.000],  loss: 14037354.000000, mae: 1435.824463, mean_q: -457.422302
 4370/5000: episode: 4370, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8844.109, mean reward: -8844.109 [-8844.109, -8844.109], mean action: 3.000 [3.000, 3.000],  loss: 12067679.000000, mae: 1253.306030, mean_q: -458.609253
 4371/5000: episode: 4371, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1274.052, mean reward: -1274.052 [-1274.052, -1274.052], mean action: 2.000 [2.000, 2.000],  loss: 11406944.000000, mae: 1299.336060, mean_q: -455.688843
 4372/5000: episode: 4372, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7922.787, mean reward: -7922.787 [-7922.787, -7922.787], mean action: 3.000 [3.000, 3.000],  loss: 16247946.000000, mae: 1408.074097, mean_q: -455.355652
 4373/5000: episode: 4373, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -216.982, mean reward: -216.982 [-216.982, -216.982], mean action: 2.000 [2.000, 2.000],  loss: 8683597.000000, mae: 1209.114990, mean_q: -456.561523
 4374/5000: episode: 4374, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -3104.175, mean reward: -3104.175 [-3104.175, -3104.175], mean action: 2.000 [2.000, 2.000],  loss: 10773612.000000, mae: 1207.492188, mean_q: -455.427307
 4375/5000: episode: 4375, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1592.427, mean reward: -1592.427 [-1592.427, -1592.427], mean action: 3.000 [3.000, 3.000],  loss: 15039866.000000, mae: 1472.649658, mean_q: -457.114136
 4376/5000: episode: 4376, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -98.211, mean reward: -98.211 [-98.211, -98.211], mean action: 2.000 [2.000, 2.000],  loss: 9318986.000000, mae: 1160.794189, mean_q: -460.086121
 4377/5000: episode: 4377, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1574.402, mean reward: -1574.402 [-1574.402, -1574.402], mean action: 2.000 [2.000, 2.000],  loss: 15708308.000000, mae: 1460.071899, mean_q: -458.664124
 4378/5000: episode: 4378, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4701.700, mean reward: -4701.700 [-4701.700, -4701.700], mean action: 2.000 [2.000, 2.000],  loss: 12213931.000000, mae: 1285.845215, mean_q: -456.874451
 4379/5000: episode: 4379, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2739.567, mean reward: -2739.567 [-2739.567, -2739.567], mean action: 2.000 [2.000, 2.000],  loss: 18796192.000000, mae: 1502.614746, mean_q: -457.150818
 4380/5000: episode: 4380, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4129.490, mean reward: -4129.490 [-4129.490, -4129.490], mean action: 2.000 [2.000, 2.000],  loss: 14744064.000000, mae: 1376.385620, mean_q: -459.444794
 4381/5000: episode: 4381, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -574.117, mean reward: -574.117 [-574.117, -574.117], mean action: 2.000 [2.000, 2.000],  loss: 10560691.000000, mae: 1175.257202, mean_q: -457.975952
 4382/5000: episode: 4382, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1531.761, mean reward: -1531.761 [-1531.761, -1531.761], mean action: 2.000 [2.000, 2.000],  loss: 16650032.000000, mae: 1414.583984, mean_q: -458.155060
 4383/5000: episode: 4383, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2011.678, mean reward: -2011.678 [-2011.678, -2011.678], mean action: 2.000 [2.000, 2.000],  loss: 8797878.000000, mae: 1175.849976, mean_q: -458.804443
 4384/5000: episode: 4384, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -709.414, mean reward: -709.414 [-709.414, -709.414], mean action: 2.000 [2.000, 2.000],  loss: 11339876.000000, mae: 1258.864746, mean_q: -459.839783
 4385/5000: episode: 4385, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2341.231, mean reward: -2341.231 [-2341.231, -2341.231], mean action: 2.000 [2.000, 2.000],  loss: 9494170.000000, mae: 1175.531860, mean_q: -458.364014
 4386/5000: episode: 4386, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3056.275, mean reward: -3056.275 [-3056.275, -3056.275], mean action: 2.000 [2.000, 2.000],  loss: 17736432.000000, mae: 1502.781616, mean_q: -459.329529
 4387/5000: episode: 4387, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -939.381, mean reward: -939.381 [-939.381, -939.381], mean action: 2.000 [2.000, 2.000],  loss: 10348906.000000, mae: 1257.473389, mean_q: -460.443237
 4388/5000: episode: 4388, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2420.760, mean reward: -2420.760 [-2420.760, -2420.760], mean action: 2.000 [2.000, 2.000],  loss: 7982625.000000, mae: 1180.040771, mean_q: -461.178040
 4389/5000: episode: 4389, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -617.203, mean reward: -617.203 [-617.203, -617.203], mean action: 2.000 [2.000, 2.000],  loss: 14097963.000000, mae: 1298.682983, mean_q: -459.974487
 4390/5000: episode: 4390, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3048.600, mean reward: -3048.600 [-3048.600, -3048.600], mean action: 2.000 [2.000, 2.000],  loss: 7932748.000000, mae: 1137.117676, mean_q: -460.768066
 4391/5000: episode: 4391, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1476.020, mean reward: -1476.020 [-1476.020, -1476.020], mean action: 2.000 [2.000, 2.000],  loss: 11120044.000000, mae: 1279.127808, mean_q: -460.707764
 4392/5000: episode: 4392, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2681.836, mean reward: -2681.836 [-2681.836, -2681.836], mean action: 2.000 [2.000, 2.000],  loss: 12660329.000000, mae: 1313.018921, mean_q: -461.974854
 4393/5000: episode: 4393, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -586.328, mean reward: -586.328 [-586.328, -586.328], mean action: 2.000 [2.000, 2.000],  loss: 12608316.000000, mae: 1189.952148, mean_q: -460.104614
 4394/5000: episode: 4394, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5720.042, mean reward: -5720.042 [-5720.042, -5720.042], mean action: 2.000 [2.000, 2.000],  loss: 9298202.000000, mae: 1215.434692, mean_q: -462.640381
 4395/5000: episode: 4395, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3842.265, mean reward: -3842.265 [-3842.265, -3842.265], mean action: 2.000 [2.000, 2.000],  loss: 14914754.000000, mae: 1410.485962, mean_q: -461.665955
 4396/5000: episode: 4396, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1282.040, mean reward: -1282.040 [-1282.040, -1282.040], mean action: 3.000 [3.000, 3.000],  loss: 8493310.000000, mae: 1108.893799, mean_q: -460.835266
 4397/5000: episode: 4397, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -155.068, mean reward: -155.068 [-155.068, -155.068], mean action: 2.000 [2.000, 2.000],  loss: 10399816.000000, mae: 1229.765625, mean_q: -461.128387
 4398/5000: episode: 4398, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -802.444, mean reward: -802.444 [-802.444, -802.444], mean action: 2.000 [2.000, 2.000],  loss: 13906186.000000, mae: 1372.507935, mean_q: -463.293396
 4399/5000: episode: 4399, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5187.144, mean reward: -5187.144 [-5187.144, -5187.144], mean action: 2.000 [2.000, 2.000],  loss: 12433176.000000, mae: 1320.074585, mean_q: -462.776367
 4400/5000: episode: 4400, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3718.178, mean reward: -3718.178 [-3718.178, -3718.178], mean action: 2.000 [2.000, 2.000],  loss: 11876684.000000, mae: 1233.006226, mean_q: -462.059448
 4401/5000: episode: 4401, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2349.059, mean reward: -2349.059 [-2349.059, -2349.059], mean action: 2.000 [2.000, 2.000],  loss: 6846440.500000, mae: 1052.611816, mean_q: -463.304688
 4402/5000: episode: 4402, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4362.875, mean reward: -4362.875 [-4362.875, -4362.875], mean action: 2.000 [2.000, 2.000],  loss: 9426962.000000, mae: 1167.989380, mean_q: -464.619690
 4403/5000: episode: 4403, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5093.641, mean reward: -5093.641 [-5093.641, -5093.641], mean action: 2.000 [2.000, 2.000],  loss: 10586760.000000, mae: 1259.221924, mean_q: -464.573364
 4404/5000: episode: 4404, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1329.066, mean reward: -1329.066 [-1329.066, -1329.066], mean action: 3.000 [3.000, 3.000],  loss: 16327306.000000, mae: 1494.663940, mean_q: -463.539673
 4405/5000: episode: 4405, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2683.985, mean reward: -2683.985 [-2683.985, -2683.985], mean action: 2.000 [2.000, 2.000],  loss: 11314412.000000, mae: 1192.821045, mean_q: -464.795166
 4406/5000: episode: 4406, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1039.768, mean reward: -1039.768 [-1039.768, -1039.768], mean action: 2.000 [2.000, 2.000],  loss: 20028360.000000, mae: 1448.320557, mean_q: -463.604279
 4407/5000: episode: 4407, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9176.322, mean reward: -9176.322 [-9176.322, -9176.322], mean action: 2.000 [2.000, 2.000],  loss: 14519557.000000, mae: 1361.865112, mean_q: -465.863831
 4408/5000: episode: 4408, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -493.761, mean reward: -493.761 [-493.761, -493.761], mean action: 2.000 [2.000, 2.000],  loss: 9458820.000000, mae: 1146.571655, mean_q: -464.796692
 4409/5000: episode: 4409, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4425.142, mean reward: -4425.142 [-4425.142, -4425.142], mean action: 2.000 [2.000, 2.000],  loss: 13919880.000000, mae: 1441.614380, mean_q: -462.990540
 4410/5000: episode: 4410, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -136.857, mean reward: -136.857 [-136.857, -136.857], mean action: 2.000 [2.000, 2.000],  loss: 10820651.000000, mae: 1168.451050, mean_q: -465.796295
 4411/5000: episode: 4411, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1578.852, mean reward: -1578.852 [-1578.852, -1578.852], mean action: 2.000 [2.000, 2.000],  loss: 10058066.000000, mae: 1222.540283, mean_q: -465.154663
 4412/5000: episode: 4412, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1105.232, mean reward: -1105.232 [-1105.232, -1105.232], mean action: 2.000 [2.000, 2.000],  loss: 10107374.000000, mae: 1317.210327, mean_q: -466.399048
 4413/5000: episode: 4413, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1321.917, mean reward: -1321.917 [-1321.917, -1321.917], mean action: 2.000 [2.000, 2.000],  loss: 9725720.000000, mae: 1199.811279, mean_q: -467.318695
 4414/5000: episode: 4414, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2051.495, mean reward: -2051.495 [-2051.495, -2051.495], mean action: 2.000 [2.000, 2.000],  loss: 8908398.000000, mae: 1214.126953, mean_q: -467.325806
 4415/5000: episode: 4415, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1312.073, mean reward: -1312.073 [-1312.073, -1312.073], mean action: 2.000 [2.000, 2.000],  loss: 6944671.000000, mae: 1100.386230, mean_q: -467.358765
 4416/5000: episode: 4416, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2162.858, mean reward: -2162.858 [-2162.858, -2162.858], mean action: 2.000 [2.000, 2.000],  loss: 9024932.000000, mae: 1090.510254, mean_q: -466.328613
 4417/5000: episode: 4417, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2696.019, mean reward: -2696.019 [-2696.019, -2696.019], mean action: 2.000 [2.000, 2.000],  loss: 11402014.000000, mae: 1269.882080, mean_q: -465.058228
 4418/5000: episode: 4418, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3049.134, mean reward: -3049.134 [-3049.134, -3049.134], mean action: 2.000 [2.000, 2.000],  loss: 19368156.000000, mae: 1532.959961, mean_q: -465.761444
 4419/5000: episode: 4419, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3267.566, mean reward: -3267.566 [-3267.566, -3267.566], mean action: 2.000 [2.000, 2.000],  loss: 10593307.000000, mae: 1237.588501, mean_q: -467.225861
 4420/5000: episode: 4420, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6033.543, mean reward: -6033.543 [-6033.543, -6033.543], mean action: 2.000 [2.000, 2.000],  loss: 9332111.000000, mae: 1168.406250, mean_q: -468.303741
 4421/5000: episode: 4421, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2228.156, mean reward: -2228.156 [-2228.156, -2228.156], mean action: 2.000 [2.000, 2.000],  loss: 9178265.000000, mae: 1151.439575, mean_q: -468.925537
 4422/5000: episode: 4422, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1948.069, mean reward: -1948.069 [-1948.069, -1948.069], mean action: 2.000 [2.000, 2.000],  loss: 13021611.000000, mae: 1282.846313, mean_q: -468.086792
 4423/5000: episode: 4423, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5276.516, mean reward: -5276.516 [-5276.516, -5276.516], mean action: 2.000 [2.000, 2.000],  loss: 10676878.000000, mae: 1288.600830, mean_q: -468.986023
 4424/5000: episode: 4424, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1365.120, mean reward: -1365.120 [-1365.120, -1365.120], mean action: 2.000 [2.000, 2.000],  loss: 14020600.000000, mae: 1424.091309, mean_q: -468.331238
 4425/5000: episode: 4425, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2041.760, mean reward: -2041.760 [-2041.760, -2041.760], mean action: 2.000 [2.000, 2.000],  loss: 10381927.000000, mae: 1147.296997, mean_q: -471.078064
 4426/5000: episode: 4426, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -785.431, mean reward: -785.431 [-785.431, -785.431], mean action: 2.000 [2.000, 2.000],  loss: 11047811.000000, mae: 1149.621582, mean_q: -466.954407
 4427/5000: episode: 4427, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4565.533, mean reward: -4565.533 [-4565.533, -4565.533], mean action: 2.000 [2.000, 2.000],  loss: 8309141.000000, mae: 1167.817505, mean_q: -470.404572
 4428/5000: episode: 4428, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3167.235, mean reward: -3167.235 [-3167.235, -3167.235], mean action: 2.000 [2.000, 2.000],  loss: 9783853.000000, mae: 1225.543457, mean_q: -470.307129
 4429/5000: episode: 4429, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3000.339, mean reward: -3000.339 [-3000.339, -3000.339], mean action: 2.000 [2.000, 2.000],  loss: 12626073.000000, mae: 1222.239868, mean_q: -469.361542
 4430/5000: episode: 4430, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2494.841, mean reward: -2494.841 [-2494.841, -2494.841], mean action: 2.000 [2.000, 2.000],  loss: 8003532.000000, mae: 1129.163574, mean_q: -470.872925
 4431/5000: episode: 4431, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4182.629, mean reward: -4182.629 [-4182.629, -4182.629], mean action: 2.000 [2.000, 2.000],  loss: 10063310.000000, mae: 1224.431030, mean_q: -470.290283
 4432/5000: episode: 4432, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1706.383, mean reward: -1706.383 [-1706.383, -1706.383], mean action: 2.000 [2.000, 2.000],  loss: 14623284.000000, mae: 1447.189575, mean_q: -468.871826
 4433/5000: episode: 4433, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1439.236, mean reward: -1439.236 [-1439.236, -1439.236], mean action: 2.000 [2.000, 2.000],  loss: 9724729.000000, mae: 1261.963623, mean_q: -472.158081
 4434/5000: episode: 4434, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1951.519, mean reward: -1951.519 [-1951.519, -1951.519], mean action: 2.000 [2.000, 2.000],  loss: 15386656.000000, mae: 1459.823975, mean_q: -469.896729
 4435/5000: episode: 4435, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3014.730, mean reward: -3014.730 [-3014.730, -3014.730], mean action: 2.000 [2.000, 2.000],  loss: 14099584.000000, mae: 1387.550537, mean_q: -469.619690
 4436/5000: episode: 4436, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -1677.175, mean reward: -1677.175 [-1677.175, -1677.175], mean action: 2.000 [2.000, 2.000],  loss: 15131806.000000, mae: 1410.062744, mean_q: -471.406799
 4437/5000: episode: 4437, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1818.991, mean reward: -1818.991 [-1818.991, -1818.991], mean action: 2.000 [2.000, 2.000],  loss: 14090710.000000, mae: 1363.769531, mean_q: -472.923523
 4438/5000: episode: 4438, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -399.283, mean reward: -399.283 [-399.283, -399.283], mean action: 2.000 [2.000, 2.000],  loss: 8820080.000000, mae: 1142.124268, mean_q: -472.980011
 4439/5000: episode: 4439, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2054.950, mean reward: -2054.950 [-2054.950, -2054.950], mean action: 2.000 [2.000, 2.000],  loss: 13623772.000000, mae: 1356.984375, mean_q: -471.422150
 4440/5000: episode: 4440, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -743.831, mean reward: -743.831 [-743.831, -743.831], mean action: 2.000 [2.000, 2.000],  loss: 11120251.000000, mae: 1231.450928, mean_q: -473.573669
 4441/5000: episode: 4441, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1888.453, mean reward: -1888.453 [-1888.453, -1888.453], mean action: 2.000 [2.000, 2.000],  loss: 7714700.500000, mae: 1081.199219, mean_q: -471.041504
 4442/5000: episode: 4442, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5235.778, mean reward: -5235.778 [-5235.778, -5235.778], mean action: 2.000 [2.000, 2.000],  loss: 17145806.000000, mae: 1400.030762, mean_q: -470.895294
 4443/5000: episode: 4443, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -6181.696, mean reward: -6181.696 [-6181.696, -6181.696], mean action: 2.000 [2.000, 2.000],  loss: 10804154.000000, mae: 1337.606934, mean_q: -472.547424
 4444/5000: episode: 4444, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1547.030, mean reward: -1547.030 [-1547.030, -1547.030], mean action: 2.000 [2.000, 2.000],  loss: 11671210.000000, mae: 1297.798462, mean_q: -472.802032
 4445/5000: episode: 4445, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7068.389, mean reward: -7068.389 [-7068.389, -7068.389], mean action: 2.000 [2.000, 2.000],  loss: 8046360.000000, mae: 1054.767822, mean_q: -474.407074
 4446/5000: episode: 4446, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -988.965, mean reward: -988.965 [-988.965, -988.965], mean action: 2.000 [2.000, 2.000],  loss: 15114433.000000, mae: 1457.503052, mean_q: -472.524902
 4447/5000: episode: 4447, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1175.910, mean reward: -1175.910 [-1175.910, -1175.910], mean action: 2.000 [2.000, 2.000],  loss: 12611008.000000, mae: 1354.852295, mean_q: -473.617310
 4448/5000: episode: 4448, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1952.544, mean reward: -1952.544 [-1952.544, -1952.544], mean action: 2.000 [2.000, 2.000],  loss: 7249272.000000, mae: 1081.539795, mean_q: -474.180420
 4449/5000: episode: 4449, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -380.608, mean reward: -380.608 [-380.608, -380.608], mean action: 2.000 [2.000, 2.000],  loss: 15310982.000000, mae: 1307.015137, mean_q: -472.490570
 4450/5000: episode: 4450, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2270.855, mean reward: -2270.855 [-2270.855, -2270.855], mean action: 2.000 [2.000, 2.000],  loss: 14913638.000000, mae: 1339.551514, mean_q: -473.198334
 4451/5000: episode: 4451, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2754.718, mean reward: -2754.718 [-2754.718, -2754.718], mean action: 2.000 [2.000, 2.000],  loss: 15318912.000000, mae: 1369.348267, mean_q: -474.039581
 4452/5000: episode: 4452, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4047.857, mean reward: -4047.857 [-4047.857, -4047.857], mean action: 2.000 [2.000, 2.000],  loss: 12686808.000000, mae: 1276.978516, mean_q: -476.098816
 4453/5000: episode: 4453, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -8007.049, mean reward: -8007.049 [-8007.049, -8007.049], mean action: 2.000 [2.000, 2.000],  loss: 14407350.000000, mae: 1357.695068, mean_q: -474.866119
 4454/5000: episode: 4454, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2374.651, mean reward: -2374.651 [-2374.651, -2374.651], mean action: 2.000 [2.000, 2.000],  loss: 14640494.000000, mae: 1404.718994, mean_q: -474.868225
 4455/5000: episode: 4455, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4408.782, mean reward: -4408.782 [-4408.782, -4408.782], mean action: 2.000 [2.000, 2.000],  loss: 11427817.000000, mae: 1257.079590, mean_q: -474.367188
 4456/5000: episode: 4456, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3502.751, mean reward: -3502.751 [-3502.751, -3502.751], mean action: 2.000 [2.000, 2.000],  loss: 7065367.000000, mae: 1072.596191, mean_q: -476.275818
 4457/5000: episode: 4457, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1892.788, mean reward: -1892.788 [-1892.788, -1892.788], mean action: 2.000 [2.000, 2.000],  loss: 8346841.500000, mae: 1098.891846, mean_q: -476.374756
 4458/5000: episode: 4458, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -33.091, mean reward: -33.091 [-33.091, -33.091], mean action: 2.000 [2.000, 2.000],  loss: 11695884.000000, mae: 1261.464111, mean_q: -477.214600
 4459/5000: episode: 4459, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6779.949, mean reward: -6779.949 [-6779.949, -6779.949], mean action: 1.000 [1.000, 1.000],  loss: 8060153.000000, mae: 1123.684814, mean_q: -477.961945
 4460/5000: episode: 4460, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1987.805, mean reward: -1987.805 [-1987.805, -1987.805], mean action: 3.000 [3.000, 3.000],  loss: 8648018.000000, mae: 1125.978516, mean_q: -478.351593
 4461/5000: episode: 4461, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -562.424, mean reward: -562.424 [-562.424, -562.424], mean action: 2.000 [2.000, 2.000],  loss: 9172650.000000, mae: 1162.239258, mean_q: -479.367493
 4462/5000: episode: 4462, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1282.603, mean reward: -1282.603 [-1282.603, -1282.603], mean action: 2.000 [2.000, 2.000],  loss: 9106616.000000, mae: 1241.693481, mean_q: -479.398804
 4463/5000: episode: 4463, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3430.854, mean reward: -3430.854 [-3430.854, -3430.854], mean action: 2.000 [2.000, 2.000],  loss: 9136183.000000, mae: 1201.507324, mean_q: -477.969116
 4464/5000: episode: 4464, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2378.064, mean reward: -2378.064 [-2378.064, -2378.064], mean action: 2.000 [2.000, 2.000],  loss: 11630015.000000, mae: 1330.536621, mean_q: -477.093933
 4465/5000: episode: 4465, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -51.468, mean reward: -51.468 [-51.468, -51.468], mean action: 2.000 [2.000, 2.000],  loss: 5666350.000000, mae: 1021.745361, mean_q: -477.397278
 4466/5000: episode: 4466, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3480.345, mean reward: -3480.345 [-3480.345, -3480.345], mean action: 2.000 [2.000, 2.000],  loss: 14395889.000000, mae: 1423.604980, mean_q: -476.877808
 4467/5000: episode: 4467, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1767.243, mean reward: -1767.243 [-1767.243, -1767.243], mean action: 2.000 [2.000, 2.000],  loss: 16810456.000000, mae: 1480.879395, mean_q: -475.596436
 4468/5000: episode: 4468, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3101.618, mean reward: -3101.618 [-3101.618, -3101.618], mean action: 3.000 [3.000, 3.000],  loss: 11526869.000000, mae: 1317.020752, mean_q: -478.206909
 4469/5000: episode: 4469, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -804.543, mean reward: -804.543 [-804.543, -804.543], mean action: 2.000 [2.000, 2.000],  loss: 15126969.000000, mae: 1404.768433, mean_q: -479.049194
 4470/5000: episode: 4470, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2688.849, mean reward: -2688.849 [-2688.849, -2688.849], mean action: 2.000 [2.000, 2.000],  loss: 13769444.000000, mae: 1325.387695, mean_q: -476.920868
 4471/5000: episode: 4471, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4636.915, mean reward: -4636.915 [-4636.915, -4636.915], mean action: 2.000 [2.000, 2.000],  loss: 13333458.000000, mae: 1455.270752, mean_q: -475.802856
 4472/5000: episode: 4472, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3464.916, mean reward: -3464.916 [-3464.916, -3464.916], mean action: 2.000 [2.000, 2.000],  loss: 6702297.000000, mae: 1072.527832, mean_q: -479.627197
 4473/5000: episode: 4473, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8568.429, mean reward: -8568.429 [-8568.429, -8568.429], mean action: 0.000 [0.000, 0.000],  loss: 15633633.000000, mae: 1449.347046, mean_q: -478.436737
 4474/5000: episode: 4474, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4736.361, mean reward: -4736.361 [-4736.361, -4736.361], mean action: 0.000 [0.000, 0.000],  loss: 10211010.000000, mae: 1236.559692, mean_q: -477.483490
 4475/5000: episode: 4475, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -16745.777, mean reward: -16745.777 [-16745.777, -16745.777], mean action: 0.000 [0.000, 0.000],  loss: 10145226.000000, mae: 1234.413574, mean_q: -479.514832
 4476/5000: episode: 4476, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4442.576, mean reward: -4442.576 [-4442.576, -4442.576], mean action: 0.000 [0.000, 0.000],  loss: 10857192.000000, mae: 1264.998047, mean_q: -480.795471
 4477/5000: episode: 4477, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3161.681, mean reward: -3161.681 [-3161.681, -3161.681], mean action: 0.000 [0.000, 0.000],  loss: 11267946.000000, mae: 1207.318604, mean_q: -481.277527
 4478/5000: episode: 4478, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3175.533, mean reward: -3175.533 [-3175.533, -3175.533], mean action: 0.000 [0.000, 0.000],  loss: 10679220.000000, mae: 1275.135498, mean_q: -480.636292
 4479/5000: episode: 4479, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3240.051, mean reward: -3240.051 [-3240.051, -3240.051], mean action: 0.000 [0.000, 0.000],  loss: 17106676.000000, mae: 1514.026855, mean_q: -479.527466
 4480/5000: episode: 4480, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -12132.789, mean reward: -12132.789 [-12132.789, -12132.789], mean action: 0.000 [0.000, 0.000],  loss: 9523131.000000, mae: 1101.410645, mean_q: -480.464661
 4481/5000: episode: 4481, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4989.720, mean reward: -4989.720 [-4989.720, -4989.720], mean action: 0.000 [0.000, 0.000],  loss: 7423277.500000, mae: 1109.585693, mean_q: -479.092590
 4482/5000: episode: 4482, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -13095.845, mean reward: -13095.845 [-13095.845, -13095.845], mean action: 0.000 [0.000, 0.000],  loss: 11042120.000000, mae: 1210.415161, mean_q: -480.683838
 4483/5000: episode: 4483, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6364.975, mean reward: -6364.975 [-6364.975, -6364.975], mean action: 3.000 [3.000, 3.000],  loss: 11458450.000000, mae: 1266.503906, mean_q: -482.103058
 4484/5000: episode: 4484, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2817.595, mean reward: -2817.595 [-2817.595, -2817.595], mean action: 0.000 [0.000, 0.000],  loss: 8677054.000000, mae: 1162.146973, mean_q: -481.677551
 4485/5000: episode: 4485, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3117.843, mean reward: -3117.843 [-3117.843, -3117.843], mean action: 3.000 [3.000, 3.000],  loss: 8916234.000000, mae: 1223.768921, mean_q: -480.764771
 4486/5000: episode: 4486, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1093.426, mean reward: -1093.426 [-1093.426, -1093.426], mean action: 3.000 [3.000, 3.000],  loss: 7400119.500000, mae: 1127.651611, mean_q: -484.375458
 4487/5000: episode: 4487, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -518.356, mean reward: -518.356 [-518.356, -518.356], mean action: 3.000 [3.000, 3.000],  loss: 7309025.000000, mae: 1061.066895, mean_q: -484.164642
 4488/5000: episode: 4488, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5778.062, mean reward: -5778.062 [-5778.062, -5778.062], mean action: 3.000 [3.000, 3.000],  loss: 12069100.000000, mae: 1210.103271, mean_q: -481.616394
 4489/5000: episode: 4489, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2581.160, mean reward: -2581.160 [-2581.160, -2581.160], mean action: 3.000 [3.000, 3.000],  loss: 12636876.000000, mae: 1392.265869, mean_q: -481.698547
 4490/5000: episode: 4490, duration: 0.047s, episode steps:   1, steps per second:  22, episode reward: -911.286, mean reward: -911.286 [-911.286, -911.286], mean action: 3.000 [3.000, 3.000],  loss: 7907410.000000, mae: 1097.144287, mean_q: -483.411835
 4491/5000: episode: 4491, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1729.123, mean reward: -1729.123 [-1729.123, -1729.123], mean action: 3.000 [3.000, 3.000],  loss: 11197017.000000, mae: 1251.921021, mean_q: -483.405334
 4492/5000: episode: 4492, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6894.276, mean reward: -6894.276 [-6894.276, -6894.276], mean action: 3.000 [3.000, 3.000],  loss: 13754314.000000, mae: 1355.336914, mean_q: -483.640381
 4493/5000: episode: 4493, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1233.527, mean reward: -1233.527 [-1233.527, -1233.527], mean action: 3.000 [3.000, 3.000],  loss: 13628598.000000, mae: 1353.954712, mean_q: -484.484802
 4494/5000: episode: 4494, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1103.147, mean reward: -1103.147 [-1103.147, -1103.147], mean action: 3.000 [3.000, 3.000],  loss: 15109442.000000, mae: 1364.947754, mean_q: -483.785889
 4495/5000: episode: 4495, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1584.683, mean reward: -1584.683 [-1584.683, -1584.683], mean action: 3.000 [3.000, 3.000],  loss: 12206391.000000, mae: 1179.694092, mean_q: -484.258118
 4496/5000: episode: 4496, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6706.232, mean reward: -6706.232 [-6706.232, -6706.232], mean action: 3.000 [3.000, 3.000],  loss: 12913768.000000, mae: 1349.972900, mean_q: -483.398651
 4497/5000: episode: 4497, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -78.384, mean reward: -78.384 [-78.384, -78.384], mean action: 3.000 [3.000, 3.000],  loss: 9872748.000000, mae: 1203.066284, mean_q: -483.686707
 4498/5000: episode: 4498, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4093.060, mean reward: -4093.060 [-4093.060, -4093.060], mean action: 3.000 [3.000, 3.000],  loss: 11131759.000000, mae: 1275.912354, mean_q: -484.648834
 4499/5000: episode: 4499, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3564.561, mean reward: -3564.561 [-3564.561, -3564.561], mean action: 3.000 [3.000, 3.000],  loss: 10676569.000000, mae: 1195.258667, mean_q: -485.593140
 4500/5000: episode: 4500, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3725.130, mean reward: -3725.130 [-3725.130, -3725.130], mean action: 3.000 [3.000, 3.000],  loss: 14008438.000000, mae: 1327.332520, mean_q: -485.272095
 4501/5000: episode: 4501, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -402.463, mean reward: -402.463 [-402.463, -402.463], mean action: 3.000 [3.000, 3.000],  loss: 13435598.000000, mae: 1357.424194, mean_q: -486.520691
 4502/5000: episode: 4502, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1316.975, mean reward: -1316.975 [-1316.975, -1316.975], mean action: 3.000 [3.000, 3.000],  loss: 11053320.000000, mae: 1236.039062, mean_q: -485.773621
 4503/5000: episode: 4503, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1722.321, mean reward: -1722.321 [-1722.321, -1722.321], mean action: 3.000 [3.000, 3.000],  loss: 11364302.000000, mae: 1224.061768, mean_q: -485.614624
 4504/5000: episode: 4504, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -75.144, mean reward: -75.144 [-75.144, -75.144], mean action: 3.000 [3.000, 3.000],  loss: 6042765.500000, mae: 1101.272705, mean_q: -487.600677
 4505/5000: episode: 4505, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -775.892, mean reward: -775.892 [-775.892, -775.892], mean action: 3.000 [3.000, 3.000],  loss: 10015438.000000, mae: 1181.659546, mean_q: -489.536011
 4506/5000: episode: 4506, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4266.497, mean reward: -4266.497 [-4266.497, -4266.497], mean action: 3.000 [3.000, 3.000],  loss: 14072078.000000, mae: 1391.479858, mean_q: -486.313293
 4507/5000: episode: 4507, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -830.214, mean reward: -830.214 [-830.214, -830.214], mean action: 3.000 [3.000, 3.000],  loss: 13957430.000000, mae: 1409.872314, mean_q: -485.564575
 4508/5000: episode: 4508, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -6757.040, mean reward: -6757.040 [-6757.040, -6757.040], mean action: 3.000 [3.000, 3.000],  loss: 12970472.000000, mae: 1346.152832, mean_q: -485.194824
 4509/5000: episode: 4509, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3056.116, mean reward: -3056.116 [-3056.116, -3056.116], mean action: 3.000 [3.000, 3.000],  loss: 11117264.000000, mae: 1253.036621, mean_q: -488.600006
 4510/5000: episode: 4510, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -932.776, mean reward: -932.776 [-932.776, -932.776], mean action: 3.000 [3.000, 3.000],  loss: 9445880.000000, mae: 1210.020996, mean_q: -487.438721
 4511/5000: episode: 4511, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1454.574, mean reward: -1454.574 [-1454.574, -1454.574], mean action: 3.000 [3.000, 3.000],  loss: 11434171.000000, mae: 1293.657104, mean_q: -486.131653
 4512/5000: episode: 4512, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3161.984, mean reward: -3161.984 [-3161.984, -3161.984], mean action: 3.000 [3.000, 3.000],  loss: 9309387.000000, mae: 1215.280029, mean_q: -489.219543
 4513/5000: episode: 4513, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2746.808, mean reward: -2746.808 [-2746.808, -2746.808], mean action: 3.000 [3.000, 3.000],  loss: 9726195.000000, mae: 1201.208008, mean_q: -487.941589
 4514/5000: episode: 4514, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1299.224, mean reward: -1299.224 [-1299.224, -1299.224], mean action: 3.000 [3.000, 3.000],  loss: 7337471.000000, mae: 1086.433594, mean_q: -487.260345
 4515/5000: episode: 4515, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1191.394, mean reward: -1191.394 [-1191.394, -1191.394], mean action: 3.000 [3.000, 3.000],  loss: 7698708.500000, mae: 1113.851074, mean_q: -490.977173
 4516/5000: episode: 4516, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6285.796, mean reward: -6285.796 [-6285.796, -6285.796], mean action: 3.000 [3.000, 3.000],  loss: 9789872.000000, mae: 1206.123779, mean_q: -490.593140
 4517/5000: episode: 4517, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -1440.161, mean reward: -1440.161 [-1440.161, -1440.161], mean action: 3.000 [3.000, 3.000],  loss: 19834064.000000, mae: 1644.303711, mean_q: -487.628601
 4518/5000: episode: 4518, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5607.494, mean reward: -5607.494 [-5607.494, -5607.494], mean action: 3.000 [3.000, 3.000],  loss: 11114084.000000, mae: 1185.349365, mean_q: -488.242279
 4519/5000: episode: 4519, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -3027.487, mean reward: -3027.487 [-3027.487, -3027.487], mean action: 3.000 [3.000, 3.000],  loss: 13049618.000000, mae: 1356.623901, mean_q: -488.427124
 4520/5000: episode: 4520, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -372.999, mean reward: -372.999 [-372.999, -372.999], mean action: 3.000 [3.000, 3.000],  loss: 7728936.500000, mae: 1127.673584, mean_q: -489.885498
 4521/5000: episode: 4521, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -4931.818, mean reward: -4931.818 [-4931.818, -4931.818], mean action: 3.000 [3.000, 3.000],  loss: 7751601.000000, mae: 1109.244385, mean_q: -490.595886
 4522/5000: episode: 4522, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1143.824, mean reward: -1143.824 [-1143.824, -1143.824], mean action: 3.000 [3.000, 3.000],  loss: 6532322.000000, mae: 1031.212891, mean_q: -492.407104
 4523/5000: episode: 4523, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3454.171, mean reward: -3454.171 [-3454.171, -3454.171], mean action: 3.000 [3.000, 3.000],  loss: 11448070.000000, mae: 1280.206299, mean_q: -490.189941
 4524/5000: episode: 4524, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3566.120, mean reward: -3566.120 [-3566.120, -3566.120], mean action: 3.000 [3.000, 3.000],  loss: 16662284.000000, mae: 1544.143677, mean_q: -489.467621
 4525/5000: episode: 4525, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -8738.097, mean reward: -8738.097 [-8738.097, -8738.097], mean action: 3.000 [3.000, 3.000],  loss: 12974374.000000, mae: 1292.863159, mean_q: -489.859558
 4526/5000: episode: 4526, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1169.095, mean reward: -1169.095 [-1169.095, -1169.095], mean action: 3.000 [3.000, 3.000],  loss: 11100558.000000, mae: 1197.166748, mean_q: -489.985718
 4527/5000: episode: 4527, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1524.981, mean reward: -1524.981 [-1524.981, -1524.981], mean action: 3.000 [3.000, 3.000],  loss: 12213810.000000, mae: 1266.503174, mean_q: -491.646454
 4528/5000: episode: 4528, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5604.788, mean reward: -5604.788 [-5604.788, -5604.788], mean action: 3.000 [3.000, 3.000],  loss: 11751479.000000, mae: 1334.150146, mean_q: -492.199097
 4529/5000: episode: 4529, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -8230.621, mean reward: -8230.621 [-8230.621, -8230.621], mean action: 3.000 [3.000, 3.000],  loss: 14382418.000000, mae: 1429.461426, mean_q: -491.200500
 4530/5000: episode: 4530, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3751.815, mean reward: -3751.815 [-3751.815, -3751.815], mean action: 3.000 [3.000, 3.000],  loss: 16902468.000000, mae: 1565.679199, mean_q: -491.585876
 4531/5000: episode: 4531, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -8852.715, mean reward: -8852.715 [-8852.715, -8852.715], mean action: 3.000 [3.000, 3.000],  loss: 11234588.000000, mae: 1303.691528, mean_q: -491.970062
 4532/5000: episode: 4532, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5532.422, mean reward: -5532.422 [-5532.422, -5532.422], mean action: 3.000 [3.000, 3.000],  loss: 5962225.000000, mae: 1041.105469, mean_q: -491.191681
 4533/5000: episode: 4533, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6792.689, mean reward: -6792.689 [-6792.689, -6792.689], mean action: 3.000 [3.000, 3.000],  loss: 13685005.000000, mae: 1375.236084, mean_q: -493.509735
 4534/5000: episode: 4534, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1466.564, mean reward: -1466.564 [-1466.564, -1466.564], mean action: 3.000 [3.000, 3.000],  loss: 6431253.500000, mae: 1051.944824, mean_q: -494.510590
 4535/5000: episode: 4535, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4025.272, mean reward: -4025.272 [-4025.272, -4025.272], mean action: 3.000 [3.000, 3.000],  loss: 11824191.000000, mae: 1321.492432, mean_q: -493.622314
 4536/5000: episode: 4536, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -798.984, mean reward: -798.984 [-798.984, -798.984], mean action: 3.000 [3.000, 3.000],  loss: 8100735.000000, mae: 1083.485596, mean_q: -494.303162
 4537/5000: episode: 4537, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1867.310, mean reward: -1867.310 [-1867.310, -1867.310], mean action: 3.000 [3.000, 3.000],  loss: 10660292.000000, mae: 1265.688721, mean_q: -493.978455
 4538/5000: episode: 4538, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4865.885, mean reward: -4865.885 [-4865.885, -4865.885], mean action: 3.000 [3.000, 3.000],  loss: 13585802.000000, mae: 1369.952271, mean_q: -492.743073
 4539/5000: episode: 4539, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -6556.697, mean reward: -6556.697 [-6556.697, -6556.697], mean action: 3.000 [3.000, 3.000],  loss: 6112306.000000, mae: 1006.840454, mean_q: -494.527374
 4540/5000: episode: 4540, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3767.424, mean reward: -3767.424 [-3767.424, -3767.424], mean action: 3.000 [3.000, 3.000],  loss: 8649634.000000, mae: 1150.961426, mean_q: -493.926575
 4541/5000: episode: 4541, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -3743.277, mean reward: -3743.277 [-3743.277, -3743.277], mean action: 3.000 [3.000, 3.000],  loss: 16945280.000000, mae: 1542.824829, mean_q: -494.172424
 4542/5000: episode: 4542, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3178.187, mean reward: -3178.187 [-3178.187, -3178.187], mean action: 3.000 [3.000, 3.000],  loss: 11511891.000000, mae: 1256.897339, mean_q: -493.742401
 4543/5000: episode: 4543, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -830.530, mean reward: -830.530 [-830.530, -830.530], mean action: 3.000 [3.000, 3.000],  loss: 18636584.000000, mae: 1636.046753, mean_q: -492.369080
 4544/5000: episode: 4544, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -310.532, mean reward: -310.532 [-310.532, -310.532], mean action: 3.000 [3.000, 3.000],  loss: 10752224.000000, mae: 1244.472412, mean_q: -496.062378
 4545/5000: episode: 4545, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2639.977, mean reward: -2639.977 [-2639.977, -2639.977], mean action: 3.000 [3.000, 3.000],  loss: 9180540.000000, mae: 1203.786743, mean_q: -494.663849
 4546/5000: episode: 4546, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -1870.961, mean reward: -1870.961 [-1870.961, -1870.961], mean action: 3.000 [3.000, 3.000],  loss: 10271646.000000, mae: 1254.927979, mean_q: -495.014374
 4547/5000: episode: 4547, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1705.927, mean reward: -1705.927 [-1705.927, -1705.927], mean action: 3.000 [3.000, 3.000],  loss: 8866136.000000, mae: 1165.153320, mean_q: -494.475037
 4548/5000: episode: 4548, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1269.157, mean reward: -1269.157 [-1269.157, -1269.157], mean action: 3.000 [3.000, 3.000],  loss: 5968506.000000, mae: 963.373108, mean_q: -497.411072
 4549/5000: episode: 4549, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -7971.461, mean reward: -7971.461 [-7971.461, -7971.461], mean action: 3.000 [3.000, 3.000],  loss: 8346971.000000, mae: 1130.905762, mean_q: -495.452209
 4550/5000: episode: 4550, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8312.705, mean reward: -8312.705 [-8312.705, -8312.705], mean action: 3.000 [3.000, 3.000],  loss: 10395814.000000, mae: 1274.557373, mean_q: -497.106628
 4551/5000: episode: 4551, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1942.588, mean reward: -1942.588 [-1942.588, -1942.588], mean action: 3.000 [3.000, 3.000],  loss: 6357466.500000, mae: 1012.400757, mean_q: -497.725159
 4552/5000: episode: 4552, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4901.037, mean reward: -4901.037 [-4901.037, -4901.037], mean action: 3.000 [3.000, 3.000],  loss: 14750519.000000, mae: 1380.190796, mean_q: -496.662354
 4553/5000: episode: 4553, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -359.897, mean reward: -359.897 [-359.897, -359.897], mean action: 3.000 [3.000, 3.000],  loss: 13045435.000000, mae: 1267.562988, mean_q: -497.015320
 4554/5000: episode: 4554, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5046.462, mean reward: -5046.462 [-5046.462, -5046.462], mean action: 3.000 [3.000, 3.000],  loss: 10326554.000000, mae: 1187.368530, mean_q: -497.669678
 4555/5000: episode: 4555, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5592.121, mean reward: -5592.121 [-5592.121, -5592.121], mean action: 3.000 [3.000, 3.000],  loss: 8107275.500000, mae: 1207.846436, mean_q: -495.889099
 4556/5000: episode: 4556, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1505.601, mean reward: -1505.601 [-1505.601, -1505.601], mean action: 3.000 [3.000, 3.000],  loss: 7356144.000000, mae: 1139.152466, mean_q: -497.814392
 4557/5000: episode: 4557, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -223.001, mean reward: -223.001 [-223.001, -223.001], mean action: 3.000 [3.000, 3.000],  loss: 11578368.000000, mae: 1389.393555, mean_q: -499.053955
 4558/5000: episode: 4558, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6.039, mean reward: -6.039 [-6.039, -6.039], mean action: 3.000 [3.000, 3.000],  loss: 10357647.000000, mae: 1211.127319, mean_q: -498.164795
 4559/5000: episode: 4559, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6295.087, mean reward: -6295.087 [-6295.087, -6295.087], mean action: 3.000 [3.000, 3.000],  loss: 10596884.000000, mae: 1224.405273, mean_q: -498.948334
 4560/5000: episode: 4560, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6638.429, mean reward: -6638.429 [-6638.429, -6638.429], mean action: 3.000 [3.000, 3.000],  loss: 9897362.000000, mae: 1234.722412, mean_q: -498.186157
 4561/5000: episode: 4561, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3857.771, mean reward: -3857.771 [-3857.771, -3857.771], mean action: 3.000 [3.000, 3.000],  loss: 17468368.000000, mae: 1599.444092, mean_q: -495.849548
 4562/5000: episode: 4562, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2995.145, mean reward: -2995.145 [-2995.145, -2995.145], mean action: 3.000 [3.000, 3.000],  loss: 12877596.000000, mae: 1367.012207, mean_q: -499.560059
 4563/5000: episode: 4563, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -708.237, mean reward: -708.237 [-708.237, -708.237], mean action: 3.000 [3.000, 3.000],  loss: 11609510.000000, mae: 1280.010742, mean_q: -499.853943
 4564/5000: episode: 4564, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5342.555, mean reward: -5342.555 [-5342.555, -5342.555], mean action: 3.000 [3.000, 3.000],  loss: 11302739.000000, mae: 1256.003296, mean_q: -500.754150
 4565/5000: episode: 4565, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7635.248, mean reward: -7635.248 [-7635.248, -7635.248], mean action: 3.000 [3.000, 3.000],  loss: 12769450.000000, mae: 1282.821777, mean_q: -500.149048
 4566/5000: episode: 4566, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -267.515, mean reward: -267.515 [-267.515, -267.515], mean action: 3.000 [3.000, 3.000],  loss: 11404946.000000, mae: 1209.089478, mean_q: -499.029327
 4567/5000: episode: 4567, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -947.787, mean reward: -947.787 [-947.787, -947.787], mean action: 3.000 [3.000, 3.000],  loss: 8743235.000000, mae: 1181.537231, mean_q: -499.784729
 4568/5000: episode: 4568, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -7489.024, mean reward: -7489.024 [-7489.024, -7489.024], mean action: 3.000 [3.000, 3.000],  loss: 11842458.000000, mae: 1362.417969, mean_q: -500.093018
 4569/5000: episode: 4569, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2163.170, mean reward: -2163.170 [-2163.170, -2163.170], mean action: 3.000 [3.000, 3.000],  loss: 5036466.500000, mae: 960.512329, mean_q: -503.753845
 4570/5000: episode: 4570, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2301.778, mean reward: -2301.778 [-2301.778, -2301.778], mean action: 3.000 [3.000, 3.000],  loss: 11039878.000000, mae: 1292.522705, mean_q: -500.046417
 4571/5000: episode: 4571, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3408.651, mean reward: -3408.651 [-3408.651, -3408.651], mean action: 3.000 [3.000, 3.000],  loss: 9793636.000000, mae: 1231.652344, mean_q: -501.869598
 4572/5000: episode: 4572, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -452.046, mean reward: -452.046 [-452.046, -452.046], mean action: 3.000 [3.000, 3.000],  loss: 10686331.000000, mae: 1298.943359, mean_q: -500.921600
 4573/5000: episode: 4573, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3737.834, mean reward: -3737.834 [-3737.834, -3737.834], mean action: 3.000 [3.000, 3.000],  loss: 9502529.000000, mae: 1186.645508, mean_q: -501.653625
 4574/5000: episode: 4574, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3082.241, mean reward: -3082.241 [-3082.241, -3082.241], mean action: 3.000 [3.000, 3.000],  loss: 14762790.000000, mae: 1366.968872, mean_q: -501.485535
 4575/5000: episode: 4575, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5590.984, mean reward: -5590.984 [-5590.984, -5590.984], mean action: 3.000 [3.000, 3.000],  loss: 10337380.000000, mae: 1200.387939, mean_q: -501.089600
 4576/5000: episode: 4576, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1957.925, mean reward: -1957.925 [-1957.925, -1957.925], mean action: 3.000 [3.000, 3.000],  loss: 5078269.500000, mae: 956.076843, mean_q: -504.038483
 4577/5000: episode: 4577, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -10087.507, mean reward: -10087.507 [-10087.507, -10087.507], mean action: 3.000 [3.000, 3.000],  loss: 9498914.000000, mae: 1186.346313, mean_q: -502.937744
 4578/5000: episode: 4578, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -11858.875, mean reward: -11858.875 [-11858.875, -11858.875], mean action: 3.000 [3.000, 3.000],  loss: 14203206.000000, mae: 1449.554199, mean_q: -503.523529
 4579/5000: episode: 4579, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2554.349, mean reward: -2554.349 [-2554.349, -2554.349], mean action: 3.000 [3.000, 3.000],  loss: 12429340.000000, mae: 1355.261230, mean_q: -504.550232
 4580/5000: episode: 4580, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2774.108, mean reward: -2774.108 [-2774.108, -2774.108], mean action: 2.000 [2.000, 2.000],  loss: 10080058.000000, mae: 1245.598877, mean_q: -502.156250
 4581/5000: episode: 4581, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6871.475, mean reward: -6871.475 [-6871.475, -6871.475], mean action: 3.000 [3.000, 3.000],  loss: 13071346.000000, mae: 1254.804688, mean_q: -502.781250
 4582/5000: episode: 4582, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3836.683, mean reward: -3836.683 [-3836.683, -3836.683], mean action: 2.000 [2.000, 2.000],  loss: 8688644.000000, mae: 1112.207520, mean_q: -503.320496
 4583/5000: episode: 4583, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6491.855, mean reward: -6491.855 [-6491.855, -6491.855], mean action: 2.000 [2.000, 2.000],  loss: 6987379.000000, mae: 1067.319580, mean_q: -506.433167
 4584/5000: episode: 4584, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1791.638, mean reward: -1791.638 [-1791.638, -1791.638], mean action: 2.000 [2.000, 2.000],  loss: 15470886.000000, mae: 1411.471313, mean_q: -503.406281
 4585/5000: episode: 4585, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -178.886, mean reward: -178.886 [-178.886, -178.886], mean action: 2.000 [2.000, 2.000],  loss: 10357599.000000, mae: 1298.928223, mean_q: -504.557892
 4586/5000: episode: 4586, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -163.084, mean reward: -163.084 [-163.084, -163.084], mean action: 2.000 [2.000, 2.000],  loss: 10678348.000000, mae: 1284.537842, mean_q: -503.801880
 4587/5000: episode: 4587, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1090.456, mean reward: -1090.456 [-1090.456, -1090.456], mean action: 2.000 [2.000, 2.000],  loss: 9821578.000000, mae: 1235.247437, mean_q: -505.203979
 4588/5000: episode: 4588, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -514.366, mean reward: -514.366 [-514.366, -514.366], mean action: 2.000 [2.000, 2.000],  loss: 8836270.000000, mae: 1170.310791, mean_q: -504.889954
 4589/5000: episode: 4589, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2717.567, mean reward: -2717.567 [-2717.567, -2717.567], mean action: 2.000 [2.000, 2.000],  loss: 6362179.500000, mae: 1033.467773, mean_q: -506.543884
 4590/5000: episode: 4590, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1252.223, mean reward: -1252.223 [-1252.223, -1252.223], mean action: 2.000 [2.000, 2.000],  loss: 10643099.000000, mae: 1192.841309, mean_q: -506.264740
 4591/5000: episode: 4591, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5214.981, mean reward: -5214.981 [-5214.981, -5214.981], mean action: 2.000 [2.000, 2.000],  loss: 9645222.000000, mae: 1169.189331, mean_q: -506.037659
 4592/5000: episode: 4592, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1927.882, mean reward: -1927.882 [-1927.882, -1927.882], mean action: 2.000 [2.000, 2.000],  loss: 8421765.000000, mae: 1172.585693, mean_q: -506.198853
 4593/5000: episode: 4593, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3061.650, mean reward: -3061.650 [-3061.650, -3061.650], mean action: 3.000 [3.000, 3.000],  loss: 9152839.000000, mae: 1235.097778, mean_q: -506.118225
 4594/5000: episode: 4594, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4895.066, mean reward: -4895.066 [-4895.066, -4895.066], mean action: 3.000 [3.000, 3.000],  loss: 13688944.000000, mae: 1304.114258, mean_q: -507.319733
 4595/5000: episode: 4595, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -647.080, mean reward: -647.080 [-647.080, -647.080], mean action: 3.000 [3.000, 3.000],  loss: 15010784.000000, mae: 1503.733276, mean_q: -506.272125
 4596/5000: episode: 4596, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1648.452, mean reward: -1648.452 [-1648.452, -1648.452], mean action: 2.000 [2.000, 2.000],  loss: 10144734.000000, mae: 1235.002686, mean_q: -507.843506
 4597/5000: episode: 4597, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6383.011, mean reward: -6383.011 [-6383.011, -6383.011], mean action: 0.000 [0.000, 0.000],  loss: 11460628.000000, mae: 1361.456543, mean_q: -506.879089
 4598/5000: episode: 4598, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -12664.798, mean reward: -12664.798 [-12664.798, -12664.798], mean action: 3.000 [3.000, 3.000],  loss: 9021340.000000, mae: 1203.182007, mean_q: -509.373840
 4599/5000: episode: 4599, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1912.826, mean reward: -1912.826 [-1912.826, -1912.826], mean action: 2.000 [2.000, 2.000],  loss: 9578288.000000, mae: 1196.755859, mean_q: -509.565521
 4600/5000: episode: 4600, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6851.145, mean reward: -6851.145 [-6851.145, -6851.145], mean action: 0.000 [0.000, 0.000],  loss: 10002412.000000, mae: 1213.407715, mean_q: -508.530212
 4601/5000: episode: 4601, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -543.743, mean reward: -543.743 [-543.743, -543.743], mean action: 2.000 [2.000, 2.000],  loss: 15582704.000000, mae: 1520.670898, mean_q: -505.453522
 4602/5000: episode: 4602, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -4365.111, mean reward: -4365.111 [-4365.111, -4365.111], mean action: 3.000 [3.000, 3.000],  loss: 9080706.000000, mae: 1168.794067, mean_q: -508.000122
 4603/5000: episode: 4603, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3417.020, mean reward: -3417.020 [-3417.020, -3417.020], mean action: 2.000 [2.000, 2.000],  loss: 7229843.000000, mae: 1144.892822, mean_q: -508.817230
 4604/5000: episode: 4604, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2118.960, mean reward: -2118.960 [-2118.960, -2118.960], mean action: 2.000 [2.000, 2.000],  loss: 14114192.000000, mae: 1438.603027, mean_q: -509.392975
 4605/5000: episode: 4605, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -84.684, mean reward: -84.684 [-84.684, -84.684], mean action: 2.000 [2.000, 2.000],  loss: 11975426.000000, mae: 1414.086792, mean_q: -509.236389
 4606/5000: episode: 4606, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -291.592, mean reward: -291.592 [-291.592, -291.592], mean action: 2.000 [2.000, 2.000],  loss: 8759032.000000, mae: 1197.404785, mean_q: -509.287476
 4607/5000: episode: 4607, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1763.297, mean reward: -1763.297 [-1763.297, -1763.297], mean action: 2.000 [2.000, 2.000],  loss: 11312847.000000, mae: 1323.777100, mean_q: -508.080719
 4608/5000: episode: 4608, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -4035.277, mean reward: -4035.277 [-4035.277, -4035.277], mean action: 3.000 [3.000, 3.000],  loss: 11378224.000000, mae: 1288.943359, mean_q: -509.835876
 4609/5000: episode: 4609, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8202.213, mean reward: -8202.213 [-8202.213, -8202.213], mean action: 3.000 [3.000, 3.000],  loss: 11047943.000000, mae: 1248.518433, mean_q: -507.736267
 4610/5000: episode: 4610, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2959.667, mean reward: -2959.667 [-2959.667, -2959.667], mean action: 3.000 [3.000, 3.000],  loss: 12850342.000000, mae: 1353.595459, mean_q: -509.171875
 4611/5000: episode: 4611, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1479.917, mean reward: -1479.917 [-1479.917, -1479.917], mean action: 2.000 [2.000, 2.000],  loss: 14145298.000000, mae: 1425.660767, mean_q: -509.309601
 4612/5000: episode: 4612, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1232.950, mean reward: -1232.950 [-1232.950, -1232.950], mean action: 2.000 [2.000, 2.000],  loss: 13649886.000000, mae: 1537.112061, mean_q: -508.912048
 4613/5000: episode: 4613, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1925.621, mean reward: -1925.621 [-1925.621, -1925.621], mean action: 2.000 [2.000, 2.000],  loss: 12673470.000000, mae: 1248.062012, mean_q: -512.026855
 4614/5000: episode: 4614, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -247.720, mean reward: -247.720 [-247.720, -247.720], mean action: 2.000 [2.000, 2.000],  loss: 13868334.000000, mae: 1335.783203, mean_q: -510.580505
 4615/5000: episode: 4615, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7542.741, mean reward: -7542.741 [-7542.741, -7542.741], mean action: 3.000 [3.000, 3.000],  loss: 6820273.500000, mae: 1092.659668, mean_q: -513.937683
 4616/5000: episode: 4616, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3240.282, mean reward: -3240.282 [-3240.282, -3240.282], mean action: 2.000 [2.000, 2.000],  loss: 7270288.000000, mae: 1077.592651, mean_q: -513.381836
 4617/5000: episode: 4617, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6557.726, mean reward: -6557.726 [-6557.726, -6557.726], mean action: 3.000 [3.000, 3.000],  loss: 16753624.000000, mae: 1567.430054, mean_q: -510.559631
 4618/5000: episode: 4618, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -76.267, mean reward: -76.267 [-76.267, -76.267], mean action: 2.000 [2.000, 2.000],  loss: 14914860.000000, mae: 1410.228394, mean_q: -512.288330
 4619/5000: episode: 4619, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1975.756, mean reward: -1975.756 [-1975.756, -1975.756], mean action: 2.000 [2.000, 2.000],  loss: 9814719.000000, mae: 1254.348389, mean_q: -511.865540
 4620/5000: episode: 4620, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1210.711, mean reward: -1210.711 [-1210.711, -1210.711], mean action: 2.000 [2.000, 2.000],  loss: 11396082.000000, mae: 1283.184937, mean_q: -512.018555
 4621/5000: episode: 4621, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3729.357, mean reward: -3729.357 [-3729.357, -3729.357], mean action: 2.000 [2.000, 2.000],  loss: 12389443.000000, mae: 1280.546509, mean_q: -513.866211
 4622/5000: episode: 4622, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2335.184, mean reward: -2335.184 [-2335.184, -2335.184], mean action: 2.000 [2.000, 2.000],  loss: 9353472.000000, mae: 1231.008789, mean_q: -509.994446
 4623/5000: episode: 4623, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1754.102, mean reward: -1754.102 [-1754.102, -1754.102], mean action: 2.000 [2.000, 2.000],  loss: 12241748.000000, mae: 1293.498291, mean_q: -512.788208
 4624/5000: episode: 4624, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5059.619, mean reward: -5059.619 [-5059.619, -5059.619], mean action: 2.000 [2.000, 2.000],  loss: 10017230.000000, mae: 1212.212158, mean_q: -513.883423
 4625/5000: episode: 4625, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -664.335, mean reward: -664.335 [-664.335, -664.335], mean action: 2.000 [2.000, 2.000],  loss: 16428242.000000, mae: 1426.519531, mean_q: -512.544006
 4626/5000: episode: 4626, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1809.470, mean reward: -1809.470 [-1809.470, -1809.470], mean action: 2.000 [2.000, 2.000],  loss: 7435477.500000, mae: 1142.207764, mean_q: -514.075684
 4627/5000: episode: 4627, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -627.056, mean reward: -627.056 [-627.056, -627.056], mean action: 2.000 [2.000, 2.000],  loss: 8531072.000000, mae: 1175.219727, mean_q: -512.997559
 4628/5000: episode: 4628, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -414.879, mean reward: -414.879 [-414.879, -414.879], mean action: 2.000 [2.000, 2.000],  loss: 10146680.000000, mae: 1290.628296, mean_q: -513.509216
 4629/5000: episode: 4629, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1747.020, mean reward: -1747.020 [-1747.020, -1747.020], mean action: 2.000 [2.000, 2.000],  loss: 9597930.000000, mae: 1234.822510, mean_q: -514.986328
 4630/5000: episode: 4630, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -999.182, mean reward: -999.182 [-999.182, -999.182], mean action: 2.000 [2.000, 2.000],  loss: 9930184.000000, mae: 1226.010620, mean_q: -515.104858
 4631/5000: episode: 4631, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -882.708, mean reward: -882.708 [-882.708, -882.708], mean action: 2.000 [2.000, 2.000],  loss: 6902558.000000, mae: 1085.479004, mean_q: -516.301514
 4632/5000: episode: 4632, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3556.608, mean reward: -3556.608 [-3556.608, -3556.608], mean action: 2.000 [2.000, 2.000],  loss: 13685934.000000, mae: 1414.977051, mean_q: -514.228516
 4633/5000: episode: 4633, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2959.594, mean reward: -2959.594 [-2959.594, -2959.594], mean action: 2.000 [2.000, 2.000],  loss: 5795018.000000, mae: 1063.715576, mean_q: -515.578491
 4634/5000: episode: 4634, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3790.944, mean reward: -3790.944 [-3790.944, -3790.944], mean action: 3.000 [3.000, 3.000],  loss: 4686359.000000, mae: 943.655762, mean_q: -517.109619
 4635/5000: episode: 4635, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -126.730, mean reward: -126.730 [-126.730, -126.730], mean action: 2.000 [2.000, 2.000],  loss: 7630935.000000, mae: 1165.826904, mean_q: -516.669067
 4636/5000: episode: 4636, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2559.949, mean reward: -2559.949 [-2559.949, -2559.949], mean action: 2.000 [2.000, 2.000],  loss: 9397324.000000, mae: 1202.439575, mean_q: -516.159912
 4637/5000: episode: 4637, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3217.412, mean reward: -3217.412 [-3217.412, -3217.412], mean action: 2.000 [2.000, 2.000],  loss: 11553685.000000, mae: 1296.948730, mean_q: -518.026855
 4638/5000: episode: 4638, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2558.708, mean reward: -2558.708 [-2558.708, -2558.708], mean action: 2.000 [2.000, 2.000],  loss: 9833147.000000, mae: 1250.020264, mean_q: -517.418823
 4639/5000: episode: 4639, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1649.852, mean reward: -1649.852 [-1649.852, -1649.852], mean action: 2.000 [2.000, 2.000],  loss: 9091990.000000, mae: 1142.468018, mean_q: -517.774902
 4640/5000: episode: 4640, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3184.372, mean reward: -3184.372 [-3184.372, -3184.372], mean action: 3.000 [3.000, 3.000],  loss: 17917250.000000, mae: 1542.694702, mean_q: -519.732849
 4641/5000: episode: 4641, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4151.734, mean reward: -4151.734 [-4151.734, -4151.734], mean action: 2.000 [2.000, 2.000],  loss: 8823221.000000, mae: 1113.716309, mean_q: -519.337463
 4642/5000: episode: 4642, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1183.883, mean reward: -1183.883 [-1183.883, -1183.883], mean action: 2.000 [2.000, 2.000],  loss: 10079526.000000, mae: 1253.243408, mean_q: -519.635864
 4643/5000: episode: 4643, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1054.086, mean reward: -1054.086 [-1054.086, -1054.086], mean action: 2.000 [2.000, 2.000],  loss: 11769352.000000, mae: 1329.158447, mean_q: -517.735107
 4644/5000: episode: 4644, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2133.765, mean reward: -2133.765 [-2133.765, -2133.765], mean action: 2.000 [2.000, 2.000],  loss: 15013410.000000, mae: 1456.517944, mean_q: -519.156128
 4645/5000: episode: 4645, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2047.488, mean reward: -2047.488 [-2047.488, -2047.488], mean action: 2.000 [2.000, 2.000],  loss: 7168091.000000, mae: 1160.742676, mean_q: -518.623657
 4646/5000: episode: 4646, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1659.116, mean reward: -1659.116 [-1659.116, -1659.116], mean action: 2.000 [2.000, 2.000],  loss: 9497455.000000, mae: 1213.449951, mean_q: -518.645630
 4647/5000: episode: 4647, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -691.464, mean reward: -691.464 [-691.464, -691.464], mean action: 2.000 [2.000, 2.000],  loss: 17308556.000000, mae: 1429.247070, mean_q: -520.363647
 4648/5000: episode: 4648, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4341.626, mean reward: -4341.626 [-4341.626, -4341.626], mean action: 2.000 [2.000, 2.000],  loss: 8937052.000000, mae: 1210.640869, mean_q: -521.209473
 4649/5000: episode: 4649, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1015.220, mean reward: -1015.220 [-1015.220, -1015.220], mean action: 2.000 [2.000, 2.000],  loss: 13275250.000000, mae: 1358.564453, mean_q: -519.256836
 4650/5000: episode: 4650, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1745.537, mean reward: -1745.537 [-1745.537, -1745.537], mean action: 2.000 [2.000, 2.000],  loss: 7303906.000000, mae: 1111.802246, mean_q: -521.361755
 4651/5000: episode: 4651, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1883.148, mean reward: -1883.148 [-1883.148, -1883.148], mean action: 2.000 [2.000, 2.000],  loss: 12695155.000000, mae: 1345.123779, mean_q: -520.783203
 4652/5000: episode: 4652, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1622.670, mean reward: -1622.670 [-1622.670, -1622.670], mean action: 2.000 [2.000, 2.000],  loss: 13837186.000000, mae: 1379.691528, mean_q: -520.566406
 4653/5000: episode: 4653, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -250.837, mean reward: -250.837 [-250.837, -250.837], mean action: 2.000 [2.000, 2.000],  loss: 11679947.000000, mae: 1363.795288, mean_q: -520.476807
 4654/5000: episode: 4654, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -489.712, mean reward: -489.712 [-489.712, -489.712], mean action: 2.000 [2.000, 2.000],  loss: 11271039.000000, mae: 1256.721069, mean_q: -520.884644
 4655/5000: episode: 4655, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1185.943, mean reward: -1185.943 [-1185.943, -1185.943], mean action: 2.000 [2.000, 2.000],  loss: 9671246.000000, mae: 1250.594727, mean_q: -519.887207
 4656/5000: episode: 4656, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3915.488, mean reward: -3915.488 [-3915.488, -3915.488], mean action: 2.000 [2.000, 2.000],  loss: 8452188.000000, mae: 1138.262451, mean_q: -522.482300
 4657/5000: episode: 4657, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4935.577, mean reward: -4935.577 [-4935.577, -4935.577], mean action: 2.000 [2.000, 2.000],  loss: 7581289.000000, mae: 1108.692871, mean_q: -520.694153
 4658/5000: episode: 4658, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -960.995, mean reward: -960.995 [-960.995, -960.995], mean action: 2.000 [2.000, 2.000],  loss: 14000196.000000, mae: 1343.203003, mean_q: -521.028809
 4659/5000: episode: 4659, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -969.167, mean reward: -969.167 [-969.167, -969.167], mean action: 2.000 [2.000, 2.000],  loss: 9424484.000000, mae: 1239.219482, mean_q: -522.677124
 4660/5000: episode: 4660, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2577.492, mean reward: -2577.492 [-2577.492, -2577.492], mean action: 2.000 [2.000, 2.000],  loss: 17387496.000000, mae: 1503.813477, mean_q: -520.894287
 4661/5000: episode: 4661, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1032.872, mean reward: -1032.872 [-1032.872, -1032.872], mean action: 2.000 [2.000, 2.000],  loss: 9463900.000000, mae: 1230.251465, mean_q: -520.302368
 4662/5000: episode: 4662, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1405.669, mean reward: -1405.669 [-1405.669, -1405.669], mean action: 2.000 [2.000, 2.000],  loss: 10084025.000000, mae: 1258.253174, mean_q: -521.983032
 4663/5000: episode: 4663, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1898.040, mean reward: -1898.040 [-1898.040, -1898.040], mean action: 2.000 [2.000, 2.000],  loss: 12388720.000000, mae: 1435.000122, mean_q: -521.357178
 4664/5000: episode: 4664, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4560.043, mean reward: -4560.043 [-4560.043, -4560.043], mean action: 2.000 [2.000, 2.000],  loss: 10064314.000000, mae: 1263.988525, mean_q: -520.962219
 4665/5000: episode: 4665, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4205.790, mean reward: -4205.790 [-4205.790, -4205.790], mean action: 2.000 [2.000, 2.000],  loss: 5330718.000000, mae: 991.705688, mean_q: -525.402710
 4666/5000: episode: 4666, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5974.066, mean reward: -5974.066 [-5974.066, -5974.066], mean action: 2.000 [2.000, 2.000],  loss: 11342512.000000, mae: 1201.309814, mean_q: -523.719788
 4667/5000: episode: 4667, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1961.122, mean reward: -1961.122 [-1961.122, -1961.122], mean action: 2.000 [2.000, 2.000],  loss: 9411929.000000, mae: 1180.034180, mean_q: -521.875610
 4668/5000: episode: 4668, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -337.540, mean reward: -337.540 [-337.540, -337.540], mean action: 2.000 [2.000, 2.000],  loss: 7653201.000000, mae: 1122.680786, mean_q: -525.429260
 4669/5000: episode: 4669, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1966.490, mean reward: -1966.490 [-1966.490, -1966.490], mean action: 3.000 [3.000, 3.000],  loss: 10626977.000000, mae: 1214.071289, mean_q: -524.370972
 4670/5000: episode: 4670, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -783.428, mean reward: -783.428 [-783.428, -783.428], mean action: 2.000 [2.000, 2.000],  loss: 9111274.000000, mae: 1189.781738, mean_q: -524.959106
 4671/5000: episode: 4671, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2854.658, mean reward: -2854.658 [-2854.658, -2854.658], mean action: 2.000 [2.000, 2.000],  loss: 9084781.000000, mae: 1242.234619, mean_q: -524.928711
 4672/5000: episode: 4672, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -71.990, mean reward: -71.990 [-71.990, -71.990], mean action: 2.000 [2.000, 2.000],  loss: 6627192.500000, mae: 1121.055176, mean_q: -524.043213
 4673/5000: episode: 4673, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -510.785, mean reward: -510.785 [-510.785, -510.785], mean action: 2.000 [2.000, 2.000],  loss: 8625229.000000, mae: 1266.350952, mean_q: -525.361450
 4674/5000: episode: 4674, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -833.313, mean reward: -833.313 [-833.313, -833.313], mean action: 2.000 [2.000, 2.000],  loss: 14407055.000000, mae: 1371.166748, mean_q: -523.525513
 4675/5000: episode: 4675, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12831.428, mean reward: -12831.428 [-12831.428, -12831.428], mean action: 0.000 [0.000, 0.000],  loss: 7608571.000000, mae: 1127.370605, mean_q: -526.320435
 4676/5000: episode: 4676, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2592.052, mean reward: -2592.052 [-2592.052, -2592.052], mean action: 0.000 [0.000, 0.000],  loss: 12744700.000000, mae: 1421.542847, mean_q: -525.452026
 4677/5000: episode: 4677, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -11761.232, mean reward: -11761.232 [-11761.232, -11761.232], mean action: 0.000 [0.000, 0.000],  loss: 9724443.000000, mae: 1250.385254, mean_q: -525.924744
 4678/5000: episode: 4678, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -9109.796, mean reward: -9109.796 [-9109.796, -9109.796], mean action: 0.000 [0.000, 0.000],  loss: 11967909.000000, mae: 1252.888672, mean_q: -527.676270
 4679/5000: episode: 4679, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9480.549, mean reward: -9480.549 [-9480.549, -9480.549], mean action: 0.000 [0.000, 0.000],  loss: 6793788.000000, mae: 1129.998535, mean_q: -529.179321
 4680/5000: episode: 4680, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3651.453, mean reward: -3651.453 [-3651.453, -3651.453], mean action: 0.000 [0.000, 0.000],  loss: 9252477.000000, mae: 1201.536133, mean_q: -526.032959
 4681/5000: episode: 4681, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -14108.080, mean reward: -14108.080 [-14108.080, -14108.080], mean action: 0.000 [0.000, 0.000],  loss: 4896616.000000, mae: 974.812500, mean_q: -528.390015
 4682/5000: episode: 4682, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4776.554, mean reward: -4776.554 [-4776.554, -4776.554], mean action: 0.000 [0.000, 0.000],  loss: 10238983.000000, mae: 1140.853271, mean_q: -526.716309
 4683/5000: episode: 4683, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -15619.573, mean reward: -15619.573 [-15619.573, -15619.573], mean action: 0.000 [0.000, 0.000],  loss: 4900295.000000, mae: 949.918579, mean_q: -529.218872
 4684/5000: episode: 4684, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5061.394, mean reward: -5061.394 [-5061.394, -5061.394], mean action: 0.000 [0.000, 0.000],  loss: 5660665.500000, mae: 1088.454590, mean_q: -529.134155
 4685/5000: episode: 4685, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3222.254, mean reward: -3222.254 [-3222.254, -3222.254], mean action: 0.000 [0.000, 0.000],  loss: 12923243.000000, mae: 1249.181030, mean_q: -528.439819
 4686/5000: episode: 4686, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6528.421, mean reward: -6528.421 [-6528.421, -6528.421], mean action: 0.000 [0.000, 0.000],  loss: 11477103.000000, mae: 1326.904297, mean_q: -526.031494
 4687/5000: episode: 4687, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10953.966, mean reward: -10953.966 [-10953.966, -10953.966], mean action: 0.000 [0.000, 0.000],  loss: 18011008.000000, mae: 1571.937744, mean_q: -527.954651
 4688/5000: episode: 4688, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -11574.977, mean reward: -11574.977 [-11574.977, -11574.977], mean action: 0.000 [0.000, 0.000],  loss: 6891062.000000, mae: 1106.820557, mean_q: -529.385742
 4689/5000: episode: 4689, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9672.932, mean reward: -9672.932 [-9672.932, -9672.932], mean action: 0.000 [0.000, 0.000],  loss: 14642510.000000, mae: 1480.276978, mean_q: -529.914124
 4690/5000: episode: 4690, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -13083.936, mean reward: -13083.936 [-13083.936, -13083.936], mean action: 0.000 [0.000, 0.000],  loss: 9853132.000000, mae: 1250.731323, mean_q: -526.546143
 4691/5000: episode: 4691, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6967.043, mean reward: -6967.043 [-6967.043, -6967.043], mean action: 0.000 [0.000, 0.000],  loss: 13069846.000000, mae: 1403.783691, mean_q: -526.675720
 4692/5000: episode: 4692, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1442.225, mean reward: -1442.225 [-1442.225, -1442.225], mean action: 0.000 [0.000, 0.000],  loss: 7849335.000000, mae: 1174.561523, mean_q: -530.919861
 4693/5000: episode: 4693, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6756.862, mean reward: -6756.862 [-6756.862, -6756.862], mean action: 0.000 [0.000, 0.000],  loss: 12113853.000000, mae: 1337.348389, mean_q: -529.063965
 4694/5000: episode: 4694, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7515.642, mean reward: -7515.642 [-7515.642, -7515.642], mean action: 0.000 [0.000, 0.000],  loss: 7866274.500000, mae: 1051.141968, mean_q: -528.127258
 4695/5000: episode: 4695, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6814.322, mean reward: -6814.322 [-6814.322, -6814.322], mean action: 0.000 [0.000, 0.000],  loss: 9860024.000000, mae: 1198.046631, mean_q: -531.314331
 4696/5000: episode: 4696, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4675.687, mean reward: -4675.687 [-4675.687, -4675.687], mean action: 0.000 [0.000, 0.000],  loss: 12769468.000000, mae: 1390.490479, mean_q: -529.160767
 4697/5000: episode: 4697, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4094.953, mean reward: -4094.953 [-4094.953, -4094.953], mean action: 0.000 [0.000, 0.000],  loss: 7627276.000000, mae: 1102.436401, mean_q: -530.921021
 4698/5000: episode: 4698, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4995.882, mean reward: -4995.882 [-4995.882, -4995.882], mean action: 0.000 [0.000, 0.000],  loss: 8740750.000000, mae: 1152.278931, mean_q: -531.453369
 4699/5000: episode: 4699, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5627.821, mean reward: -5627.821 [-5627.821, -5627.821], mean action: 0.000 [0.000, 0.000],  loss: 10646667.000000, mae: 1235.693481, mean_q: -528.834778
 4700/5000: episode: 4700, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8988.963, mean reward: -8988.963 [-8988.963, -8988.963], mean action: 0.000 [0.000, 0.000],  loss: 7806677.500000, mae: 1168.466797, mean_q: -530.421753
 4701/5000: episode: 4701, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5866.514, mean reward: -5866.514 [-5866.514, -5866.514], mean action: 0.000 [0.000, 0.000],  loss: 17120572.000000, mae: 1586.810303, mean_q: -529.557617
 4702/5000: episode: 4702, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5341.100, mean reward: -5341.100 [-5341.100, -5341.100], mean action: 0.000 [0.000, 0.000],  loss: 7034488.000000, mae: 1165.019531, mean_q: -531.884766
 4703/5000: episode: 4703, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5094.341, mean reward: -5094.341 [-5094.341, -5094.341], mean action: 0.000 [0.000, 0.000],  loss: 12846965.000000, mae: 1391.015381, mean_q: -533.223633
 4704/5000: episode: 4704, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -7037.778, mean reward: -7037.778 [-7037.778, -7037.778], mean action: 0.000 [0.000, 0.000],  loss: 9863714.000000, mae: 1273.893555, mean_q: -532.395630
 4705/5000: episode: 4705, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -10151.725, mean reward: -10151.725 [-10151.725, -10151.725], mean action: 0.000 [0.000, 0.000],  loss: 12572909.000000, mae: 1340.473755, mean_q: -531.169800
 4706/5000: episode: 4706, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9920.083, mean reward: -9920.083 [-9920.083, -9920.083], mean action: 1.000 [1.000, 1.000],  loss: 10457834.000000, mae: 1295.329590, mean_q: -531.501770
 4707/5000: episode: 4707, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2080.430, mean reward: -2080.430 [-2080.430, -2080.430], mean action: 0.000 [0.000, 0.000],  loss: 11497438.000000, mae: 1234.449463, mean_q: -530.138672
 4708/5000: episode: 4708, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6190.428, mean reward: -6190.428 [-6190.428, -6190.428], mean action: 0.000 [0.000, 0.000],  loss: 16930990.000000, mae: 1594.453125, mean_q: -531.566467
 4709/5000: episode: 4709, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4276.113, mean reward: -4276.113 [-4276.113, -4276.113], mean action: 0.000 [0.000, 0.000],  loss: 13316694.000000, mae: 1390.485107, mean_q: -531.871033
 4710/5000: episode: 4710, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1826.810, mean reward: -1826.810 [-1826.810, -1826.810], mean action: 0.000 [0.000, 0.000],  loss: 12035366.000000, mae: 1324.058472, mean_q: -533.122559
 4711/5000: episode: 4711, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -8436.234, mean reward: -8436.234 [-8436.234, -8436.234], mean action: 0.000 [0.000, 0.000],  loss: 11390067.000000, mae: 1344.931152, mean_q: -534.232361
 4712/5000: episode: 4712, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6567.988, mean reward: -6567.988 [-6567.988, -6567.988], mean action: 0.000 [0.000, 0.000],  loss: 10366110.000000, mae: 1246.886719, mean_q: -534.047974
 4713/5000: episode: 4713, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -903.326, mean reward: -903.326 [-903.326, -903.326], mean action: 2.000 [2.000, 2.000],  loss: 10736624.000000, mae: 1271.799316, mean_q: -535.247009
 4714/5000: episode: 4714, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7890.368, mean reward: -7890.368 [-7890.368, -7890.368], mean action: 0.000 [0.000, 0.000],  loss: 12702002.000000, mae: 1371.497559, mean_q: -535.809998
 4715/5000: episode: 4715, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3712.345, mean reward: -3712.345 [-3712.345, -3712.345], mean action: 0.000 [0.000, 0.000],  loss: 10356568.000000, mae: 1258.315430, mean_q: -533.733154
 4716/5000: episode: 4716, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3655.522, mean reward: -3655.522 [-3655.522, -3655.522], mean action: 0.000 [0.000, 0.000],  loss: 8537483.000000, mae: 1206.057129, mean_q: -534.648499
 4717/5000: episode: 4717, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -11288.509, mean reward: -11288.509 [-11288.509, -11288.509], mean action: 0.000 [0.000, 0.000],  loss: 8942524.000000, mae: 1172.656982, mean_q: -535.688232
 4718/5000: episode: 4718, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3576.255, mean reward: -3576.255 [-3576.255, -3576.255], mean action: 0.000 [0.000, 0.000],  loss: 12526115.000000, mae: 1378.343872, mean_q: -536.957275
 4719/5000: episode: 4719, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4992.506, mean reward: -4992.506 [-4992.506, -4992.506], mean action: 0.000 [0.000, 0.000],  loss: 8320189.000000, mae: 1206.981812, mean_q: -536.086609
 4720/5000: episode: 4720, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -7630.986, mean reward: -7630.986 [-7630.986, -7630.986], mean action: 0.000 [0.000, 0.000],  loss: 9153768.000000, mae: 1174.716797, mean_q: -537.366577
 4721/5000: episode: 4721, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5505.992, mean reward: -5505.992 [-5505.992, -5505.992], mean action: 0.000 [0.000, 0.000],  loss: 10201169.000000, mae: 1249.971924, mean_q: -535.679077
 4722/5000: episode: 4722, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6987.883, mean reward: -6987.883 [-6987.883, -6987.883], mean action: 0.000 [0.000, 0.000],  loss: 16117817.000000, mae: 1523.003174, mean_q: -537.341431
 4723/5000: episode: 4723, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -770.219, mean reward: -770.219 [-770.219, -770.219], mean action: 2.000 [2.000, 2.000],  loss: 10754820.000000, mae: 1250.652832, mean_q: -534.365601
 4724/5000: episode: 4724, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8310.289, mean reward: -8310.289 [-8310.289, -8310.289], mean action: 0.000 [0.000, 0.000],  loss: 10557658.000000, mae: 1354.095703, mean_q: -536.673096
 4725/5000: episode: 4725, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6274.822, mean reward: -6274.822 [-6274.822, -6274.822], mean action: 1.000 [1.000, 1.000],  loss: 12046813.000000, mae: 1343.776978, mean_q: -537.485291
 4726/5000: episode: 4726, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -855.673, mean reward: -855.673 [-855.673, -855.673], mean action: 0.000 [0.000, 0.000],  loss: 9713168.000000, mae: 1243.692627, mean_q: -537.278259
 4727/5000: episode: 4727, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -14071.127, mean reward: -14071.127 [-14071.127, -14071.127], mean action: 0.000 [0.000, 0.000],  loss: 9977786.000000, mae: 1293.423828, mean_q: -536.675537
 4728/5000: episode: 4728, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5106.735, mean reward: -5106.735 [-5106.735, -5106.735], mean action: 0.000 [0.000, 0.000],  loss: 5750700.000000, mae: 1069.573242, mean_q: -536.574036
 4729/5000: episode: 4729, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3728.552, mean reward: -3728.552 [-3728.552, -3728.552], mean action: 0.000 [0.000, 0.000],  loss: 11653389.000000, mae: 1298.269287, mean_q: -536.720093
 4730/5000: episode: 4730, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4943.025, mean reward: -4943.025 [-4943.025, -4943.025], mean action: 0.000 [0.000, 0.000],  loss: 14304009.000000, mae: 1503.898560, mean_q: -540.691650
 4731/5000: episode: 4731, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6982.930, mean reward: -6982.930 [-6982.930, -6982.930], mean action: 0.000 [0.000, 0.000],  loss: 13212024.000000, mae: 1395.825928, mean_q: -536.137817
 4732/5000: episode: 4732, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6411.187, mean reward: -6411.187 [-6411.187, -6411.187], mean action: 0.000 [0.000, 0.000],  loss: 11328422.000000, mae: 1262.152954, mean_q: -539.434937
 4733/5000: episode: 4733, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9449.732, mean reward: -9449.732 [-9449.732, -9449.732], mean action: 0.000 [0.000, 0.000],  loss: 12317951.000000, mae: 1380.917236, mean_q: -536.854309
 4734/5000: episode: 4734, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -8289.745, mean reward: -8289.745 [-8289.745, -8289.745], mean action: 0.000 [0.000, 0.000],  loss: 22838012.000000, mae: 1716.616333, mean_q: -536.229309
 4735/5000: episode: 4735, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5437.993, mean reward: -5437.993 [-5437.993, -5437.993], mean action: 0.000 [0.000, 0.000],  loss: 4656904.000000, mae: 969.766602, mean_q: -541.580933
 4736/5000: episode: 4736, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -10954.911, mean reward: -10954.911 [-10954.911, -10954.911], mean action: 0.000 [0.000, 0.000],  loss: 8615575.000000, mae: 1137.999268, mean_q: -540.413330
 4737/5000: episode: 4737, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3043.179, mean reward: -3043.179 [-3043.179, -3043.179], mean action: 2.000 [2.000, 2.000],  loss: 11550291.000000, mae: 1365.430908, mean_q: -539.339233
 4738/5000: episode: 4738, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -13908.948, mean reward: -13908.948 [-13908.948, -13908.948], mean action: 0.000 [0.000, 0.000],  loss: 7639732.000000, mae: 1103.459106, mean_q: -539.291626
 4739/5000: episode: 4739, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5011.795, mean reward: -5011.795 [-5011.795, -5011.795], mean action: 0.000 [0.000, 0.000],  loss: 7929814.500000, mae: 1151.712891, mean_q: -541.522705
 4740/5000: episode: 4740, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3531.489, mean reward: -3531.489 [-3531.489, -3531.489], mean action: 0.000 [0.000, 0.000],  loss: 12327664.000000, mae: 1402.753296, mean_q: -539.852173
 4741/5000: episode: 4741, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5739.902, mean reward: -5739.902 [-5739.902, -5739.902], mean action: 0.000 [0.000, 0.000],  loss: 10501830.000000, mae: 1261.746094, mean_q: -540.247681
 4742/5000: episode: 4742, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -8995.345, mean reward: -8995.345 [-8995.345, -8995.345], mean action: 0.000 [0.000, 0.000],  loss: 11866605.000000, mae: 1374.476074, mean_q: -542.455078
 4743/5000: episode: 4743, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -14837.914, mean reward: -14837.914 [-14837.914, -14837.914], mean action: 0.000 [0.000, 0.000],  loss: 11948502.000000, mae: 1321.218628, mean_q: -538.724243
 4744/5000: episode: 4744, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5296.563, mean reward: -5296.563 [-5296.563, -5296.563], mean action: 0.000 [0.000, 0.000],  loss: 15332541.000000, mae: 1506.316528, mean_q: -541.590942
 4745/5000: episode: 4745, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -6149.054, mean reward: -6149.054 [-6149.054, -6149.054], mean action: 0.000 [0.000, 0.000],  loss: 10489069.000000, mae: 1263.327637, mean_q: -539.773438
 4746/5000: episode: 4746, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -11378.256, mean reward: -11378.256 [-11378.256, -11378.256], mean action: 0.000 [0.000, 0.000],  loss: 8675336.000000, mae: 1206.876709, mean_q: -543.684937
 4747/5000: episode: 4747, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8556.141, mean reward: -8556.141 [-8556.141, -8556.141], mean action: 0.000 [0.000, 0.000],  loss: 8244084.500000, mae: 1223.638916, mean_q: -543.411377
 4748/5000: episode: 4748, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5861.618, mean reward: -5861.618 [-5861.618, -5861.618], mean action: 0.000 [0.000, 0.000],  loss: 15672528.000000, mae: 1505.810547, mean_q: -539.662354
 4749/5000: episode: 4749, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2571.818, mean reward: -2571.818 [-2571.818, -2571.818], mean action: 0.000 [0.000, 0.000],  loss: 14990360.000000, mae: 1368.158447, mean_q: -540.749939
 4750/5000: episode: 4750, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3707.181, mean reward: -3707.181 [-3707.181, -3707.181], mean action: 0.000 [0.000, 0.000],  loss: 13387195.000000, mae: 1391.713379, mean_q: -542.573975
 4751/5000: episode: 4751, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4861.714, mean reward: -4861.714 [-4861.714, -4861.714], mean action: 0.000 [0.000, 0.000],  loss: 10521888.000000, mae: 1318.953247, mean_q: -543.487366
 4752/5000: episode: 4752, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3218.247, mean reward: -3218.247 [-3218.247, -3218.247], mean action: 0.000 [0.000, 0.000],  loss: 11051825.000000, mae: 1362.443359, mean_q: -542.457764
 4753/5000: episode: 4753, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -13581.215, mean reward: -13581.215 [-13581.215, -13581.215], mean action: 0.000 [0.000, 0.000],  loss: 6507334.000000, mae: 1080.420166, mean_q: -544.477905
 4754/5000: episode: 4754, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3621.662, mean reward: -3621.662 [-3621.662, -3621.662], mean action: 0.000 [0.000, 0.000],  loss: 8954569.000000, mae: 1104.140381, mean_q: -545.486694
 4755/5000: episode: 4755, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6735.562, mean reward: -6735.562 [-6735.562, -6735.562], mean action: 0.000 [0.000, 0.000],  loss: 19112372.000000, mae: 1548.279907, mean_q: -542.932678
 4756/5000: episode: 4756, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -12740.381, mean reward: -12740.381 [-12740.381, -12740.381], mean action: 0.000 [0.000, 0.000],  loss: 12767721.000000, mae: 1349.991577, mean_q: -542.733521
 4757/5000: episode: 4757, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3447.764, mean reward: -3447.764 [-3447.764, -3447.764], mean action: 0.000 [0.000, 0.000],  loss: 12805486.000000, mae: 1384.541260, mean_q: -546.311157
 4758/5000: episode: 4758, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -16546.008, mean reward: -16546.008 [-16546.008, -16546.008], mean action: 0.000 [0.000, 0.000],  loss: 10053550.000000, mae: 1251.633667, mean_q: -542.904236
 4759/5000: episode: 4759, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9065.630, mean reward: -9065.630 [-9065.630, -9065.630], mean action: 0.000 [0.000, 0.000],  loss: 11111094.000000, mae: 1313.962280, mean_q: -545.132141
 4760/5000: episode: 4760, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -9835.831, mean reward: -9835.831 [-9835.831, -9835.831], mean action: 0.000 [0.000, 0.000],  loss: 11952823.000000, mae: 1375.039795, mean_q: -545.398438
 4761/5000: episode: 4761, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -8426.557, mean reward: -8426.557 [-8426.557, -8426.557], mean action: 0.000 [0.000, 0.000],  loss: 14726466.000000, mae: 1445.952393, mean_q: -547.822021
 4762/5000: episode: 4762, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1014.266, mean reward: -1014.266 [-1014.266, -1014.266], mean action: 0.000 [0.000, 0.000],  loss: 5647822.500000, mae: 1061.121582, mean_q: -546.088379
 4763/5000: episode: 4763, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -208.684, mean reward: -208.684 [-208.684, -208.684], mean action: 2.000 [2.000, 2.000],  loss: 7150560.000000, mae: 1089.591553, mean_q: -545.318115
 4764/5000: episode: 4764, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1395.265, mean reward: -1395.265 [-1395.265, -1395.265], mean action: 2.000 [2.000, 2.000],  loss: 7901116.000000, mae: 1104.452393, mean_q: -542.773254
 4765/5000: episode: 4765, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -920.902, mean reward: -920.902 [-920.902, -920.902], mean action: 2.000 [2.000, 2.000],  loss: 13437103.000000, mae: 1413.962158, mean_q: -546.017029
 4766/5000: episode: 4766, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -646.013, mean reward: -646.013 [-646.013, -646.013], mean action: 2.000 [2.000, 2.000],  loss: 10078727.000000, mae: 1247.176758, mean_q: -545.471008
 4767/5000: episode: 4767, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -227.765, mean reward: -227.765 [-227.765, -227.765], mean action: 2.000 [2.000, 2.000],  loss: 8493972.000000, mae: 1198.721436, mean_q: -547.285095
 4768/5000: episode: 4768, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1582.772, mean reward: -1582.772 [-1582.772, -1582.772], mean action: 2.000 [2.000, 2.000],  loss: 9990974.000000, mae: 1287.953247, mean_q: -546.799194
 4769/5000: episode: 4769, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2554.012, mean reward: -2554.012 [-2554.012, -2554.012], mean action: 2.000 [2.000, 2.000],  loss: 14683595.000000, mae: 1470.906982, mean_q: -545.514648
 4770/5000: episode: 4770, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -892.907, mean reward: -892.907 [-892.907, -892.907], mean action: 2.000 [2.000, 2.000],  loss: 14104469.000000, mae: 1498.712036, mean_q: -546.117798
 4771/5000: episode: 4771, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7717.477, mean reward: -7717.477 [-7717.477, -7717.477], mean action: 2.000 [2.000, 2.000],  loss: 13277818.000000, mae: 1393.302246, mean_q: -547.775391
 4772/5000: episode: 4772, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1322.309, mean reward: -1322.309 [-1322.309, -1322.309], mean action: 2.000 [2.000, 2.000],  loss: 7970825.500000, mae: 1211.395752, mean_q: -545.323486
 4773/5000: episode: 4773, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4767.355, mean reward: -4767.355 [-4767.355, -4767.355], mean action: 2.000 [2.000, 2.000],  loss: 13264153.000000, mae: 1406.192871, mean_q: -547.357178
 4774/5000: episode: 4774, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1977.142, mean reward: -1977.142 [-1977.142, -1977.142], mean action: 2.000 [2.000, 2.000],  loss: 9274558.000000, mae: 1262.933228, mean_q: -547.317078
 4775/5000: episode: 4775, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2740.692, mean reward: -2740.692 [-2740.692, -2740.692], mean action: 2.000 [2.000, 2.000],  loss: 11483249.000000, mae: 1338.900146, mean_q: -548.415527
 4776/5000: episode: 4776, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6110.378, mean reward: -6110.378 [-6110.378, -6110.378], mean action: 2.000 [2.000, 2.000],  loss: 9737936.000000, mae: 1281.434082, mean_q: -546.194580
 4777/5000: episode: 4777, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -67.172, mean reward: -67.172 [-67.172, -67.172], mean action: 2.000 [2.000, 2.000],  loss: 16259884.000000, mae: 1452.671997, mean_q: -547.309814
 4778/5000: episode: 4778, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -130.592, mean reward: -130.592 [-130.592, -130.592], mean action: 2.000 [2.000, 2.000],  loss: 13250780.000000, mae: 1280.318604, mean_q: -550.457520
 4779/5000: episode: 4779, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -110.603, mean reward: -110.603 [-110.603, -110.603], mean action: 2.000 [2.000, 2.000],  loss: 7446957.000000, mae: 1146.612061, mean_q: -548.617371
 4780/5000: episode: 4780, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2966.239, mean reward: -2966.239 [-2966.239, -2966.239], mean action: 2.000 [2.000, 2.000],  loss: 7591378.000000, mae: 1175.549805, mean_q: -549.407715
 4781/5000: episode: 4781, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5294.639, mean reward: -5294.639 [-5294.639, -5294.639], mean action: 2.000 [2.000, 2.000],  loss: 9649670.000000, mae: 1304.100464, mean_q: -549.251343
 4782/5000: episode: 4782, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1057.046, mean reward: -1057.046 [-1057.046, -1057.046], mean action: 2.000 [2.000, 2.000],  loss: 10844262.000000, mae: 1367.802734, mean_q: -550.497070
 4783/5000: episode: 4783, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1826.888, mean reward: -1826.888 [-1826.888, -1826.888], mean action: 2.000 [2.000, 2.000],  loss: 8847318.000000, mae: 1156.706421, mean_q: -548.738098
 4784/5000: episode: 4784, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5083.723, mean reward: -5083.723 [-5083.723, -5083.723], mean action: 2.000 [2.000, 2.000],  loss: 8417101.000000, mae: 1167.523926, mean_q: -550.291443
 4785/5000: episode: 4785, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3276.368, mean reward: -3276.368 [-3276.368, -3276.368], mean action: 2.000 [2.000, 2.000],  loss: 9269680.000000, mae: 1238.375610, mean_q: -548.934021
 4786/5000: episode: 4786, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2381.041, mean reward: -2381.041 [-2381.041, -2381.041], mean action: 2.000 [2.000, 2.000],  loss: 15480928.000000, mae: 1399.602661, mean_q: -550.007690
 4787/5000: episode: 4787, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1860.811, mean reward: -1860.811 [-1860.811, -1860.811], mean action: 2.000 [2.000, 2.000],  loss: 7132908.000000, mae: 1123.430908, mean_q: -552.124268
 4788/5000: episode: 4788, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2944.955, mean reward: -2944.955 [-2944.955, -2944.955], mean action: 2.000 [2.000, 2.000],  loss: 7228547.000000, mae: 1186.368042, mean_q: -551.563965
 4789/5000: episode: 4789, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -651.275, mean reward: -651.275 [-651.275, -651.275], mean action: 2.000 [2.000, 2.000],  loss: 8630438.000000, mae: 1217.152588, mean_q: -554.854858
 4790/5000: episode: 4790, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1978.666, mean reward: -1978.666 [-1978.666, -1978.666], mean action: 2.000 [2.000, 2.000],  loss: 11724718.000000, mae: 1338.916016, mean_q: -552.204224
 4791/5000: episode: 4791, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4272.679, mean reward: -4272.679 [-4272.679, -4272.679], mean action: 2.000 [2.000, 2.000],  loss: 10539859.000000, mae: 1337.416382, mean_q: -553.447205
 4792/5000: episode: 4792, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1262.061, mean reward: -1262.061 [-1262.061, -1262.061], mean action: 1.000 [1.000, 1.000],  loss: 11978438.000000, mae: 1398.758423, mean_q: -553.901184
 4793/5000: episode: 4793, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3109.222, mean reward: -3109.222 [-3109.222, -3109.222], mean action: 2.000 [2.000, 2.000],  loss: 11952102.000000, mae: 1349.707520, mean_q: -552.105957
 4794/5000: episode: 4794, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -3346.271, mean reward: -3346.271 [-3346.271, -3346.271], mean action: 2.000 [2.000, 2.000],  loss: 10224750.000000, mae: 1244.672485, mean_q: -553.790771
 4795/5000: episode: 4795, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1693.727, mean reward: -1693.727 [-1693.727, -1693.727], mean action: 2.000 [2.000, 2.000],  loss: 11537156.000000, mae: 1325.896240, mean_q: -551.369507
 4796/5000: episode: 4796, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5169.914, mean reward: -5169.914 [-5169.914, -5169.914], mean action: 2.000 [2.000, 2.000],  loss: 12280562.000000, mae: 1336.055664, mean_q: -555.923462
 4797/5000: episode: 4797, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -558.193, mean reward: -558.193 [-558.193, -558.193], mean action: 2.000 [2.000, 2.000],  loss: 13530553.000000, mae: 1393.147095, mean_q: -552.284668
 4798/5000: episode: 4798, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5457.445, mean reward: -5457.445 [-5457.445, -5457.445], mean action: 2.000 [2.000, 2.000],  loss: 8306418.000000, mae: 1248.127930, mean_q: -553.765320
 4799/5000: episode: 4799, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1241.299, mean reward: -1241.299 [-1241.299, -1241.299], mean action: 2.000 [2.000, 2.000],  loss: 9470994.000000, mae: 1185.492065, mean_q: -554.200989
 4800/5000: episode: 4800, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5342.467, mean reward: -5342.467 [-5342.467, -5342.467], mean action: 2.000 [2.000, 2.000],  loss: 9911713.000000, mae: 1257.620972, mean_q: -554.499268
 4801/5000: episode: 4801, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -806.034, mean reward: -806.034 [-806.034, -806.034], mean action: 2.000 [2.000, 2.000],  loss: 8488023.000000, mae: 1236.681152, mean_q: -555.017822
 4802/5000: episode: 4802, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -456.314, mean reward: -456.314 [-456.314, -456.314], mean action: 2.000 [2.000, 2.000],  loss: 11569102.000000, mae: 1367.135986, mean_q: -555.602600
 4803/5000: episode: 4803, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5489.211, mean reward: -5489.211 [-5489.211, -5489.211], mean action: 2.000 [2.000, 2.000],  loss: 10509228.000000, mae: 1262.172363, mean_q: -555.365845
 4804/5000: episode: 4804, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -183.454, mean reward: -183.454 [-183.454, -183.454], mean action: 2.000 [2.000, 2.000],  loss: 9065952.000000, mae: 1251.307373, mean_q: -553.683350
 4805/5000: episode: 4805, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1576.598, mean reward: -1576.598 [-1576.598, -1576.598], mean action: 2.000 [2.000, 2.000],  loss: 9928634.000000, mae: 1256.621948, mean_q: -554.874878
 4806/5000: episode: 4806, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4128.728, mean reward: -4128.728 [-4128.728, -4128.728], mean action: 2.000 [2.000, 2.000],  loss: 8113991.000000, mae: 1134.202148, mean_q: -556.249512
 4807/5000: episode: 4807, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5022.118, mean reward: -5022.118 [-5022.118, -5022.118], mean action: 2.000 [2.000, 2.000],  loss: 7984933.000000, mae: 1188.550781, mean_q: -556.625122
 4808/5000: episode: 4808, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2384.460, mean reward: -2384.460 [-2384.460, -2384.460], mean action: 2.000 [2.000, 2.000],  loss: 9426941.000000, mae: 1219.083008, mean_q: -555.572205
 4809/5000: episode: 4809, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -694.311, mean reward: -694.311 [-694.311, -694.311], mean action: 2.000 [2.000, 2.000],  loss: 10660958.000000, mae: 1335.119873, mean_q: -555.442810
 4810/5000: episode: 4810, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -1044.930, mean reward: -1044.930 [-1044.930, -1044.930], mean action: 2.000 [2.000, 2.000],  loss: 8258470.500000, mae: 1073.563965, mean_q: -556.709106
 4811/5000: episode: 4811, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1960.757, mean reward: -1960.757 [-1960.757, -1960.757], mean action: 2.000 [2.000, 2.000],  loss: 6683572.000000, mae: 1114.291504, mean_q: -558.203613
 4812/5000: episode: 4812, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1113.593, mean reward: -1113.593 [-1113.593, -1113.593], mean action: 3.000 [3.000, 3.000],  loss: 10457726.000000, mae: 1285.257812, mean_q: -557.385864
 4813/5000: episode: 4813, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3768.953, mean reward: -3768.953 [-3768.953, -3768.953], mean action: 2.000 [2.000, 2.000],  loss: 10877797.000000, mae: 1313.597534, mean_q: -558.317566
 4814/5000: episode: 4814, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5314.139, mean reward: -5314.139 [-5314.139, -5314.139], mean action: 2.000 [2.000, 2.000],  loss: 15720558.000000, mae: 1493.052246, mean_q: -556.539551
 4815/5000: episode: 4815, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2411.329, mean reward: -2411.329 [-2411.329, -2411.329], mean action: 2.000 [2.000, 2.000],  loss: 11190291.000000, mae: 1301.916260, mean_q: -559.151794
 4816/5000: episode: 4816, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3242.986, mean reward: -3242.986 [-3242.986, -3242.986], mean action: 2.000 [2.000, 2.000],  loss: 10686799.000000, mae: 1223.893311, mean_q: -557.409668
 4817/5000: episode: 4817, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1458.589, mean reward: -1458.589 [-1458.589, -1458.589], mean action: 2.000 [2.000, 2.000],  loss: 11403958.000000, mae: 1382.754761, mean_q: -556.627563
 4818/5000: episode: 4818, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -534.614, mean reward: -534.614 [-534.614, -534.614], mean action: 2.000 [2.000, 2.000],  loss: 9213460.000000, mae: 1250.131470, mean_q: -558.046204
 4819/5000: episode: 4819, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5628.777, mean reward: -5628.777 [-5628.777, -5628.777], mean action: 2.000 [2.000, 2.000],  loss: 13594544.000000, mae: 1383.395996, mean_q: -558.425110
 4820/5000: episode: 4820, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -959.060, mean reward: -959.060 [-959.060, -959.060], mean action: 2.000 [2.000, 2.000],  loss: 11494250.000000, mae: 1369.015747, mean_q: -559.961670
 4821/5000: episode: 4821, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2890.959, mean reward: -2890.959 [-2890.959, -2890.959], mean action: 2.000 [2.000, 2.000],  loss: 12207245.000000, mae: 1405.684082, mean_q: -560.027954
 4822/5000: episode: 4822, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2664.626, mean reward: -2664.626 [-2664.626, -2664.626], mean action: 2.000 [2.000, 2.000],  loss: 10458199.000000, mae: 1345.320068, mean_q: -558.247803
 4823/5000: episode: 4823, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1680.899, mean reward: -1680.899 [-1680.899, -1680.899], mean action: 2.000 [2.000, 2.000],  loss: 7512338.000000, mae: 1219.152100, mean_q: -559.643066
 4824/5000: episode: 4824, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1839.166, mean reward: -1839.166 [-1839.166, -1839.166], mean action: 2.000 [2.000, 2.000],  loss: 10043241.000000, mae: 1253.257812, mean_q: -558.949707
 4825/5000: episode: 4825, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3695.930, mean reward: -3695.930 [-3695.930, -3695.930], mean action: 2.000 [2.000, 2.000],  loss: 17297176.000000, mae: 1574.255859, mean_q: -559.477600
 4826/5000: episode: 4826, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2634.017, mean reward: -2634.017 [-2634.017, -2634.017], mean action: 2.000 [2.000, 2.000],  loss: 9753716.000000, mae: 1326.650391, mean_q: -560.299133
 4827/5000: episode: 4827, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2112.719, mean reward: -2112.719 [-2112.719, -2112.719], mean action: 2.000 [2.000, 2.000],  loss: 9780121.000000, mae: 1315.300415, mean_q: -562.067383
 4828/5000: episode: 4828, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1232.514, mean reward: -1232.514 [-1232.514, -1232.514], mean action: 2.000 [2.000, 2.000],  loss: 11104695.000000, mae: 1323.523071, mean_q: -557.774353
 4829/5000: episode: 4829, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2932.643, mean reward: -2932.643 [-2932.643, -2932.643], mean action: 2.000 [2.000, 2.000],  loss: 12259834.000000, mae: 1426.693115, mean_q: -558.751099
 4830/5000: episode: 4830, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1792.361, mean reward: -1792.361 [-1792.361, -1792.361], mean action: 2.000 [2.000, 2.000],  loss: 9879226.000000, mae: 1276.638428, mean_q: -560.068909
 4831/5000: episode: 4831, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -407.380, mean reward: -407.380 [-407.380, -407.380], mean action: 2.000 [2.000, 2.000],  loss: 8285416.000000, mae: 1216.630005, mean_q: -562.957703
 4832/5000: episode: 4832, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -900.544, mean reward: -900.544 [-900.544, -900.544], mean action: 2.000 [2.000, 2.000],  loss: 8283783.000000, mae: 1222.867676, mean_q: -562.125610
 4833/5000: episode: 4833, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5125.512, mean reward: -5125.512 [-5125.512, -5125.512], mean action: 2.000 [2.000, 2.000],  loss: 10955894.000000, mae: 1261.819580, mean_q: -560.317139
 4834/5000: episode: 4834, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -511.332, mean reward: -511.332 [-511.332, -511.332], mean action: 2.000 [2.000, 2.000],  loss: 10332742.000000, mae: 1329.947510, mean_q: -560.727234
 4835/5000: episode: 4835, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1784.917, mean reward: -1784.917 [-1784.917, -1784.917], mean action: 2.000 [2.000, 2.000],  loss: 12763234.000000, mae: 1345.007935, mean_q: -561.773560
 4836/5000: episode: 4836, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3490.311, mean reward: -3490.311 [-3490.311, -3490.311], mean action: 2.000 [2.000, 2.000],  loss: 8783777.000000, mae: 1208.951904, mean_q: -563.282532
 4837/5000: episode: 4837, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -151.430, mean reward: -151.430 [-151.430, -151.430], mean action: 2.000 [2.000, 2.000],  loss: 7529314.500000, mae: 1144.311157, mean_q: -563.658203
 4838/5000: episode: 4838, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1033.246, mean reward: -1033.246 [-1033.246, -1033.246], mean action: 2.000 [2.000, 2.000],  loss: 11054548.000000, mae: 1346.062256, mean_q: -562.715820
 4839/5000: episode: 4839, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -242.883, mean reward: -242.883 [-242.883, -242.883], mean action: 2.000 [2.000, 2.000],  loss: 6974044.000000, mae: 1156.559082, mean_q: -564.036072
 4840/5000: episode: 4840, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3411.819, mean reward: -3411.819 [-3411.819, -3411.819], mean action: 2.000 [2.000, 2.000],  loss: 10345964.000000, mae: 1262.389648, mean_q: -562.626587
 4841/5000: episode: 4841, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1693.561, mean reward: -1693.561 [-1693.561, -1693.561], mean action: 2.000 [2.000, 2.000],  loss: 9883824.000000, mae: 1304.014893, mean_q: -566.588501
 4842/5000: episode: 4842, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -586.780, mean reward: -586.780 [-586.780, -586.780], mean action: 2.000 [2.000, 2.000],  loss: 10538263.000000, mae: 1290.490967, mean_q: -564.994141
 4843/5000: episode: 4843, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2143.354, mean reward: -2143.354 [-2143.354, -2143.354], mean action: 2.000 [2.000, 2.000],  loss: 13491478.000000, mae: 1406.824707, mean_q: -563.820618
 4844/5000: episode: 4844, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -2335.816, mean reward: -2335.816 [-2335.816, -2335.816], mean action: 2.000 [2.000, 2.000],  loss: 10002937.000000, mae: 1320.852295, mean_q: -563.749268
 4845/5000: episode: 4845, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5629.188, mean reward: -5629.188 [-5629.188, -5629.188], mean action: 2.000 [2.000, 2.000],  loss: 11994852.000000, mae: 1323.142822, mean_q: -563.148376
 4846/5000: episode: 4846, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1812.438, mean reward: -1812.438 [-1812.438, -1812.438], mean action: 2.000 [2.000, 2.000],  loss: 9489626.000000, mae: 1173.885986, mean_q: -563.189575
 4847/5000: episode: 4847, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5135.256, mean reward: -5135.256 [-5135.256, -5135.256], mean action: 2.000 [2.000, 2.000],  loss: 8996278.000000, mae: 1254.601074, mean_q: -567.451355
 4848/5000: episode: 4848, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9174.988, mean reward: -9174.988 [-9174.988, -9174.988], mean action: 2.000 [2.000, 2.000],  loss: 10515159.000000, mae: 1225.792725, mean_q: -568.043396
 4849/5000: episode: 4849, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1630.172, mean reward: -1630.172 [-1630.172, -1630.172], mean action: 2.000 [2.000, 2.000],  loss: 7070030.500000, mae: 1117.148315, mean_q: -563.957031
 4850/5000: episode: 4850, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5305.944, mean reward: -5305.944 [-5305.944, -5305.944], mean action: 2.000 [2.000, 2.000],  loss: 5630391.000000, mae: 1074.217285, mean_q: -564.214478
 4851/5000: episode: 4851, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -956.682, mean reward: -956.682 [-956.682, -956.682], mean action: 2.000 [2.000, 2.000],  loss: 11100502.000000, mae: 1233.557129, mean_q: -566.718628
 4852/5000: episode: 4852, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2078.007, mean reward: -2078.007 [-2078.007, -2078.007], mean action: 2.000 [2.000, 2.000],  loss: 9753711.000000, mae: 1240.090332, mean_q: -568.105103
 4853/5000: episode: 4853, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3151.459, mean reward: -3151.459 [-3151.459, -3151.459], mean action: 2.000 [2.000, 2.000],  loss: 11319496.000000, mae: 1325.533936, mean_q: -565.909363
 4854/5000: episode: 4854, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1396.846, mean reward: -1396.846 [-1396.846, -1396.846], mean action: 2.000 [2.000, 2.000],  loss: 5679726.000000, mae: 1118.619385, mean_q: -567.944031
 4855/5000: episode: 4855, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1408.975, mean reward: -1408.975 [-1408.975, -1408.975], mean action: 2.000 [2.000, 2.000],  loss: 5948956.000000, mae: 995.847839, mean_q: -569.522705
 4856/5000: episode: 4856, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2651.089, mean reward: -2651.089 [-2651.089, -2651.089], mean action: 2.000 [2.000, 2.000],  loss: 7385875.000000, mae: 1142.308350, mean_q: -567.375427
 4857/5000: episode: 4857, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2533.668, mean reward: -2533.668 [-2533.668, -2533.668], mean action: 2.000 [2.000, 2.000],  loss: 5782930.000000, mae: 1102.671753, mean_q: -566.719604
 4858/5000: episode: 4858, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -647.622, mean reward: -647.622 [-647.622, -647.622], mean action: 3.000 [3.000, 3.000],  loss: 14171342.000000, mae: 1457.966553, mean_q: -563.391357
 4859/5000: episode: 4859, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4076.767, mean reward: -4076.767 [-4076.767, -4076.767], mean action: 0.000 [0.000, 0.000],  loss: 7007763.500000, mae: 1091.538086, mean_q: -568.516479
 4860/5000: episode: 4860, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2124.954, mean reward: -2124.954 [-2124.954, -2124.954], mean action: 2.000 [2.000, 2.000],  loss: 13092990.000000, mae: 1471.730713, mean_q: -565.500488
 4861/5000: episode: 4861, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4183.114, mean reward: -4183.114 [-4183.114, -4183.114], mean action: 2.000 [2.000, 2.000],  loss: 10533718.000000, mae: 1333.920166, mean_q: -567.626831
 4862/5000: episode: 4862, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -46.391, mean reward: -46.391 [-46.391, -46.391], mean action: 2.000 [2.000, 2.000],  loss: 8261146.000000, mae: 1179.174927, mean_q: -567.085083
 4863/5000: episode: 4863, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -487.005, mean reward: -487.005 [-487.005, -487.005], mean action: 2.000 [2.000, 2.000],  loss: 9137173.000000, mae: 1203.180786, mean_q: -568.346802
 4864/5000: episode: 4864, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -252.883, mean reward: -252.883 [-252.883, -252.883], mean action: 2.000 [2.000, 2.000],  loss: 12300516.000000, mae: 1344.061035, mean_q: -570.629517
 4865/5000: episode: 4865, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -82.097, mean reward: -82.097 [-82.097, -82.097], mean action: 2.000 [2.000, 2.000],  loss: 9185989.000000, mae: 1231.954102, mean_q: -569.869812
 4866/5000: episode: 4866, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2298.658, mean reward: -2298.658 [-2298.658, -2298.658], mean action: 2.000 [2.000, 2.000],  loss: 8925298.000000, mae: 1245.321777, mean_q: -570.040649
 4867/5000: episode: 4867, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5358.724, mean reward: -5358.724 [-5358.724, -5358.724], mean action: 1.000 [1.000, 1.000],  loss: 8987505.000000, mae: 1303.078369, mean_q: -568.183472
 4868/5000: episode: 4868, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1408.371, mean reward: -1408.371 [-1408.371, -1408.371], mean action: 2.000 [2.000, 2.000],  loss: 11074256.000000, mae: 1268.343140, mean_q: -569.762329
 4869/5000: episode: 4869, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1137.982, mean reward: -1137.982 [-1137.982, -1137.982], mean action: 2.000 [2.000, 2.000],  loss: 10364424.000000, mae: 1303.473633, mean_q: -570.685547
 4870/5000: episode: 4870, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -838.981, mean reward: -838.981 [-838.981, -838.981], mean action: 2.000 [2.000, 2.000],  loss: 8202560.000000, mae: 1228.872070, mean_q: -570.881531
 4871/5000: episode: 4871, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1301.940, mean reward: -1301.940 [-1301.940, -1301.940], mean action: 2.000 [2.000, 2.000],  loss: 12696638.000000, mae: 1414.207764, mean_q: -570.783447
 4872/5000: episode: 4872, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4167.026, mean reward: -4167.026 [-4167.026, -4167.026], mean action: 2.000 [2.000, 2.000],  loss: 6966882.000000, mae: 1103.334717, mean_q: -572.196411
 4873/5000: episode: 4873, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4877.831, mean reward: -4877.831 [-4877.831, -4877.831], mean action: 2.000 [2.000, 2.000],  loss: 9693460.000000, mae: 1243.610840, mean_q: -572.986816
 4874/5000: episode: 4874, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -289.722, mean reward: -289.722 [-289.722, -289.722], mean action: 2.000 [2.000, 2.000],  loss: 6707134.000000, mae: 1094.824341, mean_q: -573.472656
 4875/5000: episode: 4875, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7188.399, mean reward: -7188.399 [-7188.399, -7188.399], mean action: 2.000 [2.000, 2.000],  loss: 16084980.000000, mae: 1473.607056, mean_q: -572.651123
 4876/5000: episode: 4876, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3451.875, mean reward: -3451.875 [-3451.875, -3451.875], mean action: 2.000 [2.000, 2.000],  loss: 12441807.000000, mae: 1355.735107, mean_q: -572.177246
 4877/5000: episode: 4877, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2957.631, mean reward: -2957.631 [-2957.631, -2957.631], mean action: 1.000 [1.000, 1.000],  loss: 8479867.000000, mae: 1246.290527, mean_q: -572.709106
 4878/5000: episode: 4878, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -513.484, mean reward: -513.484 [-513.484, -513.484], mean action: 2.000 [2.000, 2.000],  loss: 7747873.000000, mae: 1199.439819, mean_q: -572.882080
 4879/5000: episode: 4879, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6447.949, mean reward: -6447.949 [-6447.949, -6447.949], mean action: 2.000 [2.000, 2.000],  loss: 7728022.500000, mae: 1194.068115, mean_q: -573.089111
 4880/5000: episode: 4880, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2376.873, mean reward: -2376.873 [-2376.873, -2376.873], mean action: 2.000 [2.000, 2.000],  loss: 6665572.000000, mae: 1137.695068, mean_q: -574.252808
 4881/5000: episode: 4881, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -481.021, mean reward: -481.021 [-481.021, -481.021], mean action: 2.000 [2.000, 2.000],  loss: 8915462.000000, mae: 1201.848389, mean_q: -572.796936
 4882/5000: episode: 4882, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6254.063, mean reward: -6254.063 [-6254.063, -6254.063], mean action: 2.000 [2.000, 2.000],  loss: 14691864.000000, mae: 1485.414551, mean_q: -574.969727
 4883/5000: episode: 4883, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -626.180, mean reward: -626.180 [-626.180, -626.180], mean action: 2.000 [2.000, 2.000],  loss: 10752783.000000, mae: 1214.438965, mean_q: -573.266357
 4884/5000: episode: 4884, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3010.224, mean reward: -3010.224 [-3010.224, -3010.224], mean action: 2.000 [2.000, 2.000],  loss: 14801998.000000, mae: 1563.186523, mean_q: -572.318359
 4885/5000: episode: 4885, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -982.230, mean reward: -982.230 [-982.230, -982.230], mean action: 2.000 [2.000, 2.000],  loss: 15877153.000000, mae: 1458.654785, mean_q: -573.940918
 4886/5000: episode: 4886, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -579.947, mean reward: -579.947 [-579.947, -579.947], mean action: 2.000 [2.000, 2.000],  loss: 15520426.000000, mae: 1404.468506, mean_q: -571.603149
 4887/5000: episode: 4887, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -859.106, mean reward: -859.106 [-859.106, -859.106], mean action: 2.000 [2.000, 2.000],  loss: 17021352.000000, mae: 1558.643311, mean_q: -575.738770
 4888/5000: episode: 4888, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1799.491, mean reward: -1799.491 [-1799.491, -1799.491], mean action: 2.000 [2.000, 2.000],  loss: 12873488.000000, mae: 1329.245117, mean_q: -572.650269
 4889/5000: episode: 4889, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5993.439, mean reward: -5993.439 [-5993.439, -5993.439], mean action: 3.000 [3.000, 3.000],  loss: 14769030.000000, mae: 1466.017090, mean_q: -573.888367
 4890/5000: episode: 4890, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1980.864, mean reward: -1980.864 [-1980.864, -1980.864], mean action: 2.000 [2.000, 2.000],  loss: 9093304.000000, mae: 1279.634766, mean_q: -577.241455
 4891/5000: episode: 4891, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1283.059, mean reward: -1283.059 [-1283.059, -1283.059], mean action: 2.000 [2.000, 2.000],  loss: 7569939.500000, mae: 1218.483398, mean_q: -576.792786
 4892/5000: episode: 4892, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1485.052, mean reward: -1485.052 [-1485.052, -1485.052], mean action: 2.000 [2.000, 2.000],  loss: 12004476.000000, mae: 1330.004395, mean_q: -573.400635
 4893/5000: episode: 4893, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3022.684, mean reward: -3022.684 [-3022.684, -3022.684], mean action: 2.000 [2.000, 2.000],  loss: 9892230.000000, mae: 1196.578979, mean_q: -574.848267
 4894/5000: episode: 4894, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1335.267, mean reward: -1335.267 [-1335.267, -1335.267], mean action: 2.000 [2.000, 2.000],  loss: 7132455.500000, mae: 1114.873535, mean_q: -575.266907
 4895/5000: episode: 4895, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4617.775, mean reward: -4617.775 [-4617.775, -4617.775], mean action: 2.000 [2.000, 2.000],  loss: 9055006.000000, mae: 1274.950684, mean_q: -577.789062
 4896/5000: episode: 4896, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1173.554, mean reward: -1173.554 [-1173.554, -1173.554], mean action: 2.000 [2.000, 2.000],  loss: 13459476.000000, mae: 1502.275146, mean_q: -575.047241
 4897/5000: episode: 4897, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -440.437, mean reward: -440.437 [-440.437, -440.437], mean action: 2.000 [2.000, 2.000],  loss: 8484927.000000, mae: 1190.352661, mean_q: -576.527954
 4898/5000: episode: 4898, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -973.278, mean reward: -973.278 [-973.278, -973.278], mean action: 2.000 [2.000, 2.000],  loss: 10849180.000000, mae: 1311.171753, mean_q: -577.882507
 4899/5000: episode: 4899, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4154.047, mean reward: -4154.047 [-4154.047, -4154.047], mean action: 2.000 [2.000, 2.000],  loss: 9577728.000000, mae: 1245.666260, mean_q: -576.695312
 4900/5000: episode: 4900, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -371.135, mean reward: -371.135 [-371.135, -371.135], mean action: 2.000 [2.000, 2.000],  loss: 9231077.000000, mae: 1238.096802, mean_q: -575.059937
 4901/5000: episode: 4901, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2419.778, mean reward: -2419.778 [-2419.778, -2419.778], mean action: 0.000 [0.000, 0.000],  loss: 11760174.000000, mae: 1377.335205, mean_q: -576.327148
 4902/5000: episode: 4902, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2227.535, mean reward: -2227.535 [-2227.535, -2227.535], mean action: 2.000 [2.000, 2.000],  loss: 9521956.000000, mae: 1230.670654, mean_q: -578.375610
 4903/5000: episode: 4903, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4348.837, mean reward: -4348.837 [-4348.837, -4348.837], mean action: 2.000 [2.000, 2.000],  loss: 9935424.000000, mae: 1293.539429, mean_q: -578.743774
 4904/5000: episode: 4904, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1610.745, mean reward: -1610.745 [-1610.745, -1610.745], mean action: 2.000 [2.000, 2.000],  loss: 8116886.500000, mae: 1164.918945, mean_q: -579.233582
 4905/5000: episode: 4905, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6427.057, mean reward: -6427.057 [-6427.057, -6427.057], mean action: 2.000 [2.000, 2.000],  loss: 10993498.000000, mae: 1247.404053, mean_q: -577.666870
 4906/5000: episode: 4906, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7402.708, mean reward: -7402.708 [-7402.708, -7402.708], mean action: 2.000 [2.000, 2.000],  loss: 9529841.000000, mae: 1200.734619, mean_q: -578.942383
 4907/5000: episode: 4907, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3312.188, mean reward: -3312.188 [-3312.188, -3312.188], mean action: 2.000 [2.000, 2.000],  loss: 7254279.000000, mae: 1151.299316, mean_q: -578.788940
 4908/5000: episode: 4908, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -874.915, mean reward: -874.915 [-874.915, -874.915], mean action: 2.000 [2.000, 2.000],  loss: 7433484.500000, mae: 1154.894775, mean_q: -582.240295
 4909/5000: episode: 4909, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1878.096, mean reward: -1878.096 [-1878.096, -1878.096], mean action: 2.000 [2.000, 2.000],  loss: 9638447.000000, mae: 1221.346191, mean_q: -579.517822
 4910/5000: episode: 4910, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3433.034, mean reward: -3433.034 [-3433.034, -3433.034], mean action: 2.000 [2.000, 2.000],  loss: 10351137.000000, mae: 1323.101807, mean_q: -582.159790
 4911/5000: episode: 4911, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3030.006, mean reward: -3030.006 [-3030.006, -3030.006], mean action: 2.000 [2.000, 2.000],  loss: 11452404.000000, mae: 1336.804199, mean_q: -578.666870
 4912/5000: episode: 4912, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2244.683, mean reward: -2244.683 [-2244.683, -2244.683], mean action: 2.000 [2.000, 2.000],  loss: 11777974.000000, mae: 1361.888672, mean_q: -578.465332
 4913/5000: episode: 4913, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1342.513, mean reward: -1342.513 [-1342.513, -1342.513], mean action: 2.000 [2.000, 2.000],  loss: 10922949.000000, mae: 1292.770264, mean_q: -578.960327
 4914/5000: episode: 4914, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -698.054, mean reward: -698.054 [-698.054, -698.054], mean action: 2.000 [2.000, 2.000],  loss: 14942800.000000, mae: 1456.585205, mean_q: -580.542847
 4915/5000: episode: 4915, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8755.991, mean reward: -8755.991 [-8755.991, -8755.991], mean action: 2.000 [2.000, 2.000],  loss: 6403593.500000, mae: 1069.470459, mean_q: -582.668823
 4916/5000: episode: 4916, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3118.191, mean reward: -3118.191 [-3118.191, -3118.191], mean action: 2.000 [2.000, 2.000],  loss: 8089193.000000, mae: 1164.909180, mean_q: -581.911194
 4917/5000: episode: 4917, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2572.127, mean reward: -2572.127 [-2572.127, -2572.127], mean action: 2.000 [2.000, 2.000],  loss: 8392459.000000, mae: 1246.886719, mean_q: -580.363647
 4918/5000: episode: 4918, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1961.822, mean reward: -1961.822 [-1961.822, -1961.822], mean action: 2.000 [2.000, 2.000],  loss: 6584434.500000, mae: 1091.177734, mean_q: -581.898682
 4919/5000: episode: 4919, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3304.168, mean reward: -3304.168 [-3304.168, -3304.168], mean action: 2.000 [2.000, 2.000],  loss: 5188781.000000, mae: 1050.242920, mean_q: -582.640442
 4920/5000: episode: 4920, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5215.258, mean reward: -5215.258 [-5215.258, -5215.258], mean action: 2.000 [2.000, 2.000],  loss: 10152135.000000, mae: 1271.801880, mean_q: -582.348145
 4921/5000: episode: 4921, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -677.863, mean reward: -677.863 [-677.863, -677.863], mean action: 2.000 [2.000, 2.000],  loss: 9445946.000000, mae: 1228.625244, mean_q: -582.353943
 4922/5000: episode: 4922, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -753.569, mean reward: -753.569 [-753.569, -753.569], mean action: 2.000 [2.000, 2.000],  loss: 6957639.500000, mae: 1134.551392, mean_q: -583.362793
 4923/5000: episode: 4923, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1705.121, mean reward: -1705.121 [-1705.121, -1705.121], mean action: 2.000 [2.000, 2.000],  loss: 8981368.000000, mae: 1254.428223, mean_q: -582.435913
 4924/5000: episode: 4924, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3523.022, mean reward: -3523.022 [-3523.022, -3523.022], mean action: 3.000 [3.000, 3.000],  loss: 8293439.500000, mae: 1190.128418, mean_q: -581.090088
 4925/5000: episode: 4925, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -11467.015, mean reward: -11467.015 [-11467.015, -11467.015], mean action: 0.000 [0.000, 0.000],  loss: 8594396.000000, mae: 1270.582275, mean_q: -583.043701
 4926/5000: episode: 4926, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5662.072, mean reward: -5662.072 [-5662.072, -5662.072], mean action: 0.000 [0.000, 0.000],  loss: 7646455.500000, mae: 1224.369629, mean_q: -583.220215
 4927/5000: episode: 4927, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5354.470, mean reward: -5354.470 [-5354.470, -5354.470], mean action: 0.000 [0.000, 0.000],  loss: 9510757.000000, mae: 1249.493530, mean_q: -583.267212
 4928/5000: episode: 4928, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -13384.857, mean reward: -13384.857 [-13384.857, -13384.857], mean action: 0.000 [0.000, 0.000],  loss: 8917751.000000, mae: 1246.338867, mean_q: -583.415039
 4929/5000: episode: 4929, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9388.411, mean reward: -9388.411 [-9388.411, -9388.411], mean action: 0.000 [0.000, 0.000],  loss: 8054641.000000, mae: 1210.335693, mean_q: -586.105591
 4930/5000: episode: 4930, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9057.429, mean reward: -9057.429 [-9057.429, -9057.429], mean action: 0.000 [0.000, 0.000],  loss: 10499426.000000, mae: 1273.417969, mean_q: -585.001953
 4931/5000: episode: 4931, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4666.120, mean reward: -4666.120 [-4666.120, -4666.120], mean action: 0.000 [0.000, 0.000],  loss: 4010267.000000, mae: 1013.544861, mean_q: -587.616150
 4932/5000: episode: 4932, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -14463.914, mean reward: -14463.914 [-14463.914, -14463.914], mean action: 0.000 [0.000, 0.000],  loss: 7222135.500000, mae: 1213.293945, mean_q: -584.546814
 4933/5000: episode: 4933, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3236.962, mean reward: -3236.962 [-3236.962, -3236.962], mean action: 0.000 [0.000, 0.000],  loss: 9807700.000000, mae: 1281.625488, mean_q: -584.220093
 4934/5000: episode: 4934, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4150.925, mean reward: -4150.925 [-4150.925, -4150.925], mean action: 0.000 [0.000, 0.000],  loss: 9559540.000000, mae: 1160.130371, mean_q: -585.578369
 4935/5000: episode: 4935, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5192.047, mean reward: -5192.047 [-5192.047, -5192.047], mean action: 0.000 [0.000, 0.000],  loss: 11541056.000000, mae: 1306.000000, mean_q: -585.171936
 4936/5000: episode: 4936, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8257.438, mean reward: -8257.438 [-8257.438, -8257.438], mean action: 0.000 [0.000, 0.000],  loss: 6370629.500000, mae: 1137.117432, mean_q: -586.816406
 4937/5000: episode: 4937, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -8039.287, mean reward: -8039.287 [-8039.287, -8039.287], mean action: 0.000 [0.000, 0.000],  loss: 15211371.000000, mae: 1498.163574, mean_q: -586.750854
 4938/5000: episode: 4938, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8980.964, mean reward: -8980.964 [-8980.964, -8980.964], mean action: 0.000 [0.000, 0.000],  loss: 12644180.000000, mae: 1377.383789, mean_q: -585.043701
 4939/5000: episode: 4939, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -1979.966, mean reward: -1979.966 [-1979.966, -1979.966], mean action: 0.000 [0.000, 0.000],  loss: 8783494.000000, mae: 1231.897461, mean_q: -587.126648
 4940/5000: episode: 4940, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2188.415, mean reward: -2188.415 [-2188.415, -2188.415], mean action: 0.000 [0.000, 0.000],  loss: 10844290.000000, mae: 1389.094727, mean_q: -586.689941
 4941/5000: episode: 4941, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3498.923, mean reward: -3498.923 [-3498.923, -3498.923], mean action: 0.000 [0.000, 0.000],  loss: 5916694.000000, mae: 1028.432373, mean_q: -586.223633
 4942/5000: episode: 4942, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -39.782, mean reward: -39.782 [-39.782, -39.782], mean action: 2.000 [2.000, 2.000],  loss: 14085091.000000, mae: 1443.653931, mean_q: -585.623657
 4943/5000: episode: 4943, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3074.447, mean reward: -3074.447 [-3074.447, -3074.447], mean action: 0.000 [0.000, 0.000],  loss: 10419823.000000, mae: 1288.381104, mean_q: -587.984192
 4944/5000: episode: 4944, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6722.881, mean reward: -6722.881 [-6722.881, -6722.881], mean action: 0.000 [0.000, 0.000],  loss: 14222838.000000, mae: 1517.774658, mean_q: -585.570190
 4945/5000: episode: 4945, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6792.926, mean reward: -6792.926 [-6792.926, -6792.926], mean action: 0.000 [0.000, 0.000],  loss: 11939520.000000, mae: 1318.885986, mean_q: -587.363525
 4946/5000: episode: 4946, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7333.726, mean reward: -7333.726 [-7333.726, -7333.726], mean action: 0.000 [0.000, 0.000],  loss: 7310898.000000, mae: 1183.990356, mean_q: -589.813904
 4947/5000: episode: 4947, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3734.771, mean reward: -3734.771 [-3734.771, -3734.771], mean action: 0.000 [0.000, 0.000],  loss: 10158525.000000, mae: 1341.935059, mean_q: -590.198730
 4948/5000: episode: 4948, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -6389.445, mean reward: -6389.445 [-6389.445, -6389.445], mean action: 0.000 [0.000, 0.000],  loss: 10912005.000000, mae: 1260.103149, mean_q: -589.531982
 4949/5000: episode: 4949, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5861.391, mean reward: -5861.391 [-5861.391, -5861.391], mean action: 0.000 [0.000, 0.000],  loss: 7474486.000000, mae: 1206.030884, mean_q: -586.902832
 4950/5000: episode: 4950, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5825.612, mean reward: -5825.612 [-5825.612, -5825.612], mean action: 0.000 [0.000, 0.000],  loss: 10983372.000000, mae: 1345.692017, mean_q: -589.034180
 4951/5000: episode: 4951, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1868.812, mean reward: -1868.812 [-1868.812, -1868.812], mean action: 2.000 [2.000, 2.000],  loss: 10139643.000000, mae: 1306.641968, mean_q: -589.294922
 4952/5000: episode: 4952, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -4289.088, mean reward: -4289.088 [-4289.088, -4289.088], mean action: 2.000 [2.000, 2.000],  loss: 6920910.000000, mae: 1230.260010, mean_q: -590.465942
 4953/5000: episode: 4953, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -15566.525, mean reward: -15566.525 [-15566.525, -15566.525], mean action: 0.000 [0.000, 0.000],  loss: 9775514.000000, mae: 1240.088867, mean_q: -590.298340
 4954/5000: episode: 4954, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6525.932, mean reward: -6525.932 [-6525.932, -6525.932], mean action: 0.000 [0.000, 0.000],  loss: 9869390.000000, mae: 1332.491699, mean_q: -588.953613
 4955/5000: episode: 4955, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -13747.878, mean reward: -13747.878 [-13747.878, -13747.878], mean action: 0.000 [0.000, 0.000],  loss: 3920996.500000, mae: 1009.199097, mean_q: -590.644775
 4956/5000: episode: 4956, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10943.010, mean reward: -10943.010 [-10943.010, -10943.010], mean action: 0.000 [0.000, 0.000],  loss: 6733379.500000, mae: 1107.874512, mean_q: -591.790344
 4957/5000: episode: 4957, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7151.131, mean reward: -7151.131 [-7151.131, -7151.131], mean action: 0.000 [0.000, 0.000],  loss: 9621288.000000, mae: 1237.948486, mean_q: -588.499268
 4958/5000: episode: 4958, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -19889.849, mean reward: -19889.849 [-19889.849, -19889.849], mean action: 0.000 [0.000, 0.000],  loss: 15944884.000000, mae: 1631.970459, mean_q: -588.649719
 4959/5000: episode: 4959, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1184.464, mean reward: -1184.464 [-1184.464, -1184.464], mean action: 3.000 [3.000, 3.000],  loss: 10543100.000000, mae: 1272.601562, mean_q: -592.792358
 4960/5000: episode: 4960, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3469.196, mean reward: -3469.196 [-3469.196, -3469.196], mean action: 0.000 [0.000, 0.000],  loss: 8706114.000000, mae: 1125.954468, mean_q: -588.874390
 4961/5000: episode: 4961, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7833.772, mean reward: -7833.772 [-7833.772, -7833.772], mean action: 0.000 [0.000, 0.000],  loss: 12654396.000000, mae: 1414.717285, mean_q: -590.618286
 4962/5000: episode: 4962, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7111.201, mean reward: -7111.201 [-7111.201, -7111.201], mean action: 0.000 [0.000, 0.000],  loss: 7036051.000000, mae: 1173.466675, mean_q: -592.393799
 4963/5000: episode: 4963, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2020.568, mean reward: -2020.568 [-2020.568, -2020.568], mean action: 3.000 [3.000, 3.000],  loss: 10760388.000000, mae: 1352.481201, mean_q: -594.629883
 4964/5000: episode: 4964, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -16660.688, mean reward: -16660.688 [-16660.688, -16660.688], mean action: 0.000 [0.000, 0.000],  loss: 13871034.000000, mae: 1471.228149, mean_q: -588.729980
 4965/5000: episode: 4965, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8159.462, mean reward: -8159.462 [-8159.462, -8159.462], mean action: 0.000 [0.000, 0.000],  loss: 17169632.000000, mae: 1461.144897, mean_q: -592.493530
 4966/5000: episode: 4966, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2796.690, mean reward: -2796.690 [-2796.690, -2796.690], mean action: 0.000 [0.000, 0.000],  loss: 9246288.000000, mae: 1224.996094, mean_q: -595.252563
 4967/5000: episode: 4967, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8518.458, mean reward: -8518.458 [-8518.458, -8518.458], mean action: 0.000 [0.000, 0.000],  loss: 11033764.000000, mae: 1366.518555, mean_q: -593.209167
 4968/5000: episode: 4968, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7438.809, mean reward: -7438.809 [-7438.809, -7438.809], mean action: 0.000 [0.000, 0.000],  loss: 12392219.000000, mae: 1381.198730, mean_q: -595.184509
 4969/5000: episode: 4969, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3549.274, mean reward: -3549.274 [-3549.274, -3549.274], mean action: 0.000 [0.000, 0.000],  loss: 12366662.000000, mae: 1453.203491, mean_q: -590.946655
 4970/5000: episode: 4970, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3583.294, mean reward: -3583.294 [-3583.294, -3583.294], mean action: 0.000 [0.000, 0.000],  loss: 10622775.000000, mae: 1356.198486, mean_q: -594.928162
 4971/5000: episode: 4971, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -13539.064, mean reward: -13539.064 [-13539.064, -13539.064], mean action: 0.000 [0.000, 0.000],  loss: 12744156.000000, mae: 1415.527344, mean_q: -592.248230
 4972/5000: episode: 4972, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -3646.464, mean reward: -3646.464 [-3646.464, -3646.464], mean action: 0.000 [0.000, 0.000],  loss: 15597281.000000, mae: 1506.713135, mean_q: -589.740967
 4973/5000: episode: 4973, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -547.225, mean reward: -547.225 [-547.225, -547.225], mean action: 3.000 [3.000, 3.000],  loss: 5267575.500000, mae: 1036.089844, mean_q: -595.199707
 4974/5000: episode: 4974, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2718.191, mean reward: -2718.191 [-2718.191, -2718.191], mean action: 0.000 [0.000, 0.000],  loss: 8413114.000000, mae: 1246.120850, mean_q: -596.311340
 4975/5000: episode: 4975, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3249.972, mean reward: -3249.972 [-3249.972, -3249.972], mean action: 0.000 [0.000, 0.000],  loss: 18095020.000000, mae: 1607.582520, mean_q: -593.304199
 4976/5000: episode: 4976, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5885.104, mean reward: -5885.104 [-5885.104, -5885.104], mean action: 0.000 [0.000, 0.000],  loss: 10379932.000000, mae: 1269.743042, mean_q: -593.134705
 4977/5000: episode: 4977, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7549.690, mean reward: -7549.690 [-7549.690, -7549.690], mean action: 0.000 [0.000, 0.000],  loss: 13744068.000000, mae: 1492.055420, mean_q: -593.160278
 4978/5000: episode: 4978, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7521.133, mean reward: -7521.133 [-7521.133, -7521.133], mean action: 0.000 [0.000, 0.000],  loss: 15774084.000000, mae: 1518.354004, mean_q: -592.434448
 4979/5000: episode: 4979, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7260.794, mean reward: -7260.794 [-7260.794, -7260.794], mean action: 0.000 [0.000, 0.000],  loss: 7940538.000000, mae: 1197.880615, mean_q: -596.199585
 4980/5000: episode: 4980, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5521.607, mean reward: -5521.607 [-5521.607, -5521.607], mean action: 0.000 [0.000, 0.000],  loss: 12530305.000000, mae: 1429.631104, mean_q: -593.768799
 4981/5000: episode: 4981, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5240.440, mean reward: -5240.440 [-5240.440, -5240.440], mean action: 0.000 [0.000, 0.000],  loss: 7741867.000000, mae: 1148.105347, mean_q: -597.667358
 4982/5000: episode: 4982, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5993.242, mean reward: -5993.242 [-5993.242, -5993.242], mean action: 0.000 [0.000, 0.000],  loss: 9749757.000000, mae: 1274.076660, mean_q: -597.445557
 4983/5000: episode: 4983, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10651.927, mean reward: -10651.927 [-10651.927, -10651.927], mean action: 0.000 [0.000, 0.000],  loss: 8926156.000000, mae: 1221.275391, mean_q: -596.349548
 4984/5000: episode: 4984, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -8342.309, mean reward: -8342.309 [-8342.309, -8342.309], mean action: 0.000 [0.000, 0.000],  loss: 13206105.000000, mae: 1448.383545, mean_q: -595.900818
 4985/5000: episode: 4985, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3367.625, mean reward: -3367.625 [-3367.625, -3367.625], mean action: 0.000 [0.000, 0.000],  loss: 11400665.000000, mae: 1367.293213, mean_q: -596.457458
 4986/5000: episode: 4986, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7337.411, mean reward: -7337.411 [-7337.411, -7337.411], mean action: 0.000 [0.000, 0.000],  loss: 10604242.000000, mae: 1435.142090, mean_q: -596.912292
 4987/5000: episode: 4987, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -9268.614, mean reward: -9268.614 [-9268.614, -9268.614], mean action: 0.000 [0.000, 0.000],  loss: 9886812.000000, mae: 1273.800781, mean_q: -595.755005
 4988/5000: episode: 4988, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6493.111, mean reward: -6493.111 [-6493.111, -6493.111], mean action: 0.000 [0.000, 0.000],  loss: 9808138.000000, mae: 1155.982422, mean_q: -597.986816
 4989/5000: episode: 4989, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1510.820, mean reward: -1510.820 [-1510.820, -1510.820], mean action: 2.000 [2.000, 2.000],  loss: 12128354.000000, mae: 1437.587646, mean_q: -599.331665
 4990/5000: episode: 4990, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5228.005, mean reward: -5228.005 [-5228.005, -5228.005], mean action: 0.000 [0.000, 0.000],  loss: 5857487.000000, mae: 1057.322754, mean_q: -599.989136
 4991/5000: episode: 4991, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8165.381, mean reward: -8165.381 [-8165.381, -8165.381], mean action: 0.000 [0.000, 0.000],  loss: 9526910.000000, mae: 1258.345093, mean_q: -600.161560
 4992/5000: episode: 4992, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9343.722, mean reward: -9343.722 [-9343.722, -9343.722], mean action: 0.000 [0.000, 0.000],  loss: 12699388.000000, mae: 1372.508301, mean_q: -599.112793
 4993/5000: episode: 4993, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5497.501, mean reward: -5497.501 [-5497.501, -5497.501], mean action: 0.000 [0.000, 0.000],  loss: 10004766.000000, mae: 1357.385620, mean_q: -598.004333
 4994/5000: episode: 4994, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10699.832, mean reward: -10699.832 [-10699.832, -10699.832], mean action: 0.000 [0.000, 0.000],  loss: 18941412.000000, mae: 1697.337891, mean_q: -596.973022
 4995/5000: episode: 4995, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5469.532, mean reward: -5469.532 [-5469.532, -5469.532], mean action: 0.000 [0.000, 0.000],  loss: 7647178.000000, mae: 1158.932861, mean_q: -602.684631
 4996/5000: episode: 4996, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5137.786, mean reward: -5137.786 [-5137.786, -5137.786], mean action: 0.000 [0.000, 0.000],  loss: 10718039.000000, mae: 1305.963745, mean_q: -602.798157
 4997/5000: episode: 4997, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7104.931, mean reward: -7104.931 [-7104.931, -7104.931], mean action: 0.000 [0.000, 0.000],  loss: 5650938.000000, mae: 1016.130554, mean_q: -599.500305
 4998/5000: episode: 4998, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -15451.839, mean reward: -15451.839 [-15451.839, -15451.839], mean action: 0.000 [0.000, 0.000],  loss: 18636286.000000, mae: 1597.576660, mean_q: -599.313599
 4999/5000: episode: 4999, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10650.433, mean reward: -10650.433 [-10650.433, -10650.433], mean action: 0.000 [0.000, 0.000],  loss: 9776194.000000, mae: 1238.709106, mean_q: -598.685852
 5000/5000: episode: 5000, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3106.061, mean reward: -3106.061 [-3106.061, -3106.061], mean action: 2.000 [2.000, 2.000],  loss: 8048075.000000, mae: 1172.533203, mean_q: -601.201050
done, took 262.191 seconds
