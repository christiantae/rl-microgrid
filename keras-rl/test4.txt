Training for 10000 steps ...
    1/10000: episode: 1, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -7853.450, mean reward: -7853.450 [-7853.450, -7853.450], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    2/10000: episode: 2, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -8207.570, mean reward: -8207.570 [-8207.570, -8207.570], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    3/10000: episode: 3, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4843.115, mean reward: -4843.115 [-4843.115, -4843.115], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    4/10000: episode: 4, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3460.802, mean reward: -3460.802 [-3460.802, -3460.802], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    5/10000: episode: 5, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -3111.000, mean reward: -3111.000 [-3111.000, -3111.000], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    6/10000: episode: 6, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1334.411, mean reward: -1334.411 [-1334.411, -1334.411], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    7/10000: episode: 7, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -7818.840, mean reward: -7818.840 [-7818.840, -7818.840], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    8/10000: episode: 8, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5150.833, mean reward: -5150.833 [-5150.833, -5150.833], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    9/10000: episode: 9, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -4381.847, mean reward: -4381.847 [-4381.847, -4381.847], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   10/10000: episode: 10, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10747.044, mean reward: -10747.044 [-10747.044, -10747.044], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   11/10000: episode: 11, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2739.181, mean reward: -2739.181 [-2739.181, -2739.181], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   12/10000: episode: 12, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -189.344, mean reward: -189.344 [-189.344, -189.344], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   13/10000: episode: 13, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -11083.751, mean reward: -11083.751 [-11083.751, -11083.751], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   14/10000: episode: 14, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -8472.097, mean reward: -8472.097 [-8472.097, -8472.097], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   15/10000: episode: 15, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -3178.606, mean reward: -3178.606 [-3178.606, -3178.606], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   16/10000: episode: 16, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2307.454, mean reward: -2307.454 [-2307.454, -2307.454], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   17/10000: episode: 17, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -8460.236, mean reward: -8460.236 [-8460.236, -8460.236], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   18/10000: episode: 18, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -6344.751, mean reward: -6344.751 [-6344.751, -6344.751], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   19/10000: episode: 19, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -8788.776, mean reward: -8788.776 [-8788.776, -8788.776], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   20/10000: episode: 20, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -14570.348, mean reward: -14570.348 [-14570.348, -14570.348], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   21/10000: episode: 21, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -12767.607, mean reward: -12767.607 [-12767.607, -12767.607], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   22/10000: episode: 22, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5333.703, mean reward: -5333.703 [-5333.703, -5333.703], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   23/10000: episode: 23, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -3300.468, mean reward: -3300.468 [-3300.468, -3300.468], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   24/10000: episode: 24, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8549.206, mean reward: -8549.206 [-8549.206, -8549.206], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   25/10000: episode: 25, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -13606.243, mean reward: -13606.243 [-13606.243, -13606.243], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   26/10000: episode: 26, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -10124.398, mean reward: -10124.398 [-10124.398, -10124.398], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   27/10000: episode: 27, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3250.239, mean reward: -3250.239 [-3250.239, -3250.239], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   28/10000: episode: 28, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -10393.124, mean reward: -10393.124 [-10393.124, -10393.124], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   29/10000: episode: 29, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -12348.867, mean reward: -12348.867 [-12348.867, -12348.867], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   30/10000: episode: 30, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7088.512, mean reward: -7088.512 [-7088.512, -7088.512], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   31/10000: episode: 31, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -2429.011, mean reward: -2429.011 [-2429.011, -2429.011], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   32/10000: episode: 32, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1142.962, mean reward: -1142.962 [-1142.962, -1142.962], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   33/10000: episode: 33, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -9459.814, mean reward: -9459.814 [-9459.814, -9459.814], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   34/10000: episode: 34, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -9113.230, mean reward: -9113.230 [-9113.230, -9113.230], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   35/10000: episode: 35, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -9936.238, mean reward: -9936.238 [-9936.238, -9936.238], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   36/10000: episode: 36, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -11798.494, mean reward: -11798.494 [-11798.494, -11798.494], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   37/10000: episode: 37, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5496.145, mean reward: -5496.145 [-5496.145, -5496.145], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   38/10000: episode: 38, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -11756.388, mean reward: -11756.388 [-11756.388, -11756.388], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   39/10000: episode: 39, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2280.790, mean reward: -2280.790 [-2280.790, -2280.790], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   40/10000: episode: 40, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5086.907, mean reward: -5086.907 [-5086.907, -5086.907], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   41/10000: episode: 41, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7031.321, mean reward: -7031.321 [-7031.321, -7031.321], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   42/10000: episode: 42, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -13604.728, mean reward: -13604.728 [-13604.728, -13604.728], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   43/10000: episode: 43, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -4117.561, mean reward: -4117.561 [-4117.561, -4117.561], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   44/10000: episode: 44, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -11837.759, mean reward: -11837.759 [-11837.759, -11837.759], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   45/10000: episode: 45, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6888.431, mean reward: -6888.431 [-6888.431, -6888.431], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   46/10000: episode: 46, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -8287.211, mean reward: -8287.211 [-8287.211, -8287.211], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   47/10000: episode: 47, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -7179.969, mean reward: -7179.969 [-7179.969, -7179.969], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   48/10000: episode: 48, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5527.376, mean reward: -5527.376 [-5527.376, -5527.376], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   49/10000: episode: 49, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4602.130, mean reward: -4602.130 [-4602.130, -4602.130], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   50/10000: episode: 50, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -8271.276, mean reward: -8271.276 [-8271.276, -8271.276], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   51/10000: episode: 51, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -13459.195, mean reward: -13459.195 [-13459.195, -13459.195], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   52/10000: episode: 52, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -8054.486, mean reward: -8054.486 [-8054.486, -8054.486], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   53/10000: episode: 53, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4496.129, mean reward: -4496.129 [-4496.129, -4496.129], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   54/10000: episode: 54, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -10526.912, mean reward: -10526.912 [-10526.912, -10526.912], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   55/10000: episode: 55, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -7910.078, mean reward: -7910.078 [-7910.078, -7910.078], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   56/10000: episode: 56, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -2924.918, mean reward: -2924.918 [-2924.918, -2924.918], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   57/10000: episode: 57, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1251.741, mean reward: -1251.741 [-1251.741, -1251.741], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   58/10000: episode: 58, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5021.852, mean reward: -5021.852 [-5021.852, -5021.852], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   59/10000: episode: 59, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10167.121, mean reward: -10167.121 [-10167.121, -10167.121], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   60/10000: episode: 60, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9035.550, mean reward: -9035.550 [-9035.550, -9035.550], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   61/10000: episode: 61, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8599.669, mean reward: -8599.669 [-8599.669, -8599.669], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   62/10000: episode: 62, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -14405.348, mean reward: -14405.348 [-14405.348, -14405.348], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   63/10000: episode: 63, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -6535.158, mean reward: -6535.158 [-6535.158, -6535.158], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   64/10000: episode: 64, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4138.994, mean reward: -4138.994 [-4138.994, -4138.994], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   65/10000: episode: 65, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4088.075, mean reward: -4088.075 [-4088.075, -4088.075], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   66/10000: episode: 66, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5573.881, mean reward: -5573.881 [-5573.881, -5573.881], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   67/10000: episode: 67, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -9495.347, mean reward: -9495.347 [-9495.347, -9495.347], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   68/10000: episode: 68, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3369.103, mean reward: -3369.103 [-3369.103, -3369.103], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   69/10000: episode: 69, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -8168.435, mean reward: -8168.435 [-8168.435, -8168.435], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   70/10000: episode: 70, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11809.206, mean reward: -11809.206 [-11809.206, -11809.206], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   71/10000: episode: 71, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11607.944, mean reward: -11607.944 [-11607.944, -11607.944], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   72/10000: episode: 72, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -10207.826, mean reward: -10207.826 [-10207.826, -10207.826], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   73/10000: episode: 73, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5848.183, mean reward: -5848.183 [-5848.183, -5848.183], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   74/10000: episode: 74, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -7964.502, mean reward: -7964.502 [-7964.502, -7964.502], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   75/10000: episode: 75, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -13569.959, mean reward: -13569.959 [-13569.959, -13569.959], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   76/10000: episode: 76, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -6708.784, mean reward: -6708.784 [-6708.784, -6708.784], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   77/10000: episode: 77, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -4541.452, mean reward: -4541.452 [-4541.452, -4541.452], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   78/10000: episode: 78, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7018.315, mean reward: -7018.315 [-7018.315, -7018.315], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   79/10000: episode: 79, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3130.563, mean reward: -3130.563 [-3130.563, -3130.563], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   80/10000: episode: 80, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2911.774, mean reward: -2911.774 [-2911.774, -2911.774], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   81/10000: episode: 81, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -7040.737, mean reward: -7040.737 [-7040.737, -7040.737], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   82/10000: episode: 82, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4337.606, mean reward: -4337.606 [-4337.606, -4337.606], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   83/10000: episode: 83, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2662.346, mean reward: -2662.346 [-2662.346, -2662.346], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   84/10000: episode: 84, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7483.060, mean reward: -7483.060 [-7483.060, -7483.060], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   85/10000: episode: 85, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4250.362, mean reward: -4250.362 [-4250.362, -4250.362], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   86/10000: episode: 86, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5333.120, mean reward: -5333.120 [-5333.120, -5333.120], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   87/10000: episode: 87, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -6471.648, mean reward: -6471.648 [-6471.648, -6471.648], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   88/10000: episode: 88, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3845.737, mean reward: -3845.737 [-3845.737, -3845.737], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   89/10000: episode: 89, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -1545.946, mean reward: -1545.946 [-1545.946, -1545.946], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   90/10000: episode: 90, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7626.293, mean reward: -7626.293 [-7626.293, -7626.293], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   91/10000: episode: 91, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -4466.810, mean reward: -4466.810 [-4466.810, -4466.810], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   92/10000: episode: 92, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1712.963, mean reward: -1712.963 [-1712.963, -1712.963], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   93/10000: episode: 93, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -9521.451, mean reward: -9521.451 [-9521.451, -9521.451], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   94/10000: episode: 94, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -9358.336, mean reward: -9358.336 [-9358.336, -9358.336], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   95/10000: episode: 95, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -14698.887, mean reward: -14698.887 [-14698.887, -14698.887], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   96/10000: episode: 96, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -2998.697, mean reward: -2998.697 [-2998.697, -2998.697], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   97/10000: episode: 97, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4043.579, mean reward: -4043.579 [-4043.579, -4043.579], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   98/10000: episode: 98, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -8979.794, mean reward: -8979.794 [-8979.794, -8979.794], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   99/10000: episode: 99, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4124.255, mean reward: -4124.255 [-4124.255, -4124.255], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  100/10000: episode: 100, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -6922.100, mean reward: -6922.100 [-6922.100, -6922.100], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  101/10000: episode: 101, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4479.489, mean reward: -4479.489 [-4479.489, -4479.489], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  102/10000: episode: 102, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -7128.524, mean reward: -7128.524 [-7128.524, -7128.524], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  103/10000: episode: 103, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -7664.150, mean reward: -7664.150 [-7664.150, -7664.150], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  104/10000: episode: 104, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -14323.650, mean reward: -14323.650 [-14323.650, -14323.650], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  105/10000: episode: 105, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -2923.268, mean reward: -2923.268 [-2923.268, -2923.268], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  106/10000: episode: 106, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -10515.714, mean reward: -10515.714 [-10515.714, -10515.714], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  107/10000: episode: 107, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5291.713, mean reward: -5291.713 [-5291.713, -5291.713], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  108/10000: episode: 108, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -8001.658, mean reward: -8001.658 [-8001.658, -8001.658], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  109/10000: episode: 109, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3152.388, mean reward: -3152.388 [-3152.388, -3152.388], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  110/10000: episode: 110, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5019.094, mean reward: -5019.094 [-5019.094, -5019.094], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  111/10000: episode: 111, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -4607.721, mean reward: -4607.721 [-4607.721, -4607.721], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  112/10000: episode: 112, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -12115.614, mean reward: -12115.614 [-12115.614, -12115.614], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  113/10000: episode: 113, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7720.549, mean reward: -7720.549 [-7720.549, -7720.549], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  114/10000: episode: 114, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2748.198, mean reward: -2748.198 [-2748.198, -2748.198], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  115/10000: episode: 115, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -14877.500, mean reward: -14877.500 [-14877.500, -14877.500], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  116/10000: episode: 116, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5277.289, mean reward: -5277.289 [-5277.289, -5277.289], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  117/10000: episode: 117, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9172.961, mean reward: -9172.961 [-9172.961, -9172.961], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  118/10000: episode: 118, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -6005.809, mean reward: -6005.809 [-6005.809, -6005.809], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  119/10000: episode: 119, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2171.412, mean reward: -2171.412 [-2171.412, -2171.412], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  120/10000: episode: 120, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -2417.835, mean reward: -2417.835 [-2417.835, -2417.835], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  121/10000: episode: 121, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9334.768, mean reward: -9334.768 [-9334.768, -9334.768], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  122/10000: episode: 122, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -12466.694, mean reward: -12466.694 [-12466.694, -12466.694], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  123/10000: episode: 123, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7527.099, mean reward: -7527.099 [-7527.099, -7527.099], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  124/10000: episode: 124, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -12497.124, mean reward: -12497.124 [-12497.124, -12497.124], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  125/10000: episode: 125, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5675.633, mean reward: -5675.633 [-5675.633, -5675.633], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  126/10000: episode: 126, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1656.191, mean reward: -1656.191 [-1656.191, -1656.191], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  127/10000: episode: 127, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5005.730, mean reward: -5005.730 [-5005.730, -5005.730], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  128/10000: episode: 128, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -13861.636, mean reward: -13861.636 [-13861.636, -13861.636], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  129/10000: episode: 129, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5730.116, mean reward: -5730.116 [-5730.116, -5730.116], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  130/10000: episode: 130, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5440.719, mean reward: -5440.719 [-5440.719, -5440.719], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  131/10000: episode: 131, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -9493.977, mean reward: -9493.977 [-9493.977, -9493.977], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  132/10000: episode: 132, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -13899.947, mean reward: -13899.947 [-13899.947, -13899.947], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  133/10000: episode: 133, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4043.288, mean reward: -4043.288 [-4043.288, -4043.288], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  134/10000: episode: 134, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1094.630, mean reward: -1094.630 [-1094.630, -1094.630], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  135/10000: episode: 135, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5025.495, mean reward: -5025.495 [-5025.495, -5025.495], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  136/10000: episode: 136, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -8231.819, mean reward: -8231.819 [-8231.819, -8231.819], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  137/10000: episode: 137, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -6031.622, mean reward: -6031.622 [-6031.622, -6031.622], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  138/10000: episode: 138, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -6072.165, mean reward: -6072.165 [-6072.165, -6072.165], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  139/10000: episode: 139, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1706.757, mean reward: -1706.757 [-1706.757, -1706.757], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  140/10000: episode: 140, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -10000.804, mean reward: -10000.804 [-10000.804, -10000.804], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  141/10000: episode: 141, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -4370.713, mean reward: -4370.713 [-4370.713, -4370.713], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  142/10000: episode: 142, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5079.950, mean reward: -5079.950 [-5079.950, -5079.950], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  143/10000: episode: 143, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4299.917, mean reward: -4299.917 [-4299.917, -4299.917], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  144/10000: episode: 144, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5382.847, mean reward: -5382.847 [-5382.847, -5382.847], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  145/10000: episode: 145, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5371.365, mean reward: -5371.365 [-5371.365, -5371.365], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  146/10000: episode: 146, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -15877.117, mean reward: -15877.117 [-15877.117, -15877.117], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  147/10000: episode: 147, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -6645.915, mean reward: -6645.915 [-6645.915, -6645.915], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  148/10000: episode: 148, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4514.566, mean reward: -4514.566 [-4514.566, -4514.566], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  149/10000: episode: 149, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -10338.889, mean reward: -10338.889 [-10338.889, -10338.889], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  150/10000: episode: 150, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2342.318, mean reward: -2342.318 [-2342.318, -2342.318], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  151/10000: episode: 151, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -8340.012, mean reward: -8340.012 [-8340.012, -8340.012], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  152/10000: episode: 152, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9479.332, mean reward: -9479.332 [-9479.332, -9479.332], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  153/10000: episode: 153, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5845.130, mean reward: -5845.130 [-5845.130, -5845.130], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  154/10000: episode: 154, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4049.626, mean reward: -4049.626 [-4049.626, -4049.626], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  155/10000: episode: 155, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -15412.524, mean reward: -15412.524 [-15412.524, -15412.524], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  156/10000: episode: 156, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2572.830, mean reward: -2572.830 [-2572.830, -2572.830], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  157/10000: episode: 157, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1956.002, mean reward: -1956.002 [-1956.002, -1956.002], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  158/10000: episode: 158, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11227.648, mean reward: -11227.648 [-11227.648, -11227.648], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  159/10000: episode: 159, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -7297.685, mean reward: -7297.685 [-7297.685, -7297.685], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  160/10000: episode: 160, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4368.483, mean reward: -4368.483 [-4368.483, -4368.483], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  161/10000: episode: 161, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -4636.712, mean reward: -4636.712 [-4636.712, -4636.712], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  162/10000: episode: 162, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1410.763, mean reward: -1410.763 [-1410.763, -1410.763], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  163/10000: episode: 163, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5443.003, mean reward: -5443.003 [-5443.003, -5443.003], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  164/10000: episode: 164, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -15934.273, mean reward: -15934.273 [-15934.273, -15934.273], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  165/10000: episode: 165, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -12874.193, mean reward: -12874.193 [-12874.193, -12874.193], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  166/10000: episode: 166, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -4785.280, mean reward: -4785.280 [-4785.280, -4785.280], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  167/10000: episode: 167, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -2266.199, mean reward: -2266.199 [-2266.199, -2266.199], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  168/10000: episode: 168, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -11401.358, mean reward: -11401.358 [-11401.358, -11401.358], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  169/10000: episode: 169, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -6803.646, mean reward: -6803.646 [-6803.646, -6803.646], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  170/10000: episode: 170, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -6144.661, mean reward: -6144.661 [-6144.661, -6144.661], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  171/10000: episode: 171, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5162.414, mean reward: -5162.414 [-5162.414, -5162.414], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  172/10000: episode: 172, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -7136.656, mean reward: -7136.656 [-7136.656, -7136.656], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  173/10000: episode: 173, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1806.166, mean reward: -1806.166 [-1806.166, -1806.166], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  174/10000: episode: 174, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5622.749, mean reward: -5622.749 [-5622.749, -5622.749], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  175/10000: episode: 175, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -1484.988, mean reward: -1484.988 [-1484.988, -1484.988], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  176/10000: episode: 176, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -10852.466, mean reward: -10852.466 [-10852.466, -10852.466], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  177/10000: episode: 177, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -8374.476, mean reward: -8374.476 [-8374.476, -8374.476], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  178/10000: episode: 178, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -11504.621, mean reward: -11504.621 [-11504.621, -11504.621], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  179/10000: episode: 179, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -10792.512, mean reward: -10792.512 [-10792.512, -10792.512], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  180/10000: episode: 180, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5708.116, mean reward: -5708.116 [-5708.116, -5708.116], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  181/10000: episode: 181, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -1184.285, mean reward: -1184.285 [-1184.285, -1184.285], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  182/10000: episode: 182, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -6428.993, mean reward: -6428.993 [-6428.993, -6428.993], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  183/10000: episode: 183, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -4792.411, mean reward: -4792.411 [-4792.411, -4792.411], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  184/10000: episode: 184, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6905.640, mean reward: -6905.640 [-6905.640, -6905.640], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  185/10000: episode: 185, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2607.585, mean reward: -2607.585 [-2607.585, -2607.585], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  186/10000: episode: 186, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5754.594, mean reward: -5754.594 [-5754.594, -5754.594], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  187/10000: episode: 187, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5596.273, mean reward: -5596.273 [-5596.273, -5596.273], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  188/10000: episode: 188, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -13752.204, mean reward: -13752.204 [-13752.204, -13752.204], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  189/10000: episode: 189, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -2458.408, mean reward: -2458.408 [-2458.408, -2458.408], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  190/10000: episode: 190, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7328.359, mean reward: -7328.359 [-7328.359, -7328.359], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  191/10000: episode: 191, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -12087.528, mean reward: -12087.528 [-12087.528, -12087.528], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  192/10000: episode: 192, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5688.973, mean reward: -5688.973 [-5688.973, -5688.973], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  193/10000: episode: 193, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7265.353, mean reward: -7265.353 [-7265.353, -7265.353], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  194/10000: episode: 194, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -7283.698, mean reward: -7283.698 [-7283.698, -7283.698], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  195/10000: episode: 195, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7074.089, mean reward: -7074.089 [-7074.089, -7074.089], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  196/10000: episode: 196, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -11860.278, mean reward: -11860.278 [-11860.278, -11860.278], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  197/10000: episode: 197, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6465.979, mean reward: -6465.979 [-6465.979, -6465.979], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  198/10000: episode: 198, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6331.021, mean reward: -6331.021 [-6331.021, -6331.021], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  199/10000: episode: 199, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -9663.873, mean reward: -9663.873 [-9663.873, -9663.873], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  200/10000: episode: 200, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6442.875, mean reward: -6442.875 [-6442.875, -6442.875], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  201/10000: episode: 201, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -15359.726, mean reward: -15359.726 [-15359.726, -15359.726], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  202/10000: episode: 202, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10336.921, mean reward: -10336.921 [-10336.921, -10336.921], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  203/10000: episode: 203, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3080.271, mean reward: -3080.271 [-3080.271, -3080.271], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  204/10000: episode: 204, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3103.088, mean reward: -3103.088 [-3103.088, -3103.088], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  205/10000: episode: 205, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -12020.660, mean reward: -12020.660 [-12020.660, -12020.660], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  206/10000: episode: 206, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6458.253, mean reward: -6458.253 [-6458.253, -6458.253], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  207/10000: episode: 207, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5541.698, mean reward: -5541.698 [-5541.698, -5541.698], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  208/10000: episode: 208, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10301.915, mean reward: -10301.915 [-10301.915, -10301.915], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  209/10000: episode: 209, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -10245.125, mean reward: -10245.125 [-10245.125, -10245.125], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  210/10000: episode: 210, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -11893.498, mean reward: -11893.498 [-11893.498, -11893.498], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  211/10000: episode: 211, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -8123.336, mean reward: -8123.336 [-8123.336, -8123.336], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  212/10000: episode: 212, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5329.278, mean reward: -5329.278 [-5329.278, -5329.278], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  213/10000: episode: 213, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -14688.347, mean reward: -14688.347 [-14688.347, -14688.347], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  214/10000: episode: 214, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -4904.872, mean reward: -4904.872 [-4904.872, -4904.872], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  215/10000: episode: 215, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3001.527, mean reward: -3001.527 [-3001.527, -3001.527], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  216/10000: episode: 216, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2669.039, mean reward: -2669.039 [-2669.039, -2669.039], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  217/10000: episode: 217, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -15333.364, mean reward: -15333.364 [-15333.364, -15333.364], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  218/10000: episode: 218, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -10635.886, mean reward: -10635.886 [-10635.886, -10635.886], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  219/10000: episode: 219, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5008.147, mean reward: -5008.147 [-5008.147, -5008.147], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  220/10000: episode: 220, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6769.295, mean reward: -6769.295 [-6769.295, -6769.295], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  221/10000: episode: 221, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -12117.675, mean reward: -12117.675 [-12117.675, -12117.675], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  222/10000: episode: 222, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10874.449, mean reward: -10874.449 [-10874.449, -10874.449], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  223/10000: episode: 223, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11727.966, mean reward: -11727.966 [-11727.966, -11727.966], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  224/10000: episode: 224, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5384.243, mean reward: -5384.243 [-5384.243, -5384.243], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  225/10000: episode: 225, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -14268.600, mean reward: -14268.600 [-14268.600, -14268.600], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  226/10000: episode: 226, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -3958.695, mean reward: -3958.695 [-3958.695, -3958.695], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  227/10000: episode: 227, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -2037.270, mean reward: -2037.270 [-2037.270, -2037.270], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  228/10000: episode: 228, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3592.863, mean reward: -3592.863 [-3592.863, -3592.863], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  229/10000: episode: 229, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3824.177, mean reward: -3824.177 [-3824.177, -3824.177], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  230/10000: episode: 230, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -2860.966, mean reward: -2860.966 [-2860.966, -2860.966], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  231/10000: episode: 231, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -8925.398, mean reward: -8925.398 [-8925.398, -8925.398], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  232/10000: episode: 232, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -3373.510, mean reward: -3373.510 [-3373.510, -3373.510], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  233/10000: episode: 233, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3017.979, mean reward: -3017.979 [-3017.979, -3017.979], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  234/10000: episode: 234, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -4253.924, mean reward: -4253.924 [-4253.924, -4253.924], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  235/10000: episode: 235, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1309.670, mean reward: -1309.670 [-1309.670, -1309.670], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  236/10000: episode: 236, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9396.623, mean reward: -9396.623 [-9396.623, -9396.623], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  237/10000: episode: 237, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3677.537, mean reward: -3677.537 [-3677.537, -3677.537], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  238/10000: episode: 238, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -2916.546, mean reward: -2916.546 [-2916.546, -2916.546], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  239/10000: episode: 239, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3650.987, mean reward: -3650.987 [-3650.987, -3650.987], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  240/10000: episode: 240, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -11085.438, mean reward: -11085.438 [-11085.438, -11085.438], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  241/10000: episode: 241, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -2451.226, mean reward: -2451.226 [-2451.226, -2451.226], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  242/10000: episode: 242, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -13531.743, mean reward: -13531.743 [-13531.743, -13531.743], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  243/10000: episode: 243, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5738.372, mean reward: -5738.372 [-5738.372, -5738.372], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  244/10000: episode: 244, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -6529.381, mean reward: -6529.381 [-6529.381, -6529.381], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  245/10000: episode: 245, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5468.478, mean reward: -5468.478 [-5468.478, -5468.478], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  246/10000: episode: 246, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -6207.057, mean reward: -6207.057 [-6207.057, -6207.057], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  247/10000: episode: 247, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2256.023, mean reward: -2256.023 [-2256.023, -2256.023], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  248/10000: episode: 248, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4897.174, mean reward: -4897.174 [-4897.174, -4897.174], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  249/10000: episode: 249, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2610.616, mean reward: -2610.616 [-2610.616, -2610.616], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  250/10000: episode: 250, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -15397.741, mean reward: -15397.741 [-15397.741, -15397.741], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  251/10000: episode: 251, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9937.956, mean reward: -9937.956 [-9937.956, -9937.956], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  252/10000: episode: 252, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -6000.631, mean reward: -6000.631 [-6000.631, -6000.631], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  253/10000: episode: 253, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9192.705, mean reward: -9192.705 [-9192.705, -9192.705], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  254/10000: episode: 254, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4668.028, mean reward: -4668.028 [-4668.028, -4668.028], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  255/10000: episode: 255, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -826.980, mean reward: -826.980 [-826.980, -826.980], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  256/10000: episode: 256, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -7784.974, mean reward: -7784.974 [-7784.974, -7784.974], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  257/10000: episode: 257, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -8978.520, mean reward: -8978.520 [-8978.520, -8978.520], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  258/10000: episode: 258, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3875.463, mean reward: -3875.463 [-3875.463, -3875.463], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  259/10000: episode: 259, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2037.576, mean reward: -2037.576 [-2037.576, -2037.576], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  260/10000: episode: 260, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -13373.870, mean reward: -13373.870 [-13373.870, -13373.870], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  261/10000: episode: 261, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -6559.405, mean reward: -6559.405 [-6559.405, -6559.405], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  262/10000: episode: 262, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -12459.247, mean reward: -12459.247 [-12459.247, -12459.247], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  263/10000: episode: 263, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -8416.187, mean reward: -8416.187 [-8416.187, -8416.187], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  264/10000: episode: 264, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -10539.759, mean reward: -10539.759 [-10539.759, -10539.759], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  265/10000: episode: 265, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3673.591, mean reward: -3673.591 [-3673.591, -3673.591], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  266/10000: episode: 266, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4964.687, mean reward: -4964.687 [-4964.687, -4964.687], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  267/10000: episode: 267, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -14396.848, mean reward: -14396.848 [-14396.848, -14396.848], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  268/10000: episode: 268, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -7355.659, mean reward: -7355.659 [-7355.659, -7355.659], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  269/10000: episode: 269, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -9302.085, mean reward: -9302.085 [-9302.085, -9302.085], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  270/10000: episode: 270, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -7482.752, mean reward: -7482.752 [-7482.752, -7482.752], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  271/10000: episode: 271, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7583.886, mean reward: -7583.886 [-7583.886, -7583.886], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  272/10000: episode: 272, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3759.295, mean reward: -3759.295 [-3759.295, -3759.295], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  273/10000: episode: 273, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -4411.459, mean reward: -4411.459 [-4411.459, -4411.459], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  274/10000: episode: 274, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -6012.792, mean reward: -6012.792 [-6012.792, -6012.792], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  275/10000: episode: 275, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2358.529, mean reward: -2358.529 [-2358.529, -2358.529], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  276/10000: episode: 276, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2279.106, mean reward: -2279.106 [-2279.106, -2279.106], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  277/10000: episode: 277, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5139.710, mean reward: -5139.710 [-5139.710, -5139.710], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  278/10000: episode: 278, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -737.665, mean reward: -737.665 [-737.665, -737.665], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  279/10000: episode: 279, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -8720.861, mean reward: -8720.861 [-8720.861, -8720.861], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  280/10000: episode: 280, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -15081.597, mean reward: -15081.597 [-15081.597, -15081.597], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  281/10000: episode: 281, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3365.531, mean reward: -3365.531 [-3365.531, -3365.531], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  282/10000: episode: 282, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -7557.757, mean reward: -7557.757 [-7557.757, -7557.757], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  283/10000: episode: 283, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -10286.433, mean reward: -10286.433 [-10286.433, -10286.433], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  284/10000: episode: 284, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5323.336, mean reward: -5323.336 [-5323.336, -5323.336], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  285/10000: episode: 285, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4594.071, mean reward: -4594.071 [-4594.071, -4594.071], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  286/10000: episode: 286, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -7603.625, mean reward: -7603.625 [-7603.625, -7603.625], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  287/10000: episode: 287, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2515.661, mean reward: -2515.661 [-2515.661, -2515.661], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  288/10000: episode: 288, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -8757.731, mean reward: -8757.731 [-8757.731, -8757.731], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  289/10000: episode: 289, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -14451.212, mean reward: -14451.212 [-14451.212, -14451.212], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  290/10000: episode: 290, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -6124.040, mean reward: -6124.040 [-6124.040, -6124.040], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  291/10000: episode: 291, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4909.033, mean reward: -4909.033 [-4909.033, -4909.033], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  292/10000: episode: 292, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10568.900, mean reward: -10568.900 [-10568.900, -10568.900], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  293/10000: episode: 293, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5712.495, mean reward: -5712.495 [-5712.495, -5712.495], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  294/10000: episode: 294, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5826.383, mean reward: -5826.383 [-5826.383, -5826.383], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  295/10000: episode: 295, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -6347.040, mean reward: -6347.040 [-6347.040, -6347.040], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  296/10000: episode: 296, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -12427.011, mean reward: -12427.011 [-12427.011, -12427.011], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  297/10000: episode: 297, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8226.097, mean reward: -8226.097 [-8226.097, -8226.097], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  298/10000: episode: 298, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4642.457, mean reward: -4642.457 [-4642.457, -4642.457], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  299/10000: episode: 299, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4148.192, mean reward: -4148.192 [-4148.192, -4148.192], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  300/10000: episode: 300, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -12157.739, mean reward: -12157.739 [-12157.739, -12157.739], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  301/10000: episode: 301, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1747.694, mean reward: -1747.694 [-1747.694, -1747.694], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  302/10000: episode: 302, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6689.573, mean reward: -6689.573 [-6689.573, -6689.573], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  303/10000: episode: 303, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -92.418, mean reward: -92.418 [-92.418, -92.418], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  304/10000: episode: 304, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3462.309, mean reward: -3462.309 [-3462.309, -3462.309], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  305/10000: episode: 305, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7516.758, mean reward: -7516.758 [-7516.758, -7516.758], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  306/10000: episode: 306, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -13531.730, mean reward: -13531.730 [-13531.730, -13531.730], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  307/10000: episode: 307, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -9474.960, mean reward: -9474.960 [-9474.960, -9474.960], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  308/10000: episode: 308, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -4893.485, mean reward: -4893.485 [-4893.485, -4893.485], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  309/10000: episode: 309, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10951.103, mean reward: -10951.103 [-10951.103, -10951.103], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  310/10000: episode: 310, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4624.627, mean reward: -4624.627 [-4624.627, -4624.627], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  311/10000: episode: 311, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4483.135, mean reward: -4483.135 [-4483.135, -4483.135], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  312/10000: episode: 312, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11868.752, mean reward: -11868.752 [-11868.752, -11868.752], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  313/10000: episode: 313, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -7796.466, mean reward: -7796.466 [-7796.466, -7796.466], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  314/10000: episode: 314, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5912.756, mean reward: -5912.756 [-5912.756, -5912.756], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  315/10000: episode: 315, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5570.657, mean reward: -5570.657 [-5570.657, -5570.657], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  316/10000: episode: 316, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -9630.401, mean reward: -9630.401 [-9630.401, -9630.401], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  317/10000: episode: 317, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2982.895, mean reward: -2982.895 [-2982.895, -2982.895], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  318/10000: episode: 318, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1466.578, mean reward: -1466.578 [-1466.578, -1466.578], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  319/10000: episode: 319, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5579.984, mean reward: -5579.984 [-5579.984, -5579.984], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  320/10000: episode: 320, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -2274.093, mean reward: -2274.093 [-2274.093, -2274.093], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  321/10000: episode: 321, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5976.105, mean reward: -5976.105 [-5976.105, -5976.105], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  322/10000: episode: 322, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -9795.151, mean reward: -9795.151 [-9795.151, -9795.151], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  323/10000: episode: 323, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -8130.663, mean reward: -8130.663 [-8130.663, -8130.663], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  324/10000: episode: 324, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -9028.702, mean reward: -9028.702 [-9028.702, -9028.702], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  325/10000: episode: 325, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -8271.125, mean reward: -8271.125 [-8271.125, -8271.125], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  326/10000: episode: 326, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11007.610, mean reward: -11007.610 [-11007.610, -11007.610], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  327/10000: episode: 327, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -12025.528, mean reward: -12025.528 [-12025.528, -12025.528], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  328/10000: episode: 328, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -10078.231, mean reward: -10078.231 [-10078.231, -10078.231], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  329/10000: episode: 329, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -11237.333, mean reward: -11237.333 [-11237.333, -11237.333], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  330/10000: episode: 330, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -11963.414, mean reward: -11963.414 [-11963.414, -11963.414], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  331/10000: episode: 331, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3119.890, mean reward: -3119.890 [-3119.890, -3119.890], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  332/10000: episode: 332, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -7588.725, mean reward: -7588.725 [-7588.725, -7588.725], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  333/10000: episode: 333, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -14675.053, mean reward: -14675.053 [-14675.053, -14675.053], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  334/10000: episode: 334, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9443.988, mean reward: -9443.988 [-9443.988, -9443.988], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  335/10000: episode: 335, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5105.705, mean reward: -5105.705 [-5105.705, -5105.705], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  336/10000: episode: 336, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5889.948, mean reward: -5889.948 [-5889.948, -5889.948], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  337/10000: episode: 337, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -13646.721, mean reward: -13646.721 [-13646.721, -13646.721], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  338/10000: episode: 338, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -10790.966, mean reward: -10790.966 [-10790.966, -10790.966], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  339/10000: episode: 339, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4787.615, mean reward: -4787.615 [-4787.615, -4787.615], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  340/10000: episode: 340, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -7868.996, mean reward: -7868.996 [-7868.996, -7868.996], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  341/10000: episode: 341, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -3210.927, mean reward: -3210.927 [-3210.927, -3210.927], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  342/10000: episode: 342, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7377.543, mean reward: -7377.543 [-7377.543, -7377.543], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  343/10000: episode: 343, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8833.863, mean reward: -8833.863 [-8833.863, -8833.863], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  344/10000: episode: 344, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -17941.264, mean reward: -17941.264 [-17941.264, -17941.264], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  345/10000: episode: 345, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -7910.333, mean reward: -7910.333 [-7910.333, -7910.333], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  346/10000: episode: 346, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -8712.014, mean reward: -8712.014 [-8712.014, -8712.014], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  347/10000: episode: 347, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -10353.977, mean reward: -10353.977 [-10353.977, -10353.977], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  348/10000: episode: 348, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -17768.092, mean reward: -17768.092 [-17768.092, -17768.092], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  349/10000: episode: 349, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -8640.158, mean reward: -8640.158 [-8640.158, -8640.158], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  350/10000: episode: 350, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2189.820, mean reward: -2189.820 [-2189.820, -2189.820], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  351/10000: episode: 351, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1976.173, mean reward: -1976.173 [-1976.173, -1976.173], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  352/10000: episode: 352, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7397.639, mean reward: -7397.639 [-7397.639, -7397.639], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  353/10000: episode: 353, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -9933.401, mean reward: -9933.401 [-9933.401, -9933.401], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  354/10000: episode: 354, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12532.864, mean reward: -12532.864 [-12532.864, -12532.864], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  355/10000: episode: 355, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -11072.460, mean reward: -11072.460 [-11072.460, -11072.460], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  356/10000: episode: 356, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -8905.597, mean reward: -8905.597 [-8905.597, -8905.597], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  357/10000: episode: 357, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5346.649, mean reward: -5346.649 [-5346.649, -5346.649], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  358/10000: episode: 358, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5723.313, mean reward: -5723.313 [-5723.313, -5723.313], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  359/10000: episode: 359, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -10017.818, mean reward: -10017.818 [-10017.818, -10017.818], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  360/10000: episode: 360, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5271.940, mean reward: -5271.940 [-5271.940, -5271.940], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  361/10000: episode: 361, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4998.333, mean reward: -4998.333 [-4998.333, -4998.333], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  362/10000: episode: 362, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -8242.971, mean reward: -8242.971 [-8242.971, -8242.971], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  363/10000: episode: 363, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -1099.837, mean reward: -1099.837 [-1099.837, -1099.837], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  364/10000: episode: 364, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -8317.922, mean reward: -8317.922 [-8317.922, -8317.922], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  365/10000: episode: 365, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -4119.760, mean reward: -4119.760 [-4119.760, -4119.760], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  366/10000: episode: 366, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -2223.357, mean reward: -2223.357 [-2223.357, -2223.357], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  367/10000: episode: 367, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -7463.704, mean reward: -7463.704 [-7463.704, -7463.704], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  368/10000: episode: 368, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -9869.540, mean reward: -9869.540 [-9869.540, -9869.540], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  369/10000: episode: 369, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -12072.110, mean reward: -12072.110 [-12072.110, -12072.110], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  370/10000: episode: 370, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -11228.582, mean reward: -11228.582 [-11228.582, -11228.582], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  371/10000: episode: 371, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3714.064, mean reward: -3714.064 [-3714.064, -3714.064], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  372/10000: episode: 372, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5442.799, mean reward: -5442.799 [-5442.799, -5442.799], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  373/10000: episode: 373, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -8261.527, mean reward: -8261.527 [-8261.527, -8261.527], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  374/10000: episode: 374, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -4430.691, mean reward: -4430.691 [-4430.691, -4430.691], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  375/10000: episode: 375, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6406.951, mean reward: -6406.951 [-6406.951, -6406.951], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  376/10000: episode: 376, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5268.115, mean reward: -5268.115 [-5268.115, -5268.115], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  377/10000: episode: 377, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -10455.150, mean reward: -10455.150 [-10455.150, -10455.150], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  378/10000: episode: 378, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2962.741, mean reward: -2962.741 [-2962.741, -2962.741], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  379/10000: episode: 379, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4087.144, mean reward: -4087.144 [-4087.144, -4087.144], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  380/10000: episode: 380, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -8011.749, mean reward: -8011.749 [-8011.749, -8011.749], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  381/10000: episode: 381, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -6809.106, mean reward: -6809.106 [-6809.106, -6809.106], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  382/10000: episode: 382, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3741.294, mean reward: -3741.294 [-3741.294, -3741.294], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  383/10000: episode: 383, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5793.311, mean reward: -5793.311 [-5793.311, -5793.311], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  384/10000: episode: 384, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -8249.949, mean reward: -8249.949 [-8249.949, -8249.949], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  385/10000: episode: 385, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3945.337, mean reward: -3945.337 [-3945.337, -3945.337], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  386/10000: episode: 386, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2831.328, mean reward: -2831.328 [-2831.328, -2831.328], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  387/10000: episode: 387, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5440.727, mean reward: -5440.727 [-5440.727, -5440.727], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  388/10000: episode: 388, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2342.934, mean reward: -2342.934 [-2342.934, -2342.934], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  389/10000: episode: 389, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7011.584, mean reward: -7011.584 [-7011.584, -7011.584], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  390/10000: episode: 390, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -3874.167, mean reward: -3874.167 [-3874.167, -3874.167], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  391/10000: episode: 391, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2751.218, mean reward: -2751.218 [-2751.218, -2751.218], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  392/10000: episode: 392, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2888.642, mean reward: -2888.642 [-2888.642, -2888.642], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  393/10000: episode: 393, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -12196.149, mean reward: -12196.149 [-12196.149, -12196.149], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  394/10000: episode: 394, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -7554.178, mean reward: -7554.178 [-7554.178, -7554.178], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  395/10000: episode: 395, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1086.979, mean reward: -1086.979 [-1086.979, -1086.979], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  396/10000: episode: 396, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -17401.770, mean reward: -17401.770 [-17401.770, -17401.770], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  397/10000: episode: 397, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9710.139, mean reward: -9710.139 [-9710.139, -9710.139], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  398/10000: episode: 398, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3345.319, mean reward: -3345.319 [-3345.319, -3345.319], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  399/10000: episode: 399, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -10461.468, mean reward: -10461.468 [-10461.468, -10461.468], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  400/10000: episode: 400, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -14864.156, mean reward: -14864.156 [-14864.156, -14864.156], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  401/10000: episode: 401, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -7649.287, mean reward: -7649.287 [-7649.287, -7649.287], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  402/10000: episode: 402, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4966.590, mean reward: -4966.590 [-4966.590, -4966.590], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  403/10000: episode: 403, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -8104.991, mean reward: -8104.991 [-8104.991, -8104.991], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  404/10000: episode: 404, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1973.745, mean reward: -1973.745 [-1973.745, -1973.745], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  405/10000: episode: 405, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -11695.982, mean reward: -11695.982 [-11695.982, -11695.982], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  406/10000: episode: 406, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -2910.102, mean reward: -2910.102 [-2910.102, -2910.102], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  407/10000: episode: 407, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -6000.346, mean reward: -6000.346 [-6000.346, -6000.346], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  408/10000: episode: 408, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -6833.437, mean reward: -6833.437 [-6833.437, -6833.437], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  409/10000: episode: 409, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2254.577, mean reward: -2254.577 [-2254.577, -2254.577], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  410/10000: episode: 410, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -8704.408, mean reward: -8704.408 [-8704.408, -8704.408], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  411/10000: episode: 411, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -12607.926, mean reward: -12607.926 [-12607.926, -12607.926], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  412/10000: episode: 412, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -11678.483, mean reward: -11678.483 [-11678.483, -11678.483], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  413/10000: episode: 413, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -6553.128, mean reward: -6553.128 [-6553.128, -6553.128], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  414/10000: episode: 414, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4449.325, mean reward: -4449.325 [-4449.325, -4449.325], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  415/10000: episode: 415, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6817.569, mean reward: -6817.569 [-6817.569, -6817.569], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  416/10000: episode: 416, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4755.889, mean reward: -4755.889 [-4755.889, -4755.889], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  417/10000: episode: 417, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -3840.175, mean reward: -3840.175 [-3840.175, -3840.175], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  418/10000: episode: 418, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -11719.200, mean reward: -11719.200 [-11719.200, -11719.200], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  419/10000: episode: 419, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4916.843, mean reward: -4916.843 [-4916.843, -4916.843], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  420/10000: episode: 420, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -8468.598, mean reward: -8468.598 [-8468.598, -8468.598], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  421/10000: episode: 421, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -1536.933, mean reward: -1536.933 [-1536.933, -1536.933], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  422/10000: episode: 422, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5300.937, mean reward: -5300.937 [-5300.937, -5300.937], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  423/10000: episode: 423, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5933.595, mean reward: -5933.595 [-5933.595, -5933.595], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  424/10000: episode: 424, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -8576.552, mean reward: -8576.552 [-8576.552, -8576.552], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  425/10000: episode: 425, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -4532.095, mean reward: -4532.095 [-4532.095, -4532.095], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  426/10000: episode: 426, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7706.260, mean reward: -7706.260 [-7706.260, -7706.260], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  427/10000: episode: 427, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -14291.449, mean reward: -14291.449 [-14291.449, -14291.449], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  428/10000: episode: 428, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3444.068, mean reward: -3444.068 [-3444.068, -3444.068], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  429/10000: episode: 429, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5320.043, mean reward: -5320.043 [-5320.043, -5320.043], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  430/10000: episode: 430, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -8079.573, mean reward: -8079.573 [-8079.573, -8079.573], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  431/10000: episode: 431, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2495.368, mean reward: -2495.368 [-2495.368, -2495.368], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  432/10000: episode: 432, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -3946.489, mean reward: -3946.489 [-3946.489, -3946.489], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  433/10000: episode: 433, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -4493.710, mean reward: -4493.710 [-4493.710, -4493.710], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  434/10000: episode: 434, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7150.175, mean reward: -7150.175 [-7150.175, -7150.175], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  435/10000: episode: 435, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -10105.945, mean reward: -10105.945 [-10105.945, -10105.945], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  436/10000: episode: 436, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -8335.228, mean reward: -8335.228 [-8335.228, -8335.228], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  437/10000: episode: 437, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -9380.320, mean reward: -9380.320 [-9380.320, -9380.320], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  438/10000: episode: 438, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6140.866, mean reward: -6140.866 [-6140.866, -6140.866], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  439/10000: episode: 439, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6110.902, mean reward: -6110.902 [-6110.902, -6110.902], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  440/10000: episode: 440, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -10418.050, mean reward: -10418.050 [-10418.050, -10418.050], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  441/10000: episode: 441, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -3458.326, mean reward: -3458.326 [-3458.326, -3458.326], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  442/10000: episode: 442, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2919.274, mean reward: -2919.274 [-2919.274, -2919.274], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  443/10000: episode: 443, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7137.627, mean reward: -7137.627 [-7137.627, -7137.627], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  444/10000: episode: 444, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -11193.588, mean reward: -11193.588 [-11193.588, -11193.588], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  445/10000: episode: 445, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5542.069, mean reward: -5542.069 [-5542.069, -5542.069], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  446/10000: episode: 446, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2281.357, mean reward: -2281.357 [-2281.357, -2281.357], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  447/10000: episode: 447, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2913.401, mean reward: -2913.401 [-2913.401, -2913.401], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  448/10000: episode: 448, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2671.605, mean reward: -2671.605 [-2671.605, -2671.605], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  449/10000: episode: 449, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1124.438, mean reward: -1124.438 [-1124.438, -1124.438], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  450/10000: episode: 450, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4128.485, mean reward: -4128.485 [-4128.485, -4128.485], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  451/10000: episode: 451, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -12751.829, mean reward: -12751.829 [-12751.829, -12751.829], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  452/10000: episode: 452, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -8358.965, mean reward: -8358.965 [-8358.965, -8358.965], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  453/10000: episode: 453, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5475.603, mean reward: -5475.603 [-5475.603, -5475.603], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  454/10000: episode: 454, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5635.304, mean reward: -5635.304 [-5635.304, -5635.304], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  455/10000: episode: 455, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -6189.636, mean reward: -6189.636 [-6189.636, -6189.636], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  456/10000: episode: 456, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -3959.657, mean reward: -3959.657 [-3959.657, -3959.657], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  457/10000: episode: 457, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -9896.370, mean reward: -9896.370 [-9896.370, -9896.370], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  458/10000: episode: 458, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7052.932, mean reward: -7052.932 [-7052.932, -7052.932], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  459/10000: episode: 459, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2371.367, mean reward: -2371.367 [-2371.367, -2371.367], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  460/10000: episode: 460, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -4856.434, mean reward: -4856.434 [-4856.434, -4856.434], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  461/10000: episode: 461, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2414.665, mean reward: -2414.665 [-2414.665, -2414.665], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  462/10000: episode: 462, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -10976.787, mean reward: -10976.787 [-10976.787, -10976.787], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  463/10000: episode: 463, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -8140.253, mean reward: -8140.253 [-8140.253, -8140.253], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  464/10000: episode: 464, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5914.693, mean reward: -5914.693 [-5914.693, -5914.693], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  465/10000: episode: 465, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5709.539, mean reward: -5709.539 [-5709.539, -5709.539], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  466/10000: episode: 466, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -7108.733, mean reward: -7108.733 [-7108.733, -7108.733], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  467/10000: episode: 467, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -13547.328, mean reward: -13547.328 [-13547.328, -13547.328], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  468/10000: episode: 468, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -11912.018, mean reward: -11912.018 [-11912.018, -11912.018], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  469/10000: episode: 469, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -9277.682, mean reward: -9277.682 [-9277.682, -9277.682], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  470/10000: episode: 470, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6386.110, mean reward: -6386.110 [-6386.110, -6386.110], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  471/10000: episode: 471, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8995.025, mean reward: -8995.025 [-8995.025, -8995.025], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  472/10000: episode: 472, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4255.789, mean reward: -4255.789 [-4255.789, -4255.789], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  473/10000: episode: 473, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -4639.620, mean reward: -4639.620 [-4639.620, -4639.620], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  474/10000: episode: 474, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -2738.113, mean reward: -2738.113 [-2738.113, -2738.113], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  475/10000: episode: 475, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2970.093, mean reward: -2970.093 [-2970.093, -2970.093], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  476/10000: episode: 476, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6382.922, mean reward: -6382.922 [-6382.922, -6382.922], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  477/10000: episode: 477, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5839.721, mean reward: -5839.721 [-5839.721, -5839.721], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  478/10000: episode: 478, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -16225.218, mean reward: -16225.218 [-16225.218, -16225.218], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  479/10000: episode: 479, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1465.564, mean reward: -1465.564 [-1465.564, -1465.564], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  480/10000: episode: 480, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -8162.257, mean reward: -8162.257 [-8162.257, -8162.257], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  481/10000: episode: 481, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -8399.268, mean reward: -8399.268 [-8399.268, -8399.268], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  482/10000: episode: 482, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -12266.304, mean reward: -12266.304 [-12266.304, -12266.304], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  483/10000: episode: 483, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2346.528, mean reward: -2346.528 [-2346.528, -2346.528], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  484/10000: episode: 484, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2593.147, mean reward: -2593.147 [-2593.147, -2593.147], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  485/10000: episode: 485, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5429.340, mean reward: -5429.340 [-5429.340, -5429.340], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  486/10000: episode: 486, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2785.505, mean reward: -2785.505 [-2785.505, -2785.505], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  487/10000: episode: 487, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -7340.316, mean reward: -7340.316 [-7340.316, -7340.316], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  488/10000: episode: 488, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -4938.935, mean reward: -4938.935 [-4938.935, -4938.935], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  489/10000: episode: 489, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2620.533, mean reward: -2620.533 [-2620.533, -2620.533], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  490/10000: episode: 490, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -12657.642, mean reward: -12657.642 [-12657.642, -12657.642], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  491/10000: episode: 491, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2047.086, mean reward: -2047.086 [-2047.086, -2047.086], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  492/10000: episode: 492, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -14587.658, mean reward: -14587.658 [-14587.658, -14587.658], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  493/10000: episode: 493, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -7240.183, mean reward: -7240.183 [-7240.183, -7240.183], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  494/10000: episode: 494, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7348.203, mean reward: -7348.203 [-7348.203, -7348.203], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  495/10000: episode: 495, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3147.861, mean reward: -3147.861 [-3147.861, -3147.861], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  496/10000: episode: 496, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -8546.744, mean reward: -8546.744 [-8546.744, -8546.744], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  497/10000: episode: 497, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5799.807, mean reward: -5799.807 [-5799.807, -5799.807], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  498/10000: episode: 498, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5602.357, mean reward: -5602.357 [-5602.357, -5602.357], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  499/10000: episode: 499, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3327.069, mean reward: -3327.069 [-3327.069, -3327.069], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  500/10000: episode: 500, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5808.030, mean reward: -5808.030 [-5808.030, -5808.030], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  501/10000: episode: 501, duration: 0.516s, episode steps:   1, steps per second:   2, episode reward: -1534.727, mean reward: -1534.727 [-1534.727, -1534.727], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  502/10000: episode: 502, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -4140.649, mean reward: -4140.649 [-4140.649, -4140.649], mean action: 0.000 [0.000, 0.000],  loss: 28797458.000000, mae: 1684.088135, mean_q: 0.472135, mean_eps: 0.095491
  503/10000: episode: 503, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -12701.491, mean reward: -12701.491 [-12701.491, -12701.491], mean action: 0.000 [0.000, 0.000],  loss: 38035704.000000, mae: 1991.939453, mean_q: 0.420597, mean_eps: 0.095482
  504/10000: episode: 504, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -10123.995, mean reward: -10123.995 [-10123.995, -10123.995], mean action: 0.000 [0.000, 0.000],  loss: 21205500.000000, mae: 1451.519653, mean_q: 0.387017, mean_eps: 0.095473
  505/10000: episode: 505, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -13192.940, mean reward: -13192.940 [-13192.940, -13192.940], mean action: 0.000 [0.000, 0.000],  loss: 21253008.000000, mae: 1414.885620, mean_q: 0.349294, mean_eps: 0.095464
  506/10000: episode: 506, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -7157.730, mean reward: -7157.730 [-7157.730, -7157.730], mean action: 0.000 [0.000, 0.000],  loss: 38714560.000000, mae: 1866.796509, mean_q: 0.303558, mean_eps: 0.095455
  507/10000: episode: 507, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -6243.253, mean reward: -6243.253 [-6243.253, -6243.253], mean action: 0.000 [0.000, 0.000],  loss: 46243136.000000, mae: 2167.942383, mean_q: 0.250406, mean_eps: 0.095446
  508/10000: episode: 508, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1541.701, mean reward: -1541.701 [-1541.701, -1541.701], mean action: 0.000 [0.000, 0.000],  loss: 32049524.000000, mae: 1769.593506, mean_q: 0.213209, mean_eps: 0.095437
  509/10000: episode: 509, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -6110.584, mean reward: -6110.584 [-6110.584, -6110.584], mean action: 0.000 [0.000, 0.000],  loss: 32255636.000000, mae: 1765.200928, mean_q: 0.169764, mean_eps: 0.095428
  510/10000: episode: 510, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7038.271, mean reward: -7038.271 [-7038.271, -7038.271], mean action: 0.000 [0.000, 0.000],  loss: 32141370.000000, mae: 1788.658936, mean_q: 0.114339, mean_eps: 0.095419
  511/10000: episode: 511, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -8394.953, mean reward: -8394.953 [-8394.953, -8394.953], mean action: 0.000 [0.000, 0.000],  loss: 25085122.000000, mae: 1553.493896, mean_q: 0.081752, mean_eps: 0.095410
  512/10000: episode: 512, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -2239.355, mean reward: -2239.355 [-2239.355, -2239.355], mean action: 2.000 [2.000, 2.000],  loss: 30630928.000000, mae: 1641.218140, mean_q: 0.045464, mean_eps: 0.095401
  513/10000: episode: 513, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5106.961, mean reward: -5106.961 [-5106.961, -5106.961], mean action: 1.000 [1.000, 1.000],  loss: 33162638.000000, mae: 1775.752197, mean_q: 0.006735, mean_eps: 0.095392
  514/10000: episode: 514, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -1188.690, mean reward: -1188.690 [-1188.690, -1188.690], mean action: 3.000 [3.000, 3.000],  loss: 34955132.000000, mae: 1803.504395, mean_q: 0.014458, mean_eps: 0.095383
  515/10000: episode: 515, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -7218.105, mean reward: -7218.105 [-7218.105, -7218.105], mean action: 0.000 [0.000, 0.000],  loss: 31300628.000000, mae: 1757.045898, mean_q: 0.013122, mean_eps: 0.095374
  516/10000: episode: 516, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -7257.280, mean reward: -7257.280 [-7257.280, -7257.280], mean action: 1.000 [1.000, 1.000],  loss: 33291028.000000, mae: 1761.194336, mean_q: 0.007700, mean_eps: 0.095365
  517/10000: episode: 517, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4345.919, mean reward: -4345.919 [-4345.919, -4345.919], mean action: 0.000 [0.000, 0.000],  loss: 29784076.000000, mae: 1753.895996, mean_q: 0.009322, mean_eps: 0.095356
  518/10000: episode: 518, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3389.720, mean reward: -3389.720 [-3389.720, -3389.720], mean action: 1.000 [1.000, 1.000],  loss: 33106044.000000, mae: 1744.862061, mean_q: 0.015921, mean_eps: 0.095347
  519/10000: episode: 519, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3994.317, mean reward: -3994.317 [-3994.317, -3994.317], mean action: 1.000 [1.000, 1.000],  loss: 26848858.000000, mae: 1641.334106, mean_q: 0.018019, mean_eps: 0.095338
  520/10000: episode: 520, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5255.067, mean reward: -5255.067 [-5255.067, -5255.067], mean action: 1.000 [1.000, 1.000],  loss: 29601860.000000, mae: 1705.747803, mean_q: 0.030115, mean_eps: 0.095329
  521/10000: episode: 521, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3545.206, mean reward: -3545.206 [-3545.206, -3545.206], mean action: 1.000 [1.000, 1.000],  loss: 39089592.000000, mae: 2002.728149, mean_q: 0.033459, mean_eps: 0.095320
  522/10000: episode: 522, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -7414.293, mean reward: -7414.293 [-7414.293, -7414.293], mean action: 1.000 [1.000, 1.000],  loss: 25565972.000000, mae: 1556.279053, mean_q: 0.040600, mean_eps: 0.095311
  523/10000: episode: 523, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4932.628, mean reward: -4932.628 [-4932.628, -4932.628], mean action: 1.000 [1.000, 1.000],  loss: 32220268.000000, mae: 1775.727295, mean_q: 0.053564, mean_eps: 0.095302
  524/10000: episode: 524, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2134.843, mean reward: -2134.843 [-2134.843, -2134.843], mean action: 1.000 [1.000, 1.000],  loss: 31134208.000000, mae: 1830.893677, mean_q: 0.059522, mean_eps: 0.095293
  525/10000: episode: 525, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1347.775, mean reward: -1347.775 [-1347.775, -1347.775], mean action: 1.000 [1.000, 1.000],  loss: 28697914.000000, mae: 1573.492432, mean_q: 0.059873, mean_eps: 0.095284
  526/10000: episode: 526, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7509.961, mean reward: -7509.961 [-7509.961, -7509.961], mean action: 1.000 [1.000, 1.000],  loss: 22561604.000000, mae: 1476.946289, mean_q: 0.065823, mean_eps: 0.095275
  527/10000: episode: 527, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2866.151, mean reward: -2866.151 [-2866.151, -2866.151], mean action: 1.000 [1.000, 1.000],  loss: 27915572.000000, mae: 1653.447388, mean_q: 0.068140, mean_eps: 0.095266
  528/10000: episode: 528, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4665.010, mean reward: -4665.010 [-4665.010, -4665.010], mean action: 1.000 [1.000, 1.000],  loss: 37793072.000000, mae: 1956.242065, mean_q: 0.066676, mean_eps: 0.095257
  529/10000: episode: 529, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -3453.504, mean reward: -3453.504 [-3453.504, -3453.504], mean action: 1.000 [1.000, 1.000],  loss: 40615568.000000, mae: 1953.335205, mean_q: 0.081421, mean_eps: 0.095248
  530/10000: episode: 530, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5788.018, mean reward: -5788.018 [-5788.018, -5788.018], mean action: 1.000 [1.000, 1.000],  loss: 32164520.000000, mae: 1772.296265, mean_q: 0.064595, mean_eps: 0.095239
  531/10000: episode: 531, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3427.320, mean reward: -3427.320 [-3427.320, -3427.320], mean action: 1.000 [1.000, 1.000],  loss: 28567788.000000, mae: 1685.221802, mean_q: 0.055896, mean_eps: 0.095230
  532/10000: episode: 532, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4547.409, mean reward: -4547.409 [-4547.409, -4547.409], mean action: 1.000 [1.000, 1.000],  loss: 29986096.000000, mae: 1705.293823, mean_q: 0.046542, mean_eps: 0.095221
  533/10000: episode: 533, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3414.549, mean reward: -3414.549 [-3414.549, -3414.549], mean action: 1.000 [1.000, 1.000],  loss: 30846092.000000, mae: 1715.679932, mean_q: 0.046627, mean_eps: 0.095212
  534/10000: episode: 534, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5917.367, mean reward: -5917.367 [-5917.367, -5917.367], mean action: 1.000 [1.000, 1.000],  loss: 35703688.000000, mae: 1828.366211, mean_q: 0.048744, mean_eps: 0.095203
  535/10000: episode: 535, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4368.965, mean reward: -4368.965 [-4368.965, -4368.965], mean action: 1.000 [1.000, 1.000],  loss: 22674018.000000, mae: 1457.269043, mean_q: 0.036408, mean_eps: 0.095194
  536/10000: episode: 536, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5576.860, mean reward: -5576.860 [-5576.860, -5576.860], mean action: 1.000 [1.000, 1.000],  loss: 39134296.000000, mae: 1987.552979, mean_q: 0.037287, mean_eps: 0.095185
  537/10000: episode: 537, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7623.010, mean reward: -7623.010 [-7623.010, -7623.010], mean action: 1.000 [1.000, 1.000],  loss: 34075472.000000, mae: 1833.307129, mean_q: 0.029827, mean_eps: 0.095176
  538/10000: episode: 538, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6595.987, mean reward: -6595.987 [-6595.987, -6595.987], mean action: 1.000 [1.000, 1.000],  loss: 34654736.000000, mae: 1846.461548, mean_q: 0.036449, mean_eps: 0.095167
  539/10000: episode: 539, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5360.076, mean reward: -5360.076 [-5360.076, -5360.076], mean action: 1.000 [1.000, 1.000],  loss: 27048184.000000, mae: 1656.256104, mean_q: 0.036763, mean_eps: 0.095158
  540/10000: episode: 540, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4464.206, mean reward: -4464.206 [-4464.206, -4464.206], mean action: 1.000 [1.000, 1.000],  loss: 33565460.000000, mae: 1750.922607, mean_q: 0.029286, mean_eps: 0.095149
  541/10000: episode: 541, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2106.272, mean reward: -2106.272 [-2106.272, -2106.272], mean action: 1.000 [1.000, 1.000],  loss: 32732108.000000, mae: 1811.348633, mean_q: 0.025221, mean_eps: 0.095140
  542/10000: episode: 542, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1532.342, mean reward: -1532.342 [-1532.342, -1532.342], mean action: 1.000 [1.000, 1.000],  loss: 22801488.000000, mae: 1480.878662, mean_q: 0.024950, mean_eps: 0.095131
  543/10000: episode: 543, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5524.735, mean reward: -5524.735 [-5524.735, -5524.735], mean action: 0.000 [0.000, 0.000],  loss: 35961512.000000, mae: 1942.030762, mean_q: 0.027049, mean_eps: 0.095122
  544/10000: episode: 544, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2776.251, mean reward: -2776.251 [-2776.251, -2776.251], mean action: 1.000 [1.000, 1.000],  loss: 24174758.000000, mae: 1593.423340, mean_q: 0.025531, mean_eps: 0.095113
  545/10000: episode: 545, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3553.695, mean reward: -3553.695 [-3553.695, -3553.695], mean action: 1.000 [1.000, 1.000],  loss: 30987932.000000, mae: 1659.336060, mean_q: 0.024035, mean_eps: 0.095104
  546/10000: episode: 546, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9836.203, mean reward: -9836.203 [-9836.203, -9836.203], mean action: 2.000 [2.000, 2.000],  loss: 28784816.000000, mae: 1667.280640, mean_q: 0.021063, mean_eps: 0.095095
  547/10000: episode: 547, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -11466.602, mean reward: -11466.602 [-11466.602, -11466.602], mean action: 1.000 [1.000, 1.000],  loss: 28701666.000000, mae: 1742.704346, mean_q: 0.005502, mean_eps: 0.095086
  548/10000: episode: 548, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5029.184, mean reward: -5029.184 [-5029.184, -5029.184], mean action: 1.000 [1.000, 1.000],  loss: 28692004.000000, mae: 1652.610596, mean_q: 0.008698, mean_eps: 0.095077
  549/10000: episode: 549, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5878.973, mean reward: -5878.973 [-5878.973, -5878.973], mean action: 1.000 [1.000, 1.000],  loss: 31044488.000000, mae: 1751.025391, mean_q: 0.002483, mean_eps: 0.095068
  550/10000: episode: 550, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -7305.733, mean reward: -7305.733 [-7305.733, -7305.733], mean action: 1.000 [1.000, 1.000],  loss: 35843328.000000, mae: 1881.923828, mean_q: -0.009029, mean_eps: 0.095059
  551/10000: episode: 551, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3546.866, mean reward: -3546.866 [-3546.866, -3546.866], mean action: 1.000 [1.000, 1.000],  loss: 28519590.000000, mae: 1709.086548, mean_q: -0.012227, mean_eps: 0.095050
  552/10000: episode: 552, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -16027.780, mean reward: -16027.780 [-16027.780, -16027.780], mean action: 0.000 [0.000, 0.000],  loss: 30620492.000000, mae: 1657.433838, mean_q: -0.021273, mean_eps: 0.095041
  553/10000: episode: 553, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3406.480, mean reward: -3406.480 [-3406.480, -3406.480], mean action: 1.000 [1.000, 1.000],  loss: 32510100.000000, mae: 1814.993896, mean_q: -0.028556, mean_eps: 0.095032
  554/10000: episode: 554, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2043.102, mean reward: -2043.102 [-2043.102, -2043.102], mean action: 1.000 [1.000, 1.000],  loss: 29110660.000000, mae: 1655.713867, mean_q: -0.033782, mean_eps: 0.095023
  555/10000: episode: 555, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4979.357, mean reward: -4979.357 [-4979.357, -4979.357], mean action: 3.000 [3.000, 3.000],  loss: 28333004.000000, mae: 1580.294678, mean_q: -0.057861, mean_eps: 0.095014
  556/10000: episode: 556, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1074.732, mean reward: -1074.732 [-1074.732, -1074.732], mean action: 1.000 [1.000, 1.000],  loss: 26026498.000000, mae: 1588.963379, mean_q: -0.058787, mean_eps: 0.095005
  557/10000: episode: 557, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3852.718, mean reward: -3852.718 [-3852.718, -3852.718], mean action: 1.000 [1.000, 1.000],  loss: 32836528.000000, mae: 1768.533691, mean_q: -0.058474, mean_eps: 0.094996
  558/10000: episode: 558, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7385.328, mean reward: -7385.328 [-7385.328, -7385.328], mean action: 1.000 [1.000, 1.000],  loss: 27672006.000000, mae: 1638.321045, mean_q: -0.072176, mean_eps: 0.094987
  559/10000: episode: 559, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4254.737, mean reward: -4254.737 [-4254.737, -4254.737], mean action: 1.000 [1.000, 1.000],  loss: 37130480.000000, mae: 1881.810669, mean_q: -0.079526, mean_eps: 0.094978
  560/10000: episode: 560, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5905.375, mean reward: -5905.375 [-5905.375, -5905.375], mean action: 1.000 [1.000, 1.000],  loss: 29138756.000000, mae: 1643.895752, mean_q: -0.088966, mean_eps: 0.094969
  561/10000: episode: 561, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1967.215, mean reward: -1967.215 [-1967.215, -1967.215], mean action: 1.000 [1.000, 1.000],  loss: 36050716.000000, mae: 1877.101318, mean_q: -0.087231, mean_eps: 0.094960
  562/10000: episode: 562, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2574.953, mean reward: -2574.953 [-2574.953, -2574.953], mean action: 1.000 [1.000, 1.000],  loss: 28096396.000000, mae: 1729.238770, mean_q: -0.105349, mean_eps: 0.094951
  563/10000: episode: 563, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4326.829, mean reward: -4326.829 [-4326.829, -4326.829], mean action: 1.000 [1.000, 1.000],  loss: 26397630.000000, mae: 1625.550415, mean_q: -0.116702, mean_eps: 0.094942
  564/10000: episode: 564, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3487.620, mean reward: -3487.620 [-3487.620, -3487.620], mean action: 2.000 [2.000, 2.000],  loss: 14823355.000000, mae: 1118.161621, mean_q: -0.133361, mean_eps: 0.094933
  565/10000: episode: 565, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3909.010, mean reward: -3909.010 [-3909.010, -3909.010], mean action: 1.000 [1.000, 1.000],  loss: 29655904.000000, mae: 1629.110840, mean_q: -0.131375, mean_eps: 0.094924
  566/10000: episode: 566, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3428.371, mean reward: -3428.371 [-3428.371, -3428.371], mean action: 1.000 [1.000, 1.000],  loss: 29963624.000000, mae: 1678.944946, mean_q: -0.142037, mean_eps: 0.094915
  567/10000: episode: 567, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4531.078, mean reward: -4531.078 [-4531.078, -4531.078], mean action: 1.000 [1.000, 1.000],  loss: 19908832.000000, mae: 1317.099854, mean_q: -0.160545, mean_eps: 0.094906
  568/10000: episode: 568, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5568.739, mean reward: -5568.739 [-5568.739, -5568.739], mean action: 1.000 [1.000, 1.000],  loss: 26550164.000000, mae: 1611.928833, mean_q: -0.164268, mean_eps: 0.094897
  569/10000: episode: 569, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1414.741, mean reward: -1414.741 [-1414.741, -1414.741], mean action: 1.000 [1.000, 1.000],  loss: 30402298.000000, mae: 1700.439209, mean_q: -0.175720, mean_eps: 0.094888
  570/10000: episode: 570, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -592.262, mean reward: -592.262 [-592.262, -592.262], mean action: 1.000 [1.000, 1.000],  loss: 25221568.000000, mae: 1615.531250, mean_q: -0.187787, mean_eps: 0.094879
  571/10000: episode: 571, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5430.144, mean reward: -5430.144 [-5430.144, -5430.144], mean action: 1.000 [1.000, 1.000],  loss: 27507206.000000, mae: 1568.774414, mean_q: -0.195523, mean_eps: 0.094870
  572/10000: episode: 572, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4003.768, mean reward: -4003.768 [-4003.768, -4003.768], mean action: 1.000 [1.000, 1.000],  loss: 28568216.000000, mae: 1678.251099, mean_q: -0.212095, mean_eps: 0.094861
  573/10000: episode: 573, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2542.079, mean reward: -2542.079 [-2542.079, -2542.079], mean action: 1.000 [1.000, 1.000],  loss: 36316192.000000, mae: 1866.686157, mean_q: -0.221452, mean_eps: 0.094852
  574/10000: episode: 574, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2050.122, mean reward: -2050.122 [-2050.122, -2050.122], mean action: 1.000 [1.000, 1.000],  loss: 26911980.000000, mae: 1540.986450, mean_q: -0.228330, mean_eps: 0.094843
  575/10000: episode: 575, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -3028.525, mean reward: -3028.525 [-3028.525, -3028.525], mean action: 1.000 [1.000, 1.000],  loss: 24868168.000000, mae: 1636.168457, mean_q: -0.243613, mean_eps: 0.094834
  576/10000: episode: 576, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5302.180, mean reward: -5302.180 [-5302.180, -5302.180], mean action: 1.000 [1.000, 1.000],  loss: 29463624.000000, mae: 1647.091675, mean_q: -0.254767, mean_eps: 0.094825
  577/10000: episode: 577, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6256.286, mean reward: -6256.286 [-6256.286, -6256.286], mean action: 1.000 [1.000, 1.000],  loss: 23784536.000000, mae: 1516.371338, mean_q: -0.274927, mean_eps: 0.094816
  578/10000: episode: 578, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9788.841, mean reward: -9788.841 [-9788.841, -9788.841], mean action: 1.000 [1.000, 1.000],  loss: 32628000.000000, mae: 1815.105225, mean_q: -0.287695, mean_eps: 0.094807
  579/10000: episode: 579, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5638.761, mean reward: -5638.761 [-5638.761, -5638.761], mean action: 1.000 [1.000, 1.000],  loss: 24299998.000000, mae: 1594.988525, mean_q: -0.302950, mean_eps: 0.094798
  580/10000: episode: 580, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2388.014, mean reward: -2388.014 [-2388.014, -2388.014], mean action: 1.000 [1.000, 1.000],  loss: 36742336.000000, mae: 1876.415527, mean_q: -0.305309, mean_eps: 0.094789
  581/10000: episode: 581, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5702.602, mean reward: -5702.602 [-5702.602, -5702.602], mean action: 1.000 [1.000, 1.000],  loss: 26636092.000000, mae: 1666.309082, mean_q: -0.317301, mean_eps: 0.094780
  582/10000: episode: 582, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4957.713, mean reward: -4957.713 [-4957.713, -4957.713], mean action: 1.000 [1.000, 1.000],  loss: 27679880.000000, mae: 1679.412109, mean_q: -0.332080, mean_eps: 0.094771
  583/10000: episode: 583, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3315.971, mean reward: -3315.971 [-3315.971, -3315.971], mean action: 1.000 [1.000, 1.000],  loss: 30698516.000000, mae: 1678.485107, mean_q: -0.348599, mean_eps: 0.094762
  584/10000: episode: 584, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3756.633, mean reward: -3756.633 [-3756.633, -3756.633], mean action: 1.000 [1.000, 1.000],  loss: 41174760.000000, mae: 2048.725098, mean_q: -0.352577, mean_eps: 0.094753
  585/10000: episode: 585, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3500.319, mean reward: -3500.319 [-3500.319, -3500.319], mean action: 1.000 [1.000, 1.000],  loss: 22816236.000000, mae: 1458.450684, mean_q: -0.380589, mean_eps: 0.094744
  586/10000: episode: 586, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1938.538, mean reward: -1938.538 [-1938.538, -1938.538], mean action: 1.000 [1.000, 1.000],  loss: 28356572.000000, mae: 1637.537231, mean_q: -0.408280, mean_eps: 0.094735
  587/10000: episode: 587, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1772.161, mean reward: -1772.161 [-1772.161, -1772.161], mean action: 1.000 [1.000, 1.000],  loss: 26721864.000000, mae: 1558.976318, mean_q: -0.416979, mean_eps: 0.094726
  588/10000: episode: 588, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5243.946, mean reward: -5243.946 [-5243.946, -5243.946], mean action: 1.000 [1.000, 1.000],  loss: 28522508.000000, mae: 1664.726196, mean_q: -0.430157, mean_eps: 0.094717
  589/10000: episode: 589, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5081.984, mean reward: -5081.984 [-5081.984, -5081.984], mean action: 1.000 [1.000, 1.000],  loss: 20601184.000000, mae: 1435.164551, mean_q: -0.448210, mean_eps: 0.094708
  590/10000: episode: 590, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4542.036, mean reward: -4542.036 [-4542.036, -4542.036], mean action: 1.000 [1.000, 1.000],  loss: 30120118.000000, mae: 1713.466187, mean_q: -0.461510, mean_eps: 0.094699
  591/10000: episode: 591, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3846.537, mean reward: -3846.537 [-3846.537, -3846.537], mean action: 1.000 [1.000, 1.000],  loss: 32776204.000000, mae: 1829.480957, mean_q: -0.475453, mean_eps: 0.094690
  592/10000: episode: 592, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6403.277, mean reward: -6403.277 [-6403.277, -6403.277], mean action: 1.000 [1.000, 1.000],  loss: 21579100.000000, mae: 1458.975342, mean_q: -0.493427, mean_eps: 0.094681
  593/10000: episode: 593, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6387.710, mean reward: -6387.710 [-6387.710, -6387.710], mean action: 1.000 [1.000, 1.000],  loss: 27052424.000000, mae: 1559.004150, mean_q: -0.525183, mean_eps: 0.094672
  594/10000: episode: 594, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1401.937, mean reward: -1401.937 [-1401.937, -1401.937], mean action: 1.000 [1.000, 1.000],  loss: 35138416.000000, mae: 1820.315552, mean_q: -0.542143, mean_eps: 0.094663
  595/10000: episode: 595, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1683.196, mean reward: -1683.196 [-1683.196, -1683.196], mean action: 1.000 [1.000, 1.000],  loss: 24199592.000000, mae: 1580.037109, mean_q: -0.568423, mean_eps: 0.094654
  596/10000: episode: 596, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2240.010, mean reward: -2240.010 [-2240.010, -2240.010], mean action: 1.000 [1.000, 1.000],  loss: 23200484.000000, mae: 1482.536377, mean_q: -0.586934, mean_eps: 0.094645
  597/10000: episode: 597, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5501.093, mean reward: -5501.093 [-5501.093, -5501.093], mean action: 1.000 [1.000, 1.000],  loss: 29122716.000000, mae: 1649.824707, mean_q: -0.607975, mean_eps: 0.094636
  598/10000: episode: 598, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4475.964, mean reward: -4475.964 [-4475.964, -4475.964], mean action: 1.000 [1.000, 1.000],  loss: 25057352.000000, mae: 1587.888916, mean_q: -0.625375, mean_eps: 0.094627
  599/10000: episode: 599, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3486.756, mean reward: -3486.756 [-3486.756, -3486.756], mean action: 1.000 [1.000, 1.000],  loss: 28056472.000000, mae: 1737.396729, mean_q: -0.648391, mean_eps: 0.094618
  600/10000: episode: 600, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -7788.228, mean reward: -7788.228 [-7788.228, -7788.228], mean action: 1.000 [1.000, 1.000],  loss: 19951620.000000, mae: 1305.116333, mean_q: -0.679575, mean_eps: 0.094609
  601/10000: episode: 601, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6006.853, mean reward: -6006.853 [-6006.853, -6006.853], mean action: 1.000 [1.000, 1.000],  loss: 32507148.000000, mae: 1735.231079, mean_q: -0.700055, mean_eps: 0.094600
  602/10000: episode: 602, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1700.892, mean reward: -1700.892 [-1700.892, -1700.892], mean action: 1.000 [1.000, 1.000],  loss: 28749012.000000, mae: 1647.812500, mean_q: -0.727123, mean_eps: 0.094591
  603/10000: episode: 603, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5509.315, mean reward: -5509.315 [-5509.315, -5509.315], mean action: 1.000 [1.000, 1.000],  loss: 34575616.000000, mae: 1705.767090, mean_q: -0.753960, mean_eps: 0.094582
  604/10000: episode: 604, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7365.956, mean reward: -7365.956 [-7365.956, -7365.956], mean action: 1.000 [1.000, 1.000],  loss: 24023898.000000, mae: 1531.384277, mean_q: -0.786737, mean_eps: 0.094573
  605/10000: episode: 605, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5276.918, mean reward: -5276.918 [-5276.918, -5276.918], mean action: 1.000 [1.000, 1.000],  loss: 25845384.000000, mae: 1589.665161, mean_q: -0.815411, mean_eps: 0.094564
  606/10000: episode: 606, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -679.146, mean reward: -679.146 [-679.146, -679.146], mean action: 2.000 [2.000, 2.000],  loss: 31539782.000000, mae: 1815.607910, mean_q: -0.835656, mean_eps: 0.094555
  607/10000: episode: 607, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3759.386, mean reward: -3759.386 [-3759.386, -3759.386], mean action: 1.000 [1.000, 1.000],  loss: 25481684.000000, mae: 1559.716064, mean_q: -0.864987, mean_eps: 0.094546
  608/10000: episode: 608, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -816.113, mean reward: -816.113 [-816.113, -816.113], mean action: 1.000 [1.000, 1.000],  loss: 27035812.000000, mae: 1639.815918, mean_q: -0.893546, mean_eps: 0.094537
  609/10000: episode: 609, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5031.620, mean reward: -5031.620 [-5031.620, -5031.620], mean action: 1.000 [1.000, 1.000],  loss: 32713724.000000, mae: 1737.450684, mean_q: -0.909488, mean_eps: 0.094528
  610/10000: episode: 610, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3725.959, mean reward: -3725.959 [-3725.959, -3725.959], mean action: 1.000 [1.000, 1.000],  loss: 28087052.000000, mae: 1679.392334, mean_q: -0.939002, mean_eps: 0.094519
  611/10000: episode: 611, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5149.012, mean reward: -5149.012 [-5149.012, -5149.012], mean action: 1.000 [1.000, 1.000],  loss: 27376392.000000, mae: 1709.648804, mean_q: -0.970148, mean_eps: 0.094510
  612/10000: episode: 612, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1357.921, mean reward: -1357.921 [-1357.921, -1357.921], mean action: 3.000 [3.000, 3.000],  loss: 26871956.000000, mae: 1618.218750, mean_q: -0.999009, mean_eps: 0.094501
  613/10000: episode: 613, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -214.721, mean reward: -214.721 [-214.721, -214.721], mean action: 3.000 [3.000, 3.000],  loss: 34393344.000000, mae: 1803.669922, mean_q: -1.016488, mean_eps: 0.094492
  614/10000: episode: 614, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -9875.193, mean reward: -9875.193 [-9875.193, -9875.193], mean action: 1.000 [1.000, 1.000],  loss: 32241764.000000, mae: 1746.951172, mean_q: -1.039016, mean_eps: 0.094483
  615/10000: episode: 615, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7956.351, mean reward: -7956.351 [-7956.351, -7956.351], mean action: 1.000 [1.000, 1.000],  loss: 25262300.000000, mae: 1560.395996, mean_q: -1.065378, mean_eps: 0.094474
  616/10000: episode: 616, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -217.950, mean reward: -217.950 [-217.950, -217.950], mean action: 3.000 [3.000, 3.000],  loss: 24688988.000000, mae: 1522.795166, mean_q: -1.089930, mean_eps: 0.094465
  617/10000: episode: 617, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -466.258, mean reward: -466.258 [-466.258, -466.258], mean action: 3.000 [3.000, 3.000],  loss: 40231200.000000, mae: 2046.533569, mean_q: -1.103348, mean_eps: 0.094456
  618/10000: episode: 618, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -620.745, mean reward: -620.745 [-620.745, -620.745], mean action: 1.000 [1.000, 1.000],  loss: 20736988.000000, mae: 1405.942139, mean_q: -1.135056, mean_eps: 0.094447
  619/10000: episode: 619, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1294.042, mean reward: -1294.042 [-1294.042, -1294.042], mean action: 1.000 [1.000, 1.000],  loss: 27599352.000000, mae: 1643.981689, mean_q: -1.153035, mean_eps: 0.094438
  620/10000: episode: 620, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -74.350, mean reward: -74.350 [-74.350, -74.350], mean action: 3.000 [3.000, 3.000],  loss: 28830944.000000, mae: 1626.310059, mean_q: -1.167381, mean_eps: 0.094429
  621/10000: episode: 621, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -120.371, mean reward: -120.371 [-120.371, -120.371], mean action: 3.000 [3.000, 3.000],  loss: 21834566.000000, mae: 1509.102295, mean_q: -1.200094, mean_eps: 0.094420
  622/10000: episode: 622, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6962.131, mean reward: -6962.131 [-6962.131, -6962.131], mean action: 1.000 [1.000, 1.000],  loss: 28509492.000000, mae: 1626.175537, mean_q: -1.230791, mean_eps: 0.094411
  623/10000: episode: 623, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3119.462, mean reward: -3119.462 [-3119.462, -3119.462], mean action: 3.000 [3.000, 3.000],  loss: 23441602.000000, mae: 1506.697266, mean_q: -1.237943, mean_eps: 0.094402
  624/10000: episode: 624, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4132.098, mean reward: -4132.098 [-4132.098, -4132.098], mean action: 3.000 [3.000, 3.000],  loss: 26817904.000000, mae: 1660.049561, mean_q: -1.259769, mean_eps: 0.094393
  625/10000: episode: 625, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2026.702, mean reward: -2026.702 [-2026.702, -2026.702], mean action: 3.000 [3.000, 3.000],  loss: 29439168.000000, mae: 1587.256836, mean_q: -1.274424, mean_eps: 0.094384
  626/10000: episode: 626, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -7181.642, mean reward: -7181.642 [-7181.642, -7181.642], mean action: 0.000 [0.000, 0.000],  loss: 32927332.000000, mae: 1765.997559, mean_q: -1.287541, mean_eps: 0.094375
  627/10000: episode: 627, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2074.246, mean reward: -2074.246 [-2074.246, -2074.246], mean action: 3.000 [3.000, 3.000],  loss: 31572824.000000, mae: 1749.543213, mean_q: -1.305743, mean_eps: 0.094366
  628/10000: episode: 628, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8278.679, mean reward: -8278.679 [-8278.679, -8278.679], mean action: 3.000 [3.000, 3.000],  loss: 23197652.000000, mae: 1471.342773, mean_q: -1.327580, mean_eps: 0.094357
  629/10000: episode: 629, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -589.403, mean reward: -589.403 [-589.403, -589.403], mean action: 3.000 [3.000, 3.000],  loss: 38268880.000000, mae: 1955.173096, mean_q: -1.340536, mean_eps: 0.094348
  630/10000: episode: 630, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1350.758, mean reward: -1350.758 [-1350.758, -1350.758], mean action: 3.000 [3.000, 3.000],  loss: 21735012.000000, mae: 1408.006592, mean_q: -1.358924, mean_eps: 0.094339
  631/10000: episode: 631, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7181.141, mean reward: -7181.141 [-7181.141, -7181.141], mean action: 3.000 [3.000, 3.000],  loss: 21837730.000000, mae: 1445.954224, mean_q: -1.377936, mean_eps: 0.094330
  632/10000: episode: 632, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -361.939, mean reward: -361.939 [-361.939, -361.939], mean action: 3.000 [3.000, 3.000],  loss: 28227920.000000, mae: 1572.250977, mean_q: -1.406613, mean_eps: 0.094321
  633/10000: episode: 633, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2432.032, mean reward: -2432.032 [-2432.032, -2432.032], mean action: 3.000 [3.000, 3.000],  loss: 19506676.000000, mae: 1423.688232, mean_q: -1.416239, mean_eps: 0.094312
  634/10000: episode: 634, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2251.586, mean reward: -2251.586 [-2251.586, -2251.586], mean action: 3.000 [3.000, 3.000],  loss: 29310096.000000, mae: 1604.009521, mean_q: -1.434206, mean_eps: 0.094303
  635/10000: episode: 635, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8057.415, mean reward: -8057.415 [-8057.415, -8057.415], mean action: 3.000 [3.000, 3.000],  loss: 18614912.000000, mae: 1338.203003, mean_q: -1.449822, mean_eps: 0.094294
  636/10000: episode: 636, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -674.637, mean reward: -674.637 [-674.637, -674.637], mean action: 3.000 [3.000, 3.000],  loss: 19784298.000000, mae: 1438.103394, mean_q: -1.479707, mean_eps: 0.094285
  637/10000: episode: 637, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2330.807, mean reward: -2330.807 [-2330.807, -2330.807], mean action: 3.000 [3.000, 3.000],  loss: 32516110.000000, mae: 1849.022461, mean_q: -1.500073, mean_eps: 0.094276
  638/10000: episode: 638, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2979.841, mean reward: -2979.841 [-2979.841, -2979.841], mean action: 3.000 [3.000, 3.000],  loss: 25204318.000000, mae: 1651.765259, mean_q: -1.536700, mean_eps: 0.094267
  639/10000: episode: 639, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -14.129, mean reward: -14.129 [-14.129, -14.129], mean action: 3.000 [3.000, 3.000],  loss: 28028062.000000, mae: 1621.952637, mean_q: -1.552589, mean_eps: 0.094258
  640/10000: episode: 640, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -3459.301, mean reward: -3459.301 [-3459.301, -3459.301], mean action: 3.000 [3.000, 3.000],  loss: 28181930.000000, mae: 1634.723145, mean_q: -1.561677, mean_eps: 0.094249
  641/10000: episode: 641, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4648.737, mean reward: -4648.737 [-4648.737, -4648.737], mean action: 3.000 [3.000, 3.000],  loss: 30943486.000000, mae: 1755.575562, mean_q: -1.618769, mean_eps: 0.094240
  642/10000: episode: 642, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8903.783, mean reward: -8903.783 [-8903.783, -8903.783], mean action: 3.000 [3.000, 3.000],  loss: 30261474.000000, mae: 1742.338867, mean_q: -1.629733, mean_eps: 0.094231
  643/10000: episode: 643, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -259.537, mean reward: -259.537 [-259.537, -259.537], mean action: 3.000 [3.000, 3.000],  loss: 25105312.000000, mae: 1572.260376, mean_q: -1.679279, mean_eps: 0.094222
  644/10000: episode: 644, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -606.018, mean reward: -606.018 [-606.018, -606.018], mean action: 3.000 [3.000, 3.000],  loss: 34549200.000000, mae: 1800.311890, mean_q: -1.696991, mean_eps: 0.094213
  645/10000: episode: 645, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -343.800, mean reward: -343.800 [-343.800, -343.800], mean action: 3.000 [3.000, 3.000],  loss: 24603894.000000, mae: 1511.105713, mean_q: -1.738014, mean_eps: 0.094204
  646/10000: episode: 646, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2391.499, mean reward: -2391.499 [-2391.499, -2391.499], mean action: 3.000 [3.000, 3.000],  loss: 22088068.000000, mae: 1382.376099, mean_q: -1.764690, mean_eps: 0.094195
  647/10000: episode: 647, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5379.556, mean reward: -5379.556 [-5379.556, -5379.556], mean action: 3.000 [3.000, 3.000],  loss: 20918212.000000, mae: 1401.226562, mean_q: -1.793349, mean_eps: 0.094186
  648/10000: episode: 648, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -11.772, mean reward: -11.772 [-11.772, -11.772], mean action: 2.000 [2.000, 2.000],  loss: 19548980.000000, mae: 1318.815674, mean_q: -1.813417, mean_eps: 0.094177
  649/10000: episode: 649, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1277.530, mean reward: -1277.530 [-1277.530, -1277.530], mean action: 2.000 [2.000, 2.000],  loss: 22859148.000000, mae: 1510.353271, mean_q: -1.831670, mean_eps: 0.094168
  650/10000: episode: 650, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4480.724, mean reward: -4480.724 [-4480.724, -4480.724], mean action: 2.000 [2.000, 2.000],  loss: 24462252.000000, mae: 1536.735352, mean_q: -1.842063, mean_eps: 0.094159
  651/10000: episode: 651, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4173.224, mean reward: -4173.224 [-4173.224, -4173.224], mean action: 2.000 [2.000, 2.000],  loss: 24039248.000000, mae: 1429.043945, mean_q: -1.849082, mean_eps: 0.094150
  652/10000: episode: 652, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -140.135, mean reward: -140.135 [-140.135, -140.135], mean action: 2.000 [2.000, 2.000],  loss: 19391008.000000, mae: 1323.344727, mean_q: -1.875453, mean_eps: 0.094141
  653/10000: episode: 653, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1278.331, mean reward: -1278.331 [-1278.331, -1278.331], mean action: 2.000 [2.000, 2.000],  loss: 16802676.000000, mae: 1278.888794, mean_q: -1.906438, mean_eps: 0.094132
  654/10000: episode: 654, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2457.642, mean reward: -2457.642 [-2457.642, -2457.642], mean action: 2.000 [2.000, 2.000],  loss: 24719424.000000, mae: 1531.871582, mean_q: -1.931022, mean_eps: 0.094123
  655/10000: episode: 655, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4248.601, mean reward: -4248.601 [-4248.601, -4248.601], mean action: 2.000 [2.000, 2.000],  loss: 27393830.000000, mae: 1566.433350, mean_q: -1.936504, mean_eps: 0.094114
  656/10000: episode: 656, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -9749.932, mean reward: -9749.932 [-9749.932, -9749.932], mean action: 2.000 [2.000, 2.000],  loss: 27800650.000000, mae: 1600.063721, mean_q: -1.970524, mean_eps: 0.094105
  657/10000: episode: 657, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -7391.991, mean reward: -7391.991 [-7391.991, -7391.991], mean action: 2.000 [2.000, 2.000],  loss: 22885028.000000, mae: 1485.245117, mean_q: -1.990562, mean_eps: 0.094096
  658/10000: episode: 658, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -740.585, mean reward: -740.585 [-740.585, -740.585], mean action: 2.000 [2.000, 2.000],  loss: 21788174.000000, mae: 1392.082520, mean_q: -2.012913, mean_eps: 0.094087
  659/10000: episode: 659, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1943.910, mean reward: -1943.910 [-1943.910, -1943.910], mean action: 2.000 [2.000, 2.000],  loss: 25924696.000000, mae: 1585.756836, mean_q: -2.045273, mean_eps: 0.094078
  660/10000: episode: 660, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7365.903, mean reward: -7365.903 [-7365.903, -7365.903], mean action: 2.000 [2.000, 2.000],  loss: 25997230.000000, mae: 1496.630615, mean_q: -2.075032, mean_eps: 0.094069
  661/10000: episode: 661, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3278.937, mean reward: -3278.937 [-3278.937, -3278.937], mean action: 2.000 [2.000, 2.000],  loss: 27028728.000000, mae: 1642.687622, mean_q: -2.095849, mean_eps: 0.094060
  662/10000: episode: 662, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3290.680, mean reward: -3290.680 [-3290.680, -3290.680], mean action: 2.000 [2.000, 2.000],  loss: 26661820.000000, mae: 1540.611938, mean_q: -2.123685, mean_eps: 0.094051
  663/10000: episode: 663, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2233.491, mean reward: -2233.491 [-2233.491, -2233.491], mean action: 2.000 [2.000, 2.000],  loss: 25528484.000000, mae: 1486.565918, mean_q: -2.146458, mean_eps: 0.094042
  664/10000: episode: 664, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4978.867, mean reward: -4978.867 [-4978.867, -4978.867], mean action: 2.000 [2.000, 2.000],  loss: 32419626.000000, mae: 1675.981934, mean_q: -2.168714, mean_eps: 0.094033
  665/10000: episode: 665, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5048.774, mean reward: -5048.774 [-5048.774, -5048.774], mean action: 2.000 [2.000, 2.000],  loss: 22927700.000000, mae: 1523.213135, mean_q: -2.202835, mean_eps: 0.094024
  666/10000: episode: 666, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1092.046, mean reward: -1092.046 [-1092.046, -1092.046], mean action: 2.000 [2.000, 2.000],  loss: 32005228.000000, mae: 1733.938721, mean_q: -2.237812, mean_eps: 0.094015
  667/10000: episode: 667, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1552.564, mean reward: -1552.564 [-1552.564, -1552.564], mean action: 2.000 [2.000, 2.000],  loss: 22550558.000000, mae: 1490.513550, mean_q: -2.265715, mean_eps: 0.094006
  668/10000: episode: 668, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5925.367, mean reward: -5925.367 [-5925.367, -5925.367], mean action: 2.000 [2.000, 2.000],  loss: 29570918.000000, mae: 1632.777100, mean_q: -2.301975, mean_eps: 0.093997
  669/10000: episode: 669, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5801.969, mean reward: -5801.969 [-5801.969, -5801.969], mean action: 2.000 [2.000, 2.000],  loss: 30232240.000000, mae: 1689.050293, mean_q: -2.326978, mean_eps: 0.093988
  670/10000: episode: 670, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2062.399, mean reward: -2062.399 [-2062.399, -2062.399], mean action: 2.000 [2.000, 2.000],  loss: 19579564.000000, mae: 1342.503906, mean_q: -2.375459, mean_eps: 0.093979
  671/10000: episode: 671, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -935.700, mean reward: -935.700 [-935.700, -935.700], mean action: 2.000 [2.000, 2.000],  loss: 27270408.000000, mae: 1651.189575, mean_q: -2.393468, mean_eps: 0.093970
  672/10000: episode: 672, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2152.586, mean reward: -2152.586 [-2152.586, -2152.586], mean action: 3.000 [3.000, 3.000],  loss: 31033652.000000, mae: 1748.788452, mean_q: -2.427205, mean_eps: 0.093961
  673/10000: episode: 673, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1400.782, mean reward: -1400.782 [-1400.782, -1400.782], mean action: 2.000 [2.000, 2.000],  loss: 32867640.000000, mae: 1748.362671, mean_q: -2.453828, mean_eps: 0.093952
  674/10000: episode: 674, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4550.271, mean reward: -4550.271 [-4550.271, -4550.271], mean action: 0.000 [0.000, 0.000],  loss: 23289404.000000, mae: 1440.368652, mean_q: -2.505610, mean_eps: 0.093943
  675/10000: episode: 675, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1584.511, mean reward: -1584.511 [-1584.511, -1584.511], mean action: 2.000 [2.000, 2.000],  loss: 35510792.000000, mae: 1796.874756, mean_q: -2.514589, mean_eps: 0.093934
  676/10000: episode: 676, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1669.496, mean reward: -1669.496 [-1669.496, -1669.496], mean action: 2.000 [2.000, 2.000],  loss: 21633058.000000, mae: 1439.458374, mean_q: -2.568221, mean_eps: 0.093925
  677/10000: episode: 677, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3128.276, mean reward: -3128.276 [-3128.276, -3128.276], mean action: 2.000 [2.000, 2.000],  loss: 25177316.000000, mae: 1561.829834, mean_q: -2.586547, mean_eps: 0.093916
  678/10000: episode: 678, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2980.953, mean reward: -2980.953 [-2980.953, -2980.953], mean action: 2.000 [2.000, 2.000],  loss: 31082704.000000, mae: 1681.382080, mean_q: -2.617938, mean_eps: 0.093907
  679/10000: episode: 679, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3691.224, mean reward: -3691.224 [-3691.224, -3691.224], mean action: 2.000 [2.000, 2.000],  loss: 20700606.000000, mae: 1365.397583, mean_q: -2.661318, mean_eps: 0.093898
  680/10000: episode: 680, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3307.971, mean reward: -3307.971 [-3307.971, -3307.971], mean action: 2.000 [2.000, 2.000],  loss: 20716792.000000, mae: 1376.325562, mean_q: -2.697613, mean_eps: 0.093889
  681/10000: episode: 681, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1697.330, mean reward: -1697.330 [-1697.330, -1697.330], mean action: 2.000 [2.000, 2.000],  loss: 28357304.000000, mae: 1638.989868, mean_q: -2.745190, mean_eps: 0.093880
  682/10000: episode: 682, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3461.530, mean reward: -3461.530 [-3461.530, -3461.530], mean action: 2.000 [2.000, 2.000],  loss: 24598672.000000, mae: 1399.002197, mean_q: -2.774470, mean_eps: 0.093871
  683/10000: episode: 683, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4752.906, mean reward: -4752.906 [-4752.906, -4752.906], mean action: 2.000 [2.000, 2.000],  loss: 29437404.000000, mae: 1655.124268, mean_q: -2.801325, mean_eps: 0.093862
  684/10000: episode: 684, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -667.003, mean reward: -667.003 [-667.003, -667.003], mean action: 2.000 [2.000, 2.000],  loss: 30071818.000000, mae: 1724.462524, mean_q: -2.856460, mean_eps: 0.093853
  685/10000: episode: 685, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1454.294, mean reward: -1454.294 [-1454.294, -1454.294], mean action: 2.000 [2.000, 2.000],  loss: 30338182.000000, mae: 1714.142090, mean_q: -2.886478, mean_eps: 0.093844
  686/10000: episode: 686, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1321.917, mean reward: -1321.917 [-1321.917, -1321.917], mean action: 2.000 [2.000, 2.000],  loss: 25751982.000000, mae: 1445.387207, mean_q: -2.935682, mean_eps: 0.093835
  687/10000: episode: 687, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1587.775, mean reward: -1587.775 [-1587.775, -1587.775], mean action: 2.000 [2.000, 2.000],  loss: 21064514.000000, mae: 1363.885376, mean_q: -2.974151, mean_eps: 0.093826
  688/10000: episode: 688, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8305.285, mean reward: -8305.285 [-8305.285, -8305.285], mean action: 2.000 [2.000, 2.000],  loss: 30291684.000000, mae: 1673.479004, mean_q: -3.005203, mean_eps: 0.093817
  689/10000: episode: 689, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2140.399, mean reward: -2140.399 [-2140.399, -2140.399], mean action: 2.000 [2.000, 2.000],  loss: 24171936.000000, mae: 1469.077148, mean_q: -3.035375, mean_eps: 0.093808
  690/10000: episode: 690, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -4575.311, mean reward: -4575.311 [-4575.311, -4575.311], mean action: 3.000 [3.000, 3.000],  loss: 26168112.000000, mae: 1501.914307, mean_q: -3.058064, mean_eps: 0.093799
  691/10000: episode: 691, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7259.185, mean reward: -7259.185 [-7259.185, -7259.185], mean action: 2.000 [2.000, 2.000],  loss: 26261830.000000, mae: 1628.968262, mean_q: -3.096190, mean_eps: 0.093790
  692/10000: episode: 692, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1315.227, mean reward: -1315.227 [-1315.227, -1315.227], mean action: 2.000 [2.000, 2.000],  loss: 21906684.000000, mae: 1502.500977, mean_q: -3.135269, mean_eps: 0.093781
  693/10000: episode: 693, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2227.357, mean reward: -2227.357 [-2227.357, -2227.357], mean action: 2.000 [2.000, 2.000],  loss: 32844170.000000, mae: 1777.409180, mean_q: -3.176550, mean_eps: 0.093772
  694/10000: episode: 694, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3178.842, mean reward: -3178.842 [-3178.842, -3178.842], mean action: 2.000 [2.000, 2.000],  loss: 21809396.000000, mae: 1466.374634, mean_q: -3.226620, mean_eps: 0.093763
  695/10000: episode: 695, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -835.272, mean reward: -835.272 [-835.272, -835.272], mean action: 2.000 [2.000, 2.000],  loss: 30842668.000000, mae: 1668.978394, mean_q: -3.272614, mean_eps: 0.093754
  696/10000: episode: 696, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8118.538, mean reward: -8118.538 [-8118.538, -8118.538], mean action: 2.000 [2.000, 2.000],  loss: 27340772.000000, mae: 1611.671875, mean_q: -3.325084, mean_eps: 0.093745
  697/10000: episode: 697, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -3286.332, mean reward: -3286.332 [-3286.332, -3286.332], mean action: 2.000 [2.000, 2.000],  loss: 26387188.000000, mae: 1567.233398, mean_q: -3.371555, mean_eps: 0.093736
  698/10000: episode: 698, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2049.442, mean reward: -2049.442 [-2049.442, -2049.442], mean action: 2.000 [2.000, 2.000],  loss: 28764744.000000, mae: 1614.735352, mean_q: -3.417035, mean_eps: 0.093727
  699/10000: episode: 699, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -8201.069, mean reward: -8201.069 [-8201.069, -8201.069], mean action: 0.000 [0.000, 0.000],  loss: 27661894.000000, mae: 1577.703613, mean_q: -3.467033, mean_eps: 0.093718
  700/10000: episode: 700, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4126.671, mean reward: -4126.671 [-4126.671, -4126.671], mean action: 2.000 [2.000, 2.000],  loss: 23522444.000000, mae: 1569.173828, mean_q: -3.518552, mean_eps: 0.093709
  701/10000: episode: 701, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1585.904, mean reward: -1585.904 [-1585.904, -1585.904], mean action: 2.000 [2.000, 2.000],  loss: 26246326.000000, mae: 1518.843140, mean_q: -3.568540, mean_eps: 0.093700
  702/10000: episode: 702, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3263.416, mean reward: -3263.416 [-3263.416, -3263.416], mean action: 2.000 [2.000, 2.000],  loss: 28147190.000000, mae: 1669.742920, mean_q: -3.604411, mean_eps: 0.093691
  703/10000: episode: 703, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1149.615, mean reward: -1149.615 [-1149.615, -1149.615], mean action: 2.000 [2.000, 2.000],  loss: 17968924.000000, mae: 1285.932373, mean_q: -3.660083, mean_eps: 0.093682
  704/10000: episode: 704, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1232.142, mean reward: -1232.142 [-1232.142, -1232.142], mean action: 2.000 [2.000, 2.000],  loss: 39659076.000000, mae: 1953.020996, mean_q: -3.685893, mean_eps: 0.093673
  705/10000: episode: 705, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -612.847, mean reward: -612.847 [-612.847, -612.847], mean action: 2.000 [2.000, 2.000],  loss: 30331756.000000, mae: 1728.214844, mean_q: -3.736698, mean_eps: 0.093664
  706/10000: episode: 706, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3079.066, mean reward: -3079.066 [-3079.066, -3079.066], mean action: 2.000 [2.000, 2.000],  loss: 31996070.000000, mae: 1767.157471, mean_q: -3.799593, mean_eps: 0.093655
  707/10000: episode: 707, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2573.400, mean reward: -2573.400 [-2573.400, -2573.400], mean action: 2.000 [2.000, 2.000],  loss: 33331968.000000, mae: 1781.805420, mean_q: -3.836226, mean_eps: 0.093646
  708/10000: episode: 708, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2165.703, mean reward: -2165.703 [-2165.703, -2165.703], mean action: 2.000 [2.000, 2.000],  loss: 24057900.000000, mae: 1459.734009, mean_q: -3.903225, mean_eps: 0.093637
  709/10000: episode: 709, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2185.442, mean reward: -2185.442 [-2185.442, -2185.442], mean action: 2.000 [2.000, 2.000],  loss: 22570858.000000, mae: 1390.787598, mean_q: -3.956734, mean_eps: 0.093628
  710/10000: episode: 710, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2686.810, mean reward: -2686.810 [-2686.810, -2686.810], mean action: 2.000 [2.000, 2.000],  loss: 33930072.000000, mae: 1787.729248, mean_q: -4.006354, mean_eps: 0.093619
  711/10000: episode: 711, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3812.471, mean reward: -3812.471 [-3812.471, -3812.471], mean action: 2.000 [2.000, 2.000],  loss: 20513764.000000, mae: 1354.466309, mean_q: -4.063591, mean_eps: 0.093610
  712/10000: episode: 712, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -15.893, mean reward: -15.893 [-15.893, -15.893], mean action: 2.000 [2.000, 2.000],  loss: 17815908.000000, mae: 1288.660522, mean_q: -4.152236, mean_eps: 0.093601
  713/10000: episode: 713, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2828.799, mean reward: -2828.799 [-2828.799, -2828.799], mean action: 2.000 [2.000, 2.000],  loss: 21275298.000000, mae: 1327.965576, mean_q: -4.204060, mean_eps: 0.093592
  714/10000: episode: 714, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6980.232, mean reward: -6980.232 [-6980.232, -6980.232], mean action: 2.000 [2.000, 2.000],  loss: 16466124.000000, mae: 1133.360596, mean_q: -4.253456, mean_eps: 0.093583
  715/10000: episode: 715, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -216.774, mean reward: -216.774 [-216.774, -216.774], mean action: 2.000 [2.000, 2.000],  loss: 18590318.000000, mae: 1378.422852, mean_q: -4.305031, mean_eps: 0.093574
  716/10000: episode: 716, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1640.121, mean reward: -1640.121 [-1640.121, -1640.121], mean action: 2.000 [2.000, 2.000],  loss: 22237060.000000, mae: 1401.724609, mean_q: -4.351040, mean_eps: 0.093565
  717/10000: episode: 717, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4473.415, mean reward: -4473.415 [-4473.415, -4473.415], mean action: 0.000 [0.000, 0.000],  loss: 25276452.000000, mae: 1614.868530, mean_q: -4.381474, mean_eps: 0.093556
  718/10000: episode: 718, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1012.489, mean reward: -1012.489 [-1012.489, -1012.489], mean action: 2.000 [2.000, 2.000],  loss: 25654864.000000, mae: 1479.855225, mean_q: -4.458426, mean_eps: 0.093547
  719/10000: episode: 719, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6695.825, mean reward: -6695.825 [-6695.825, -6695.825], mean action: 2.000 [2.000, 2.000],  loss: 19427048.000000, mae: 1432.751953, mean_q: -4.486729, mean_eps: 0.093538
  720/10000: episode: 720, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -466.804, mean reward: -466.804 [-466.804, -466.804], mean action: 2.000 [2.000, 2.000],  loss: 24055166.000000, mae: 1492.159546, mean_q: -4.560528, mean_eps: 0.093529
  721/10000: episode: 721, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7.138, mean reward: -7.138 [-7.138, -7.138], mean action: 2.000 [2.000, 2.000],  loss: 21488912.000000, mae: 1435.437744, mean_q: -4.630572, mean_eps: 0.093520
  722/10000: episode: 722, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4720.872, mean reward: -4720.872 [-4720.872, -4720.872], mean action: 0.000 [0.000, 0.000],  loss: 26985184.000000, mae: 1534.331177, mean_q: -4.697802, mean_eps: 0.093511
  723/10000: episode: 723, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2606.592, mean reward: -2606.592 [-2606.592, -2606.592], mean action: 2.000 [2.000, 2.000],  loss: 16987126.000000, mae: 1267.852539, mean_q: -4.794048, mean_eps: 0.093502
  724/10000: episode: 724, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4341.043, mean reward: -4341.043 [-4341.043, -4341.043], mean action: 1.000 [1.000, 1.000],  loss: 18391156.000000, mae: 1287.972290, mean_q: -4.835061, mean_eps: 0.093493
  725/10000: episode: 725, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4391.206, mean reward: -4391.206 [-4391.206, -4391.206], mean action: 2.000 [2.000, 2.000],  loss: 26109240.000000, mae: 1635.597656, mean_q: -4.875277, mean_eps: 0.093484
  726/10000: episode: 726, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -121.931, mean reward: -121.931 [-121.931, -121.931], mean action: 2.000 [2.000, 2.000],  loss: 28800416.000000, mae: 1584.517090, mean_q: -4.969155, mean_eps: 0.093475
  727/10000: episode: 727, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6059.111, mean reward: -6059.111 [-6059.111, -6059.111], mean action: 2.000 [2.000, 2.000],  loss: 33025350.000000, mae: 1718.299316, mean_q: -4.989291, mean_eps: 0.093466
  728/10000: episode: 728, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2539.178, mean reward: -2539.178 [-2539.178, -2539.178], mean action: 2.000 [2.000, 2.000],  loss: 17905028.000000, mae: 1258.680420, mean_q: -5.100019, mean_eps: 0.093457
  729/10000: episode: 729, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -997.707, mean reward: -997.707 [-997.707, -997.707], mean action: 2.000 [2.000, 2.000],  loss: 27356184.000000, mae: 1574.679688, mean_q: -5.156347, mean_eps: 0.093448
  730/10000: episode: 730, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2182.176, mean reward: -2182.176 [-2182.176, -2182.176], mean action: 2.000 [2.000, 2.000],  loss: 16597741.000000, mae: 1213.923462, mean_q: -5.276444, mean_eps: 0.093439
  731/10000: episode: 731, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8591.727, mean reward: -8591.727 [-8591.727, -8591.727], mean action: 2.000 [2.000, 2.000],  loss: 32604398.000000, mae: 1699.152344, mean_q: -5.312801, mean_eps: 0.093430
  732/10000: episode: 732, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3329.306, mean reward: -3329.306 [-3329.306, -3329.306], mean action: 2.000 [2.000, 2.000],  loss: 23266080.000000, mae: 1390.588379, mean_q: -5.393486, mean_eps: 0.093421
  733/10000: episode: 733, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2918.711, mean reward: -2918.711 [-2918.711, -2918.711], mean action: 0.000 [0.000, 0.000],  loss: 28271582.000000, mae: 1700.600220, mean_q: -5.452629, mean_eps: 0.093412
  734/10000: episode: 734, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4685.136, mean reward: -4685.136 [-4685.136, -4685.136], mean action: 2.000 [2.000, 2.000],  loss: 28475698.000000, mae: 1717.904297, mean_q: -5.517690, mean_eps: 0.093403
  735/10000: episode: 735, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1246.652, mean reward: -1246.652 [-1246.652, -1246.652], mean action: 2.000 [2.000, 2.000],  loss: 23672114.000000, mae: 1516.666138, mean_q: -5.631854, mean_eps: 0.093394
  736/10000: episode: 736, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6984.596, mean reward: -6984.596 [-6984.596, -6984.596], mean action: 1.000 [1.000, 1.000],  loss: 37158580.000000, mae: 1870.073242, mean_q: -5.702261, mean_eps: 0.093385
  737/10000: episode: 737, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3013.821, mean reward: -3013.821 [-3013.821, -3013.821], mean action: 2.000 [2.000, 2.000],  loss: 31491304.000000, mae: 1641.082275, mean_q: -5.816651, mean_eps: 0.093376
  738/10000: episode: 738, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2066.121, mean reward: -2066.121 [-2066.121, -2066.121], mean action: 2.000 [2.000, 2.000],  loss: 25570716.000000, mae: 1480.262207, mean_q: -5.900787, mean_eps: 0.093367
  739/10000: episode: 739, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3586.769, mean reward: -3586.769 [-3586.769, -3586.769], mean action: 2.000 [2.000, 2.000],  loss: 24826188.000000, mae: 1485.605225, mean_q: -5.980774, mean_eps: 0.093358
  740/10000: episode: 740, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -1718.374, mean reward: -1718.374 [-1718.374, -1718.374], mean action: 2.000 [2.000, 2.000],  loss: 28270170.000000, mae: 1621.879639, mean_q: -6.063076, mean_eps: 0.093349
  741/10000: episode: 741, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1014.382, mean reward: -1014.382 [-1014.382, -1014.382], mean action: 2.000 [2.000, 2.000],  loss: 19696156.000000, mae: 1322.433594, mean_q: -6.156955, mean_eps: 0.093340
  742/10000: episode: 742, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1633.014, mean reward: -1633.014 [-1633.014, -1633.014], mean action: 2.000 [2.000, 2.000],  loss: 26180356.000000, mae: 1502.656494, mean_q: -6.187992, mean_eps: 0.093331
  743/10000: episode: 743, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4240.524, mean reward: -4240.524 [-4240.524, -4240.524], mean action: 2.000 [2.000, 2.000],  loss: 23781584.000000, mae: 1460.517334, mean_q: -6.292665, mean_eps: 0.093322
  744/10000: episode: 744, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7134.019, mean reward: -7134.019 [-7134.019, -7134.019], mean action: 2.000 [2.000, 2.000],  loss: 24317560.000000, mae: 1447.743652, mean_q: -6.347727, mean_eps: 0.093313
  745/10000: episode: 745, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -958.147, mean reward: -958.147 [-958.147, -958.147], mean action: 2.000 [2.000, 2.000],  loss: 17212624.000000, mae: 1252.661255, mean_q: -6.433579, mean_eps: 0.093304
  746/10000: episode: 746, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -2092.152, mean reward: -2092.152 [-2092.152, -2092.152], mean action: 2.000 [2.000, 2.000],  loss: 29441912.000000, mae: 1535.348511, mean_q: -6.488165, mean_eps: 0.093295
  747/10000: episode: 747, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -4871.723, mean reward: -4871.723 [-4871.723, -4871.723], mean action: 2.000 [2.000, 2.000],  loss: 22413854.000000, mae: 1462.235352, mean_q: -6.557479, mean_eps: 0.093286
  748/10000: episode: 748, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2471.118, mean reward: -2471.118 [-2471.118, -2471.118], mean action: 2.000 [2.000, 2.000],  loss: 32839268.000000, mae: 1748.107544, mean_q: -6.642774, mean_eps: 0.093277
  749/10000: episode: 749, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -124.987, mean reward: -124.987 [-124.987, -124.987], mean action: 2.000 [2.000, 2.000],  loss: 19912176.000000, mae: 1309.592651, mean_q: -6.740190, mean_eps: 0.093268
  750/10000: episode: 750, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2239.731, mean reward: -2239.731 [-2239.731, -2239.731], mean action: 2.000 [2.000, 2.000],  loss: 26640484.000000, mae: 1645.837158, mean_q: -6.782339, mean_eps: 0.093259
  751/10000: episode: 751, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2910.245, mean reward: -2910.245 [-2910.245, -2910.245], mean action: 2.000 [2.000, 2.000],  loss: 25532900.000000, mae: 1496.885498, mean_q: -6.834206, mean_eps: 0.093250
  752/10000: episode: 752, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -941.454, mean reward: -941.454 [-941.454, -941.454], mean action: 2.000 [2.000, 2.000],  loss: 23565758.000000, mae: 1446.930908, mean_q: -6.936483, mean_eps: 0.093241
  753/10000: episode: 753, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2602.142, mean reward: -2602.142 [-2602.142, -2602.142], mean action: 2.000 [2.000, 2.000],  loss: 26335652.000000, mae: 1560.068237, mean_q: -7.019086, mean_eps: 0.093232
  754/10000: episode: 754, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1932.491, mean reward: -1932.491 [-1932.491, -1932.491], mean action: 2.000 [2.000, 2.000],  loss: 28431212.000000, mae: 1598.989502, mean_q: -7.077701, mean_eps: 0.093223
  755/10000: episode: 755, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3980.117, mean reward: -3980.117 [-3980.117, -3980.117], mean action: 2.000 [2.000, 2.000],  loss: 23588092.000000, mae: 1569.951660, mean_q: -7.153871, mean_eps: 0.093214
  756/10000: episode: 756, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5939.252, mean reward: -5939.252 [-5939.252, -5939.252], mean action: 2.000 [2.000, 2.000],  loss: 21964904.000000, mae: 1487.122070, mean_q: -7.247245, mean_eps: 0.093205
  757/10000: episode: 757, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -7290.726, mean reward: -7290.726 [-7290.726, -7290.726], mean action: 3.000 [3.000, 3.000],  loss: 25064370.000000, mae: 1459.297119, mean_q: -7.332407, mean_eps: 0.093196
  758/10000: episode: 758, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -3252.381, mean reward: -3252.381 [-3252.381, -3252.381], mean action: 2.000 [2.000, 2.000],  loss: 27174410.000000, mae: 1637.637207, mean_q: -7.394578, mean_eps: 0.093187
  759/10000: episode: 759, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2689.760, mean reward: -2689.760 [-2689.760, -2689.760], mean action: 3.000 [3.000, 3.000],  loss: 21297602.000000, mae: 1417.749023, mean_q: -7.472692, mean_eps: 0.093178
  760/10000: episode: 760, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -6730.827, mean reward: -6730.827 [-6730.827, -6730.827], mean action: 3.000 [3.000, 3.000],  loss: 22374920.000000, mae: 1480.194336, mean_q: -7.545317, mean_eps: 0.093169
  761/10000: episode: 761, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1835.686, mean reward: -1835.686 [-1835.686, -1835.686], mean action: 0.000 [0.000, 0.000],  loss: 22894836.000000, mae: 1445.613403, mean_q: -7.603833, mean_eps: 0.093160
  762/10000: episode: 762, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2177.530, mean reward: -2177.530 [-2177.530, -2177.530], mean action: 3.000 [3.000, 3.000],  loss: 26583488.000000, mae: 1570.319824, mean_q: -7.650951, mean_eps: 0.093151
  763/10000: episode: 763, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -8049.626, mean reward: -8049.626 [-8049.626, -8049.626], mean action: 1.000 [1.000, 1.000],  loss: 25176960.000000, mae: 1433.731812, mean_q: -7.739299, mean_eps: 0.093142
  764/10000: episode: 764, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2744.542, mean reward: -2744.542 [-2744.542, -2744.542], mean action: 2.000 [2.000, 2.000],  loss: 38314472.000000, mae: 1900.317383, mean_q: -7.745884, mean_eps: 0.093133
  765/10000: episode: 765, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2440.498, mean reward: -2440.498 [-2440.498, -2440.498], mean action: 3.000 [3.000, 3.000],  loss: 30288596.000000, mae: 1653.569946, mean_q: -7.843449, mean_eps: 0.093124
  766/10000: episode: 766, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -760.558, mean reward: -760.558 [-760.558, -760.558], mean action: 3.000 [3.000, 3.000],  loss: 17800476.000000, mae: 1257.538086, mean_q: -7.957650, mean_eps: 0.093115
  767/10000: episode: 767, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -195.128, mean reward: -195.128 [-195.128, -195.128], mean action: 3.000 [3.000, 3.000],  loss: 14876656.000000, mae: 1116.553467, mean_q: -8.006104, mean_eps: 0.093106
  768/10000: episode: 768, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -927.530, mean reward: -927.530 [-927.530, -927.530], mean action: 3.000 [3.000, 3.000],  loss: 25969202.000000, mae: 1563.142090, mean_q: -8.062423, mean_eps: 0.093097
  769/10000: episode: 769, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4248.176, mean reward: -4248.176 [-4248.176, -4248.176], mean action: 3.000 [3.000, 3.000],  loss: 18373890.000000, mae: 1378.166748, mean_q: -8.171871, mean_eps: 0.093088
  770/10000: episode: 770, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1422.853, mean reward: -1422.853 [-1422.853, -1422.853], mean action: 3.000 [3.000, 3.000],  loss: 18002290.000000, mae: 1208.821777, mean_q: -8.279743, mean_eps: 0.093079
  771/10000: episode: 771, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -668.949, mean reward: -668.949 [-668.949, -668.949], mean action: 3.000 [3.000, 3.000],  loss: 18785184.000000, mae: 1321.829956, mean_q: -8.363710, mean_eps: 0.093070
  772/10000: episode: 772, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -7372.944, mean reward: -7372.944 [-7372.944, -7372.944], mean action: 3.000 [3.000, 3.000],  loss: 21129720.000000, mae: 1427.052734, mean_q: -8.440002, mean_eps: 0.093061
  773/10000: episode: 773, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -4988.172, mean reward: -4988.172 [-4988.172, -4988.172], mean action: 3.000 [3.000, 3.000],  loss: 21890914.000000, mae: 1435.181152, mean_q: -8.532639, mean_eps: 0.093052
  774/10000: episode: 774, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -4564.197, mean reward: -4564.197 [-4564.197, -4564.197], mean action: 3.000 [3.000, 3.000],  loss: 25961056.000000, mae: 1553.181030, mean_q: -8.640881, mean_eps: 0.093043
  775/10000: episode: 775, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1489.227, mean reward: -1489.227 [-1489.227, -1489.227], mean action: 3.000 [3.000, 3.000],  loss: 23857502.000000, mae: 1552.322021, mean_q: -8.725741, mean_eps: 0.093034
  776/10000: episode: 776, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1334.390, mean reward: -1334.390 [-1334.390, -1334.390], mean action: 3.000 [3.000, 3.000],  loss: 22750156.000000, mae: 1453.821777, mean_q: -8.863637, mean_eps: 0.093025
  777/10000: episode: 777, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -618.416, mean reward: -618.416 [-618.416, -618.416], mean action: 3.000 [3.000, 3.000],  loss: 19458078.000000, mae: 1356.709717, mean_q: -8.988398, mean_eps: 0.093016
  778/10000: episode: 778, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5864.560, mean reward: -5864.560 [-5864.560, -5864.560], mean action: 3.000 [3.000, 3.000],  loss: 28048452.000000, mae: 1618.995117, mean_q: -9.023962, mean_eps: 0.093007
  779/10000: episode: 779, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -410.310, mean reward: -410.310 [-410.310, -410.310], mean action: 3.000 [3.000, 3.000],  loss: 22318842.000000, mae: 1441.059937, mean_q: -9.176579, mean_eps: 0.092998
  780/10000: episode: 780, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1183.535, mean reward: -1183.535 [-1183.535, -1183.535], mean action: 3.000 [3.000, 3.000],  loss: 16216175.000000, mae: 1222.584717, mean_q: -9.310285, mean_eps: 0.092989
  781/10000: episode: 781, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -7300.190, mean reward: -7300.190 [-7300.190, -7300.190], mean action: 3.000 [3.000, 3.000],  loss: 24085418.000000, mae: 1447.118652, mean_q: -9.353361, mean_eps: 0.092980
  782/10000: episode: 782, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6654.810, mean reward: -6654.810 [-6654.810, -6654.810], mean action: 3.000 [3.000, 3.000],  loss: 23090032.000000, mae: 1515.545166, mean_q: -9.495266, mean_eps: 0.092971
  783/10000: episode: 783, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -153.669, mean reward: -153.669 [-153.669, -153.669], mean action: 3.000 [3.000, 3.000],  loss: 18957544.000000, mae: 1386.586426, mean_q: -9.600771, mean_eps: 0.092962
  784/10000: episode: 784, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2033.173, mean reward: -2033.173 [-2033.173, -2033.173], mean action: 3.000 [3.000, 3.000],  loss: 25480846.000000, mae: 1602.529419, mean_q: -9.691385, mean_eps: 0.092953
  785/10000: episode: 785, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -557.181, mean reward: -557.181 [-557.181, -557.181], mean action: 3.000 [3.000, 3.000],  loss: 20582776.000000, mae: 1353.465088, mean_q: -9.831800, mean_eps: 0.092944
  786/10000: episode: 786, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3060.103, mean reward: -3060.103 [-3060.103, -3060.103], mean action: 3.000 [3.000, 3.000],  loss: 17919572.000000, mae: 1309.714355, mean_q: -9.920082, mean_eps: 0.092935
  787/10000: episode: 787, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -178.029, mean reward: -178.029 [-178.029, -178.029], mean action: 3.000 [3.000, 3.000],  loss: 28559924.000000, mae: 1625.303101, mean_q: -9.966882, mean_eps: 0.092926
  788/10000: episode: 788, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3367.938, mean reward: -3367.938 [-3367.938, -3367.938], mean action: 3.000 [3.000, 3.000],  loss: 30892638.000000, mae: 1673.506592, mean_q: -10.119319, mean_eps: 0.092917
  789/10000: episode: 789, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -6769.626, mean reward: -6769.626 [-6769.626, -6769.626], mean action: 3.000 [3.000, 3.000],  loss: 22875626.000000, mae: 1370.308594, mean_q: -10.196871, mean_eps: 0.092908
  790/10000: episode: 790, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3410.467, mean reward: -3410.467 [-3410.467, -3410.467], mean action: 3.000 [3.000, 3.000],  loss: 25984022.000000, mae: 1612.783936, mean_q: -10.303522, mean_eps: 0.092899
  791/10000: episode: 791, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -360.374, mean reward: -360.374 [-360.374, -360.374], mean action: 3.000 [3.000, 3.000],  loss: 27768168.000000, mae: 1668.685547, mean_q: -10.405234, mean_eps: 0.092890
  792/10000: episode: 792, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5736.243, mean reward: -5736.243 [-5736.243, -5736.243], mean action: 3.000 [3.000, 3.000],  loss: 25939464.000000, mae: 1479.926392, mean_q: -10.512815, mean_eps: 0.092881
  793/10000: episode: 793, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -8068.354, mean reward: -8068.354 [-8068.354, -8068.354], mean action: 3.000 [3.000, 3.000],  loss: 18778148.000000, mae: 1284.084595, mean_q: -10.676399, mean_eps: 0.092872
  794/10000: episode: 794, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4144.438, mean reward: -4144.438 [-4144.438, -4144.438], mean action: 3.000 [3.000, 3.000],  loss: 29595586.000000, mae: 1681.581787, mean_q: -10.776957, mean_eps: 0.092863
  795/10000: episode: 795, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -806.551, mean reward: -806.551 [-806.551, -806.551], mean action: 3.000 [3.000, 3.000],  loss: 27231984.000000, mae: 1600.946899, mean_q: -10.926823, mean_eps: 0.092854
  796/10000: episode: 796, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -8040.260, mean reward: -8040.260 [-8040.260, -8040.260], mean action: 3.000 [3.000, 3.000],  loss: 19360398.000000, mae: 1322.974487, mean_q: -11.107035, mean_eps: 0.092845
  797/10000: episode: 797, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -2182.288, mean reward: -2182.288 [-2182.288, -2182.288], mean action: 3.000 [3.000, 3.000],  loss: 12779894.000000, mae: 1158.374268, mean_q: -11.248981, mean_eps: 0.092836
  798/10000: episode: 798, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -4264.801, mean reward: -4264.801 [-4264.801, -4264.801], mean action: 3.000 [3.000, 3.000],  loss: 30021424.000000, mae: 1611.103027, mean_q: -11.276585, mean_eps: 0.092827
  799/10000: episode: 799, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3189.151, mean reward: -3189.151 [-3189.151, -3189.151], mean action: 2.000 [2.000, 2.000],  loss: 16115220.000000, mae: 1177.159668, mean_q: -11.547744, mean_eps: 0.092818
  800/10000: episode: 800, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -896.460, mean reward: -896.460 [-896.460, -896.460], mean action: 2.000 [2.000, 2.000],  loss: 22884950.000000, mae: 1472.575562, mean_q: -11.565820, mean_eps: 0.092809
  801/10000: episode: 801, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -794.950, mean reward: -794.950 [-794.950, -794.950], mean action: 2.000 [2.000, 2.000],  loss: 22334190.000000, mae: 1449.106445, mean_q: -11.694244, mean_eps: 0.092800
  802/10000: episode: 802, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -577.853, mean reward: -577.853 [-577.853, -577.853], mean action: 2.000 [2.000, 2.000],  loss: 21816940.000000, mae: 1430.199463, mean_q: -11.846146, mean_eps: 0.092791
  803/10000: episode: 803, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4549.982, mean reward: -4549.982 [-4549.982, -4549.982], mean action: 2.000 [2.000, 2.000],  loss: 23389612.000000, mae: 1434.798462, mean_q: -11.980514, mean_eps: 0.092782
  804/10000: episode: 804, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -536.950, mean reward: -536.950 [-536.950, -536.950], mean action: 2.000 [2.000, 2.000],  loss: 19718456.000000, mae: 1346.238281, mean_q: -12.086349, mean_eps: 0.092773
  805/10000: episode: 805, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -1731.063, mean reward: -1731.063 [-1731.063, -1731.063], mean action: 2.000 [2.000, 2.000],  loss: 30929404.000000, mae: 1699.708252, mean_q: -12.199465, mean_eps: 0.092764
  806/10000: episode: 806, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -2173.095, mean reward: -2173.095 [-2173.095, -2173.095], mean action: 1.000 [1.000, 1.000],  loss: 26084416.000000, mae: 1522.116699, mean_q: -12.299782, mean_eps: 0.092755
  807/10000: episode: 807, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3448.133, mean reward: -3448.133 [-3448.133, -3448.133], mean action: 2.000 [2.000, 2.000],  loss: 19426594.000000, mae: 1291.297363, mean_q: -12.475345, mean_eps: 0.092746
  808/10000: episode: 808, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -63.975, mean reward: -63.975 [-63.975, -63.975], mean action: 2.000 [2.000, 2.000],  loss: 29165862.000000, mae: 1770.190674, mean_q: -12.505970, mean_eps: 0.092737
  809/10000: episode: 809, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4922.315, mean reward: -4922.315 [-4922.315, -4922.315], mean action: 2.000 [2.000, 2.000],  loss: 23408056.000000, mae: 1475.441895, mean_q: -12.644205, mean_eps: 0.092728
  810/10000: episode: 810, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1082.402, mean reward: -1082.402 [-1082.402, -1082.402], mean action: 2.000 [2.000, 2.000],  loss: 22305062.000000, mae: 1393.602173, mean_q: -12.840352, mean_eps: 0.092719
  811/10000: episode: 811, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2514.401, mean reward: -2514.401 [-2514.401, -2514.401], mean action: 2.000 [2.000, 2.000],  loss: 19750594.000000, mae: 1377.918213, mean_q: -12.970222, mean_eps: 0.092710
  812/10000: episode: 812, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7079.559, mean reward: -7079.559 [-7079.559, -7079.559], mean action: 2.000 [2.000, 2.000],  loss: 22068162.000000, mae: 1379.090332, mean_q: -13.037010, mean_eps: 0.092701
  813/10000: episode: 813, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -2451.533, mean reward: -2451.533 [-2451.533, -2451.533], mean action: 2.000 [2.000, 2.000],  loss: 30682058.000000, mae: 1669.306396, mean_q: -13.149149, mean_eps: 0.092692
  814/10000: episode: 814, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5186.236, mean reward: -5186.236 [-5186.236, -5186.236], mean action: 2.000 [2.000, 2.000],  loss: 27240698.000000, mae: 1684.440186, mean_q: -13.293425, mean_eps: 0.092683
  815/10000: episode: 815, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3860.678, mean reward: -3860.678 [-3860.678, -3860.678], mean action: 2.000 [2.000, 2.000],  loss: 23191010.000000, mae: 1424.410278, mean_q: -13.461113, mean_eps: 0.092674
  816/10000: episode: 816, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -6535.216, mean reward: -6535.216 [-6535.216, -6535.216], mean action: 0.000 [0.000, 0.000],  loss: 19277072.000000, mae: 1276.785034, mean_q: -13.579038, mean_eps: 0.092665
  817/10000: episode: 817, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2856.535, mean reward: -2856.535 [-2856.535, -2856.535], mean action: 2.000 [2.000, 2.000],  loss: 21768824.000000, mae: 1358.622925, mean_q: -13.722923, mean_eps: 0.092656
  818/10000: episode: 818, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -911.862, mean reward: -911.862 [-911.862, -911.862], mean action: 2.000 [2.000, 2.000],  loss: 18780304.000000, mae: 1326.194336, mean_q: -13.880951, mean_eps: 0.092647
  819/10000: episode: 819, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2334.672, mean reward: -2334.672 [-2334.672, -2334.672], mean action: 2.000 [2.000, 2.000],  loss: 19538220.000000, mae: 1326.757935, mean_q: -14.016431, mean_eps: 0.092638
  820/10000: episode: 820, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -892.669, mean reward: -892.669 [-892.669, -892.669], mean action: 2.000 [2.000, 2.000],  loss: 19890876.000000, mae: 1360.857056, mean_q: -14.147441, mean_eps: 0.092629
  821/10000: episode: 821, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3389.083, mean reward: -3389.083 [-3389.083, -3389.083], mean action: 2.000 [2.000, 2.000],  loss: 27715132.000000, mae: 1542.521484, mean_q: -14.274248, mean_eps: 0.092620
  822/10000: episode: 822, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2886.956, mean reward: -2886.956 [-2886.956, -2886.956], mean action: 2.000 [2.000, 2.000],  loss: 20942058.000000, mae: 1389.471802, mean_q: -14.405456, mean_eps: 0.092611
  823/10000: episode: 823, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -141.200, mean reward: -141.200 [-141.200, -141.200], mean action: 2.000 [2.000, 2.000],  loss: 16961560.000000, mae: 1292.901733, mean_q: -14.535611, mean_eps: 0.092602
  824/10000: episode: 824, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -9566.164, mean reward: -9566.164 [-9566.164, -9566.164], mean action: 2.000 [2.000, 2.000],  loss: 20492444.000000, mae: 1294.512573, mean_q: -14.656034, mean_eps: 0.092593
  825/10000: episode: 825, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4359.840, mean reward: -4359.840 [-4359.840, -4359.840], mean action: 2.000 [2.000, 2.000],  loss: 25294772.000000, mae: 1441.397949, mean_q: -14.818536, mean_eps: 0.092584
  826/10000: episode: 826, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1946.086, mean reward: -1946.086 [-1946.086, -1946.086], mean action: 2.000 [2.000, 2.000],  loss: 14464802.000000, mae: 1137.590820, mean_q: -14.956064, mean_eps: 0.092575
  827/10000: episode: 827, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5577.488, mean reward: -5577.488 [-5577.488, -5577.488], mean action: 2.000 [2.000, 2.000],  loss: 34076164.000000, mae: 1791.105103, mean_q: -15.036383, mean_eps: 0.092566
  828/10000: episode: 828, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2226.788, mean reward: -2226.788 [-2226.788, -2226.788], mean action: 2.000 [2.000, 2.000],  loss: 22089206.000000, mae: 1346.407104, mean_q: -15.243627, mean_eps: 0.092557
  829/10000: episode: 829, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1167.821, mean reward: -1167.821 [-1167.821, -1167.821], mean action: 2.000 [2.000, 2.000],  loss: 24904576.000000, mae: 1471.987305, mean_q: -15.392208, mean_eps: 0.092548
  830/10000: episode: 830, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1604.193, mean reward: -1604.193 [-1604.193, -1604.193], mean action: 3.000 [3.000, 3.000],  loss: 26649816.000000, mae: 1527.739868, mean_q: -15.484037, mean_eps: 0.092539
  831/10000: episode: 831, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -843.536, mean reward: -843.536 [-843.536, -843.536], mean action: 2.000 [2.000, 2.000],  loss: 8925476.000000, mae: 896.341675, mean_q: -15.709494, mean_eps: 0.092530
  832/10000: episode: 832, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1240.853, mean reward: -1240.853 [-1240.853, -1240.853], mean action: 2.000 [2.000, 2.000],  loss: 18275568.000000, mae: 1243.433350, mean_q: -15.877203, mean_eps: 0.092521
  833/10000: episode: 833, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4637.908, mean reward: -4637.908 [-4637.908, -4637.908], mean action: 2.000 [2.000, 2.000],  loss: 23376184.000000, mae: 1531.305664, mean_q: -15.906265, mean_eps: 0.092512
  834/10000: episode: 834, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2876.046, mean reward: -2876.046 [-2876.046, -2876.046], mean action: 2.000 [2.000, 2.000],  loss: 21663484.000000, mae: 1454.189941, mean_q: -16.058590, mean_eps: 0.092503
  835/10000: episode: 835, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -692.920, mean reward: -692.920 [-692.920, -692.920], mean action: 2.000 [2.000, 2.000],  loss: 31727944.000000, mae: 1596.358398, mean_q: -16.143372, mean_eps: 0.092494
  836/10000: episode: 836, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4275.010, mean reward: -4275.010 [-4275.010, -4275.010], mean action: 2.000 [2.000, 2.000],  loss: 17622822.000000, mae: 1275.525391, mean_q: -16.381817, mean_eps: 0.092485
  837/10000: episode: 837, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -569.166, mean reward: -569.166 [-569.166, -569.166], mean action: 2.000 [2.000, 2.000],  loss: 36092976.000000, mae: 1775.824707, mean_q: -16.389732, mean_eps: 0.092476
  838/10000: episode: 838, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -1499.690, mean reward: -1499.690 [-1499.690, -1499.690], mean action: 2.000 [2.000, 2.000],  loss: 19938668.000000, mae: 1323.203369, mean_q: -16.673220, mean_eps: 0.092467
  839/10000: episode: 839, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1434.280, mean reward: -1434.280 [-1434.280, -1434.280], mean action: 2.000 [2.000, 2.000],  loss: 25881750.000000, mae: 1551.624634, mean_q: -16.825008, mean_eps: 0.092458
  840/10000: episode: 840, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4070.247, mean reward: -4070.247 [-4070.247, -4070.247], mean action: 2.000 [2.000, 2.000],  loss: 27912312.000000, mae: 1585.359131, mean_q: -16.896391, mean_eps: 0.092449
  841/10000: episode: 841, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2044.229, mean reward: -2044.229 [-2044.229, -2044.229], mean action: 2.000 [2.000, 2.000],  loss: 35222624.000000, mae: 1866.197021, mean_q: -17.052177, mean_eps: 0.092440
  842/10000: episode: 842, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2657.977, mean reward: -2657.977 [-2657.977, -2657.977], mean action: 2.000 [2.000, 2.000],  loss: 29258648.000000, mae: 1676.510742, mean_q: -17.319290, mean_eps: 0.092431
  843/10000: episode: 843, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -526.049, mean reward: -526.049 [-526.049, -526.049], mean action: 2.000 [2.000, 2.000],  loss: 28359534.000000, mae: 1528.391479, mean_q: -17.493137, mean_eps: 0.092422
  844/10000: episode: 844, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4264.797, mean reward: -4264.797 [-4264.797, -4264.797], mean action: 2.000 [2.000, 2.000],  loss: 25681088.000000, mae: 1485.567383, mean_q: -17.652117, mean_eps: 0.092413
  845/10000: episode: 845, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5354.427, mean reward: -5354.427 [-5354.427, -5354.427], mean action: 2.000 [2.000, 2.000],  loss: 30025424.000000, mae: 1694.649902, mean_q: -17.785877, mean_eps: 0.092404
  846/10000: episode: 846, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -953.931, mean reward: -953.931 [-953.931, -953.931], mean action: 2.000 [2.000, 2.000],  loss: 21686064.000000, mae: 1465.042480, mean_q: -18.050190, mean_eps: 0.092395
  847/10000: episode: 847, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1417.123, mean reward: -1417.123 [-1417.123, -1417.123], mean action: 2.000 [2.000, 2.000],  loss: 20855906.000000, mae: 1369.410278, mean_q: -18.191788, mean_eps: 0.092386
  848/10000: episode: 848, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2392.464, mean reward: -2392.464 [-2392.464, -2392.464], mean action: 2.000 [2.000, 2.000],  loss: 23963592.000000, mae: 1445.355469, mean_q: -18.373249, mean_eps: 0.092377
  849/10000: episode: 849, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2639.338, mean reward: -2639.338 [-2639.338, -2639.338], mean action: 1.000 [1.000, 1.000],  loss: 24683540.000000, mae: 1523.178223, mean_q: -18.481041, mean_eps: 0.092368
  850/10000: episode: 850, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3382.652, mean reward: -3382.652 [-3382.652, -3382.652], mean action: 2.000 [2.000, 2.000],  loss: 38726196.000000, mae: 1880.490479, mean_q: -18.638405, mean_eps: 0.092359
  851/10000: episode: 851, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -1291.890, mean reward: -1291.890 [-1291.890, -1291.890], mean action: 2.000 [2.000, 2.000],  loss: 18617240.000000, mae: 1297.935791, mean_q: -18.924271, mean_eps: 0.092350
  852/10000: episode: 852, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1545.213, mean reward: -1545.213 [-1545.213, -1545.213], mean action: 2.000 [2.000, 2.000],  loss: 26713674.000000, mae: 1542.528320, mean_q: -19.072317, mean_eps: 0.092341
  853/10000: episode: 853, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4107.658, mean reward: -4107.658 [-4107.658, -4107.658], mean action: 2.000 [2.000, 2.000],  loss: 17245292.000000, mae: 1235.013672, mean_q: -19.289736, mean_eps: 0.092332
  854/10000: episode: 854, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -6661.627, mean reward: -6661.627 [-6661.627, -6661.627], mean action: 2.000 [2.000, 2.000],  loss: 20520718.000000, mae: 1302.294189, mean_q: -19.456091, mean_eps: 0.092323
  855/10000: episode: 855, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -154.351, mean reward: -154.351 [-154.351, -154.351], mean action: 2.000 [2.000, 2.000],  loss: 21870146.000000, mae: 1302.188232, mean_q: -19.651070, mean_eps: 0.092314
  856/10000: episode: 856, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3138.940, mean reward: -3138.940 [-3138.940, -3138.940], mean action: 2.000 [2.000, 2.000],  loss: 16928932.000000, mae: 1184.555420, mean_q: -19.796547, mean_eps: 0.092305
  857/10000: episode: 857, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -11121.927, mean reward: -11121.927 [-11121.927, -11121.927], mean action: 2.000 [2.000, 2.000],  loss: 17383576.000000, mae: 1256.654053, mean_q: -19.984066, mean_eps: 0.092296
  858/10000: episode: 858, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -7105.762, mean reward: -7105.762 [-7105.762, -7105.762], mean action: 2.000 [2.000, 2.000],  loss: 13769124.000000, mae: 1098.114258, mean_q: -20.157225, mean_eps: 0.092287
  859/10000: episode: 859, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5497.349, mean reward: -5497.349 [-5497.349, -5497.349], mean action: 2.000 [2.000, 2.000],  loss: 18929802.000000, mae: 1281.526367, mean_q: -20.276619, mean_eps: 0.092278
  860/10000: episode: 860, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -4148.140, mean reward: -4148.140 [-4148.140, -4148.140], mean action: 2.000 [2.000, 2.000],  loss: 19724918.000000, mae: 1306.895996, mean_q: -20.572460, mean_eps: 0.092269
  861/10000: episode: 861, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1811.725, mean reward: -1811.725 [-1811.725, -1811.725], mean action: 3.000 [3.000, 3.000],  loss: 23611646.000000, mae: 1463.458984, mean_q: -20.618734, mean_eps: 0.092260
  862/10000: episode: 862, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -1829.540, mean reward: -1829.540 [-1829.540, -1829.540], mean action: 2.000 [2.000, 2.000],  loss: 14062482.000000, mae: 1163.575195, mean_q: -20.872482, mean_eps: 0.092251
  863/10000: episode: 863, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -284.528, mean reward: -284.528 [-284.528, -284.528], mean action: 2.000 [2.000, 2.000],  loss: 17726632.000000, mae: 1299.494263, mean_q: -21.083469, mean_eps: 0.092242
  864/10000: episode: 864, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2248.944, mean reward: -2248.944 [-2248.944, -2248.944], mean action: 2.000 [2.000, 2.000],  loss: 21898956.000000, mae: 1408.756104, mean_q: -21.172287, mean_eps: 0.092233
  865/10000: episode: 865, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1420.518, mean reward: -1420.518 [-1420.518, -1420.518], mean action: 2.000 [2.000, 2.000],  loss: 18197856.000000, mae: 1250.995605, mean_q: -21.391216, mean_eps: 0.092224
  866/10000: episode: 866, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8815.567, mean reward: -8815.567 [-8815.567, -8815.567], mean action: 2.000 [2.000, 2.000],  loss: 23543224.000000, mae: 1367.077637, mean_q: -21.600796, mean_eps: 0.092215
  867/10000: episode: 867, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3965.257, mean reward: -3965.257 [-3965.257, -3965.257], mean action: 2.000 [2.000, 2.000],  loss: 25852272.000000, mae: 1559.812744, mean_q: -21.708216, mean_eps: 0.092206
  868/10000: episode: 868, duration: 0.065s, episode steps:   1, steps per second:  16, episode reward: -4714.881, mean reward: -4714.881 [-4714.881, -4714.881], mean action: 2.000 [2.000, 2.000],  loss: 26013328.000000, mae: 1433.457764, mean_q: -21.904591, mean_eps: 0.092197
  869/10000: episode: 869, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5424.199, mean reward: -5424.199 [-5424.199, -5424.199], mean action: 2.000 [2.000, 2.000],  loss: 18081740.000000, mae: 1290.384888, mean_q: -22.180511, mean_eps: 0.092188
  870/10000: episode: 870, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -489.943, mean reward: -489.943 [-489.943, -489.943], mean action: 2.000 [2.000, 2.000],  loss: 18359488.000000, mae: 1242.244873, mean_q: -22.353708, mean_eps: 0.092179
  871/10000: episode: 871, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -7518.590, mean reward: -7518.590 [-7518.590, -7518.590], mean action: 1.000 [1.000, 1.000],  loss: 26129604.000000, mae: 1537.122314, mean_q: -22.446510, mean_eps: 0.092170
  872/10000: episode: 872, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -1084.390, mean reward: -1084.390 [-1084.390, -1084.390], mean action: 2.000 [2.000, 2.000],  loss: 28978506.000000, mae: 1567.857422, mean_q: -22.708252, mean_eps: 0.092161
  873/10000: episode: 873, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1555.473, mean reward: -1555.473 [-1555.473, -1555.473], mean action: 2.000 [2.000, 2.000],  loss: 27731146.000000, mae: 1631.557129, mean_q: -22.830641, mean_eps: 0.092152
  874/10000: episode: 874, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -2782.092, mean reward: -2782.092 [-2782.092, -2782.092], mean action: 2.000 [2.000, 2.000],  loss: 27882686.000000, mae: 1524.778198, mean_q: -23.039867, mean_eps: 0.092143
  875/10000: episode: 875, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -95.346, mean reward: -95.346 [-95.346, -95.346], mean action: 2.000 [2.000, 2.000],  loss: 20447700.000000, mae: 1315.028809, mean_q: -23.334492, mean_eps: 0.092134
  876/10000: episode: 876, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -22.011, mean reward: -22.011 [-22.011, -22.011], mean action: 2.000 [2.000, 2.000],  loss: 27533068.000000, mae: 1648.946655, mean_q: -23.441002, mean_eps: 0.092125
  877/10000: episode: 877, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -3228.657, mean reward: -3228.657 [-3228.657, -3228.657], mean action: 2.000 [2.000, 2.000],  loss: 29551012.000000, mae: 1642.465576, mean_q: -23.752069, mean_eps: 0.092116
  878/10000: episode: 878, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -1933.805, mean reward: -1933.805 [-1933.805, -1933.805], mean action: 2.000 [2.000, 2.000],  loss: 21819560.000000, mae: 1447.113403, mean_q: -23.882643, mean_eps: 0.092107
  879/10000: episode: 879, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -2540.070, mean reward: -2540.070 [-2540.070, -2540.070], mean action: 3.000 [3.000, 3.000],  loss: 24820920.000000, mae: 1412.233398, mean_q: -24.186218, mean_eps: 0.092098
  880/10000: episode: 880, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -2801.899, mean reward: -2801.899 [-2801.899, -2801.899], mean action: 2.000 [2.000, 2.000],  loss: 15896624.000000, mae: 1122.237549, mean_q: -24.488480, mean_eps: 0.092089
  881/10000: episode: 881, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1159.403, mean reward: -1159.403 [-1159.403, -1159.403], mean action: 2.000 [2.000, 2.000],  loss: 24577910.000000, mae: 1442.142212, mean_q: -24.677885, mean_eps: 0.092080
  882/10000: episode: 882, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5196.147, mean reward: -5196.147 [-5196.147, -5196.147], mean action: 2.000 [2.000, 2.000],  loss: 25899138.000000, mae: 1583.507446, mean_q: -24.788481, mean_eps: 0.092071
  883/10000: episode: 883, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1405.364, mean reward: -1405.364 [-1405.364, -1405.364], mean action: 2.000 [2.000, 2.000],  loss: 15169124.000000, mae: 1162.279907, mean_q: -25.120146, mean_eps: 0.092062
  884/10000: episode: 884, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -44.431, mean reward: -44.431 [-44.431, -44.431], mean action: 2.000 [2.000, 2.000],  loss: 20536390.000000, mae: 1407.498047, mean_q: -25.253635, mean_eps: 0.092053
  885/10000: episode: 885, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -2472.226, mean reward: -2472.226 [-2472.226, -2472.226], mean action: 2.000 [2.000, 2.000],  loss: 19533912.000000, mae: 1282.079102, mean_q: -25.549576, mean_eps: 0.092044
  886/10000: episode: 886, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -181.583, mean reward: -181.583 [-181.583, -181.583], mean action: 2.000 [2.000, 2.000],  loss: 26070380.000000, mae: 1532.888062, mean_q: -25.642014, mean_eps: 0.092035
  887/10000: episode: 887, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -4752.098, mean reward: -4752.098 [-4752.098, -4752.098], mean action: 2.000 [2.000, 2.000],  loss: 19334656.000000, mae: 1297.129395, mean_q: -25.941792, mean_eps: 0.092026
  888/10000: episode: 888, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -1586.472, mean reward: -1586.472 [-1586.472, -1586.472], mean action: 2.000 [2.000, 2.000],  loss: 23729746.000000, mae: 1431.520264, mean_q: -26.087530, mean_eps: 0.092017
  889/10000: episode: 889, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6100.736, mean reward: -6100.736 [-6100.736, -6100.736], mean action: 0.000 [0.000, 0.000],  loss: 22716280.000000, mae: 1485.272461, mean_q: -26.276730, mean_eps: 0.092008
  890/10000: episode: 890, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3192.812, mean reward: -3192.812 [-3192.812, -3192.812], mean action: 2.000 [2.000, 2.000],  loss: 20763450.000000, mae: 1278.751953, mean_q: -26.501442, mean_eps: 0.091999
  891/10000: episode: 891, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1431.521, mean reward: -1431.521 [-1431.521, -1431.521], mean action: 1.000 [1.000, 1.000],  loss: 14249168.000000, mae: 1128.300781, mean_q: -26.853748, mean_eps: 0.091990
  892/10000: episode: 892, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3671.177, mean reward: -3671.177 [-3671.177, -3671.177], mean action: 2.000 [2.000, 2.000],  loss: 33401464.000000, mae: 1647.558350, mean_q: -27.036942, mean_eps: 0.091981
  893/10000: episode: 893, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1058.658, mean reward: -1058.658 [-1058.658, -1058.658], mean action: 2.000 [2.000, 2.000],  loss: 21961524.000000, mae: 1369.479126, mean_q: -27.167219, mean_eps: 0.091972
  894/10000: episode: 894, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1281.763, mean reward: -1281.763 [-1281.763, -1281.763], mean action: 2.000 [2.000, 2.000],  loss: 18834344.000000, mae: 1367.617920, mean_q: -27.488758, mean_eps: 0.091963
  895/10000: episode: 895, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2660.364, mean reward: -2660.364 [-2660.364, -2660.364], mean action: 2.000 [2.000, 2.000],  loss: 18603976.000000, mae: 1286.397461, mean_q: -27.666203, mean_eps: 0.091954
  896/10000: episode: 896, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2747.393, mean reward: -2747.393 [-2747.393, -2747.393], mean action: 2.000 [2.000, 2.000],  loss: 22028264.000000, mae: 1448.373535, mean_q: -27.949160, mean_eps: 0.091945
  897/10000: episode: 897, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -3099.922, mean reward: -3099.922 [-3099.922, -3099.922], mean action: 2.000 [2.000, 2.000],  loss: 21966292.000000, mae: 1292.587280, mean_q: -28.240055, mean_eps: 0.091936
  898/10000: episode: 898, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5.624, mean reward: -5.624 [-5.624, -5.624], mean action: 2.000 [2.000, 2.000],  loss: 14848349.000000, mae: 1157.071533, mean_q: -28.342438, mean_eps: 0.091927
  899/10000: episode: 899, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -9862.698, mean reward: -9862.698 [-9862.698, -9862.698], mean action: 2.000 [2.000, 2.000],  loss: 25829528.000000, mae: 1461.017578, mean_q: -28.554026, mean_eps: 0.091918
  900/10000: episode: 900, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5021.899, mean reward: -5021.899 [-5021.899, -5021.899], mean action: 2.000 [2.000, 2.000],  loss: 19233948.000000, mae: 1198.498169, mean_q: -28.973598, mean_eps: 0.091909
  901/10000: episode: 901, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -3984.358, mean reward: -3984.358 [-3984.358, -3984.358], mean action: 3.000 [3.000, 3.000],  loss: 18948716.000000, mae: 1285.350830, mean_q: -29.074022, mean_eps: 0.091900
  902/10000: episode: 902, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -4239.825, mean reward: -4239.825 [-4239.825, -4239.825], mean action: 2.000 [2.000, 2.000],  loss: 19295356.000000, mae: 1250.449219, mean_q: -29.270750, mean_eps: 0.091891
  903/10000: episode: 903, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4774.011, mean reward: -4774.011 [-4774.011, -4774.011], mean action: 3.000 [3.000, 3.000],  loss: 23789754.000000, mae: 1455.598145, mean_q: -29.608286, mean_eps: 0.091882
  904/10000: episode: 904, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -119.190, mean reward: -119.190 [-119.190, -119.190], mean action: 3.000 [3.000, 3.000],  loss: 20638168.000000, mae: 1258.537354, mean_q: -29.781799, mean_eps: 0.091873
  905/10000: episode: 905, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -563.512, mean reward: -563.512 [-563.512, -563.512], mean action: 3.000 [3.000, 3.000],  loss: 23097232.000000, mae: 1432.375488, mean_q: -30.009222, mean_eps: 0.091864
  906/10000: episode: 906, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2584.877, mean reward: -2584.877 [-2584.877, -2584.877], mean action: 3.000 [3.000, 3.000],  loss: 21450080.000000, mae: 1408.522705, mean_q: -30.079933, mean_eps: 0.091855
  907/10000: episode: 907, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2493.405, mean reward: -2493.405 [-2493.405, -2493.405], mean action: 3.000 [3.000, 3.000],  loss: 17218684.000000, mae: 1264.247070, mean_q: -30.372299, mean_eps: 0.091846
  908/10000: episode: 908, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -305.580, mean reward: -305.580 [-305.580, -305.580], mean action: 3.000 [3.000, 3.000],  loss: 19721738.000000, mae: 1379.142334, mean_q: -30.739288, mean_eps: 0.091837
  909/10000: episode: 909, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2867.230, mean reward: -2867.230 [-2867.230, -2867.230], mean action: 3.000 [3.000, 3.000],  loss: 24074356.000000, mae: 1479.896973, mean_q: -30.911686, mean_eps: 0.091828
  910/10000: episode: 910, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2018.625, mean reward: -2018.625 [-2018.625, -2018.625], mean action: 3.000 [3.000, 3.000],  loss: 20879756.000000, mae: 1402.503418, mean_q: -31.160116, mean_eps: 0.091819
  911/10000: episode: 911, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -858.515, mean reward: -858.515 [-858.515, -858.515], mean action: 3.000 [3.000, 3.000],  loss: 20869958.000000, mae: 1384.330322, mean_q: -31.307091, mean_eps: 0.091810
  912/10000: episode: 912, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4872.964, mean reward: -4872.964 [-4872.964, -4872.964], mean action: 3.000 [3.000, 3.000],  loss: 24124124.000000, mae: 1450.076172, mean_q: -31.758305, mean_eps: 0.091801
  913/10000: episode: 913, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3661.050, mean reward: -3661.050 [-3661.050, -3661.050], mean action: 3.000 [3.000, 3.000],  loss: 20781592.000000, mae: 1394.535522, mean_q: -31.877769, mean_eps: 0.091792
  914/10000: episode: 914, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -3394.960, mean reward: -3394.960 [-3394.960, -3394.960], mean action: 3.000 [3.000, 3.000],  loss: 20579428.000000, mae: 1424.094971, mean_q: -32.103905, mean_eps: 0.091783
  915/10000: episode: 915, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2230.080, mean reward: -2230.080 [-2230.080, -2230.080], mean action: 3.000 [3.000, 3.000],  loss: 18525564.000000, mae: 1367.882568, mean_q: -32.470661, mean_eps: 0.091774
  916/10000: episode: 916, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5.761, mean reward: -5.761 [-5.761, -5.761], mean action: 3.000 [3.000, 3.000],  loss: 20720624.000000, mae: 1363.751953, mean_q: -32.639053, mean_eps: 0.091765
  917/10000: episode: 917, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -1894.562, mean reward: -1894.562 [-1894.562, -1894.562], mean action: 3.000 [3.000, 3.000],  loss: 23861704.000000, mae: 1503.498047, mean_q: -32.937649, mean_eps: 0.091756
  918/10000: episode: 918, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1434.101, mean reward: -1434.101 [-1434.101, -1434.101], mean action: 3.000 [3.000, 3.000],  loss: 17426084.000000, mae: 1246.256104, mean_q: -33.213844, mean_eps: 0.091747
  919/10000: episode: 919, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3155.280, mean reward: -3155.280 [-3155.280, -3155.280], mean action: 3.000 [3.000, 3.000],  loss: 18944060.000000, mae: 1315.937744, mean_q: -33.557972, mean_eps: 0.091738
  920/10000: episode: 920, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -445.469, mean reward: -445.469 [-445.469, -445.469], mean action: 3.000 [3.000, 3.000],  loss: 30197340.000000, mae: 1701.030151, mean_q: -33.791267, mean_eps: 0.091729
  921/10000: episode: 921, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2353.738, mean reward: -2353.738 [-2353.738, -2353.738], mean action: 3.000 [3.000, 3.000],  loss: 18887814.000000, mae: 1383.807129, mean_q: -34.103725, mean_eps: 0.091720
  922/10000: episode: 922, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2644.221, mean reward: -2644.221 [-2644.221, -2644.221], mean action: 3.000 [3.000, 3.000],  loss: 17249848.000000, mae: 1251.425781, mean_q: -34.435352, mean_eps: 0.091711
  923/10000: episode: 923, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1802.737, mean reward: -1802.737 [-1802.737, -1802.737], mean action: 3.000 [3.000, 3.000],  loss: 28190616.000000, mae: 1650.413696, mean_q: -34.586784, mean_eps: 0.091702
  924/10000: episode: 924, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2868.435, mean reward: -2868.435 [-2868.435, -2868.435], mean action: 3.000 [3.000, 3.000],  loss: 16922036.000000, mae: 1177.479004, mean_q: -35.012306, mean_eps: 0.091693
  925/10000: episode: 925, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -3334.634, mean reward: -3334.634 [-3334.634, -3334.634], mean action: 3.000 [3.000, 3.000],  loss: 26391892.000000, mae: 1464.056885, mean_q: -35.137306, mean_eps: 0.091684
  926/10000: episode: 926, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -773.678, mean reward: -773.678 [-773.678, -773.678], mean action: 3.000 [3.000, 3.000],  loss: 23249366.000000, mae: 1375.181519, mean_q: -35.371964, mean_eps: 0.091675
  927/10000: episode: 927, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4972.052, mean reward: -4972.052 [-4972.052, -4972.052], mean action: 3.000 [3.000, 3.000],  loss: 11864789.000000, mae: 1004.208740, mean_q: -35.805695, mean_eps: 0.091666
  928/10000: episode: 928, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1609.644, mean reward: -1609.644 [-1609.644, -1609.644], mean action: 3.000 [3.000, 3.000],  loss: 17371808.000000, mae: 1228.571777, mean_q: -36.036720, mean_eps: 0.091657
  929/10000: episode: 929, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -6214.295, mean reward: -6214.295 [-6214.295, -6214.295], mean action: 3.000 [3.000, 3.000],  loss: 13606941.000000, mae: 1098.720581, mean_q: -36.367031, mean_eps: 0.091648
  930/10000: episode: 930, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2527.612, mean reward: -2527.612 [-2527.612, -2527.612], mean action: 1.000 [1.000, 1.000],  loss: 23211688.000000, mae: 1524.518921, mean_q: -36.474533, mean_eps: 0.091639
  931/10000: episode: 931, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3011.260, mean reward: -3011.260 [-3011.260, -3011.260], mean action: 3.000 [3.000, 3.000],  loss: 19806664.000000, mae: 1344.087646, mean_q: -36.855877, mean_eps: 0.091630
  932/10000: episode: 932, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8629.047, mean reward: -8629.047 [-8629.047, -8629.047], mean action: 3.000 [3.000, 3.000],  loss: 15992967.000000, mae: 1202.190674, mean_q: -37.100147, mean_eps: 0.091621
  933/10000: episode: 933, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -1987.255, mean reward: -1987.255 [-1987.255, -1987.255], mean action: 3.000 [3.000, 3.000],  loss: 21773092.000000, mae: 1441.695312, mean_q: -37.298580, mean_eps: 0.091612
  934/10000: episode: 934, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -31.729, mean reward: -31.729 [-31.729, -31.729], mean action: 3.000 [3.000, 3.000],  loss: 18659772.000000, mae: 1285.968018, mean_q: -37.530037, mean_eps: 0.091603
  935/10000: episode: 935, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8768.474, mean reward: -8768.474 [-8768.474, -8768.474], mean action: 3.000 [3.000, 3.000],  loss: 29621390.000000, mae: 1596.889160, mean_q: -37.800690, mean_eps: 0.091594
  936/10000: episode: 936, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4557.841, mean reward: -4557.841 [-4557.841, -4557.841], mean action: 3.000 [3.000, 3.000],  loss: 13627637.000000, mae: 1156.579346, mean_q: -38.023941, mean_eps: 0.091585
  937/10000: episode: 937, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1396.382, mean reward: -1396.382 [-1396.382, -1396.382], mean action: 1.000 [1.000, 1.000],  loss: 26530174.000000, mae: 1603.234619, mean_q: -38.454407, mean_eps: 0.091576
  938/10000: episode: 938, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -584.068, mean reward: -584.068 [-584.068, -584.068], mean action: 3.000 [3.000, 3.000],  loss: 18566380.000000, mae: 1216.347900, mean_q: -38.773331, mean_eps: 0.091567
  939/10000: episode: 939, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -852.729, mean reward: -852.729 [-852.729, -852.729], mean action: 3.000 [3.000, 3.000],  loss: 28668790.000000, mae: 1579.508911, mean_q: -38.980045, mean_eps: 0.091558
  940/10000: episode: 940, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -56.171, mean reward: -56.171 [-56.171, -56.171], mean action: 3.000 [3.000, 3.000],  loss: 28127608.000000, mae: 1584.016113, mean_q: -39.077904, mean_eps: 0.091549
  941/10000: episode: 941, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3414.558, mean reward: -3414.558 [-3414.558, -3414.558], mean action: 3.000 [3.000, 3.000],  loss: 17890652.000000, mae: 1286.243286, mean_q: -39.806763, mean_eps: 0.091540
  942/10000: episode: 942, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2106.993, mean reward: -2106.993 [-2106.993, -2106.993], mean action: 3.000 [3.000, 3.000],  loss: 17223832.000000, mae: 1257.536987, mean_q: -39.876068, mean_eps: 0.091531
  943/10000: episode: 943, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1842.410, mean reward: -1842.410 [-1842.410, -1842.410], mean action: 3.000 [3.000, 3.000],  loss: 18690412.000000, mae: 1362.761353, mean_q: -40.083542, mean_eps: 0.091522
  944/10000: episode: 944, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2143.495, mean reward: -2143.495 [-2143.495, -2143.495], mean action: 3.000 [3.000, 3.000],  loss: 29858580.000000, mae: 1692.819580, mean_q: -40.320221, mean_eps: 0.091513
  945/10000: episode: 945, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -4096.613, mean reward: -4096.613 [-4096.613, -4096.613], mean action: 3.000 [3.000, 3.000],  loss: 15578220.000000, mae: 1237.723389, mean_q: -40.747036, mean_eps: 0.091504
  946/10000: episode: 946, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3561.456, mean reward: -3561.456 [-3561.456, -3561.456], mean action: 3.000 [3.000, 3.000],  loss: 25341084.000000, mae: 1502.944092, mean_q: -40.950439, mean_eps: 0.091495
  947/10000: episode: 947, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5077.357, mean reward: -5077.357 [-5077.357, -5077.357], mean action: 3.000 [3.000, 3.000],  loss: 25398416.000000, mae: 1497.896240, mean_q: -41.247406, mean_eps: 0.091486
  948/10000: episode: 948, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8720.333, mean reward: -8720.333 [-8720.333, -8720.333], mean action: 3.000 [3.000, 3.000],  loss: 20741128.000000, mae: 1395.478760, mean_q: -41.617687, mean_eps: 0.091477
  949/10000: episode: 949, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2521.793, mean reward: -2521.793 [-2521.793, -2521.793], mean action: 3.000 [3.000, 3.000],  loss: 17896684.000000, mae: 1219.239258, mean_q: -41.954926, mean_eps: 0.091468
  950/10000: episode: 950, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1993.352, mean reward: -1993.352 [-1993.352, -1993.352], mean action: 3.000 [3.000, 3.000],  loss: 18692480.000000, mae: 1290.026123, mean_q: -42.266098, mean_eps: 0.091459
  951/10000: episode: 951, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -9345.996, mean reward: -9345.996 [-9345.996, -9345.996], mean action: 3.000 [3.000, 3.000],  loss: 16199352.000000, mae: 1260.377686, mean_q: -42.719406, mean_eps: 0.091450
  952/10000: episode: 952, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -3166.204, mean reward: -3166.204 [-3166.204, -3166.204], mean action: 3.000 [3.000, 3.000],  loss: 17882780.000000, mae: 1259.477295, mean_q: -42.899017, mean_eps: 0.091441
  953/10000: episode: 953, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -978.063, mean reward: -978.063 [-978.063, -978.063], mean action: 3.000 [3.000, 3.000],  loss: 20962092.000000, mae: 1256.808960, mean_q: -43.271400, mean_eps: 0.091432
  954/10000: episode: 954, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -10268.402, mean reward: -10268.402 [-10268.402, -10268.402], mean action: 0.000 [0.000, 0.000],  loss: 18252308.000000, mae: 1331.319458, mean_q: -43.635056, mean_eps: 0.091423
  955/10000: episode: 955, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -413.458, mean reward: -413.458 [-413.458, -413.458], mean action: 3.000 [3.000, 3.000],  loss: 16340960.000000, mae: 1276.857422, mean_q: -44.002529, mean_eps: 0.091414
  956/10000: episode: 956, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5.688, mean reward: -5.688 [-5.688, -5.688], mean action: 3.000 [3.000, 3.000],  loss: 14283004.000000, mae: 1151.450684, mean_q: -44.377598, mean_eps: 0.091405
  957/10000: episode: 957, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5266.824, mean reward: -5266.824 [-5266.824, -5266.824], mean action: 3.000 [3.000, 3.000],  loss: 26276952.000000, mae: 1532.901489, mean_q: -44.505932, mean_eps: 0.091396
  958/10000: episode: 958, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6849.477, mean reward: -6849.477 [-6849.477, -6849.477], mean action: 3.000 [3.000, 3.000],  loss: 15878911.000000, mae: 1206.964844, mean_q: -44.880615, mean_eps: 0.091387
  959/10000: episode: 959, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -4363.275, mean reward: -4363.275 [-4363.275, -4363.275], mean action: 2.000 [2.000, 2.000],  loss: 22915426.000000, mae: 1506.152588, mean_q: -45.303570, mean_eps: 0.091378
  960/10000: episode: 960, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -8625.771, mean reward: -8625.771 [-8625.771, -8625.771], mean action: 0.000 [0.000, 0.000],  loss: 17401928.000000, mae: 1268.359985, mean_q: -45.583363, mean_eps: 0.091369
  961/10000: episode: 961, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -4584.523, mean reward: -4584.523 [-4584.523, -4584.523], mean action: 0.000 [0.000, 0.000],  loss: 23544772.000000, mae: 1446.329590, mean_q: -45.730877, mean_eps: 0.091360
  962/10000: episode: 962, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2418.673, mean reward: -2418.673 [-2418.673, -2418.673], mean action: 2.000 [2.000, 2.000],  loss: 27072884.000000, mae: 1612.236084, mean_q: -46.089005, mean_eps: 0.091351
  963/10000: episode: 963, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2747.573, mean reward: -2747.573 [-2747.573, -2747.573], mean action: 2.000 [2.000, 2.000],  loss: 11765719.000000, mae: 1057.735229, mean_q: -46.645054, mean_eps: 0.091342
  964/10000: episode: 964, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3552.654, mean reward: -3552.654 [-3552.654, -3552.654], mean action: 2.000 [2.000, 2.000],  loss: 15198215.000000, mae: 1278.522705, mean_q: -46.843876, mean_eps: 0.091333
  965/10000: episode: 965, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5021.206, mean reward: -5021.206 [-5021.206, -5021.206], mean action: 2.000 [2.000, 2.000],  loss: 19811792.000000, mae: 1246.202637, mean_q: -47.075058, mean_eps: 0.091324
  966/10000: episode: 966, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -3504.398, mean reward: -3504.398 [-3504.398, -3504.398], mean action: 2.000 [2.000, 2.000],  loss: 20807748.000000, mae: 1396.475586, mean_q: -47.388229, mean_eps: 0.091315
  967/10000: episode: 967, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -2711.144, mean reward: -2711.144 [-2711.144, -2711.144], mean action: 2.000 [2.000, 2.000],  loss: 24860610.000000, mae: 1445.582764, mean_q: -47.922970, mean_eps: 0.091306
  968/10000: episode: 968, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -2162.838, mean reward: -2162.838 [-2162.838, -2162.838], mean action: 2.000 [2.000, 2.000],  loss: 19232016.000000, mae: 1383.296143, mean_q: -48.029968, mean_eps: 0.091297
  969/10000: episode: 969, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -697.142, mean reward: -697.142 [-697.142, -697.142], mean action: 2.000 [2.000, 2.000],  loss: 19009972.000000, mae: 1293.650513, mean_q: -48.383701, mean_eps: 0.091288
  970/10000: episode: 970, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -856.279, mean reward: -856.279 [-856.279, -856.279], mean action: 2.000 [2.000, 2.000],  loss: 23974632.000000, mae: 1393.037842, mean_q: -48.471470, mean_eps: 0.091279
  971/10000: episode: 971, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2240.682, mean reward: -2240.682 [-2240.682, -2240.682], mean action: 2.000 [2.000, 2.000],  loss: 27014056.000000, mae: 1586.834229, mean_q: -48.776451, mean_eps: 0.091270
  972/10000: episode: 972, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2429.626, mean reward: -2429.626 [-2429.626, -2429.626], mean action: 2.000 [2.000, 2.000],  loss: 17378282.000000, mae: 1137.744873, mean_q: -49.679085, mean_eps: 0.091261
  973/10000: episode: 973, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7009.053, mean reward: -7009.053 [-7009.053, -7009.053], mean action: 2.000 [2.000, 2.000],  loss: 21750548.000000, mae: 1406.778809, mean_q: -49.651375, mean_eps: 0.091252
  974/10000: episode: 974, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1967.560, mean reward: -1967.560 [-1967.560, -1967.560], mean action: 2.000 [2.000, 2.000],  loss: 14345390.000000, mae: 1210.946411, mean_q: -50.126068, mean_eps: 0.091243
  975/10000: episode: 975, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -1959.758, mean reward: -1959.758 [-1959.758, -1959.758], mean action: 2.000 [2.000, 2.000],  loss: 19386276.000000, mae: 1357.479736, mean_q: -50.311451, mean_eps: 0.091234
  976/10000: episode: 976, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -288.293, mean reward: -288.293 [-288.293, -288.293], mean action: 2.000 [2.000, 2.000],  loss: 27038776.000000, mae: 1618.388184, mean_q: -50.785583, mean_eps: 0.091225
  977/10000: episode: 977, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3652.745, mean reward: -3652.745 [-3652.745, -3652.745], mean action: 3.000 [3.000, 3.000],  loss: 13669269.000000, mae: 1116.807739, mean_q: -51.083759, mean_eps: 0.091216
  978/10000: episode: 978, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2241.622, mean reward: -2241.622 [-2241.622, -2241.622], mean action: 1.000 [1.000, 1.000],  loss: 15291285.000000, mae: 1102.498779, mean_q: -51.400574, mean_eps: 0.091207
  979/10000: episode: 979, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2502.123, mean reward: -2502.123 [-2502.123, -2502.123], mean action: 2.000 [2.000, 2.000],  loss: 17343226.000000, mae: 1279.585815, mean_q: -51.566265, mean_eps: 0.091198
  980/10000: episode: 980, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2659.441, mean reward: -2659.441 [-2659.441, -2659.441], mean action: 2.000 [2.000, 2.000],  loss: 22837698.000000, mae: 1367.624390, mean_q: -52.001183, mean_eps: 0.091189
  981/10000: episode: 981, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1726.041, mean reward: -1726.041 [-1726.041, -1726.041], mean action: 2.000 [2.000, 2.000],  loss: 27083076.000000, mae: 1601.256104, mean_q: -52.175564, mean_eps: 0.091180
  982/10000: episode: 982, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1473.204, mean reward: -1473.204 [-1473.204, -1473.204], mean action: 2.000 [2.000, 2.000],  loss: 16465032.000000, mae: 1218.667236, mean_q: -52.684158, mean_eps: 0.091171
  983/10000: episode: 983, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1637.228, mean reward: -1637.228 [-1637.228, -1637.228], mean action: 2.000 [2.000, 2.000],  loss: 24953900.000000, mae: 1508.571167, mean_q: -52.979988, mean_eps: 0.091162
  984/10000: episode: 984, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -135.176, mean reward: -135.176 [-135.176, -135.176], mean action: 2.000 [2.000, 2.000],  loss: 14361246.000000, mae: 1234.843750, mean_q: -53.311310, mean_eps: 0.091153
  985/10000: episode: 985, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -626.915, mean reward: -626.915 [-626.915, -626.915], mean action: 1.000 [1.000, 1.000],  loss: 21838062.000000, mae: 1407.615234, mean_q: -53.549431, mean_eps: 0.091144
  986/10000: episode: 986, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5300.557, mean reward: -5300.557 [-5300.557, -5300.557], mean action: 2.000 [2.000, 2.000],  loss: 21587350.000000, mae: 1360.182617, mean_q: -54.092731, mean_eps: 0.091135
  987/10000: episode: 987, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -356.884, mean reward: -356.884 [-356.884, -356.884], mean action: 2.000 [2.000, 2.000],  loss: 17740360.000000, mae: 1272.968994, mean_q: -54.660759, mean_eps: 0.091126
  988/10000: episode: 988, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5487.458, mean reward: -5487.458 [-5487.458, -5487.458], mean action: 2.000 [2.000, 2.000],  loss: 26042112.000000, mae: 1500.642578, mean_q: -54.849148, mean_eps: 0.091117
  989/10000: episode: 989, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -579.298, mean reward: -579.298 [-579.298, -579.298], mean action: 2.000 [2.000, 2.000],  loss: 18446552.000000, mae: 1291.425537, mean_q: -55.163712, mean_eps: 0.091108
  990/10000: episode: 990, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5873.592, mean reward: -5873.592 [-5873.592, -5873.592], mean action: 2.000 [2.000, 2.000],  loss: 25946160.000000, mae: 1652.074463, mean_q: -55.455719, mean_eps: 0.091099
  991/10000: episode: 991, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -1925.171, mean reward: -1925.171 [-1925.171, -1925.171], mean action: 2.000 [2.000, 2.000],  loss: 22973466.000000, mae: 1486.670166, mean_q: -55.853134, mean_eps: 0.091090
  992/10000: episode: 992, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -6800.683, mean reward: -6800.683 [-6800.683, -6800.683], mean action: 2.000 [2.000, 2.000],  loss: 12028703.000000, mae: 1102.472656, mean_q: -56.596741, mean_eps: 0.091081
  993/10000: episode: 993, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -982.110, mean reward: -982.110 [-982.110, -982.110], mean action: 2.000 [2.000, 2.000],  loss: 12911350.000000, mae: 1146.331299, mean_q: -56.921951, mean_eps: 0.091072
  994/10000: episode: 994, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -7026.744, mean reward: -7026.744 [-7026.744, -7026.744], mean action: 2.000 [2.000, 2.000],  loss: 20574804.000000, mae: 1391.927490, mean_q: -57.203751, mean_eps: 0.091063
  995/10000: episode: 995, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -2697.477, mean reward: -2697.477 [-2697.477, -2697.477], mean action: 2.000 [2.000, 2.000],  loss: 24021136.000000, mae: 1475.291504, mean_q: -57.674110, mean_eps: 0.091054
  996/10000: episode: 996, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5205.333, mean reward: -5205.333 [-5205.333, -5205.333], mean action: 0.000 [0.000, 0.000],  loss: 19737874.000000, mae: 1354.161743, mean_q: -57.878853, mean_eps: 0.091045
  997/10000: episode: 997, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1732.336, mean reward: -1732.336 [-1732.336, -1732.336], mean action: 2.000 [2.000, 2.000],  loss: 14863633.000000, mae: 1257.763062, mean_q: -58.468235, mean_eps: 0.091036
  998/10000: episode: 998, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1286.330, mean reward: -1286.330 [-1286.330, -1286.330], mean action: 2.000 [2.000, 2.000],  loss: 36077392.000000, mae: 1860.100464, mean_q: -58.571045, mean_eps: 0.091027
  999/10000: episode: 999, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1283.079, mean reward: -1283.079 [-1283.079, -1283.079], mean action: 2.000 [2.000, 2.000],  loss: 24293732.000000, mae: 1507.677490, mean_q: -59.032104, mean_eps: 0.091018
 1000/10000: episode: 1000, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1990.762, mean reward: -1990.762 [-1990.762, -1990.762], mean action: 2.000 [2.000, 2.000],  loss: 20937332.000000, mae: 1363.834473, mean_q: -59.707542, mean_eps: 0.091009
 1001/10000: episode: 1001, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -3216.579, mean reward: -3216.579 [-3216.579, -3216.579], mean action: 2.000 [2.000, 2.000],  loss: 15942073.000000, mae: 1220.575684, mean_q: -60.155941, mean_eps: 0.091000
 1002/10000: episode: 1002, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -11666.380, mean reward: -11666.380 [-11666.380, -11666.380], mean action: 0.000 [0.000, 0.000],  loss: 18861686.000000, mae: 1299.905518, mean_q: -60.623520, mean_eps: 0.090991
 1003/10000: episode: 1003, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2446.298, mean reward: -2446.298 [-2446.298, -2446.298], mean action: 2.000 [2.000, 2.000],  loss: 20191024.000000, mae: 1495.032227, mean_q: -60.829330, mean_eps: 0.090982
 1004/10000: episode: 1004, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4544.517, mean reward: -4544.517 [-4544.517, -4544.517], mean action: 2.000 [2.000, 2.000],  loss: 18584318.000000, mae: 1307.331787, mean_q: -61.348679, mean_eps: 0.090973
 1005/10000: episode: 1005, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1574.788, mean reward: -1574.788 [-1574.788, -1574.788], mean action: 2.000 [2.000, 2.000],  loss: 26642188.000000, mae: 1595.147949, mean_q: -61.484741, mean_eps: 0.090964
 1006/10000: episode: 1006, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -4003.137, mean reward: -4003.137 [-4003.137, -4003.137], mean action: 2.000 [2.000, 2.000],  loss: 20622528.000000, mae: 1367.857544, mean_q: -62.333855, mean_eps: 0.090955
 1007/10000: episode: 1007, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -2107.008, mean reward: -2107.008 [-2107.008, -2107.008], mean action: 2.000 [2.000, 2.000],  loss: 12002655.000000, mae: 1077.639771, mean_q: -62.925369, mean_eps: 0.090946
 1008/10000: episode: 1008, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -672.534, mean reward: -672.534 [-672.534, -672.534], mean action: 2.000 [2.000, 2.000],  loss: 18887568.000000, mae: 1301.364258, mean_q: -63.024948, mean_eps: 0.090937
 1009/10000: episode: 1009, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1800.104, mean reward: -1800.104 [-1800.104, -1800.104], mean action: 2.000 [2.000, 2.000],  loss: 16723509.000000, mae: 1202.760254, mean_q: -63.394672, mean_eps: 0.090928
 1010/10000: episode: 1010, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3847.407, mean reward: -3847.407 [-3847.407, -3847.407], mean action: 3.000 [3.000, 3.000],  loss: 13976202.000000, mae: 1169.444214, mean_q: -64.067291, mean_eps: 0.090919
 1011/10000: episode: 1011, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1246.587, mean reward: -1246.587 [-1246.587, -1246.587], mean action: 3.000 [3.000, 3.000],  loss: 14492352.000000, mae: 1170.671875, mean_q: -64.258293, mean_eps: 0.090910
 1012/10000: episode: 1012, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -9149.441, mean reward: -9149.441 [-9149.441, -9149.441], mean action: 3.000 [3.000, 3.000],  loss: 17358668.000000, mae: 1230.800171, mean_q: -64.743629, mean_eps: 0.090901
 1013/10000: episode: 1013, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2768.116, mean reward: -2768.116 [-2768.116, -2768.116], mean action: 3.000 [3.000, 3.000],  loss: 20801768.000000, mae: 1332.274536, mean_q: -65.087036, mean_eps: 0.090892
 1014/10000: episode: 1014, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -776.239, mean reward: -776.239 [-776.239, -776.239], mean action: 3.000 [3.000, 3.000],  loss: 12883034.000000, mae: 1132.304688, mean_q: -65.503204, mean_eps: 0.090883
 1015/10000: episode: 1015, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3300.756, mean reward: -3300.756 [-3300.756, -3300.756], mean action: 3.000 [3.000, 3.000],  loss: 16333352.000000, mae: 1212.606934, mean_q: -65.840179, mean_eps: 0.090874
 1016/10000: episode: 1016, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2860.276, mean reward: -2860.276 [-2860.276, -2860.276], mean action: 3.000 [3.000, 3.000],  loss: 17934096.000000, mae: 1336.204834, mean_q: -65.998901, mean_eps: 0.090865
 1017/10000: episode: 1017, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -6736.926, mean reward: -6736.926 [-6736.926, -6736.926], mean action: 3.000 [3.000, 3.000],  loss: 20732484.000000, mae: 1295.445190, mean_q: -66.167320, mean_eps: 0.090856
 1018/10000: episode: 1018, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -629.999, mean reward: -629.999 [-629.999, -629.999], mean action: 3.000 [3.000, 3.000],  loss: 22934914.000000, mae: 1443.767090, mean_q: -66.650856, mean_eps: 0.090847
 1019/10000: episode: 1019, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3647.732, mean reward: -3647.732 [-3647.732, -3647.732], mean action: 3.000 [3.000, 3.000],  loss: 21972636.000000, mae: 1399.063843, mean_q: -67.100784, mean_eps: 0.090838
 1020/10000: episode: 1020, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -7497.829, mean reward: -7497.829 [-7497.829, -7497.829], mean action: 3.000 [3.000, 3.000],  loss: 15028028.000000, mae: 1126.740356, mean_q: -67.449692, mean_eps: 0.090829
 1021/10000: episode: 1021, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1927.402, mean reward: -1927.402 [-1927.402, -1927.402], mean action: 3.000 [3.000, 3.000],  loss: 20550060.000000, mae: 1417.388184, mean_q: -67.624176, mean_eps: 0.090820
 1022/10000: episode: 1022, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -95.889, mean reward: -95.889 [-95.889, -95.889], mean action: 3.000 [3.000, 3.000],  loss: 9385490.000000, mae: 1021.953369, mean_q: -68.651848, mean_eps: 0.090811
 1023/10000: episode: 1023, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5091.645, mean reward: -5091.645 [-5091.645, -5091.645], mean action: 3.000 [3.000, 3.000],  loss: 18719416.000000, mae: 1314.989136, mean_q: -68.522255, mean_eps: 0.090802
 1024/10000: episode: 1024, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2044.635, mean reward: -2044.635 [-2044.635, -2044.635], mean action: 3.000 [3.000, 3.000],  loss: 28629954.000000, mae: 1607.494263, mean_q: -68.502136, mean_eps: 0.090793
 1025/10000: episode: 1025, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3590.752, mean reward: -3590.752 [-3590.752, -3590.752], mean action: 3.000 [3.000, 3.000],  loss: 21092476.000000, mae: 1403.268555, mean_q: -69.243134, mean_eps: 0.090784
 1026/10000: episode: 1026, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -6701.611, mean reward: -6701.611 [-6701.611, -6701.611], mean action: 3.000 [3.000, 3.000],  loss: 14621979.000000, mae: 1132.241455, mean_q: -69.869919, mean_eps: 0.090775
 1027/10000: episode: 1027, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -4487.091, mean reward: -4487.091 [-4487.091, -4487.091], mean action: 3.000 [3.000, 3.000],  loss: 18893648.000000, mae: 1336.127686, mean_q: -70.486115, mean_eps: 0.090766
 1028/10000: episode: 1028, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2114.088, mean reward: -2114.088 [-2114.088, -2114.088], mean action: 3.000 [3.000, 3.000],  loss: 16019008.000000, mae: 1168.952393, mean_q: -70.638535, mean_eps: 0.090757
 1029/10000: episode: 1029, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6478.928, mean reward: -6478.928 [-6478.928, -6478.928], mean action: 3.000 [3.000, 3.000],  loss: 24673528.000000, mae: 1561.337769, mean_q: -70.957962, mean_eps: 0.090748
 1030/10000: episode: 1030, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7030.163, mean reward: -7030.163 [-7030.163, -7030.163], mean action: 3.000 [3.000, 3.000],  loss: 22171256.000000, mae: 1448.397217, mean_q: -71.468781, mean_eps: 0.090739
 1031/10000: episode: 1031, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6930.496, mean reward: -6930.496 [-6930.496, -6930.496], mean action: 3.000 [3.000, 3.000],  loss: 20726884.000000, mae: 1319.888672, mean_q: -71.750153, mean_eps: 0.090730
 1032/10000: episode: 1032, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2235.023, mean reward: -2235.023 [-2235.023, -2235.023], mean action: 0.000 [0.000, 0.000],  loss: 21840438.000000, mae: 1385.680786, mean_q: -72.178864, mean_eps: 0.090721
 1033/10000: episode: 1033, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -4148.080, mean reward: -4148.080 [-4148.080, -4148.080], mean action: 3.000 [3.000, 3.000],  loss: 13860289.000000, mae: 1131.804565, mean_q: -72.943970, mean_eps: 0.090712
 1034/10000: episode: 1034, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5934.377, mean reward: -5934.377 [-5934.377, -5934.377], mean action: 3.000 [3.000, 3.000],  loss: 23767466.000000, mae: 1508.114136, mean_q: -72.969421, mean_eps: 0.090703
 1035/10000: episode: 1035, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4139.142, mean reward: -4139.142 [-4139.142, -4139.142], mean action: 3.000 [3.000, 3.000],  loss: 12442527.000000, mae: 1072.489746, mean_q: -73.675102, mean_eps: 0.090694
 1036/10000: episode: 1036, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -4054.941, mean reward: -4054.941 [-4054.941, -4054.941], mean action: 3.000 [3.000, 3.000],  loss: 20239678.000000, mae: 1271.322144, mean_q: -74.198395, mean_eps: 0.090685
 1037/10000: episode: 1037, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2103.380, mean reward: -2103.380 [-2103.380, -2103.380], mean action: 3.000 [3.000, 3.000],  loss: 24315880.000000, mae: 1437.097046, mean_q: -74.385345, mean_eps: 0.090676
 1038/10000: episode: 1038, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -600.278, mean reward: -600.278 [-600.278, -600.278], mean action: 3.000 [3.000, 3.000],  loss: 18347354.000000, mae: 1291.531250, mean_q: -74.946518, mean_eps: 0.090667
 1039/10000: episode: 1039, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -358.081, mean reward: -358.081 [-358.081, -358.081], mean action: 3.000 [3.000, 3.000],  loss: 27993432.000000, mae: 1628.562622, mean_q: -75.085510, mean_eps: 0.090658
 1040/10000: episode: 1040, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -42.022, mean reward: -42.022 [-42.022, -42.022], mean action: 3.000 [3.000, 3.000],  loss: 21995538.000000, mae: 1383.994141, mean_q: -75.761536, mean_eps: 0.090649
 1041/10000: episode: 1041, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1088.091, mean reward: -1088.091 [-1088.091, -1088.091], mean action: 3.000 [3.000, 3.000],  loss: 20332528.000000, mae: 1385.030273, mean_q: -76.152496, mean_eps: 0.090640
 1042/10000: episode: 1042, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -6214.321, mean reward: -6214.321 [-6214.321, -6214.321], mean action: 3.000 [3.000, 3.000],  loss: 15663196.000000, mae: 1292.732422, mean_q: -76.495377, mean_eps: 0.090631
 1043/10000: episode: 1043, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5828.009, mean reward: -5828.009 [-5828.009, -5828.009], mean action: 3.000 [3.000, 3.000],  loss: 27022530.000000, mae: 1608.387695, mean_q: -76.849777, mean_eps: 0.090622
 1044/10000: episode: 1044, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -815.837, mean reward: -815.837 [-815.837, -815.837], mean action: 3.000 [3.000, 3.000],  loss: 18708962.000000, mae: 1242.394043, mean_q: -77.427208, mean_eps: 0.090613
 1045/10000: episode: 1045, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2202.508, mean reward: -2202.508 [-2202.508, -2202.508], mean action: 3.000 [3.000, 3.000],  loss: 22020684.000000, mae: 1441.567383, mean_q: -77.934891, mean_eps: 0.090604
 1046/10000: episode: 1046, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3010.104, mean reward: -3010.104 [-3010.104, -3010.104], mean action: 3.000 [3.000, 3.000],  loss: 23085632.000000, mae: 1466.186401, mean_q: -78.326035, mean_eps: 0.090595
 1047/10000: episode: 1047, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2547.374, mean reward: -2547.374 [-2547.374, -2547.374], mean action: 2.000 [2.000, 2.000],  loss: 22766352.000000, mae: 1449.371094, mean_q: -79.043640, mean_eps: 0.090586
 1048/10000: episode: 1048, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -10574.369, mean reward: -10574.369 [-10574.369, -10574.369], mean action: 3.000 [3.000, 3.000],  loss: 25573404.000000, mae: 1499.623047, mean_q: -79.221825, mean_eps: 0.090577
 1049/10000: episode: 1049, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1913.449, mean reward: -1913.449 [-1913.449, -1913.449], mean action: 3.000 [3.000, 3.000],  loss: 15249186.000000, mae: 1183.747803, mean_q: -79.704231, mean_eps: 0.090568
 1050/10000: episode: 1050, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1581.516, mean reward: -1581.516 [-1581.516, -1581.516], mean action: 3.000 [3.000, 3.000],  loss: 21682964.000000, mae: 1398.877686, mean_q: -80.262878, mean_eps: 0.090559
 1051/10000: episode: 1051, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1635.705, mean reward: -1635.705 [-1635.705, -1635.705], mean action: 3.000 [3.000, 3.000],  loss: 18473828.000000, mae: 1344.251709, mean_q: -80.736687, mean_eps: 0.090550
 1052/10000: episode: 1052, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -6427.885, mean reward: -6427.885 [-6427.885, -6427.885], mean action: 3.000 [3.000, 3.000],  loss: 15419728.000000, mae: 1197.233154, mean_q: -81.338173, mean_eps: 0.090541
 1053/10000: episode: 1053, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -525.750, mean reward: -525.750 [-525.750, -525.750], mean action: 3.000 [3.000, 3.000],  loss: 17796808.000000, mae: 1323.074707, mean_q: -81.705856, mean_eps: 0.090532
 1054/10000: episode: 1054, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -4977.340, mean reward: -4977.340 [-4977.340, -4977.340], mean action: 3.000 [3.000, 3.000],  loss: 17415088.000000, mae: 1201.868408, mean_q: -82.087708, mean_eps: 0.090523
 1055/10000: episode: 1055, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5006.544, mean reward: -5006.544 [-5006.544, -5006.544], mean action: 3.000 [3.000, 3.000],  loss: 16083651.000000, mae: 1221.844727, mean_q: -83.348610, mean_eps: 0.090514
 1056/10000: episode: 1056, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2386.128, mean reward: -2386.128 [-2386.128, -2386.128], mean action: 3.000 [3.000, 3.000],  loss: 19986686.000000, mae: 1314.939209, mean_q: -83.364716, mean_eps: 0.090505
 1057/10000: episode: 1057, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1611.907, mean reward: -1611.907 [-1611.907, -1611.907], mean action: 3.000 [3.000, 3.000],  loss: 14634602.000000, mae: 1160.928467, mean_q: -84.063293, mean_eps: 0.090496
 1058/10000: episode: 1058, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1855.477, mean reward: -1855.477 [-1855.477, -1855.477], mean action: 3.000 [3.000, 3.000],  loss: 16662060.000000, mae: 1289.307861, mean_q: -84.294144, mean_eps: 0.090487
 1059/10000: episode: 1059, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1745.565, mean reward: -1745.565 [-1745.565, -1745.565], mean action: 3.000 [3.000, 3.000],  loss: 19613760.000000, mae: 1258.948242, mean_q: -84.790497, mean_eps: 0.090478
 1060/10000: episode: 1060, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -2546.829, mean reward: -2546.829 [-2546.829, -2546.829], mean action: 3.000 [3.000, 3.000],  loss: 15966098.000000, mae: 1174.949829, mean_q: -85.153603, mean_eps: 0.090469
 1061/10000: episode: 1061, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -30.719, mean reward: -30.719 [-30.719, -30.719], mean action: 2.000 [2.000, 2.000],  loss: 22288062.000000, mae: 1385.711670, mean_q: -85.491760, mean_eps: 0.090460
 1062/10000: episode: 1062, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1254.755, mean reward: -1254.755 [-1254.755, -1254.755], mean action: 2.000 [2.000, 2.000],  loss: 22631904.000000, mae: 1361.606445, mean_q: -86.041641, mean_eps: 0.090451
 1063/10000: episode: 1063, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -44.494, mean reward: -44.494 [-44.494, -44.494], mean action: 2.000 [2.000, 2.000],  loss: 21281564.000000, mae: 1410.354614, mean_q: -86.626129, mean_eps: 0.090442
 1064/10000: episode: 1064, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -6836.083, mean reward: -6836.083 [-6836.083, -6836.083], mean action: 2.000 [2.000, 2.000],  loss: 15072136.000000, mae: 1128.121704, mean_q: -87.319870, mean_eps: 0.090433
 1065/10000: episode: 1065, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3616.597, mean reward: -3616.597 [-3616.597, -3616.597], mean action: 2.000 [2.000, 2.000],  loss: 20538492.000000, mae: 1406.856201, mean_q: -87.466217, mean_eps: 0.090424
 1066/10000: episode: 1066, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -219.686, mean reward: -219.686 [-219.686, -219.686], mean action: 2.000 [2.000, 2.000],  loss: 25264034.000000, mae: 1583.763916, mean_q: -87.729034, mean_eps: 0.090415
 1067/10000: episode: 1067, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -6089.086, mean reward: -6089.086 [-6089.086, -6089.086], mean action: 2.000 [2.000, 2.000],  loss: 11796346.000000, mae: 1047.741089, mean_q: -88.655518, mean_eps: 0.090406
 1068/10000: episode: 1068, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -673.281, mean reward: -673.281 [-673.281, -673.281], mean action: 2.000 [2.000, 2.000],  loss: 20297850.000000, mae: 1405.302979, mean_q: -88.956512, mean_eps: 0.090397
 1069/10000: episode: 1069, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -391.782, mean reward: -391.782 [-391.782, -391.782], mean action: 2.000 [2.000, 2.000],  loss: 14825084.000000, mae: 1198.059082, mean_q: -89.502373, mean_eps: 0.090388
 1070/10000: episode: 1070, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -361.074, mean reward: -361.074 [-361.074, -361.074], mean action: 2.000 [2.000, 2.000],  loss: 12261334.000000, mae: 1111.515869, mean_q: -89.584137, mean_eps: 0.090379
 1071/10000: episode: 1071, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -121.590, mean reward: -121.590 [-121.590, -121.590], mean action: 2.000 [2.000, 2.000],  loss: 21628240.000000, mae: 1463.924683, mean_q: -90.242737, mean_eps: 0.090370
 1072/10000: episode: 1072, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -437.133, mean reward: -437.133 [-437.133, -437.133], mean action: 2.000 [2.000, 2.000],  loss: 16109838.000000, mae: 1226.300781, mean_q: -91.005417, mean_eps: 0.090361
 1073/10000: episode: 1073, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -1907.340, mean reward: -1907.340 [-1907.340, -1907.340], mean action: 2.000 [2.000, 2.000],  loss: 17822360.000000, mae: 1260.416992, mean_q: -91.299576, mean_eps: 0.090352
 1074/10000: episode: 1074, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2604.956, mean reward: -2604.956 [-2604.956, -2604.956], mean action: 2.000 [2.000, 2.000],  loss: 14679902.000000, mae: 1061.718750, mean_q: -91.626663, mean_eps: 0.090343
 1075/10000: episode: 1075, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -779.175, mean reward: -779.175 [-779.175, -779.175], mean action: 2.000 [2.000, 2.000],  loss: 18190252.000000, mae: 1300.263062, mean_q: -92.272049, mean_eps: 0.090334
 1076/10000: episode: 1076, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -6684.981, mean reward: -6684.981 [-6684.981, -6684.981], mean action: 2.000 [2.000, 2.000],  loss: 18251300.000000, mae: 1358.787231, mean_q: -92.965118, mean_eps: 0.090325
 1077/10000: episode: 1077, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -6791.888, mean reward: -6791.888 [-6791.888, -6791.888], mean action: 2.000 [2.000, 2.000],  loss: 25571452.000000, mae: 1573.973633, mean_q: -92.970657, mean_eps: 0.090316
 1078/10000: episode: 1078, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1735.786, mean reward: -1735.786 [-1735.786, -1735.786], mean action: 2.000 [2.000, 2.000],  loss: 24340752.000000, mae: 1566.552246, mean_q: -93.485733, mean_eps: 0.090307
 1079/10000: episode: 1079, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5.586, mean reward: -5.586 [-5.586, -5.586], mean action: 2.000 [2.000, 2.000],  loss: 19627284.000000, mae: 1360.857178, mean_q: -94.295403, mean_eps: 0.090298
 1080/10000: episode: 1080, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -95.235, mean reward: -95.235 [-95.235, -95.235], mean action: 2.000 [2.000, 2.000],  loss: 18703152.000000, mae: 1364.162354, mean_q: -94.908302, mean_eps: 0.090289
 1081/10000: episode: 1081, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -603.609, mean reward: -603.609 [-603.609, -603.609], mean action: 2.000 [2.000, 2.000],  loss: 19891118.000000, mae: 1344.354980, mean_q: -95.492447, mean_eps: 0.090280
 1082/10000: episode: 1082, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -271.696, mean reward: -271.696 [-271.696, -271.696], mean action: 2.000 [2.000, 2.000],  loss: 24670590.000000, mae: 1509.582031, mean_q: -95.824768, mean_eps: 0.090271
 1083/10000: episode: 1083, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1567.252, mean reward: -1567.252 [-1567.252, -1567.252], mean action: 2.000 [2.000, 2.000],  loss: 18877238.000000, mae: 1229.793823, mean_q: -96.580200, mean_eps: 0.090262
 1084/10000: episode: 1084, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5074.408, mean reward: -5074.408 [-5074.408, -5074.408], mean action: 2.000 [2.000, 2.000],  loss: 19620854.000000, mae: 1330.177979, mean_q: -97.056656, mean_eps: 0.090253
 1085/10000: episode: 1085, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1482.287, mean reward: -1482.287 [-1482.287, -1482.287], mean action: 2.000 [2.000, 2.000],  loss: 12447486.000000, mae: 1027.693359, mean_q: -97.835480, mean_eps: 0.090244
 1086/10000: episode: 1086, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2929.058, mean reward: -2929.058 [-2929.058, -2929.058], mean action: 2.000 [2.000, 2.000],  loss: 23176156.000000, mae: 1434.312256, mean_q: -97.781662, mean_eps: 0.090235
 1087/10000: episode: 1087, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -3761.958, mean reward: -3761.958 [-3761.958, -3761.958], mean action: 2.000 [2.000, 2.000],  loss: 19322140.000000, mae: 1306.645874, mean_q: -98.469391, mean_eps: 0.090226
 1088/10000: episode: 1088, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -7602.806, mean reward: -7602.806 [-7602.806, -7602.806], mean action: 2.000 [2.000, 2.000],  loss: 18943804.000000, mae: 1331.266113, mean_q: -99.164642, mean_eps: 0.090217
 1089/10000: episode: 1089, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1926.283, mean reward: -1926.283 [-1926.283, -1926.283], mean action: 2.000 [2.000, 2.000],  loss: 16167456.000000, mae: 1248.918701, mean_q: -99.832481, mean_eps: 0.090208
 1090/10000: episode: 1090, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -712.156, mean reward: -712.156 [-712.156, -712.156], mean action: 2.000 [2.000, 2.000],  loss: 15592562.000000, mae: 1202.642944, mean_q: -100.207756, mean_eps: 0.090199
 1091/10000: episode: 1091, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -588.332, mean reward: -588.332 [-588.332, -588.332], mean action: 2.000 [2.000, 2.000],  loss: 25185240.000000, mae: 1498.955566, mean_q: -100.116150, mean_eps: 0.090190
 1092/10000: episode: 1092, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5309.162, mean reward: -5309.162 [-5309.162, -5309.162], mean action: 2.000 [2.000, 2.000],  loss: 18633146.000000, mae: 1324.881104, mean_q: -101.000488, mean_eps: 0.090181
 1093/10000: episode: 1093, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2576.510, mean reward: -2576.510 [-2576.510, -2576.510], mean action: 2.000 [2.000, 2.000],  loss: 18702472.000000, mae: 1201.421875, mean_q: -101.785713, mean_eps: 0.090172
 1094/10000: episode: 1094, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1639.729, mean reward: -1639.729 [-1639.729, -1639.729], mean action: 2.000 [2.000, 2.000],  loss: 23543694.000000, mae: 1542.649902, mean_q: -102.049065, mean_eps: 0.090163
 1095/10000: episode: 1095, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3637.418, mean reward: -3637.418 [-3637.418, -3637.418], mean action: 2.000 [2.000, 2.000],  loss: 25429098.000000, mae: 1531.905396, mean_q: -102.485718, mean_eps: 0.090154
 1096/10000: episode: 1096, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -6111.812, mean reward: -6111.812 [-6111.812, -6111.812], mean action: 3.000 [3.000, 3.000],  loss: 21204718.000000, mae: 1317.937134, mean_q: -103.458488, mean_eps: 0.090145
 1097/10000: episode: 1097, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1091.381, mean reward: -1091.381 [-1091.381, -1091.381], mean action: 2.000 [2.000, 2.000],  loss: 17850824.000000, mae: 1290.094238, mean_q: -103.970261, mean_eps: 0.090136
 1098/10000: episode: 1098, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -8609.468, mean reward: -8609.468 [-8609.468, -8609.468], mean action: 1.000 [1.000, 1.000],  loss: 24628962.000000, mae: 1508.441162, mean_q: -104.357742, mean_eps: 0.090127
 1099/10000: episode: 1099, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -90.538, mean reward: -90.538 [-90.538, -90.538], mean action: 2.000 [2.000, 2.000],  loss: 18408402.000000, mae: 1377.714233, mean_q: -105.042641, mean_eps: 0.090118
 1100/10000: episode: 1100, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -1582.147, mean reward: -1582.147 [-1582.147, -1582.147], mean action: 2.000 [2.000, 2.000],  loss: 23576992.000000, mae: 1611.865356, mean_q: -105.588455, mean_eps: 0.090109
 1101/10000: episode: 1101, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5002.285, mean reward: -5002.285 [-5002.285, -5002.285], mean action: 2.000 [2.000, 2.000],  loss: 21409724.000000, mae: 1503.794434, mean_q: -106.189758, mean_eps: 0.090100
 1102/10000: episode: 1102, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4371.710, mean reward: -4371.710 [-4371.710, -4371.710], mean action: 2.000 [2.000, 2.000],  loss: 21600212.000000, mae: 1337.925537, mean_q: -106.840935, mean_eps: 0.090091
 1103/10000: episode: 1103, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2468.975, mean reward: -2468.975 [-2468.975, -2468.975], mean action: 2.000 [2.000, 2.000],  loss: 17154720.000000, mae: 1232.980713, mean_q: -107.615105, mean_eps: 0.090082
 1104/10000: episode: 1104, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -1524.464, mean reward: -1524.464 [-1524.464, -1524.464], mean action: 2.000 [2.000, 2.000],  loss: 16265054.000000, mae: 1315.656250, mean_q: -108.074493, mean_eps: 0.090073
 1105/10000: episode: 1105, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1100.879, mean reward: -1100.879 [-1100.879, -1100.879], mean action: 2.000 [2.000, 2.000],  loss: 17928736.000000, mae: 1369.262695, mean_q: -108.503815, mean_eps: 0.090064
 1106/10000: episode: 1106, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3005.519, mean reward: -3005.519 [-3005.519, -3005.519], mean action: 2.000 [2.000, 2.000],  loss: 16690235.000000, mae: 1198.666260, mean_q: -109.253632, mean_eps: 0.090055
 1107/10000: episode: 1107, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -3536.363, mean reward: -3536.363 [-3536.363, -3536.363], mean action: 2.000 [2.000, 2.000],  loss: 16667407.000000, mae: 1314.272461, mean_q: -109.961487, mean_eps: 0.090046
 1108/10000: episode: 1108, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -64.765, mean reward: -64.765 [-64.765, -64.765], mean action: 2.000 [2.000, 2.000],  loss: 22587288.000000, mae: 1383.534058, mean_q: -110.480408, mean_eps: 0.090037
 1109/10000: episode: 1109, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -1718.070, mean reward: -1718.070 [-1718.070, -1718.070], mean action: 2.000 [2.000, 2.000],  loss: 15463912.000000, mae: 1188.757568, mean_q: -111.093765, mean_eps: 0.090028
 1110/10000: episode: 1110, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -1788.841, mean reward: -1788.841 [-1788.841, -1788.841], mean action: 2.000 [2.000, 2.000],  loss: 10744109.000000, mae: 1044.736816, mean_q: -112.022247, mean_eps: 0.090019
 1111/10000: episode: 1111, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -1747.007, mean reward: -1747.007 [-1747.007, -1747.007], mean action: 2.000 [2.000, 2.000],  loss: 16478176.000000, mae: 1227.802002, mean_q: -111.998093, mean_eps: 0.090010
 1112/10000: episode: 1112, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -1565.226, mean reward: -1565.226 [-1565.226, -1565.226], mean action: 2.000 [2.000, 2.000],  loss: 12311882.000000, mae: 1127.851440, mean_q: -113.076248, mean_eps: 0.090001
 1113/10000: episode: 1113, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1587.130, mean reward: -1587.130 [-1587.130, -1587.130], mean action: 2.000 [2.000, 2.000],  loss: 16103521.000000, mae: 1233.628052, mean_q: -113.228638, mean_eps: 0.089992
 1114/10000: episode: 1114, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -564.077, mean reward: -564.077 [-564.077, -564.077], mean action: 2.000 [2.000, 2.000],  loss: 18001040.000000, mae: 1386.456909, mean_q: -113.681816, mean_eps: 0.089983
 1115/10000: episode: 1115, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3246.091, mean reward: -3246.091 [-3246.091, -3246.091], mean action: 2.000 [2.000, 2.000],  loss: 25303052.000000, mae: 1582.757812, mean_q: -113.914948, mean_eps: 0.089974
 1116/10000: episode: 1116, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -7021.755, mean reward: -7021.755 [-7021.755, -7021.755], mean action: 2.000 [2.000, 2.000],  loss: 17474500.000000, mae: 1164.580078, mean_q: -114.311722, mean_eps: 0.089965
 1117/10000: episode: 1117, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -2632.311, mean reward: -2632.311 [-2632.311, -2632.311], mean action: 2.000 [2.000, 2.000],  loss: 20001384.000000, mae: 1385.643799, mean_q: -115.522644, mean_eps: 0.089956
 1118/10000: episode: 1118, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4112.826, mean reward: -4112.826 [-4112.826, -4112.826], mean action: 2.000 [2.000, 2.000],  loss: 15883352.000000, mae: 1138.044434, mean_q: -116.533844, mean_eps: 0.089947
 1119/10000: episode: 1119, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2396.422, mean reward: -2396.422 [-2396.422, -2396.422], mean action: 2.000 [2.000, 2.000],  loss: 17884234.000000, mae: 1296.438232, mean_q: -116.116875, mean_eps: 0.089938
 1120/10000: episode: 1120, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2328.241, mean reward: -2328.241 [-2328.241, -2328.241], mean action: 2.000 [2.000, 2.000],  loss: 18287166.000000, mae: 1356.576904, mean_q: -117.111053, mean_eps: 0.089929
 1121/10000: episode: 1121, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5144.207, mean reward: -5144.207 [-5144.207, -5144.207], mean action: 2.000 [2.000, 2.000],  loss: 22440578.000000, mae: 1423.489990, mean_q: -117.565216, mean_eps: 0.089920
 1122/10000: episode: 1122, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3950.230, mean reward: -3950.230 [-3950.230, -3950.230], mean action: 1.000 [1.000, 1.000],  loss: 19207590.000000, mae: 1323.729492, mean_q: -117.853668, mean_eps: 0.089911
 1123/10000: episode: 1123, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5417.811, mean reward: -5417.811 [-5417.811, -5417.811], mean action: 2.000 [2.000, 2.000],  loss: 20374846.000000, mae: 1417.072754, mean_q: -118.572983, mean_eps: 0.089902
 1124/10000: episode: 1124, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2887.595, mean reward: -2887.595 [-2887.595, -2887.595], mean action: 2.000 [2.000, 2.000],  loss: 22099648.000000, mae: 1509.484375, mean_q: -119.333206, mean_eps: 0.089893
 1125/10000: episode: 1125, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2076.990, mean reward: -2076.990 [-2076.990, -2076.990], mean action: 2.000 [2.000, 2.000],  loss: 13625400.000000, mae: 1193.982666, mean_q: -119.957809, mean_eps: 0.089884
 1126/10000: episode: 1126, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -919.684, mean reward: -919.684 [-919.684, -919.684], mean action: 2.000 [2.000, 2.000],  loss: 28037062.000000, mae: 1731.844360, mean_q: -120.172897, mean_eps: 0.089875
 1127/10000: episode: 1127, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -6597.765, mean reward: -6597.765 [-6597.765, -6597.765], mean action: 2.000 [2.000, 2.000],  loss: 16526778.000000, mae: 1330.202148, mean_q: -121.563744, mean_eps: 0.089866
 1128/10000: episode: 1128, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -2813.556, mean reward: -2813.556 [-2813.556, -2813.556], mean action: 2.000 [2.000, 2.000],  loss: 16356486.000000, mae: 1358.847290, mean_q: -121.939438, mean_eps: 0.089857
 1129/10000: episode: 1129, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1326.269, mean reward: -1326.269 [-1326.269, -1326.269], mean action: 2.000 [2.000, 2.000],  loss: 17486534.000000, mae: 1300.719604, mean_q: -122.406754, mean_eps: 0.089848
 1130/10000: episode: 1130, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -660.509, mean reward: -660.509 [-660.509, -660.509], mean action: 2.000 [2.000, 2.000],  loss: 16202803.000000, mae: 1305.614258, mean_q: -123.370895, mean_eps: 0.089839
 1131/10000: episode: 1131, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -615.084, mean reward: -615.084 [-615.084, -615.084], mean action: 2.000 [2.000, 2.000],  loss: 20268648.000000, mae: 1369.144897, mean_q: -123.306335, mean_eps: 0.089830
 1132/10000: episode: 1132, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2642.040, mean reward: -2642.040 [-2642.040, -2642.040], mean action: 2.000 [2.000, 2.000],  loss: 16477063.000000, mae: 1194.147217, mean_q: -124.390228, mean_eps: 0.089821
 1133/10000: episode: 1133, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3638.980, mean reward: -3638.980 [-3638.980, -3638.980], mean action: 2.000 [2.000, 2.000],  loss: 20803972.000000, mae: 1389.104736, mean_q: -124.643723, mean_eps: 0.089812
 1134/10000: episode: 1134, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5578.352, mean reward: -5578.352 [-5578.352, -5578.352], mean action: 2.000 [2.000, 2.000],  loss: 21024832.000000, mae: 1473.590210, mean_q: -125.312881, mean_eps: 0.089803
 1135/10000: episode: 1135, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2816.229, mean reward: -2816.229 [-2816.229, -2816.229], mean action: 2.000 [2.000, 2.000],  loss: 15290448.000000, mae: 1267.514648, mean_q: -125.992996, mean_eps: 0.089794
 1136/10000: episode: 1136, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1609.648, mean reward: -1609.648 [-1609.648, -1609.648], mean action: 2.000 [2.000, 2.000],  loss: 19672192.000000, mae: 1462.307617, mean_q: -126.493713, mean_eps: 0.089785
 1137/10000: episode: 1137, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -29.426, mean reward: -29.426 [-29.426, -29.426], mean action: 2.000 [2.000, 2.000],  loss: 16597629.000000, mae: 1304.468506, mean_q: -127.342216, mean_eps: 0.089776
 1138/10000: episode: 1138, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1108.191, mean reward: -1108.191 [-1108.191, -1108.191], mean action: 2.000 [2.000, 2.000],  loss: 19576998.000000, mae: 1332.184814, mean_q: -127.111809, mean_eps: 0.089767
 1139/10000: episode: 1139, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3963.398, mean reward: -3963.398 [-3963.398, -3963.398], mean action: 2.000 [2.000, 2.000],  loss: 15351051.000000, mae: 1196.033203, mean_q: -129.091431, mean_eps: 0.089758
 1140/10000: episode: 1140, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5576.481, mean reward: -5576.481 [-5576.481, -5576.481], mean action: 2.000 [2.000, 2.000],  loss: 20664120.000000, mae: 1403.298096, mean_q: -129.458862, mean_eps: 0.089749
 1141/10000: episode: 1141, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1273.221, mean reward: -1273.221 [-1273.221, -1273.221], mean action: 2.000 [2.000, 2.000],  loss: 14547037.000000, mae: 1206.869751, mean_q: -129.745972, mean_eps: 0.089740
 1142/10000: episode: 1142, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -4109.847, mean reward: -4109.847 [-4109.847, -4109.847], mean action: 2.000 [2.000, 2.000],  loss: 15408052.000000, mae: 1269.250000, mean_q: -130.591736, mean_eps: 0.089731
 1143/10000: episode: 1143, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -8902.854, mean reward: -8902.854 [-8902.854, -8902.854], mean action: 1.000 [1.000, 1.000],  loss: 19796836.000000, mae: 1314.231812, mean_q: -131.099701, mean_eps: 0.089722
 1144/10000: episode: 1144, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1903.569, mean reward: -1903.569 [-1903.569, -1903.569], mean action: 2.000 [2.000, 2.000],  loss: 10550398.000000, mae: 1058.540161, mean_q: -132.362122, mean_eps: 0.089713
 1145/10000: episode: 1145, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1945.621, mean reward: -1945.621 [-1945.621, -1945.621], mean action: 2.000 [2.000, 2.000],  loss: 16785232.000000, mae: 1255.608643, mean_q: -132.606415, mean_eps: 0.089704
 1146/10000: episode: 1146, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3884.625, mean reward: -3884.625 [-3884.625, -3884.625], mean action: 2.000 [2.000, 2.000],  loss: 18785590.000000, mae: 1344.022705, mean_q: -133.474930, mean_eps: 0.089695
 1147/10000: episode: 1147, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -64.959, mean reward: -64.959 [-64.959, -64.959], mean action: 2.000 [2.000, 2.000],  loss: 9070034.000000, mae: 938.990723, mean_q: -134.202789, mean_eps: 0.089686
 1148/10000: episode: 1148, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2974.229, mean reward: -2974.229 [-2974.229, -2974.229], mean action: 2.000 [2.000, 2.000],  loss: 17083040.000000, mae: 1220.255127, mean_q: -134.257538, mean_eps: 0.089677
 1149/10000: episode: 1149, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -548.953, mean reward: -548.953 [-548.953, -548.953], mean action: 2.000 [2.000, 2.000],  loss: 14125229.000000, mae: 1244.137695, mean_q: -135.271454, mean_eps: 0.089668
 1150/10000: episode: 1150, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -1163.465, mean reward: -1163.465 [-1163.465, -1163.465], mean action: 2.000 [2.000, 2.000],  loss: 21170294.000000, mae: 1392.981567, mean_q: -135.257843, mean_eps: 0.089659
 1151/10000: episode: 1151, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -735.409, mean reward: -735.409 [-735.409, -735.409], mean action: 2.000 [2.000, 2.000],  loss: 15358249.000000, mae: 1274.037842, mean_q: -136.778931, mean_eps: 0.089650
 1152/10000: episode: 1152, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -1590.958, mean reward: -1590.958 [-1590.958, -1590.958], mean action: 2.000 [2.000, 2.000],  loss: 15764712.000000, mae: 1316.052490, mean_q: -137.768463, mean_eps: 0.089641
 1153/10000: episode: 1153, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3477.359, mean reward: -3477.359 [-3477.359, -3477.359], mean action: 2.000 [2.000, 2.000],  loss: 13698248.000000, mae: 1132.119629, mean_q: -137.797119, mean_eps: 0.089632
 1154/10000: episode: 1154, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -37.992, mean reward: -37.992 [-37.992, -37.992], mean action: 2.000 [2.000, 2.000],  loss: 23923062.000000, mae: 1462.476074, mean_q: -138.044434, mean_eps: 0.089623
 1155/10000: episode: 1155, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -819.582, mean reward: -819.582 [-819.582, -819.582], mean action: 2.000 [2.000, 2.000],  loss: 16100829.000000, mae: 1269.273926, mean_q: -139.386307, mean_eps: 0.089614
 1156/10000: episode: 1156, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1867.375, mean reward: -1867.375 [-1867.375, -1867.375], mean action: 2.000 [2.000, 2.000],  loss: 18566408.000000, mae: 1262.042725, mean_q: -139.655243, mean_eps: 0.089605
 1157/10000: episode: 1157, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -872.883, mean reward: -872.883 [-872.883, -872.883], mean action: 2.000 [2.000, 2.000],  loss: 16416655.000000, mae: 1301.821777, mean_q: -140.226425, mean_eps: 0.089596
 1158/10000: episode: 1158, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -146.794, mean reward: -146.794 [-146.794, -146.794], mean action: 2.000 [2.000, 2.000],  loss: 18968126.000000, mae: 1349.483887, mean_q: -141.271515, mean_eps: 0.089587
 1159/10000: episode: 1159, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -60.614, mean reward: -60.614 [-60.614, -60.614], mean action: 2.000 [2.000, 2.000],  loss: 20018408.000000, mae: 1312.225342, mean_q: -141.065430, mean_eps: 0.089578
 1160/10000: episode: 1160, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1900.722, mean reward: -1900.722 [-1900.722, -1900.722], mean action: 2.000 [2.000, 2.000],  loss: 17944658.000000, mae: 1405.621582, mean_q: -142.156296, mean_eps: 0.089569
 1161/10000: episode: 1161, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -446.941, mean reward: -446.941 [-446.941, -446.941], mean action: 2.000 [2.000, 2.000],  loss: 18993088.000000, mae: 1323.315796, mean_q: -143.305222, mean_eps: 0.089560
 1162/10000: episode: 1162, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -4926.916, mean reward: -4926.916 [-4926.916, -4926.916], mean action: 2.000 [2.000, 2.000],  loss: 14767124.000000, mae: 1122.152100, mean_q: -143.986023, mean_eps: 0.089551
 1163/10000: episode: 1163, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2116.791, mean reward: -2116.791 [-2116.791, -2116.791], mean action: 2.000 [2.000, 2.000],  loss: 10175299.000000, mae: 1107.165161, mean_q: -144.956268, mean_eps: 0.089542
 1164/10000: episode: 1164, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2771.868, mean reward: -2771.868 [-2771.868, -2771.868], mean action: 2.000 [2.000, 2.000],  loss: 25143242.000000, mae: 1512.958374, mean_q: -145.249481, mean_eps: 0.089533
 1165/10000: episode: 1165, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -517.434, mean reward: -517.434 [-517.434, -517.434], mean action: 2.000 [2.000, 2.000],  loss: 17766460.000000, mae: 1409.400757, mean_q: -146.448853, mean_eps: 0.089524
 1166/10000: episode: 1166, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3825.299, mean reward: -3825.299 [-3825.299, -3825.299], mean action: 2.000 [2.000, 2.000],  loss: 17592468.000000, mae: 1285.012573, mean_q: -146.281509, mean_eps: 0.089515
 1167/10000: episode: 1167, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3988.571, mean reward: -3988.571 [-3988.571, -3988.571], mean action: 2.000 [2.000, 2.000],  loss: 16454603.000000, mae: 1197.604248, mean_q: -147.503464, mean_eps: 0.089506
 1168/10000: episode: 1168, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1126.075, mean reward: -1126.075 [-1126.075, -1126.075], mean action: 2.000 [2.000, 2.000],  loss: 15418249.000000, mae: 1295.442383, mean_q: -148.088470, mean_eps: 0.089497
 1169/10000: episode: 1169, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1110.669, mean reward: -1110.669 [-1110.669, -1110.669], mean action: 2.000 [2.000, 2.000],  loss: 18584378.000000, mae: 1353.193604, mean_q: -148.519821, mean_eps: 0.089488
 1170/10000: episode: 1170, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -2054.403, mean reward: -2054.403 [-2054.403, -2054.403], mean action: 2.000 [2.000, 2.000],  loss: 12446110.000000, mae: 1025.364990, mean_q: -149.343658, mean_eps: 0.089479
 1171/10000: episode: 1171, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -338.602, mean reward: -338.602 [-338.602, -338.602], mean action: 3.000 [3.000, 3.000],  loss: 17364880.000000, mae: 1326.681885, mean_q: -150.009583, mean_eps: 0.089470
 1172/10000: episode: 1172, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -107.082, mean reward: -107.082 [-107.082, -107.082], mean action: 2.000 [2.000, 2.000],  loss: 14736664.000000, mae: 1204.044556, mean_q: -151.033234, mean_eps: 0.089461
 1173/10000: episode: 1173, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5808.809, mean reward: -5808.809 [-5808.809, -5808.809], mean action: 2.000 [2.000, 2.000],  loss: 14907060.000000, mae: 1163.777588, mean_q: -151.586212, mean_eps: 0.089452
 1174/10000: episode: 1174, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2795.305, mean reward: -2795.305 [-2795.305, -2795.305], mean action: 2.000 [2.000, 2.000],  loss: 15000785.000000, mae: 1290.501099, mean_q: -152.316086, mean_eps: 0.089443
 1175/10000: episode: 1175, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1831.873, mean reward: -1831.873 [-1831.873, -1831.873], mean action: 2.000 [2.000, 2.000],  loss: 14296750.000000, mae: 1264.586426, mean_q: -152.464874, mean_eps: 0.089434
 1176/10000: episode: 1176, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -6350.478, mean reward: -6350.478 [-6350.478, -6350.478], mean action: 2.000 [2.000, 2.000],  loss: 18427800.000000, mae: 1401.763916, mean_q: -153.374451, mean_eps: 0.089425
 1177/10000: episode: 1177, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2119.833, mean reward: -2119.833 [-2119.833, -2119.833], mean action: 2.000 [2.000, 2.000],  loss: 12349048.000000, mae: 1135.591919, mean_q: -154.725540, mean_eps: 0.089416
 1178/10000: episode: 1178, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -549.841, mean reward: -549.841 [-549.841, -549.841], mean action: 3.000 [3.000, 3.000],  loss: 18870568.000000, mae: 1420.411987, mean_q: -154.630249, mean_eps: 0.089407
 1179/10000: episode: 1179, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -801.520, mean reward: -801.520 [-801.520, -801.520], mean action: 2.000 [2.000, 2.000],  loss: 16590586.000000, mae: 1188.631348, mean_q: -155.701004, mean_eps: 0.089398
 1180/10000: episode: 1180, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -517.122, mean reward: -517.122 [-517.122, -517.122], mean action: 2.000 [2.000, 2.000],  loss: 20063376.000000, mae: 1450.139648, mean_q: -155.933060, mean_eps: 0.089389
 1181/10000: episode: 1181, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -759.837, mean reward: -759.837 [-759.837, -759.837], mean action: 2.000 [2.000, 2.000],  loss: 14790532.000000, mae: 1165.142090, mean_q: -157.547913, mean_eps: 0.089380
 1182/10000: episode: 1182, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2983.214, mean reward: -2983.214 [-2983.214, -2983.214], mean action: 2.000 [2.000, 2.000],  loss: 12472968.000000, mae: 1153.185425, mean_q: -157.900681, mean_eps: 0.089371
 1183/10000: episode: 1183, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2385.206, mean reward: -2385.206 [-2385.206, -2385.206], mean action: 3.000 [3.000, 3.000],  loss: 14987478.000000, mae: 1258.696289, mean_q: -159.139954, mean_eps: 0.089362
 1184/10000: episode: 1184, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -986.009, mean reward: -986.009 [-986.009, -986.009], mean action: 2.000 [2.000, 2.000],  loss: 18678046.000000, mae: 1333.272339, mean_q: -159.037796, mean_eps: 0.089353
 1185/10000: episode: 1185, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1074.907, mean reward: -1074.907 [-1074.907, -1074.907], mean action: 2.000 [2.000, 2.000],  loss: 21852592.000000, mae: 1434.433105, mean_q: -159.414017, mean_eps: 0.089344
 1186/10000: episode: 1186, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1959.415, mean reward: -1959.415 [-1959.415, -1959.415], mean action: 2.000 [2.000, 2.000],  loss: 14966392.000000, mae: 1253.845459, mean_q: -160.738831, mean_eps: 0.089335
 1187/10000: episode: 1187, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6264.324, mean reward: -6264.324 [-6264.324, -6264.324], mean action: 1.000 [1.000, 1.000],  loss: 13413624.000000, mae: 1202.395386, mean_q: -161.558792, mean_eps: 0.089326
 1188/10000: episode: 1188, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2795.455, mean reward: -2795.455 [-2795.455, -2795.455], mean action: 2.000 [2.000, 2.000],  loss: 18434394.000000, mae: 1412.285400, mean_q: -161.755386, mean_eps: 0.089317
 1189/10000: episode: 1189, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3238.943, mean reward: -3238.943 [-3238.943, -3238.943], mean action: 2.000 [2.000, 2.000],  loss: 17325940.000000, mae: 1358.071899, mean_q: -162.709641, mean_eps: 0.089308
 1190/10000: episode: 1190, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5153.315, mean reward: -5153.315 [-5153.315, -5153.315], mean action: 2.000 [2.000, 2.000],  loss: 13265723.000000, mae: 1179.781372, mean_q: -163.554291, mean_eps: 0.089299
 1191/10000: episode: 1191, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4368.286, mean reward: -4368.286 [-4368.286, -4368.286], mean action: 2.000 [2.000, 2.000],  loss: 20062380.000000, mae: 1360.303833, mean_q: -164.306168, mean_eps: 0.089290
 1192/10000: episode: 1192, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1060.175, mean reward: -1060.175 [-1060.175, -1060.175], mean action: 2.000 [2.000, 2.000],  loss: 18772588.000000, mae: 1385.333984, mean_q: -164.700058, mean_eps: 0.089281
 1193/10000: episode: 1193, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -3015.344, mean reward: -3015.344 [-3015.344, -3015.344], mean action: 2.000 [2.000, 2.000],  loss: 19066330.000000, mae: 1376.388550, mean_q: -165.324646, mean_eps: 0.089272
 1194/10000: episode: 1194, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4458.045, mean reward: -4458.045 [-4458.045, -4458.045], mean action: 1.000 [1.000, 1.000],  loss: 24420110.000000, mae: 1527.230957, mean_q: -166.163757, mean_eps: 0.089263
 1195/10000: episode: 1195, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2073.624, mean reward: -2073.624 [-2073.624, -2073.624], mean action: 2.000 [2.000, 2.000],  loss: 23013922.000000, mae: 1442.241089, mean_q: -166.521820, mean_eps: 0.089254
 1196/10000: episode: 1196, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -994.345, mean reward: -994.345 [-994.345, -994.345], mean action: 2.000 [2.000, 2.000],  loss: 21244824.000000, mae: 1398.418213, mean_q: -167.996964, mean_eps: 0.089245
 1197/10000: episode: 1197, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2057.999, mean reward: -2057.999 [-2057.999, -2057.999], mean action: 2.000 [2.000, 2.000],  loss: 16249522.000000, mae: 1260.023804, mean_q: -169.253525, mean_eps: 0.089236
 1198/10000: episode: 1198, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -881.564, mean reward: -881.564 [-881.564, -881.564], mean action: 2.000 [2.000, 2.000],  loss: 10823814.000000, mae: 1159.718506, mean_q: -170.230179, mean_eps: 0.089227
 1199/10000: episode: 1199, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5887.586, mean reward: -5887.586 [-5887.586, -5887.586], mean action: 3.000 [3.000, 3.000],  loss: 7601831.500000, mae: 939.828186, mean_q: -170.791870, mean_eps: 0.089218