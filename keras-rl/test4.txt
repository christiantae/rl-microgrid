Training for 1000 steps ...
   1/1000: episode: 1, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -8651.611, mean reward: -8651.611 [-8651.611, -8651.611], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   2/1000: episode: 2, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -9440.515, mean reward: -9440.515 [-9440.515, -9440.515], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   3/1000: episode: 3, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -10527.015, mean reward: -10527.015 [-10527.015, -10527.015], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   4/1000: episode: 4, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -222.433, mean reward: -222.433 [-222.433, -222.433], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   5/1000: episode: 5, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3299.220, mean reward: -3299.220 [-3299.220, -3299.220], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   6/1000: episode: 6, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6607.480, mean reward: -6607.480 [-6607.480, -6607.480], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   7/1000: episode: 7, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2987.740, mean reward: -2987.740 [-2987.740, -2987.740], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   8/1000: episode: 8, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -7510.963, mean reward: -7510.963 [-7510.963, -7510.963], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   9/1000: episode: 9, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1772.853, mean reward: -1772.853 [-1772.853, -1772.853], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  10/1000: episode: 10, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -565.241, mean reward: -565.241 [-565.241, -565.241], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  11/1000: episode: 11, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -6306.659, mean reward: -6306.659 [-6306.659, -6306.659], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  12/1000: episode: 12, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2306.231, mean reward: -2306.231 [-2306.231, -2306.231], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  13/1000: episode: 13, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4925.129, mean reward: -4925.129 [-4925.129, -4925.129], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  14/1000: episode: 14, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8817.268, mean reward: -8817.268 [-8817.268, -8817.268], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  15/1000: episode: 15, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6434.518, mean reward: -6434.518 [-6434.518, -6434.518], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  16/1000: episode: 16, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2617.041, mean reward: -2617.041 [-2617.041, -2617.041], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  17/1000: episode: 17, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7827.598, mean reward: -7827.598 [-7827.598, -7827.598], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  18/1000: episode: 18, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -8646.266, mean reward: -8646.266 [-8646.266, -8646.266], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  19/1000: episode: 19, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1795.694, mean reward: -1795.694 [-1795.694, -1795.694], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  20/1000: episode: 20, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6983.098, mean reward: -6983.098 [-6983.098, -6983.098], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  21/1000: episode: 21, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6363.984, mean reward: -6363.984 [-6363.984, -6363.984], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  22/1000: episode: 22, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3268.434, mean reward: -3268.434 [-3268.434, -3268.434], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  23/1000: episode: 23, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -963.486, mean reward: -963.486 [-963.486, -963.486], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  24/1000: episode: 24, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1433.424, mean reward: -1433.424 [-1433.424, -1433.424], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  25/1000: episode: 25, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -11599.027, mean reward: -11599.027 [-11599.027, -11599.027], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  26/1000: episode: 26, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4711.049, mean reward: -4711.049 [-4711.049, -4711.049], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  27/1000: episode: 27, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -7334.281, mean reward: -7334.281 [-7334.281, -7334.281], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  28/1000: episode: 28, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -382.911, mean reward: -382.911 [-382.911, -382.911], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  29/1000: episode: 29, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2305.940, mean reward: -2305.940 [-2305.940, -2305.940], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  30/1000: episode: 30, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3615.627, mean reward: -3615.627 [-3615.627, -3615.627], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  31/1000: episode: 31, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -605.596, mean reward: -605.596 [-605.596, -605.596], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  32/1000: episode: 32, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2660.556, mean reward: -2660.556 [-2660.556, -2660.556], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  33/1000: episode: 33, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1966.083, mean reward: -1966.083 [-1966.083, -1966.083], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  34/1000: episode: 34, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2300.984, mean reward: -2300.984 [-2300.984, -2300.984], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  35/1000: episode: 35, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3385.628, mean reward: -3385.628 [-3385.628, -3385.628], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  36/1000: episode: 36, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -10311.377, mean reward: -10311.377 [-10311.377, -10311.377], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  37/1000: episode: 37, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4526.065, mean reward: -4526.065 [-4526.065, -4526.065], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  38/1000: episode: 38, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1335.180, mean reward: -1335.180 [-1335.180, -1335.180], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  39/1000: episode: 39, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -7970.384, mean reward: -7970.384 [-7970.384, -7970.384], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  40/1000: episode: 40, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -8729.855, mean reward: -8729.855 [-8729.855, -8729.855], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  41/1000: episode: 41, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5323.873, mean reward: -5323.873 [-5323.873, -5323.873], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  42/1000: episode: 42, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6959.636, mean reward: -6959.636 [-6959.636, -6959.636], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  43/1000: episode: 43, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -12089.857, mean reward: -12089.857 [-12089.857, -12089.857], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  44/1000: episode: 44, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1928.390, mean reward: -1928.390 [-1928.390, -1928.390], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  45/1000: episode: 45, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -6359.041, mean reward: -6359.041 [-6359.041, -6359.041], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  46/1000: episode: 46, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -309.518, mean reward: -309.518 [-309.518, -309.518], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  47/1000: episode: 47, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -10251.295, mean reward: -10251.295 [-10251.295, -10251.295], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  48/1000: episode: 48, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4952.478, mean reward: -4952.478 [-4952.478, -4952.478], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  49/1000: episode: 49, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3896.765, mean reward: -3896.765 [-3896.765, -3896.765], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  50/1000: episode: 50, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3655.174, mean reward: -3655.174 [-3655.174, -3655.174], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  51/1000: episode: 51, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -644.170, mean reward: -644.170 [-644.170, -644.170], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  52/1000: episode: 52, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1954.861, mean reward: -1954.861 [-1954.861, -1954.861], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  53/1000: episode: 53, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2172.766, mean reward: -2172.766 [-2172.766, -2172.766], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  54/1000: episode: 54, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2407.401, mean reward: -2407.401 [-2407.401, -2407.401], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  55/1000: episode: 55, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -676.546, mean reward: -676.546 [-676.546, -676.546], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  56/1000: episode: 56, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -904.624, mean reward: -904.624 [-904.624, -904.624], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  57/1000: episode: 57, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2557.146, mean reward: -2557.146 [-2557.146, -2557.146], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  58/1000: episode: 58, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -550.112, mean reward: -550.112 [-550.112, -550.112], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  59/1000: episode: 59, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6554.543, mean reward: -6554.543 [-6554.543, -6554.543], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  60/1000: episode: 60, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2697.955, mean reward: -2697.955 [-2697.955, -2697.955], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  61/1000: episode: 61, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2162.974, mean reward: -2162.974 [-2162.974, -2162.974], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  62/1000: episode: 62, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4619.223, mean reward: -4619.223 [-4619.223, -4619.223], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  63/1000: episode: 63, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1169.229, mean reward: -1169.229 [-1169.229, -1169.229], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  64/1000: episode: 64, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -12338.257, mean reward: -12338.257 [-12338.257, -12338.257], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  65/1000: episode: 65, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -10484.998, mean reward: -10484.998 [-10484.998, -10484.998], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  66/1000: episode: 66, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -2239.708, mean reward: -2239.708 [-2239.708, -2239.708], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  67/1000: episode: 67, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3627.353, mean reward: -3627.353 [-3627.353, -3627.353], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  68/1000: episode: 68, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5003.735, mean reward: -5003.735 [-5003.735, -5003.735], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  69/1000: episode: 69, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -1029.383, mean reward: -1029.383 [-1029.383, -1029.383], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  70/1000: episode: 70, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4918.625, mean reward: -4918.625 [-4918.625, -4918.625], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  71/1000: episode: 71, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4380.819, mean reward: -4380.819 [-4380.819, -4380.819], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  72/1000: episode: 72, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -6665.550, mean reward: -6665.550 [-6665.550, -6665.550], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  73/1000: episode: 73, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5461.540, mean reward: -5461.540 [-5461.540, -5461.540], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  74/1000: episode: 74, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4152.938, mean reward: -4152.938 [-4152.938, -4152.938], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  75/1000: episode: 75, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2181.264, mean reward: -2181.264 [-2181.264, -2181.264], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  76/1000: episode: 76, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -3621.043, mean reward: -3621.043 [-3621.043, -3621.043], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  77/1000: episode: 77, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5385.598, mean reward: -5385.598 [-5385.598, -5385.598], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  78/1000: episode: 78, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5304.932, mean reward: -5304.932 [-5304.932, -5304.932], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  79/1000: episode: 79, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -416.976, mean reward: -416.976 [-416.976, -416.976], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  80/1000: episode: 80, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4531.992, mean reward: -4531.992 [-4531.992, -4531.992], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  81/1000: episode: 81, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -20.058, mean reward: -20.058 [-20.058, -20.058], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  82/1000: episode: 82, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1700.937, mean reward: -1700.937 [-1700.937, -1700.937], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  83/1000: episode: 83, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2720.562, mean reward: -2720.562 [-2720.562, -2720.562], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  84/1000: episode: 84, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -10669.157, mean reward: -10669.157 [-10669.157, -10669.157], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  85/1000: episode: 85, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4370.928, mean reward: -4370.928 [-4370.928, -4370.928], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  86/1000: episode: 86, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -4053.911, mean reward: -4053.911 [-4053.911, -4053.911], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  87/1000: episode: 87, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3747.653, mean reward: -3747.653 [-3747.653, -3747.653], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  88/1000: episode: 88, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6239.728, mean reward: -6239.728 [-6239.728, -6239.728], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  89/1000: episode: 89, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6639.687, mean reward: -6639.687 [-6639.687, -6639.687], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  90/1000: episode: 90, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6048.698, mean reward: -6048.698 [-6048.698, -6048.698], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  91/1000: episode: 91, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -6804.956, mean reward: -6804.956 [-6804.956, -6804.956], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  92/1000: episode: 92, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3050.556, mean reward: -3050.556 [-3050.556, -3050.556], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  93/1000: episode: 93, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -13376.206, mean reward: -13376.206 [-13376.206, -13376.206], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  94/1000: episode: 94, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5176.016, mean reward: -5176.016 [-5176.016, -5176.016], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  95/1000: episode: 95, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1772.350, mean reward: -1772.350 [-1772.350, -1772.350], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  96/1000: episode: 96, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -497.017, mean reward: -497.017 [-497.017, -497.017], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  97/1000: episode: 97, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -265.015, mean reward: -265.015 [-265.015, -265.015], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  98/1000: episode: 98, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1343.459, mean reward: -1343.459 [-1343.459, -1343.459], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  99/1000: episode: 99, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2898.915, mean reward: -2898.915 [-2898.915, -2898.915], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 100/1000: episode: 100, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -13446.639, mean reward: -13446.639 [-13446.639, -13446.639], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 101/1000: episode: 101, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -236.861, mean reward: -236.861 [-236.861, -236.861], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 102/1000: episode: 102, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4920.801, mean reward: -4920.801 [-4920.801, -4920.801], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 103/1000: episode: 103, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3696.492, mean reward: -3696.492 [-3696.492, -3696.492], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 104/1000: episode: 104, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5376.248, mean reward: -5376.248 [-5376.248, -5376.248], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 105/1000: episode: 105, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -312.784, mean reward: -312.784 [-312.784, -312.784], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 106/1000: episode: 106, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -8274.288, mean reward: -8274.288 [-8274.288, -8274.288], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 107/1000: episode: 107, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4201.031, mean reward: -4201.031 [-4201.031, -4201.031], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 108/1000: episode: 108, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -9013.926, mean reward: -9013.926 [-9013.926, -9013.926], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 109/1000: episode: 109, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3249.945, mean reward: -3249.945 [-3249.945, -3249.945], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 110/1000: episode: 110, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -150.465, mean reward: -150.465 [-150.465, -150.465], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 111/1000: episode: 111, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4966.781, mean reward: -4966.781 [-4966.781, -4966.781], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 112/1000: episode: 112, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5488.029, mean reward: -5488.029 [-5488.029, -5488.029], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 113/1000: episode: 113, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3054.382, mean reward: -3054.382 [-3054.382, -3054.382], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 114/1000: episode: 114, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2150.519, mean reward: -2150.519 [-2150.519, -2150.519], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 115/1000: episode: 115, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3480.674, mean reward: -3480.674 [-3480.674, -3480.674], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 116/1000: episode: 116, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5523.740, mean reward: -5523.740 [-5523.740, -5523.740], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 117/1000: episode: 117, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3710.565, mean reward: -3710.565 [-3710.565, -3710.565], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 118/1000: episode: 118, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -10901.050, mean reward: -10901.050 [-10901.050, -10901.050], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 119/1000: episode: 119, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1675.530, mean reward: -1675.530 [-1675.530, -1675.530], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 120/1000: episode: 120, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6734.378, mean reward: -6734.378 [-6734.378, -6734.378], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 121/1000: episode: 121, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3876.324, mean reward: -3876.324 [-3876.324, -3876.324], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 122/1000: episode: 122, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7320.081, mean reward: -7320.081 [-7320.081, -7320.081], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 123/1000: episode: 123, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1445.477, mean reward: -1445.477 [-1445.477, -1445.477], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 124/1000: episode: 124, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -4097.122, mean reward: -4097.122 [-4097.122, -4097.122], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 125/1000: episode: 125, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3095.507, mean reward: -3095.507 [-3095.507, -3095.507], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 126/1000: episode: 126, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -8000.318, mean reward: -8000.318 [-8000.318, -8000.318], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 127/1000: episode: 127, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -467.795, mean reward: -467.795 [-467.795, -467.795], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 128/1000: episode: 128, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -201.850, mean reward: -201.850 [-201.850, -201.850], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 129/1000: episode: 129, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7000.948, mean reward: -7000.948 [-7000.948, -7000.948], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 130/1000: episode: 130, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2748.368, mean reward: -2748.368 [-2748.368, -2748.368], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 131/1000: episode: 131, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4754.324, mean reward: -4754.324 [-4754.324, -4754.324], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 132/1000: episode: 132, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9615.371, mean reward: -9615.371 [-9615.371, -9615.371], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 133/1000: episode: 133, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5235.463, mean reward: -5235.463 [-5235.463, -5235.463], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 134/1000: episode: 134, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2104.890, mean reward: -2104.890 [-2104.890, -2104.890], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 135/1000: episode: 135, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3150.169, mean reward: -3150.169 [-3150.169, -3150.169], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 136/1000: episode: 136, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3717.314, mean reward: -3717.314 [-3717.314, -3717.314], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 137/1000: episode: 137, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2787.318, mean reward: -2787.318 [-2787.318, -2787.318], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 138/1000: episode: 138, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1677.759, mean reward: -1677.759 [-1677.759, -1677.759], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 139/1000: episode: 139, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7263.363, mean reward: -7263.363 [-7263.363, -7263.363], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 140/1000: episode: 140, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5255.028, mean reward: -5255.028 [-5255.028, -5255.028], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 141/1000: episode: 141, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -201.734, mean reward: -201.734 [-201.734, -201.734], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 142/1000: episode: 142, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -10071.989, mean reward: -10071.989 [-10071.989, -10071.989], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 143/1000: episode: 143, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3185.228, mean reward: -3185.228 [-3185.228, -3185.228], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 144/1000: episode: 144, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2449.179, mean reward: -2449.179 [-2449.179, -2449.179], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 145/1000: episode: 145, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4058.106, mean reward: -4058.106 [-4058.106, -4058.106], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 146/1000: episode: 146, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -8735.273, mean reward: -8735.273 [-8735.273, -8735.273], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 147/1000: episode: 147, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1815.558, mean reward: -1815.558 [-1815.558, -1815.558], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 148/1000: episode: 148, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5956.258, mean reward: -5956.258 [-5956.258, -5956.258], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 149/1000: episode: 149, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2139.596, mean reward: -2139.596 [-2139.596, -2139.596], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 150/1000: episode: 150, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6803.364, mean reward: -6803.364 [-6803.364, -6803.364], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 151/1000: episode: 151, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3401.038, mean reward: -3401.038 [-3401.038, -3401.038], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 152/1000: episode: 152, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5345.377, mean reward: -5345.377 [-5345.377, -5345.377], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 153/1000: episode: 153, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -696.111, mean reward: -696.111 [-696.111, -696.111], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 154/1000: episode: 154, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -17870.103, mean reward: -17870.103 [-17870.103, -17870.103], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 155/1000: episode: 155, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -2195.171, mean reward: -2195.171 [-2195.171, -2195.171], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 156/1000: episode: 156, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -90.679, mean reward: -90.679 [-90.679, -90.679], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 157/1000: episode: 157, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4184.905, mean reward: -4184.905 [-4184.905, -4184.905], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 158/1000: episode: 158, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3148.129, mean reward: -3148.129 [-3148.129, -3148.129], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 159/1000: episode: 159, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1361.561, mean reward: -1361.561 [-1361.561, -1361.561], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 160/1000: episode: 160, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6788.788, mean reward: -6788.788 [-6788.788, -6788.788], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 161/1000: episode: 161, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -3603.062, mean reward: -3603.062 [-3603.062, -3603.062], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 162/1000: episode: 162, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3940.671, mean reward: -3940.671 [-3940.671, -3940.671], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 163/1000: episode: 163, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2273.584, mean reward: -2273.584 [-2273.584, -2273.584], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 164/1000: episode: 164, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -894.236, mean reward: -894.236 [-894.236, -894.236], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 165/1000: episode: 165, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1675.985, mean reward: -1675.985 [-1675.985, -1675.985], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 166/1000: episode: 166, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4795.342, mean reward: -4795.342 [-4795.342, -4795.342], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 167/1000: episode: 167, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -7105.292, mean reward: -7105.292 [-7105.292, -7105.292], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 168/1000: episode: 168, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -233.567, mean reward: -233.567 [-233.567, -233.567], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 169/1000: episode: 169, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4650.494, mean reward: -4650.494 [-4650.494, -4650.494], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 170/1000: episode: 170, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1668.982, mean reward: -1668.982 [-1668.982, -1668.982], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 171/1000: episode: 171, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4967.581, mean reward: -4967.581 [-4967.581, -4967.581], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 172/1000: episode: 172, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -46.994, mean reward: -46.994 [-46.994, -46.994], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 173/1000: episode: 173, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -1550.046, mean reward: -1550.046 [-1550.046, -1550.046], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 174/1000: episode: 174, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -7509.535, mean reward: -7509.535 [-7509.535, -7509.535], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 175/1000: episode: 175, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2336.930, mean reward: -2336.930 [-2336.930, -2336.930], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 176/1000: episode: 176, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2635.478, mean reward: -2635.478 [-2635.478, -2635.478], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 177/1000: episode: 177, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4035.013, mean reward: -4035.013 [-4035.013, -4035.013], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 178/1000: episode: 178, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1732.935, mean reward: -1732.935 [-1732.935, -1732.935], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 179/1000: episode: 179, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -6140.321, mean reward: -6140.321 [-6140.321, -6140.321], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 180/1000: episode: 180, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -9519.937, mean reward: -9519.937 [-9519.937, -9519.937], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 181/1000: episode: 181, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4175.381, mean reward: -4175.381 [-4175.381, -4175.381], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 182/1000: episode: 182, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2211.124, mean reward: -2211.124 [-2211.124, -2211.124], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 183/1000: episode: 183, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3484.434, mean reward: -3484.434 [-3484.434, -3484.434], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 184/1000: episode: 184, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2357.680, mean reward: -2357.680 [-2357.680, -2357.680], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 185/1000: episode: 185, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2662.808, mean reward: -2662.808 [-2662.808, -2662.808], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 186/1000: episode: 186, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -514.651, mean reward: -514.651 [-514.651, -514.651], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 187/1000: episode: 187, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2246.441, mean reward: -2246.441 [-2246.441, -2246.441], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 188/1000: episode: 188, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4401.147, mean reward: -4401.147 [-4401.147, -4401.147], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 189/1000: episode: 189, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4671.796, mean reward: -4671.796 [-4671.796, -4671.796], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 190/1000: episode: 190, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7722.006, mean reward: -7722.006 [-7722.006, -7722.006], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 191/1000: episode: 191, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3268.943, mean reward: -3268.943 [-3268.943, -3268.943], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 192/1000: episode: 192, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6570.432, mean reward: -6570.432 [-6570.432, -6570.432], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 193/1000: episode: 193, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -3917.762, mean reward: -3917.762 [-3917.762, -3917.762], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 194/1000: episode: 194, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8565.934, mean reward: -8565.934 [-8565.934, -8565.934], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 195/1000: episode: 195, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -691.106, mean reward: -691.106 [-691.106, -691.106], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 196/1000: episode: 196, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3450.013, mean reward: -3450.013 [-3450.013, -3450.013], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 197/1000: episode: 197, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2733.380, mean reward: -2733.380 [-2733.380, -2733.380], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 198/1000: episode: 198, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -9901.679, mean reward: -9901.679 [-9901.679, -9901.679], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 199/1000: episode: 199, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -323.289, mean reward: -323.289 [-323.289, -323.289], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 200/1000: episode: 200, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -7020.527, mean reward: -7020.527 [-7020.527, -7020.527], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 201/1000: episode: 201, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -8734.043, mean reward: -8734.043 [-8734.043, -8734.043], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 202/1000: episode: 202, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -2909.207, mean reward: -2909.207 [-2909.207, -2909.207], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 203/1000: episode: 203, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6314.541, mean reward: -6314.541 [-6314.541, -6314.541], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 204/1000: episode: 204, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2904.515, mean reward: -2904.515 [-2904.515, -2904.515], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 205/1000: episode: 205, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1021.588, mean reward: -1021.588 [-1021.588, -1021.588], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 206/1000: episode: 206, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4163.273, mean reward: -4163.273 [-4163.273, -4163.273], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 207/1000: episode: 207, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7406.696, mean reward: -7406.696 [-7406.696, -7406.696], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 208/1000: episode: 208, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4323.896, mean reward: -4323.896 [-4323.896, -4323.896], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 209/1000: episode: 209, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3103.083, mean reward: -3103.083 [-3103.083, -3103.083], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 210/1000: episode: 210, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -7045.021, mean reward: -7045.021 [-7045.021, -7045.021], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 211/1000: episode: 211, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7477.726, mean reward: -7477.726 [-7477.726, -7477.726], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 212/1000: episode: 212, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -4578.309, mean reward: -4578.309 [-4578.309, -4578.309], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 213/1000: episode: 213, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3067.632, mean reward: -3067.632 [-3067.632, -3067.632], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 214/1000: episode: 214, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -3081.186, mean reward: -3081.186 [-3081.186, -3081.186], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 215/1000: episode: 215, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -1106.974, mean reward: -1106.974 [-1106.974, -1106.974], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 216/1000: episode: 216, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -1165.014, mean reward: -1165.014 [-1165.014, -1165.014], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 217/1000: episode: 217, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -9339.234, mean reward: -9339.234 [-9339.234, -9339.234], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 218/1000: episode: 218, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8818.847, mean reward: -8818.847 [-8818.847, -8818.847], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 219/1000: episode: 219, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4856.689, mean reward: -4856.689 [-4856.689, -4856.689], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 220/1000: episode: 220, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -10635.761, mean reward: -10635.761 [-10635.761, -10635.761], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 221/1000: episode: 221, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4773.724, mean reward: -4773.724 [-4773.724, -4773.724], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 222/1000: episode: 222, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9924.515, mean reward: -9924.515 [-9924.515, -9924.515], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 223/1000: episode: 223, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2461.214, mean reward: -2461.214 [-2461.214, -2461.214], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 224/1000: episode: 224, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7342.208, mean reward: -7342.208 [-7342.208, -7342.208], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 225/1000: episode: 225, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -8760.889, mean reward: -8760.889 [-8760.889, -8760.889], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 226/1000: episode: 226, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -993.588, mean reward: -993.588 [-993.588, -993.588], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 227/1000: episode: 227, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5772.277, mean reward: -5772.277 [-5772.277, -5772.277], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 228/1000: episode: 228, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6587.563, mean reward: -6587.563 [-6587.563, -6587.563], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 229/1000: episode: 229, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -16620.767, mean reward: -16620.767 [-16620.767, -16620.767], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 230/1000: episode: 230, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -787.945, mean reward: -787.945 [-787.945, -787.945], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 231/1000: episode: 231, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1498.925, mean reward: -1498.925 [-1498.925, -1498.925], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 232/1000: episode: 232, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4803.080, mean reward: -4803.080 [-4803.080, -4803.080], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 233/1000: episode: 233, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -2255.724, mean reward: -2255.724 [-2255.724, -2255.724], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 234/1000: episode: 234, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -7566.968, mean reward: -7566.968 [-7566.968, -7566.968], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 235/1000: episode: 235, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -11720.947, mean reward: -11720.947 [-11720.947, -11720.947], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 236/1000: episode: 236, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -4215.055, mean reward: -4215.055 [-4215.055, -4215.055], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 237/1000: episode: 237, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1725.210, mean reward: -1725.210 [-1725.210, -1725.210], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 238/1000: episode: 238, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -8938.795, mean reward: -8938.795 [-8938.795, -8938.795], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 239/1000: episode: 239, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1988.808, mean reward: -1988.808 [-1988.808, -1988.808], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 240/1000: episode: 240, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -8722.981, mean reward: -8722.981 [-8722.981, -8722.981], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 241/1000: episode: 241, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4840.571, mean reward: -4840.571 [-4840.571, -4840.571], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 242/1000: episode: 242, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -747.076, mean reward: -747.076 [-747.076, -747.076], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 243/1000: episode: 243, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -12755.773, mean reward: -12755.773 [-12755.773, -12755.773], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 244/1000: episode: 244, duration: 0.047s, episode steps:   1, steps per second:  22, episode reward: -87.294, mean reward: -87.294 [-87.294, -87.294], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 245/1000: episode: 245, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4240.594, mean reward: -4240.594 [-4240.594, -4240.594], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 246/1000: episode: 246, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7792.113, mean reward: -7792.113 [-7792.113, -7792.113], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 247/1000: episode: 247, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7867.556, mean reward: -7867.556 [-7867.556, -7867.556], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 248/1000: episode: 248, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6902.871, mean reward: -6902.871 [-6902.871, -6902.871], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 249/1000: episode: 249, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -3691.269, mean reward: -3691.269 [-3691.269, -3691.269], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 250/1000: episode: 250, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -8828.154, mean reward: -8828.154 [-8828.154, -8828.154], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 251/1000: episode: 251, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4285.049, mean reward: -4285.049 [-4285.049, -4285.049], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 252/1000: episode: 252, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -955.632, mean reward: -955.632 [-955.632, -955.632], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 253/1000: episode: 253, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5741.163, mean reward: -5741.163 [-5741.163, -5741.163], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 254/1000: episode: 254, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3230.408, mean reward: -3230.408 [-3230.408, -3230.408], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 255/1000: episode: 255, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -8156.965, mean reward: -8156.965 [-8156.965, -8156.965], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 256/1000: episode: 256, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -978.950, mean reward: -978.950 [-978.950, -978.950], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 257/1000: episode: 257, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4304.142, mean reward: -4304.142 [-4304.142, -4304.142], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 258/1000: episode: 258, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -325.142, mean reward: -325.142 [-325.142, -325.142], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 259/1000: episode: 259, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1286.588, mean reward: -1286.588 [-1286.588, -1286.588], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 260/1000: episode: 260, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7892.163, mean reward: -7892.163 [-7892.163, -7892.163], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 261/1000: episode: 261, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8767.178, mean reward: -8767.178 [-8767.178, -8767.178], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 262/1000: episode: 262, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3186.642, mean reward: -3186.642 [-3186.642, -3186.642], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 263/1000: episode: 263, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4766.143, mean reward: -4766.143 [-4766.143, -4766.143], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 264/1000: episode: 264, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4952.221, mean reward: -4952.221 [-4952.221, -4952.221], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 265/1000: episode: 265, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1419.766, mean reward: -1419.766 [-1419.766, -1419.766], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 266/1000: episode: 266, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -2367.096, mean reward: -2367.096 [-2367.096, -2367.096], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 267/1000: episode: 267, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5866.941, mean reward: -5866.941 [-5866.941, -5866.941], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 268/1000: episode: 268, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2793.998, mean reward: -2793.998 [-2793.998, -2793.998], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 269/1000: episode: 269, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -10239.284, mean reward: -10239.284 [-10239.284, -10239.284], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 270/1000: episode: 270, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -8926.630, mean reward: -8926.630 [-8926.630, -8926.630], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 271/1000: episode: 271, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1799.908, mean reward: -1799.908 [-1799.908, -1799.908], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 272/1000: episode: 272, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3872.651, mean reward: -3872.651 [-3872.651, -3872.651], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 273/1000: episode: 273, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3335.961, mean reward: -3335.961 [-3335.961, -3335.961], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 274/1000: episode: 274, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -897.679, mean reward: -897.679 [-897.679, -897.679], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 275/1000: episode: 275, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1234.727, mean reward: -1234.727 [-1234.727, -1234.727], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 276/1000: episode: 276, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2208.078, mean reward: -2208.078 [-2208.078, -2208.078], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 277/1000: episode: 277, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -2387.643, mean reward: -2387.643 [-2387.643, -2387.643], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 278/1000: episode: 278, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5501.026, mean reward: -5501.026 [-5501.026, -5501.026], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 279/1000: episode: 279, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2526.027, mean reward: -2526.027 [-2526.027, -2526.027], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 280/1000: episode: 280, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -13767.391, mean reward: -13767.391 [-13767.391, -13767.391], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 281/1000: episode: 281, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4649.599, mean reward: -4649.599 [-4649.599, -4649.599], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 282/1000: episode: 282, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -7386.894, mean reward: -7386.894 [-7386.894, -7386.894], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 283/1000: episode: 283, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -950.180, mean reward: -950.180 [-950.180, -950.180], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 284/1000: episode: 284, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -12621.458, mean reward: -12621.458 [-12621.458, -12621.458], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 285/1000: episode: 285, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -212.220, mean reward: -212.220 [-212.220, -212.220], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 286/1000: episode: 286, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9091.355, mean reward: -9091.355 [-9091.355, -9091.355], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 287/1000: episode: 287, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -6959.900, mean reward: -6959.900 [-6959.900, -6959.900], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 288/1000: episode: 288, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -8154.510, mean reward: -8154.510 [-8154.510, -8154.510], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 289/1000: episode: 289, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5824.951, mean reward: -5824.951 [-5824.951, -5824.951], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 290/1000: episode: 290, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -6535.809, mean reward: -6535.809 [-6535.809, -6535.809], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 291/1000: episode: 291, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -7555.823, mean reward: -7555.823 [-7555.823, -7555.823], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 292/1000: episode: 292, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5119.459, mean reward: -5119.459 [-5119.459, -5119.459], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 293/1000: episode: 293, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3616.964, mean reward: -3616.964 [-3616.964, -3616.964], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 294/1000: episode: 294, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -4247.247, mean reward: -4247.247 [-4247.247, -4247.247], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 295/1000: episode: 295, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2289.801, mean reward: -2289.801 [-2289.801, -2289.801], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 296/1000: episode: 296, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -8536.521, mean reward: -8536.521 [-8536.521, -8536.521], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 297/1000: episode: 297, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2944.990, mean reward: -2944.990 [-2944.990, -2944.990], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 298/1000: episode: 298, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2963.544, mean reward: -2963.544 [-2963.544, -2963.544], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 299/1000: episode: 299, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -4413.491, mean reward: -4413.491 [-4413.491, -4413.491], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 300/1000: episode: 300, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2834.300, mean reward: -2834.300 [-2834.300, -2834.300], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 301/1000: episode: 301, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -13497.684, mean reward: -13497.684 [-13497.684, -13497.684], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 302/1000: episode: 302, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7745.094, mean reward: -7745.094 [-7745.094, -7745.094], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 303/1000: episode: 303, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -9284.342, mean reward: -9284.342 [-9284.342, -9284.342], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 304/1000: episode: 304, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4115.706, mean reward: -4115.706 [-4115.706, -4115.706], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 305/1000: episode: 305, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -9279.940, mean reward: -9279.940 [-9279.940, -9279.940], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 306/1000: episode: 306, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -580.259, mean reward: -580.259 [-580.259, -580.259], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 307/1000: episode: 307, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2588.844, mean reward: -2588.844 [-2588.844, -2588.844], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 308/1000: episode: 308, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -6359.776, mean reward: -6359.776 [-6359.776, -6359.776], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 309/1000: episode: 309, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6710.896, mean reward: -6710.896 [-6710.896, -6710.896], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 310/1000: episode: 310, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7325.730, mean reward: -7325.730 [-7325.730, -7325.730], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 311/1000: episode: 311, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -9468.599, mean reward: -9468.599 [-9468.599, -9468.599], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 312/1000: episode: 312, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -3175.182, mean reward: -3175.182 [-3175.182, -3175.182], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 313/1000: episode: 313, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2296.885, mean reward: -2296.885 [-2296.885, -2296.885], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 314/1000: episode: 314, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2505.329, mean reward: -2505.329 [-2505.329, -2505.329], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 315/1000: episode: 315, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4168.394, mean reward: -4168.394 [-4168.394, -4168.394], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 316/1000: episode: 316, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -9711.208, mean reward: -9711.208 [-9711.208, -9711.208], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 317/1000: episode: 317, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -333.401, mean reward: -333.401 [-333.401, -333.401], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 318/1000: episode: 318, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -8945.491, mean reward: -8945.491 [-8945.491, -8945.491], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 319/1000: episode: 319, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3151.469, mean reward: -3151.469 [-3151.469, -3151.469], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 320/1000: episode: 320, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4417.969, mean reward: -4417.969 [-4417.969, -4417.969], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 321/1000: episode: 321, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1265.161, mean reward: -1265.161 [-1265.161, -1265.161], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 322/1000: episode: 322, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3848.677, mean reward: -3848.677 [-3848.677, -3848.677], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 323/1000: episode: 323, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4212.211, mean reward: -4212.211 [-4212.211, -4212.211], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 324/1000: episode: 324, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -2720.025, mean reward: -2720.025 [-2720.025, -2720.025], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 325/1000: episode: 325, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -3461.900, mean reward: -3461.900 [-3461.900, -3461.900], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 326/1000: episode: 326, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -8236.211, mean reward: -8236.211 [-8236.211, -8236.211], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 327/1000: episode: 327, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -7510.658, mean reward: -7510.658 [-7510.658, -7510.658], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 328/1000: episode: 328, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -816.771, mean reward: -816.771 [-816.771, -816.771], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 329/1000: episode: 329, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -9941.786, mean reward: -9941.786 [-9941.786, -9941.786], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 330/1000: episode: 330, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -9492.190, mean reward: -9492.190 [-9492.190, -9492.190], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 331/1000: episode: 331, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1448.583, mean reward: -1448.583 [-1448.583, -1448.583], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 332/1000: episode: 332, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4743.436, mean reward: -4743.436 [-4743.436, -4743.436], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 333/1000: episode: 333, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -13631.827, mean reward: -13631.827 [-13631.827, -13631.827], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 334/1000: episode: 334, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2094.508, mean reward: -2094.508 [-2094.508, -2094.508], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 335/1000: episode: 335, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4196.489, mean reward: -4196.489 [-4196.489, -4196.489], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 336/1000: episode: 336, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4675.774, mean reward: -4675.774 [-4675.774, -4675.774], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 337/1000: episode: 337, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -11849.594, mean reward: -11849.594 [-11849.594, -11849.594], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 338/1000: episode: 338, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -10861.024, mean reward: -10861.024 [-10861.024, -10861.024], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 339/1000: episode: 339, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -9398.625, mean reward: -9398.625 [-9398.625, -9398.625], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 340/1000: episode: 340, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -1897.305, mean reward: -1897.305 [-1897.305, -1897.305], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 341/1000: episode: 341, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4689.002, mean reward: -4689.002 [-4689.002, -4689.002], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 342/1000: episode: 342, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1686.372, mean reward: -1686.372 [-1686.372, -1686.372], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 343/1000: episode: 343, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4997.060, mean reward: -4997.060 [-4997.060, -4997.060], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 344/1000: episode: 344, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -9081.295, mean reward: -9081.295 [-9081.295, -9081.295], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 345/1000: episode: 345, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -2661.598, mean reward: -2661.598 [-2661.598, -2661.598], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 346/1000: episode: 346, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -556.333, mean reward: -556.333 [-556.333, -556.333], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 347/1000: episode: 347, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -3140.098, mean reward: -3140.098 [-3140.098, -3140.098], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 348/1000: episode: 348, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8257.035, mean reward: -8257.035 [-8257.035, -8257.035], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 349/1000: episode: 349, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5345.636, mean reward: -5345.636 [-5345.636, -5345.636], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 350/1000: episode: 350, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1116.577, mean reward: -1116.577 [-1116.577, -1116.577], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 351/1000: episode: 351, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4590.839, mean reward: -4590.839 [-4590.839, -4590.839], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 352/1000: episode: 352, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -2134.062, mean reward: -2134.062 [-2134.062, -2134.062], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 353/1000: episode: 353, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -10810.125, mean reward: -10810.125 [-10810.125, -10810.125], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 354/1000: episode: 354, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3924.921, mean reward: -3924.921 [-3924.921, -3924.921], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 355/1000: episode: 355, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4717.996, mean reward: -4717.996 [-4717.996, -4717.996], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 356/1000: episode: 356, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1547.500, mean reward: -1547.500 [-1547.500, -1547.500], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 357/1000: episode: 357, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1987.396, mean reward: -1987.396 [-1987.396, -1987.396], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 358/1000: episode: 358, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1792.891, mean reward: -1792.891 [-1792.891, -1792.891], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 359/1000: episode: 359, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -3259.031, mean reward: -3259.031 [-3259.031, -3259.031], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 360/1000: episode: 360, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -6222.549, mean reward: -6222.549 [-6222.549, -6222.549], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 361/1000: episode: 361, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -4701.450, mean reward: -4701.450 [-4701.450, -4701.450], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 362/1000: episode: 362, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -2276.166, mean reward: -2276.166 [-2276.166, -2276.166], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 363/1000: episode: 363, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -10746.912, mean reward: -10746.912 [-10746.912, -10746.912], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 364/1000: episode: 364, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4883.078, mean reward: -4883.078 [-4883.078, -4883.078], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 365/1000: episode: 365, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6250.536, mean reward: -6250.536 [-6250.536, -6250.536], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 366/1000: episode: 366, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -307.373, mean reward: -307.373 [-307.373, -307.373], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 367/1000: episode: 367, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4126.350, mean reward: -4126.350 [-4126.350, -4126.350], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 368/1000: episode: 368, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -918.593, mean reward: -918.593 [-918.593, -918.593], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 369/1000: episode: 369, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -9554.177, mean reward: -9554.177 [-9554.177, -9554.177], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 370/1000: episode: 370, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -11350.452, mean reward: -11350.452 [-11350.452, -11350.452], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 371/1000: episode: 371, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4128.454, mean reward: -4128.454 [-4128.454, -4128.454], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 372/1000: episode: 372, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7047.359, mean reward: -7047.359 [-7047.359, -7047.359], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 373/1000: episode: 373, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7554.967, mean reward: -7554.967 [-7554.967, -7554.967], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 374/1000: episode: 374, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7202.757, mean reward: -7202.757 [-7202.757, -7202.757], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 375/1000: episode: 375, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -8884.443, mean reward: -8884.443 [-8884.443, -8884.443], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 376/1000: episode: 376, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5004.252, mean reward: -5004.252 [-5004.252, -5004.252], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 377/1000: episode: 377, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -543.488, mean reward: -543.488 [-543.488, -543.488], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 378/1000: episode: 378, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2681.876, mean reward: -2681.876 [-2681.876, -2681.876], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 379/1000: episode: 379, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -7884.753, mean reward: -7884.753 [-7884.753, -7884.753], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 380/1000: episode: 380, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -10604.593, mean reward: -10604.593 [-10604.593, -10604.593], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 381/1000: episode: 381, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -7058.830, mean reward: -7058.830 [-7058.830, -7058.830], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 382/1000: episode: 382, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6247.062, mean reward: -6247.062 [-6247.062, -6247.062], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 383/1000: episode: 383, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -189.995, mean reward: -189.995 [-189.995, -189.995], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 384/1000: episode: 384, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8705.035, mean reward: -8705.035 [-8705.035, -8705.035], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 385/1000: episode: 385, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3250.187, mean reward: -3250.187 [-3250.187, -3250.187], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 386/1000: episode: 386, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2088.826, mean reward: -2088.826 [-2088.826, -2088.826], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 387/1000: episode: 387, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3316.189, mean reward: -3316.189 [-3316.189, -3316.189], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 388/1000: episode: 388, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -8195.570, mean reward: -8195.570 [-8195.570, -8195.570], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 389/1000: episode: 389, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -4598.764, mean reward: -4598.764 [-4598.764, -4598.764], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 390/1000: episode: 390, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -2775.347, mean reward: -2775.347 [-2775.347, -2775.347], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 391/1000: episode: 391, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6604.729, mean reward: -6604.729 [-6604.729, -6604.729], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 392/1000: episode: 392, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4383.767, mean reward: -4383.767 [-4383.767, -4383.767], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 393/1000: episode: 393, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2428.299, mean reward: -2428.299 [-2428.299, -2428.299], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 394/1000: episode: 394, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3916.888, mean reward: -3916.888 [-3916.888, -3916.888], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 395/1000: episode: 395, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -9323.545, mean reward: -9323.545 [-9323.545, -9323.545], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 396/1000: episode: 396, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5181.529, mean reward: -5181.529 [-5181.529, -5181.529], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 397/1000: episode: 397, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3891.461, mean reward: -3891.461 [-3891.461, -3891.461], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 398/1000: episode: 398, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8330.681, mean reward: -8330.681 [-8330.681, -8330.681], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 399/1000: episode: 399, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6267.387, mean reward: -6267.387 [-6267.387, -6267.387], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 400/1000: episode: 400, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -12261.517, mean reward: -12261.517 [-12261.517, -12261.517], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 401/1000: episode: 401, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5064.301, mean reward: -5064.301 [-5064.301, -5064.301], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 402/1000: episode: 402, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3307.734, mean reward: -3307.734 [-3307.734, -3307.734], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 403/1000: episode: 403, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -7260.842, mean reward: -7260.842 [-7260.842, -7260.842], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 404/1000: episode: 404, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2912.367, mean reward: -2912.367 [-2912.367, -2912.367], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 405/1000: episode: 405, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -6049.551, mean reward: -6049.551 [-6049.551, -6049.551], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 406/1000: episode: 406, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -6342.677, mean reward: -6342.677 [-6342.677, -6342.677], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 407/1000: episode: 407, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -712.096, mean reward: -712.096 [-712.096, -712.096], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 408/1000: episode: 408, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5563.883, mean reward: -5563.883 [-5563.883, -5563.883], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 409/1000: episode: 409, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5571.412, mean reward: -5571.412 [-5571.412, -5571.412], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 410/1000: episode: 410, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -337.409, mean reward: -337.409 [-337.409, -337.409], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 411/1000: episode: 411, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -3550.668, mean reward: -3550.668 [-3550.668, -3550.668], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 412/1000: episode: 412, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -14457.643, mean reward: -14457.643 [-14457.643, -14457.643], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 413/1000: episode: 413, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6940.205, mean reward: -6940.205 [-6940.205, -6940.205], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 414/1000: episode: 414, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -120.550, mean reward: -120.550 [-120.550, -120.550], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 415/1000: episode: 415, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3379.216, mean reward: -3379.216 [-3379.216, -3379.216], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 416/1000: episode: 416, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3566.804, mean reward: -3566.804 [-3566.804, -3566.804], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 417/1000: episode: 417, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2173.561, mean reward: -2173.561 [-2173.561, -2173.561], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 418/1000: episode: 418, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3638.414, mean reward: -3638.414 [-3638.414, -3638.414], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 419/1000: episode: 419, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -9908.110, mean reward: -9908.110 [-9908.110, -9908.110], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 420/1000: episode: 420, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4403.227, mean reward: -4403.227 [-4403.227, -4403.227], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 421/1000: episode: 421, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1674.736, mean reward: -1674.736 [-1674.736, -1674.736], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 422/1000: episode: 422, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2815.888, mean reward: -2815.888 [-2815.888, -2815.888], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 423/1000: episode: 423, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5194.517, mean reward: -5194.517 [-5194.517, -5194.517], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 424/1000: episode: 424, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -11544.404, mean reward: -11544.404 [-11544.404, -11544.404], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 425/1000: episode: 425, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -3326.015, mean reward: -3326.015 [-3326.015, -3326.015], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 426/1000: episode: 426, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -6816.126, mean reward: -6816.126 [-6816.126, -6816.126], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 427/1000: episode: 427, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -3533.162, mean reward: -3533.162 [-3533.162, -3533.162], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 428/1000: episode: 428, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2720.287, mean reward: -2720.287 [-2720.287, -2720.287], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 429/1000: episode: 429, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7221.957, mean reward: -7221.957 [-7221.957, -7221.957], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 430/1000: episode: 430, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7156.582, mean reward: -7156.582 [-7156.582, -7156.582], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 431/1000: episode: 431, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -9790.954, mean reward: -9790.954 [-9790.954, -9790.954], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 432/1000: episode: 432, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1661.077, mean reward: -1661.077 [-1661.077, -1661.077], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 433/1000: episode: 433, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5666.788, mean reward: -5666.788 [-5666.788, -5666.788], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 434/1000: episode: 434, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3156.605, mean reward: -3156.605 [-3156.605, -3156.605], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 435/1000: episode: 435, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -4498.331, mean reward: -4498.331 [-4498.331, -4498.331], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 436/1000: episode: 436, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -6240.164, mean reward: -6240.164 [-6240.164, -6240.164], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 437/1000: episode: 437, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -1467.317, mean reward: -1467.317 [-1467.317, -1467.317], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 438/1000: episode: 438, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1346.250, mean reward: -1346.250 [-1346.250, -1346.250], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 439/1000: episode: 439, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7774.712, mean reward: -7774.712 [-7774.712, -7774.712], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 440/1000: episode: 440, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -11035.010, mean reward: -11035.010 [-11035.010, -11035.010], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 441/1000: episode: 441, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7953.476, mean reward: -7953.476 [-7953.476, -7953.476], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 442/1000: episode: 442, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -6443.212, mean reward: -6443.212 [-6443.212, -6443.212], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 443/1000: episode: 443, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1724.249, mean reward: -1724.249 [-1724.249, -1724.249], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 444/1000: episode: 444, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -2180.040, mean reward: -2180.040 [-2180.040, -2180.040], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 445/1000: episode: 445, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -16006.442, mean reward: -16006.442 [-16006.442, -16006.442], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 446/1000: episode: 446, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -7077.379, mean reward: -7077.379 [-7077.379, -7077.379], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 447/1000: episode: 447, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5682.848, mean reward: -5682.848 [-5682.848, -5682.848], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 448/1000: episode: 448, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5550.158, mean reward: -5550.158 [-5550.158, -5550.158], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 449/1000: episode: 449, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -12735.509, mean reward: -12735.509 [-12735.509, -12735.509], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 450/1000: episode: 450, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -293.495, mean reward: -293.495 [-293.495, -293.495], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 451/1000: episode: 451, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -3071.449, mean reward: -3071.449 [-3071.449, -3071.449], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 452/1000: episode: 452, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7256.012, mean reward: -7256.012 [-7256.012, -7256.012], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 453/1000: episode: 453, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -3801.620, mean reward: -3801.620 [-3801.620, -3801.620], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 454/1000: episode: 454, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -8439.462, mean reward: -8439.462 [-8439.462, -8439.462], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 455/1000: episode: 455, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -9617.856, mean reward: -9617.856 [-9617.856, -9617.856], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 456/1000: episode: 456, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -6133.397, mean reward: -6133.397 [-6133.397, -6133.397], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 457/1000: episode: 457, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3572.044, mean reward: -3572.044 [-3572.044, -3572.044], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 458/1000: episode: 458, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2513.511, mean reward: -2513.511 [-2513.511, -2513.511], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 459/1000: episode: 459, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4357.796, mean reward: -4357.796 [-4357.796, -4357.796], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 460/1000: episode: 460, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -1441.828, mean reward: -1441.828 [-1441.828, -1441.828], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 461/1000: episode: 461, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1149.343, mean reward: -1149.343 [-1149.343, -1149.343], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 462/1000: episode: 462, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -1902.957, mean reward: -1902.957 [-1902.957, -1902.957], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 463/1000: episode: 463, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1938.277, mean reward: -1938.277 [-1938.277, -1938.277], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 464/1000: episode: 464, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5240.507, mean reward: -5240.507 [-5240.507, -5240.507], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 465/1000: episode: 465, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -1670.674, mean reward: -1670.674 [-1670.674, -1670.674], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 466/1000: episode: 466, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -6533.861, mean reward: -6533.861 [-6533.861, -6533.861], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 467/1000: episode: 467, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1197.274, mean reward: -1197.274 [-1197.274, -1197.274], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 468/1000: episode: 468, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -2030.578, mean reward: -2030.578 [-2030.578, -2030.578], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 469/1000: episode: 469, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1808.364, mean reward: -1808.364 [-1808.364, -1808.364], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 470/1000: episode: 470, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -822.650, mean reward: -822.650 [-822.650, -822.650], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 471/1000: episode: 471, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7248.755, mean reward: -7248.755 [-7248.755, -7248.755], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 472/1000: episode: 472, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -3087.973, mean reward: -3087.973 [-3087.973, -3087.973], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 473/1000: episode: 473, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -11494.537, mean reward: -11494.537 [-11494.537, -11494.537], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 474/1000: episode: 474, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -10852.154, mean reward: -10852.154 [-10852.154, -10852.154], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 475/1000: episode: 475, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -8460.851, mean reward: -8460.851 [-8460.851, -8460.851], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 476/1000: episode: 476, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -3105.132, mean reward: -3105.132 [-3105.132, -3105.132], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 477/1000: episode: 477, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3015.121, mean reward: -3015.121 [-3015.121, -3015.121], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 478/1000: episode: 478, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -2709.866, mean reward: -2709.866 [-2709.866, -2709.866], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 479/1000: episode: 479, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -13457.677, mean reward: -13457.677 [-13457.677, -13457.677], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 480/1000: episode: 480, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -12598.814, mean reward: -12598.814 [-12598.814, -12598.814], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 481/1000: episode: 481, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -190.972, mean reward: -190.972 [-190.972, -190.972], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 482/1000: episode: 482, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3637.993, mean reward: -3637.993 [-3637.993, -3637.993], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 483/1000: episode: 483, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4062.450, mean reward: -4062.450 [-4062.450, -4062.450], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 484/1000: episode: 484, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -11335.328, mean reward: -11335.328 [-11335.328, -11335.328], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 485/1000: episode: 485, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7466.469, mean reward: -7466.469 [-7466.469, -7466.469], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 486/1000: episode: 486, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -7.264, mean reward: -7.264 [-7.264, -7.264], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 487/1000: episode: 487, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5982.935, mean reward: -5982.935 [-5982.935, -5982.935], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 488/1000: episode: 488, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -1555.025, mean reward: -1555.025 [-1555.025, -1555.025], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 489/1000: episode: 489, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1753.760, mean reward: -1753.760 [-1753.760, -1753.760], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 490/1000: episode: 490, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5114.326, mean reward: -5114.326 [-5114.326, -5114.326], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 491/1000: episode: 491, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -72.524, mean reward: -72.524 [-72.524, -72.524], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 492/1000: episode: 492, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2410.626, mean reward: -2410.626 [-2410.626, -2410.626], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 493/1000: episode: 493, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -265.765, mean reward: -265.765 [-265.765, -265.765], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 494/1000: episode: 494, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5235.680, mean reward: -5235.680 [-5235.680, -5235.680], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 495/1000: episode: 495, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -12385.495, mean reward: -12385.495 [-12385.495, -12385.495], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 496/1000: episode: 496, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -8202.934, mean reward: -8202.934 [-8202.934, -8202.934], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 497/1000: episode: 497, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -7006.087, mean reward: -7006.087 [-7006.087, -7006.087], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 498/1000: episode: 498, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -11061.709, mean reward: -11061.709 [-11061.709, -11061.709], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 499/1000: episode: 499, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2415.790, mean reward: -2415.790 [-2415.790, -2415.790], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 500/1000: episode: 500, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -7060.117, mean reward: -7060.117 [-7060.117, -7060.117], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 501/1000: episode: 501, duration: 0.531s, episode steps:   1, steps per second:   2, episode reward: -12656.726, mean reward: -12656.726 [-12656.726, -12656.726], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --, mean_eps: --
 502/1000: episode: 502, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5500.204, mean reward: -5500.204 [-5500.204, -5500.204], mean action: 0.000 [0.000, 0.000],  loss: 20345604.000000, mae: 1325.258179, mean_q: 0.810467, mean_eps: 0.504010
 503/1000: episode: 503, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -187.661, mean reward: -187.661 [-187.661, -187.661], mean action: 2.000 [2.000, 2.000],  loss: 18179062.000000, mae: 1223.916992, mean_q: 0.789456, mean_eps: 0.503020
 504/1000: episode: 504, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5976.019, mean reward: -5976.019 [-5976.019, -5976.019], mean action: 0.000 [0.000, 0.000],  loss: 18701712.000000, mae: 1307.961548, mean_q: 0.763896, mean_eps: 0.502030
 505/1000: episode: 505, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6749.274, mean reward: -6749.274 [-6749.274, -6749.274], mean action: 0.000 [0.000, 0.000],  loss: 28318824.000000, mae: 1551.171631, mean_q: 0.735088, mean_eps: 0.501040
 506/1000: episode: 506, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -3886.460, mean reward: -3886.460 [-3886.460, -3886.460], mean action: 1.000 [1.000, 1.000],  loss: 18298238.000000, mae: 1244.607178, mean_q: 0.712905, mean_eps: 0.500050
 507/1000: episode: 507, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5924.540, mean reward: -5924.540 [-5924.540, -5924.540], mean action: 0.000 [0.000, 0.000],  loss: 13226723.000000, mae: 1090.775391, mean_q: 0.688437, mean_eps: 0.499060
 508/1000: episode: 508, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6584.299, mean reward: -6584.299 [-6584.299, -6584.299], mean action: 0.000 [0.000, 0.000],  loss: 24781278.000000, mae: 1440.154907, mean_q: 0.659693, mean_eps: 0.498070
 509/1000: episode: 509, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5892.268, mean reward: -5892.268 [-5892.268, -5892.268], mean action: 1.000 [1.000, 1.000],  loss: 20612238.000000, mae: 1348.945312, mean_q: 0.636914, mean_eps: 0.497080
 510/1000: episode: 510, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1390.865, mean reward: -1390.865 [-1390.865, -1390.865], mean action: 2.000 [2.000, 2.000],  loss: 16492498.000000, mae: 1234.145264, mean_q: 0.619567, mean_eps: 0.496090
 511/1000: episode: 511, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9361.358, mean reward: -9361.358 [-9361.358, -9361.358], mean action: 0.000 [0.000, 0.000],  loss: 15197832.000000, mae: 1176.665161, mean_q: 0.599373, mean_eps: 0.495100
 512/1000: episode: 512, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7347.423, mean reward: -7347.423 [-7347.423, -7347.423], mean action: 0.000 [0.000, 0.000],  loss: 18333034.000000, mae: 1281.734619, mean_q: 0.576138, mean_eps: 0.494110
 513/1000: episode: 513, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -1566.318, mean reward: -1566.318 [-1566.318, -1566.318], mean action: 0.000 [0.000, 0.000],  loss: 15434926.000000, mae: 1168.370117, mean_q: 0.551855, mean_eps: 0.493120
 514/1000: episode: 514, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -10693.854, mean reward: -10693.854 [-10693.854, -10693.854], mean action: 0.000 [0.000, 0.000],  loss: 24551076.000000, mae: 1427.938354, mean_q: 0.531203, mean_eps: 0.492130
 515/1000: episode: 515, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2189.600, mean reward: -2189.600 [-2189.600, -2189.600], mean action: 0.000 [0.000, 0.000],  loss: 17704726.000000, mae: 1226.859863, mean_q: 0.508456, mean_eps: 0.491140
 516/1000: episode: 516, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -309.877, mean reward: -309.877 [-309.877, -309.877], mean action: 2.000 [2.000, 2.000],  loss: 15110134.000000, mae: 1086.603027, mean_q: 0.489382, mean_eps: 0.490150
 517/1000: episode: 517, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1397.469, mean reward: -1397.469 [-1397.469, -1397.469], mean action: 2.000 [2.000, 2.000],  loss: 21927432.000000, mae: 1390.497192, mean_q: 0.473695, mean_eps: 0.489160
 518/1000: episode: 518, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1062.629, mean reward: -1062.629 [-1062.629, -1062.629], mean action: 2.000 [2.000, 2.000],  loss: 25626276.000000, mae: 1546.177979, mean_q: 0.450121, mean_eps: 0.488170
 519/1000: episode: 519, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -9014.746, mean reward: -9014.746 [-9014.746, -9014.746], mean action: 0.000 [0.000, 0.000],  loss: 17189140.000000, mae: 1175.665527, mean_q: 0.435565, mean_eps: 0.487180
 520/1000: episode: 520, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2875.688, mean reward: -2875.688 [-2875.688, -2875.688], mean action: 3.000 [3.000, 3.000],  loss: 10153516.000000, mae: 962.546448, mean_q: 0.420030, mean_eps: 0.486190
 521/1000: episode: 521, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5582.034, mean reward: -5582.034 [-5582.034, -5582.034], mean action: 3.000 [3.000, 3.000],  loss: 22288862.000000, mae: 1418.608521, mean_q: 0.412417, mean_eps: 0.485200
 522/1000: episode: 522, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -8310.943, mean reward: -8310.943 [-8310.943, -8310.943], mean action: 0.000 [0.000, 0.000],  loss: 14594261.000000, mae: 1109.053345, mean_q: 0.394006, mean_eps: 0.484210
 523/1000: episode: 523, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5731.534, mean reward: -5731.534 [-5731.534, -5731.534], mean action: 0.000 [0.000, 0.000],  loss: 17006644.000000, mae: 1214.503784, mean_q: 0.381383, mean_eps: 0.483220
 524/1000: episode: 524, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8050.216, mean reward: -8050.216 [-8050.216, -8050.216], mean action: 3.000 [3.000, 3.000],  loss: 21338670.000000, mae: 1406.306152, mean_q: 0.362678, mean_eps: 0.482230
 525/1000: episode: 525, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -850.139, mean reward: -850.139 [-850.139, -850.139], mean action: 3.000 [3.000, 3.000],  loss: 15797912.000000, mae: 1014.876221, mean_q: 0.351833, mean_eps: 0.481240
 526/1000: episode: 526, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7554.738, mean reward: -7554.738 [-7554.738, -7554.738], mean action: 0.000 [0.000, 0.000],  loss: 18250020.000000, mae: 1255.897827, mean_q: 0.334011, mean_eps: 0.480250
 527/1000: episode: 527, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4420.032, mean reward: -4420.032 [-4420.032, -4420.032], mean action: 0.000 [0.000, 0.000],  loss: 21039084.000000, mae: 1301.679199, mean_q: 0.320783, mean_eps: 0.479260
 528/1000: episode: 528, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4495.298, mean reward: -4495.298 [-4495.298, -4495.298], mean action: 0.000 [0.000, 0.000],  loss: 15082073.000000, mae: 1143.901978, mean_q: 0.308556, mean_eps: 0.478270
 529/1000: episode: 529, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -9767.647, mean reward: -9767.647 [-9767.647, -9767.647], mean action: 0.000 [0.000, 0.000],  loss: 15720514.000000, mae: 1150.939819, mean_q: 0.293450, mean_eps: 0.477280
 530/1000: episode: 530, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5205.031, mean reward: -5205.031 [-5205.031, -5205.031], mean action: 0.000 [0.000, 0.000],  loss: 19908872.000000, mae: 1259.174316, mean_q: 0.280820, mean_eps: 0.476290
 531/1000: episode: 531, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2966.265, mean reward: -2966.265 [-2966.265, -2966.265], mean action: 2.000 [2.000, 2.000],  loss: 22631308.000000, mae: 1495.202637, mean_q: 0.263178, mean_eps: 0.475300
 532/1000: episode: 532, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2526.637, mean reward: -2526.637 [-2526.637, -2526.637], mean action: 0.000 [0.000, 0.000],  loss: 15373782.000000, mae: 1155.728394, mean_q: 0.253779, mean_eps: 0.474310
 533/1000: episode: 533, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7555.475, mean reward: -7555.475 [-7555.475, -7555.475], mean action: 0.000 [0.000, 0.000],  loss: 16223133.000000, mae: 1191.946045, mean_q: 0.241557, mean_eps: 0.473320
 534/1000: episode: 534, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8997.806, mean reward: -8997.806 [-8997.806, -8997.806], mean action: 1.000 [1.000, 1.000],  loss: 15460040.000000, mae: 1232.676514, mean_q: 0.225199, mean_eps: 0.472330
 535/1000: episode: 535, duration: 0.047s, episode steps:   1, steps per second:  22, episode reward: -5409.595, mean reward: -5409.595 [-5409.595, -5409.595], mean action: 0.000 [0.000, 0.000],  loss: 21438796.000000, mae: 1334.214844, mean_q: 0.212673, mean_eps: 0.471340
 536/1000: episode: 536, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -872.826, mean reward: -872.826 [-872.826, -872.826], mean action: 3.000 [3.000, 3.000],  loss: 14620288.000000, mae: 1159.700195, mean_q: 0.202080, mean_eps: 0.470350
 537/1000: episode: 537, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4397.289, mean reward: -4397.289 [-4397.289, -4397.289], mean action: 3.000 [3.000, 3.000],  loss: 21424534.000000, mae: 1378.225098, mean_q: 0.186973, mean_eps: 0.469360
 538/1000: episode: 538, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3881.013, mean reward: -3881.013 [-3881.013, -3881.013], mean action: 3.000 [3.000, 3.000],  loss: 17504660.000000, mae: 1132.855103, mean_q: 0.177967, mean_eps: 0.468370
 539/1000: episode: 539, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3461.822, mean reward: -3461.822 [-3461.822, -3461.822], mean action: 1.000 [1.000, 1.000],  loss: 21668684.000000, mae: 1467.807617, mean_q: 0.163730, mean_eps: 0.467380
 540/1000: episode: 540, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -329.256, mean reward: -329.256 [-329.256, -329.256], mean action: 3.000 [3.000, 3.000],  loss: 16888742.000000, mae: 1219.235229, mean_q: 0.156359, mean_eps: 0.466390
 541/1000: episode: 541, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4623.910, mean reward: -4623.910 [-4623.910, -4623.910], mean action: 1.000 [1.000, 1.000],  loss: 24139894.000000, mae: 1495.402954, mean_q: 0.153079, mean_eps: 0.465400
 542/1000: episode: 542, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3653.259, mean reward: -3653.259 [-3653.259, -3653.259], mean action: 2.000 [2.000, 2.000],  loss: 24773304.000000, mae: 1376.057495, mean_q: 0.160109, mean_eps: 0.464410
 543/1000: episode: 543, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3510.666, mean reward: -3510.666 [-3510.666, -3510.666], mean action: 2.000 [2.000, 2.000],  loss: 16422514.000000, mae: 1186.822021, mean_q: 0.170178, mean_eps: 0.463420
 544/1000: episode: 544, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1372.988, mean reward: -1372.988 [-1372.988, -1372.988], mean action: 1.000 [1.000, 1.000],  loss: 17226116.000000, mae: 1116.529175, mean_q: 0.178808, mean_eps: 0.462430
 545/1000: episode: 545, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6472.279, mean reward: -6472.279 [-6472.279, -6472.279], mean action: 2.000 [2.000, 2.000],  loss: 11637654.000000, mae: 1049.624634, mean_q: 0.187720, mean_eps: 0.461440
 546/1000: episode: 546, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3331.605, mean reward: -3331.605 [-3331.605, -3331.605], mean action: 1.000 [1.000, 1.000],  loss: 19839856.000000, mae: 1342.122559, mean_q: 0.197399, mean_eps: 0.460450
 547/1000: episode: 547, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4112.129, mean reward: -4112.129 [-4112.129, -4112.129], mean action: 0.000 [0.000, 0.000],  loss: 16972108.000000, mae: 1196.942017, mean_q: 0.206631, mean_eps: 0.459460
 548/1000: episode: 548, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5851.749, mean reward: -5851.749 [-5851.749, -5851.749], mean action: 1.000 [1.000, 1.000],  loss: 16104776.000000, mae: 1195.995483, mean_q: 0.216852, mean_eps: 0.458470
 549/1000: episode: 549, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1093.880, mean reward: -1093.880 [-1093.880, -1093.880], mean action: 2.000 [2.000, 2.000],  loss: 20383762.000000, mae: 1372.287842, mean_q: 0.226071, mean_eps: 0.457480
 550/1000: episode: 550, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5434.904, mean reward: -5434.904 [-5434.904, -5434.904], mean action: 1.000 [1.000, 1.000],  loss: 22322972.000000, mae: 1459.535156, mean_q: 0.230849, mean_eps: 0.456490
 551/1000: episode: 551, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3407.031, mean reward: -3407.031 [-3407.031, -3407.031], mean action: 1.000 [1.000, 1.000],  loss: 17478072.000000, mae: 1220.442627, mean_q: 0.239729, mean_eps: 0.455500
 552/1000: episode: 552, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -9362.648, mean reward: -9362.648 [-9362.648, -9362.648], mean action: 1.000 [1.000, 1.000],  loss: 14651411.000000, mae: 1114.522949, mean_q: 0.245692, mean_eps: 0.454510
 553/1000: episode: 553, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3180.599, mean reward: -3180.599 [-3180.599, -3180.599], mean action: 1.000 [1.000, 1.000],  loss: 20276092.000000, mae: 1330.175537, mean_q: 0.255690, mean_eps: 0.453520
 554/1000: episode: 554, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4859.447, mean reward: -4859.447 [-4859.447, -4859.447], mean action: 1.000 [1.000, 1.000],  loss: 16756194.000000, mae: 1248.104492, mean_q: 0.263737, mean_eps: 0.452530
 555/1000: episode: 555, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1284.894, mean reward: -1284.894 [-1284.894, -1284.894], mean action: 1.000 [1.000, 1.000],  loss: 16468126.000000, mae: 1191.572510, mean_q: 0.268988, mean_eps: 0.451540
 556/1000: episode: 556, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6514.244, mean reward: -6514.244 [-6514.244, -6514.244], mean action: 1.000 [1.000, 1.000],  loss: 16902640.000000, mae: 1126.099121, mean_q: 0.276839, mean_eps: 0.450550
 557/1000: episode: 557, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7338.001, mean reward: -7338.001 [-7338.001, -7338.001], mean action: 1.000 [1.000, 1.000],  loss: 21149918.000000, mae: 1336.041748, mean_q: 0.287035, mean_eps: 0.449560
 558/1000: episode: 558, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -7121.104, mean reward: -7121.104 [-7121.104, -7121.104], mean action: 1.000 [1.000, 1.000],  loss: 12535752.000000, mae: 1058.494873, mean_q: 0.294080, mean_eps: 0.448570
 559/1000: episode: 559, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2414.254, mean reward: -2414.254 [-2414.254, -2414.254], mean action: 1.000 [1.000, 1.000],  loss: 18820856.000000, mae: 1281.285156, mean_q: 0.303245, mean_eps: 0.447580
 560/1000: episode: 560, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5680.363, mean reward: -5680.363 [-5680.363, -5680.363], mean action: 1.000 [1.000, 1.000],  loss: 14867411.000000, mae: 1182.519409, mean_q: 0.309554, mean_eps: 0.446590
 561/1000: episode: 561, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -2016.344, mean reward: -2016.344 [-2016.344, -2016.344], mean action: 1.000 [1.000, 1.000],  loss: 11500691.000000, mae: 970.954285, mean_q: 0.315383, mean_eps: 0.445600
 562/1000: episode: 562, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5485.564, mean reward: -5485.564 [-5485.564, -5485.564], mean action: 1.000 [1.000, 1.000],  loss: 21535154.000000, mae: 1464.756958, mean_q: 0.318983, mean_eps: 0.444610
 563/1000: episode: 563, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -6711.710, mean reward: -6711.710 [-6711.710, -6711.710], mean action: 3.000 [3.000, 3.000],  loss: 13355386.000000, mae: 1049.520264, mean_q: 0.322793, mean_eps: 0.443620
 564/1000: episode: 564, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -13688.016, mean reward: -13688.016 [-13688.016, -13688.016], mean action: 0.000 [0.000, 0.000],  loss: 18307508.000000, mae: 1181.328979, mean_q: 0.327030, mean_eps: 0.442630
 565/1000: episode: 565, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3496.494, mean reward: -3496.494 [-3496.494, -3496.494], mean action: 1.000 [1.000, 1.000],  loss: 17338718.000000, mae: 1290.032227, mean_q: 0.338598, mean_eps: 0.441640
 566/1000: episode: 566, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7485.508, mean reward: -7485.508 [-7485.508, -7485.508], mean action: 2.000 [2.000, 2.000],  loss: 20439738.000000, mae: 1276.957153, mean_q: 0.343140, mean_eps: 0.440650
 567/1000: episode: 567, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5675.536, mean reward: -5675.536 [-5675.536, -5675.536], mean action: 1.000 [1.000, 1.000],  loss: 15102210.000000, mae: 1144.065186, mean_q: 0.356583, mean_eps: 0.439660
 568/1000: episode: 568, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -8842.050, mean reward: -8842.050 [-8842.050, -8842.050], mean action: 1.000 [1.000, 1.000],  loss: 18017332.000000, mae: 1213.296509, mean_q: 0.369877, mean_eps: 0.438670
 569/1000: episode: 569, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4012.545, mean reward: -4012.545 [-4012.545, -4012.545], mean action: 2.000 [2.000, 2.000],  loss: 16819218.000000, mae: 1216.430420, mean_q: 0.387609, mean_eps: 0.437680
 570/1000: episode: 570, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6876.322, mean reward: -6876.322 [-6876.322, -6876.322], mean action: 1.000 [1.000, 1.000],  loss: 14389099.000000, mae: 1052.845825, mean_q: 0.393300, mean_eps: 0.436690
 571/1000: episode: 571, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -889.959, mean reward: -889.959 [-889.959, -889.959], mean action: 1.000 [1.000, 1.000],  loss: 16809268.000000, mae: 1198.781738, mean_q: 0.408227, mean_eps: 0.435700
 572/1000: episode: 572, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -783.631, mean reward: -783.631 [-783.631, -783.631], mean action: 1.000 [1.000, 1.000],  loss: 15462160.000000, mae: 1132.498657, mean_q: 0.416611, mean_eps: 0.434710
 573/1000: episode: 573, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3718.217, mean reward: -3718.217 [-3718.217, -3718.217], mean action: 1.000 [1.000, 1.000],  loss: 16521697.000000, mae: 1226.267578, mean_q: 0.432182, mean_eps: 0.433720
 574/1000: episode: 574, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6003.172, mean reward: -6003.172 [-6003.172, -6003.172], mean action: 1.000 [1.000, 1.000],  loss: 18397128.000000, mae: 1249.611572, mean_q: 0.446285, mean_eps: 0.432730
 575/1000: episode: 575, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7487.140, mean reward: -7487.140 [-7487.140, -7487.140], mean action: 3.000 [3.000, 3.000],  loss: 18437092.000000, mae: 1295.175781, mean_q: 0.461477, mean_eps: 0.431740
 576/1000: episode: 576, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1560.242, mean reward: -1560.242 [-1560.242, -1560.242], mean action: 3.000 [3.000, 3.000],  loss: 15393907.000000, mae: 1152.893188, mean_q: 0.480746, mean_eps: 0.430750
 577/1000: episode: 577, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -3907.980, mean reward: -3907.980 [-3907.980, -3907.980], mean action: 1.000 [1.000, 1.000],  loss: 18995040.000000, mae: 1275.631104, mean_q: 0.498757, mean_eps: 0.429760
 578/1000: episode: 578, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5138.990, mean reward: -5138.990 [-5138.990, -5138.990], mean action: 1.000 [1.000, 1.000],  loss: 19339754.000000, mae: 1321.746338, mean_q: 0.507073, mean_eps: 0.428770
 579/1000: episode: 579, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9818.870, mean reward: -9818.870 [-9818.870, -9818.870], mean action: 1.000 [1.000, 1.000],  loss: 13751299.000000, mae: 1008.080750, mean_q: 0.522794, mean_eps: 0.427780
 580/1000: episode: 580, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8759.389, mean reward: -8759.389 [-8759.389, -8759.389], mean action: 3.000 [3.000, 3.000],  loss: 21600960.000000, mae: 1324.334961, mean_q: 0.527798, mean_eps: 0.426790
 581/1000: episode: 581, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4297.449, mean reward: -4297.449 [-4297.449, -4297.449], mean action: 1.000 [1.000, 1.000],  loss: 24879288.000000, mae: 1498.801270, mean_q: 0.544570, mean_eps: 0.425800
 582/1000: episode: 582, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6536.854, mean reward: -6536.854 [-6536.854, -6536.854], mean action: 1.000 [1.000, 1.000],  loss: 25405516.000000, mae: 1489.800049, mean_q: 0.566123, mean_eps: 0.424810
 583/1000: episode: 583, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2623.329, mean reward: -2623.329 [-2623.329, -2623.329], mean action: 1.000 [1.000, 1.000],  loss: 17486994.000000, mae: 1265.200928, mean_q: 0.583948, mean_eps: 0.423820
 584/1000: episode: 584, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2990.460, mean reward: -2990.460 [-2990.460, -2990.460], mean action: 2.000 [2.000, 2.000],  loss: 17669052.000000, mae: 1201.380371, mean_q: 0.604459, mean_eps: 0.422830
 585/1000: episode: 585, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6456.919, mean reward: -6456.919 [-6456.919, -6456.919], mean action: 1.000 [1.000, 1.000],  loss: 17953376.000000, mae: 1319.661377, mean_q: 0.620781, mean_eps: 0.421840
 586/1000: episode: 586, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -3822.017, mean reward: -3822.017 [-3822.017, -3822.017], mean action: 1.000 [1.000, 1.000],  loss: 20918276.000000, mae: 1331.946045, mean_q: 0.639574, mean_eps: 0.420850
 587/1000: episode: 587, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2382.397, mean reward: -2382.397 [-2382.397, -2382.397], mean action: 1.000 [1.000, 1.000],  loss: 19081000.000000, mae: 1289.882568, mean_q: 0.647712, mean_eps: 0.419860
 588/1000: episode: 588, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -9450.836, mean reward: -9450.836 [-9450.836, -9450.836], mean action: 1.000 [1.000, 1.000],  loss: 21282860.000000, mae: 1260.771240, mean_q: 0.664250, mean_eps: 0.418870
 589/1000: episode: 589, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6040.239, mean reward: -6040.239 [-6040.239, -6040.239], mean action: 1.000 [1.000, 1.000],  loss: 19918816.000000, mae: 1190.575928, mean_q: 0.672980, mean_eps: 0.417880
 590/1000: episode: 590, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1594.927, mean reward: -1594.927 [-1594.927, -1594.927], mean action: 1.000 [1.000, 1.000],  loss: 14964310.000000, mae: 1071.372314, mean_q: 0.692719, mean_eps: 0.416890
 591/1000: episode: 591, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -92.923, mean reward: -92.923 [-92.923, -92.923], mean action: 3.000 [3.000, 3.000],  loss: 10439609.000000, mae: 942.869995, mean_q: 0.703351, mean_eps: 0.415900
 592/1000: episode: 592, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4397.186, mean reward: -4397.186 [-4397.186, -4397.186], mean action: 3.000 [3.000, 3.000],  loss: 17205332.000000, mae: 1251.815430, mean_q: 0.722578, mean_eps: 0.414910
 593/1000: episode: 593, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5072.400, mean reward: -5072.400 [-5072.400, -5072.400], mean action: 1.000 [1.000, 1.000],  loss: 24041260.000000, mae: 1316.601074, mean_q: 0.732638, mean_eps: 0.413920
 594/1000: episode: 594, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7326.019, mean reward: -7326.019 [-7326.019, -7326.019], mean action: 1.000 [1.000, 1.000],  loss: 20932850.000000, mae: 1317.083252, mean_q: 0.744289, mean_eps: 0.412930
 595/1000: episode: 595, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8023.994, mean reward: -8023.994 [-8023.994, -8023.994], mean action: 1.000 [1.000, 1.000],  loss: 21015016.000000, mae: 1359.302246, mean_q: 0.758975, mean_eps: 0.411940
 596/1000: episode: 596, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -11594.005, mean reward: -11594.005 [-11594.005, -11594.005], mean action: 0.000 [0.000, 0.000],  loss: 15089383.000000, mae: 1151.234863, mean_q: 0.766650, mean_eps: 0.410950
 597/1000: episode: 597, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5890.584, mean reward: -5890.584 [-5890.584, -5890.584], mean action: 1.000 [1.000, 1.000],  loss: 21937276.000000, mae: 1502.162964, mean_q: 0.777474, mean_eps: 0.409960
 598/1000: episode: 598, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3365.650, mean reward: -3365.650 [-3365.650, -3365.650], mean action: 2.000 [2.000, 2.000],  loss: 18518546.000000, mae: 1273.506592, mean_q: 0.782440, mean_eps: 0.408970
 599/1000: episode: 599, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4155.736, mean reward: -4155.736 [-4155.736, -4155.736], mean action: 0.000 [0.000, 0.000],  loss: 24157928.000000, mae: 1535.123535, mean_q: 0.788018, mean_eps: 0.407980
 600/1000: episode: 600, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -8027.234, mean reward: -8027.234 [-8027.234, -8027.234], mean action: 0.000 [0.000, 0.000],  loss: 27627896.000000, mae: 1605.571655, mean_q: 0.794726, mean_eps: 0.406990
 601/1000: episode: 601, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6032.509, mean reward: -6032.509 [-6032.509, -6032.509], mean action: 1.000 [1.000, 1.000],  loss: 17139912.000000, mae: 1202.953369, mean_q: 0.813347, mean_eps: 0.406000
 602/1000: episode: 602, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4338.848, mean reward: -4338.848 [-4338.848, -4338.848], mean action: 1.000 [1.000, 1.000],  loss: 14865165.000000, mae: 1162.040039, mean_q: 0.819541, mean_eps: 0.405010
 603/1000: episode: 603, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4248.776, mean reward: -4248.776 [-4248.776, -4248.776], mean action: 1.000 [1.000, 1.000],  loss: 19236032.000000, mae: 1402.983032, mean_q: 0.823311, mean_eps: 0.404020
 604/1000: episode: 604, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3115.120, mean reward: -3115.120 [-3115.120, -3115.120], mean action: 1.000 [1.000, 1.000],  loss: 13203807.000000, mae: 1079.639038, mean_q: 0.839608, mean_eps: 0.403030
 605/1000: episode: 605, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7004.973, mean reward: -7004.973 [-7004.973, -7004.973], mean action: 0.000 [0.000, 0.000],  loss: 20408268.000000, mae: 1251.671509, mean_q: 0.846519, mean_eps: 0.402040
 606/1000: episode: 606, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1999.857, mean reward: -1999.857 [-1999.857, -1999.857], mean action: 1.000 [1.000, 1.000],  loss: 14546542.000000, mae: 1123.721802, mean_q: 0.861205, mean_eps: 0.401050
 607/1000: episode: 607, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9731.344, mean reward: -9731.344 [-9731.344, -9731.344], mean action: 0.000 [0.000, 0.000],  loss: 15478670.000000, mae: 1180.000610, mean_q: 0.872639, mean_eps: 0.400060
 608/1000: episode: 608, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5213.922, mean reward: -5213.922 [-5213.922, -5213.922], mean action: 1.000 [1.000, 1.000],  loss: 12876547.000000, mae: 1068.516479, mean_q: 0.881117, mean_eps: 0.399070
 609/1000: episode: 609, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2247.644, mean reward: -2247.644 [-2247.644, -2247.644], mean action: 0.000 [0.000, 0.000],  loss: 19608242.000000, mae: 1361.088623, mean_q: 0.883997, mean_eps: 0.398080
 610/1000: episode: 610, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -422.799, mean reward: -422.799 [-422.799, -422.799], mean action: 1.000 [1.000, 1.000],  loss: 22181272.000000, mae: 1319.228027, mean_q: 0.904192, mean_eps: 0.397090
 611/1000: episode: 611, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1168.480, mean reward: -1168.480 [-1168.480, -1168.480], mean action: 1.000 [1.000, 1.000],  loss: 13905641.000000, mae: 1063.641724, mean_q: 0.916472, mean_eps: 0.396100
 612/1000: episode: 612, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -835.723, mean reward: -835.723 [-835.723, -835.723], mean action: 2.000 [2.000, 2.000],  loss: 20568672.000000, mae: 1355.476562, mean_q: 0.914686, mean_eps: 0.395110
 613/1000: episode: 613, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -511.929, mean reward: -511.929 [-511.929, -511.929], mean action: 3.000 [3.000, 3.000],  loss: 15322112.000000, mae: 1156.032471, mean_q: 0.940910, mean_eps: 0.394120
 614/1000: episode: 614, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7587.385, mean reward: -7587.385 [-7587.385, -7587.385], mean action: 1.000 [1.000, 1.000],  loss: 13551365.000000, mae: 1119.466675, mean_q: 0.950848, mean_eps: 0.393130
 615/1000: episode: 615, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3811.609, mean reward: -3811.609 [-3811.609, -3811.609], mean action: 1.000 [1.000, 1.000],  loss: 19230716.000000, mae: 1301.409668, mean_q: 0.951527, mean_eps: 0.392140
 616/1000: episode: 616, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7031.658, mean reward: -7031.658 [-7031.658, -7031.658], mean action: 1.000 [1.000, 1.000],  loss: 15431156.000000, mae: 1107.951538, mean_q: 0.967587, mean_eps: 0.391150
 617/1000: episode: 617, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9434.823, mean reward: -9434.823 [-9434.823, -9434.823], mean action: 0.000 [0.000, 0.000],  loss: 15607164.000000, mae: 1190.008545, mean_q: 0.972010, mean_eps: 0.390160
 618/1000: episode: 618, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -14809.529, mean reward: -14809.529 [-14809.529, -14809.529], mean action: 0.000 [0.000, 0.000],  loss: 13030300.000000, mae: 1043.135986, mean_q: 0.988263, mean_eps: 0.389170
 619/1000: episode: 619, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -7077.488, mean reward: -7077.488 [-7077.488, -7077.488], mean action: 1.000 [1.000, 1.000],  loss: 12957576.000000, mae: 1050.535889, mean_q: 0.990367, mean_eps: 0.388180
 620/1000: episode: 620, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4827.859, mean reward: -4827.859 [-4827.859, -4827.859], mean action: 1.000 [1.000, 1.000],  loss: 18327658.000000, mae: 1240.190552, mean_q: 0.985945, mean_eps: 0.387190
 621/1000: episode: 621, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -904.416, mean reward: -904.416 [-904.416, -904.416], mean action: 2.000 [2.000, 2.000],  loss: 12848093.000000, mae: 1078.702759, mean_q: 0.993702, mean_eps: 0.386200
 622/1000: episode: 622, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4464.473, mean reward: -4464.473 [-4464.473, -4464.473], mean action: 1.000 [1.000, 1.000],  loss: 13081214.000000, mae: 976.316528, mean_q: 1.005077, mean_eps: 0.385210
 623/1000: episode: 623, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1077.725, mean reward: -1077.725 [-1077.725, -1077.725], mean action: 1.000 [1.000, 1.000],  loss: 13203298.000000, mae: 1077.542236, mean_q: 1.020990, mean_eps: 0.384220
 624/1000: episode: 624, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5006.518, mean reward: -5006.518 [-5006.518, -5006.518], mean action: 1.000 [1.000, 1.000],  loss: 17897256.000000, mae: 1278.862793, mean_q: 1.015245, mean_eps: 0.383230
 625/1000: episode: 625, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6221.534, mean reward: -6221.534 [-6221.534, -6221.534], mean action: 1.000 [1.000, 1.000],  loss: 18215820.000000, mae: 1257.462891, mean_q: 1.020470, mean_eps: 0.382240
 626/1000: episode: 626, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4715.909, mean reward: -4715.909 [-4715.909, -4715.909], mean action: 1.000 [1.000, 1.000],  loss: 9255613.000000, mae: 951.833252, mean_q: 1.031648, mean_eps: 0.381250
 627/1000: episode: 627, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7160.504, mean reward: -7160.504 [-7160.504, -7160.504], mean action: 1.000 [1.000, 1.000],  loss: 13103277.000000, mae: 1118.339355, mean_q: 1.030819, mean_eps: 0.380260
 628/1000: episode: 628, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3708.274, mean reward: -3708.274 [-3708.274, -3708.274], mean action: 2.000 [2.000, 2.000],  loss: 19865568.000000, mae: 1342.884888, mean_q: 1.044536, mean_eps: 0.379270
 629/1000: episode: 629, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -14136.654, mean reward: -14136.654 [-14136.654, -14136.654], mean action: 0.000 [0.000, 0.000],  loss: 16960470.000000, mae: 1201.854004, mean_q: 1.042184, mean_eps: 0.378280
 630/1000: episode: 630, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5710.334, mean reward: -5710.334 [-5710.334, -5710.334], mean action: 1.000 [1.000, 1.000],  loss: 15542242.000000, mae: 1206.992432, mean_q: 1.038894, mean_eps: 0.377290
 631/1000: episode: 631, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -609.156, mean reward: -609.156 [-609.156, -609.156], mean action: 1.000 [1.000, 1.000],  loss: 15754808.000000, mae: 1107.593872, mean_q: 1.057297, mean_eps: 0.376300
 632/1000: episode: 632, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6615.659, mean reward: -6615.659 [-6615.659, -6615.659], mean action: 0.000 [0.000, 0.000],  loss: 10430264.000000, mae: 971.688843, mean_q: 1.071269, mean_eps: 0.375310
 633/1000: episode: 633, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4513.851, mean reward: -4513.851 [-4513.851, -4513.851], mean action: 1.000 [1.000, 1.000],  loss: 18282996.000000, mae: 1197.014160, mean_q: 1.072268, mean_eps: 0.374320
 634/1000: episode: 634, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2366.594, mean reward: -2366.594 [-2366.594, -2366.594], mean action: 3.000 [3.000, 3.000],  loss: 22687812.000000, mae: 1344.677979, mean_q: 1.071506, mean_eps: 0.373330
 635/1000: episode: 635, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -859.832, mean reward: -859.832 [-859.832, -859.832], mean action: 2.000 [2.000, 2.000],  loss: 11083132.000000, mae: 1002.350464, mean_q: 1.083687, mean_eps: 0.372340
 636/1000: episode: 636, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3599.745, mean reward: -3599.745 [-3599.745, -3599.745], mean action: 1.000 [1.000, 1.000],  loss: 15521530.000000, mae: 1115.042480, mean_q: 1.097018, mean_eps: 0.371350
 637/1000: episode: 637, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -3768.119, mean reward: -3768.119 [-3768.119, -3768.119], mean action: 1.000 [1.000, 1.000],  loss: 20444078.000000, mae: 1267.372559, mean_q: 1.084615, mean_eps: 0.370360
 638/1000: episode: 638, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1947.848, mean reward: -1947.848 [-1947.848, -1947.848], mean action: 3.000 [3.000, 3.000],  loss: 18147522.000000, mae: 1334.794922, mean_q: 1.101784, mean_eps: 0.369370
 639/1000: episode: 639, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4468.319, mean reward: -4468.319 [-4468.319, -4468.319], mean action: 1.000 [1.000, 1.000],  loss: 14469689.000000, mae: 1156.731934, mean_q: 1.099809, mean_eps: 0.368380
 640/1000: episode: 640, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -4386.161, mean reward: -4386.161 [-4386.161, -4386.161], mean action: 1.000 [1.000, 1.000],  loss: 19156700.000000, mae: 1253.628174, mean_q: 1.101855, mean_eps: 0.367390
 641/1000: episode: 641, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4682.154, mean reward: -4682.154 [-4682.154, -4682.154], mean action: 1.000 [1.000, 1.000],  loss: 14144518.000000, mae: 1113.982178, mean_q: 1.100650, mean_eps: 0.366400
 642/1000: episode: 642, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7335.446, mean reward: -7335.446 [-7335.446, -7335.446], mean action: 1.000 [1.000, 1.000],  loss: 18306294.000000, mae: 1263.543457, mean_q: 1.105734, mean_eps: 0.365410
 643/1000: episode: 643, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5299.734, mean reward: -5299.734 [-5299.734, -5299.734], mean action: 0.000 [0.000, 0.000],  loss: 17259560.000000, mae: 1253.672485, mean_q: 1.112252, mean_eps: 0.364420
 644/1000: episode: 644, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8362.331, mean reward: -8362.331 [-8362.331, -8362.331], mean action: 0.000 [0.000, 0.000],  loss: 10433436.000000, mae: 991.323669, mean_q: 1.126538, mean_eps: 0.363430
 645/1000: episode: 645, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6719.251, mean reward: -6719.251 [-6719.251, -6719.251], mean action: 1.000 [1.000, 1.000],  loss: 25688940.000000, mae: 1472.312256, mean_q: 1.106084, mean_eps: 0.362440
 646/1000: episode: 646, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -7410.458, mean reward: -7410.458 [-7410.458, -7410.458], mean action: 1.000 [1.000, 1.000],  loss: 14116290.000000, mae: 1130.334229, mean_q: 1.127678, mean_eps: 0.361450
 647/1000: episode: 647, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7789.361, mean reward: -7789.361 [-7789.361, -7789.361], mean action: 1.000 [1.000, 1.000],  loss: 16087717.000000, mae: 1226.013306, mean_q: 1.123385, mean_eps: 0.360460
 648/1000: episode: 648, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -765.306, mean reward: -765.306 [-765.306, -765.306], mean action: 1.000 [1.000, 1.000],  loss: 18113736.000000, mae: 1220.000244, mean_q: 1.115904, mean_eps: 0.359470
 649/1000: episode: 649, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3013.453, mean reward: -3013.453 [-3013.453, -3013.453], mean action: 1.000 [1.000, 1.000],  loss: 19509836.000000, mae: 1369.040894, mean_q: 1.121157, mean_eps: 0.358480
 650/1000: episode: 650, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -747.690, mean reward: -747.690 [-747.690, -747.690], mean action: 1.000 [1.000, 1.000],  loss: 10798822.000000, mae: 918.113525, mean_q: 1.127419, mean_eps: 0.357490
 651/1000: episode: 651, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -4326.232, mean reward: -4326.232 [-4326.232, -4326.232], mean action: 1.000 [1.000, 1.000],  loss: 15962509.000000, mae: 1185.056885, mean_q: 1.130279, mean_eps: 0.356500
 652/1000: episode: 652, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3336.446, mean reward: -3336.446 [-3336.446, -3336.446], mean action: 1.000 [1.000, 1.000],  loss: 19472316.000000, mae: 1235.890503, mean_q: 1.126664, mean_eps: 0.355510
 653/1000: episode: 653, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6539.446, mean reward: -6539.446 [-6539.446, -6539.446], mean action: 1.000 [1.000, 1.000],  loss: 15915161.000000, mae: 1222.390137, mean_q: 1.143639, mean_eps: 0.354520
 654/1000: episode: 654, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1159.397, mean reward: -1159.397 [-1159.397, -1159.397], mean action: 1.000 [1.000, 1.000],  loss: 15393148.000000, mae: 1230.067627, mean_q: 1.130898, mean_eps: 0.353530
 655/1000: episode: 655, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -600.185, mean reward: -600.185 [-600.185, -600.185], mean action: 2.000 [2.000, 2.000],  loss: 17506186.000000, mae: 1170.342285, mean_q: 1.138154, mean_eps: 0.352540
 656/1000: episode: 656, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8671.289, mean reward: -8671.289 [-8671.289, -8671.289], mean action: 1.000 [1.000, 1.000],  loss: 21145640.000000, mae: 1384.799316, mean_q: 1.134169, mean_eps: 0.351550
 657/1000: episode: 657, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8444.216, mean reward: -8444.216 [-8444.216, -8444.216], mean action: 1.000 [1.000, 1.000],  loss: 17846924.000000, mae: 1280.127441, mean_q: 1.137563, mean_eps: 0.350560
 658/1000: episode: 658, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -6133.056, mean reward: -6133.056 [-6133.056, -6133.056], mean action: 1.000 [1.000, 1.000],  loss: 15627595.000000, mae: 1127.495239, mean_q: 1.136165, mean_eps: 0.349570
 659/1000: episode: 659, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3130.477, mean reward: -3130.477 [-3130.477, -3130.477], mean action: 1.000 [1.000, 1.000],  loss: 24514526.000000, mae: 1510.500000, mean_q: 1.130231, mean_eps: 0.348580
 660/1000: episode: 660, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8247.214, mean reward: -8247.214 [-8247.214, -8247.214], mean action: 1.000 [1.000, 1.000],  loss: 17845946.000000, mae: 1219.015381, mean_q: 1.122387, mean_eps: 0.347590
 661/1000: episode: 661, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4948.333, mean reward: -4948.333 [-4948.333, -4948.333], mean action: 3.000 [3.000, 3.000],  loss: 23224334.000000, mae: 1484.917725, mean_q: 1.120831, mean_eps: 0.346600
 662/1000: episode: 662, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1238.228, mean reward: -1238.228 [-1238.228, -1238.228], mean action: 1.000 [1.000, 1.000],  loss: 18969216.000000, mae: 1295.300049, mean_q: 1.126905, mean_eps: 0.345610
 663/1000: episode: 663, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6648.887, mean reward: -6648.887 [-6648.887, -6648.887], mean action: 1.000 [1.000, 1.000],  loss: 13369259.000000, mae: 1079.170654, mean_q: 1.138003, mean_eps: 0.344620
 664/1000: episode: 664, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3725.551, mean reward: -3725.551 [-3725.551, -3725.551], mean action: 1.000 [1.000, 1.000],  loss: 21700044.000000, mae: 1400.414795, mean_q: 1.127678, mean_eps: 0.343630
 665/1000: episode: 665, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -937.992, mean reward: -937.992 [-937.992, -937.992], mean action: 3.000 [3.000, 3.000],  loss: 21194082.000000, mae: 1426.946045, mean_q: 1.139571, mean_eps: 0.342640
 666/1000: episode: 666, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7171.958, mean reward: -7171.958 [-7171.958, -7171.958], mean action: 1.000 [1.000, 1.000],  loss: 21884772.000000, mae: 1436.783081, mean_q: 1.131653, mean_eps: 0.341650
 667/1000: episode: 667, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3160.940, mean reward: -3160.940 [-3160.940, -3160.940], mean action: 1.000 [1.000, 1.000],  loss: 15981920.000000, mae: 1181.304810, mean_q: 1.141960, mean_eps: 0.340660
 668/1000: episode: 668, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7024.581, mean reward: -7024.581 [-7024.581, -7024.581], mean action: 3.000 [3.000, 3.000],  loss: 20420704.000000, mae: 1337.019043, mean_q: 1.131545, mean_eps: 0.339670
 669/1000: episode: 669, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1474.418, mean reward: -1474.418 [-1474.418, -1474.418], mean action: 3.000 [3.000, 3.000],  loss: 17865732.000000, mae: 1234.622925, mean_q: 1.131527, mean_eps: 0.338680
 670/1000: episode: 670, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5821.615, mean reward: -5821.615 [-5821.615, -5821.615], mean action: 1.000 [1.000, 1.000],  loss: 19678380.000000, mae: 1273.315063, mean_q: 1.130115, mean_eps: 0.337690
 671/1000: episode: 671, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -3668.609, mean reward: -3668.609 [-3668.609, -3668.609], mean action: 1.000 [1.000, 1.000],  loss: 17335642.000000, mae: 1294.978516, mean_q: 1.134366, mean_eps: 0.336700
 672/1000: episode: 672, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -8431.838, mean reward: -8431.838 [-8431.838, -8431.838], mean action: 1.000 [1.000, 1.000],  loss: 14015415.000000, mae: 1104.725098, mean_q: 1.135522, mean_eps: 0.335710
 673/1000: episode: 673, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1895.474, mean reward: -1895.474 [-1895.474, -1895.474], mean action: 2.000 [2.000, 2.000],  loss: 13423853.000000, mae: 1120.251221, mean_q: 1.126424, mean_eps: 0.334720
 674/1000: episode: 674, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4033.669, mean reward: -4033.669 [-4033.669, -4033.669], mean action: 1.000 [1.000, 1.000],  loss: 21208056.000000, mae: 1413.849121, mean_q: 1.118688, mean_eps: 0.333730
 675/1000: episode: 675, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3371.895, mean reward: -3371.895 [-3371.895, -3371.895], mean action: 1.000 [1.000, 1.000],  loss: 17776144.000000, mae: 1270.783447, mean_q: 1.116178, mean_eps: 0.332740
 676/1000: episode: 676, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3041.653, mean reward: -3041.653 [-3041.653, -3041.653], mean action: 1.000 [1.000, 1.000],  loss: 20131654.000000, mae: 1316.464722, mean_q: 1.095981, mean_eps: 0.331750
 677/1000: episode: 677, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -661.989, mean reward: -661.989 [-661.989, -661.989], mean action: 1.000 [1.000, 1.000],  loss: 14054803.000000, mae: 1123.500000, mean_q: 1.088694, mean_eps: 0.330760
 678/1000: episode: 678, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5113.862, mean reward: -5113.862 [-5113.862, -5113.862], mean action: 1.000 [1.000, 1.000],  loss: 18117912.000000, mae: 1223.971924, mean_q: 1.074825, mean_eps: 0.329770
 679/1000: episode: 679, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8623.749, mean reward: -8623.749 [-8623.749, -8623.749], mean action: 1.000 [1.000, 1.000],  loss: 23224064.000000, mae: 1442.511230, mean_q: 1.061498, mean_eps: 0.328780
 680/1000: episode: 680, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4412.943, mean reward: -4412.943 [-4412.943, -4412.943], mean action: 1.000 [1.000, 1.000],  loss: 10980635.000000, mae: 1011.104553, mean_q: 1.065460, mean_eps: 0.327790
 681/1000: episode: 681, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1766.895, mean reward: -1766.895 [-1766.895, -1766.895], mean action: 1.000 [1.000, 1.000],  loss: 14861755.000000, mae: 1085.270264, mean_q: 1.060170, mean_eps: 0.326800
 682/1000: episode: 682, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8553.843, mean reward: -8553.843 [-8553.843, -8553.843], mean action: 1.000 [1.000, 1.000],  loss: 14097742.000000, mae: 1138.379028, mean_q: 1.062598, mean_eps: 0.325810
 683/1000: episode: 683, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3389.750, mean reward: -3389.750 [-3389.750, -3389.750], mean action: 1.000 [1.000, 1.000],  loss: 13837573.000000, mae: 1146.175781, mean_q: 1.051741, mean_eps: 0.324820
 684/1000: episode: 684, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3421.681, mean reward: -3421.681 [-3421.681, -3421.681], mean action: 1.000 [1.000, 1.000],  loss: 20264682.000000, mae: 1327.553223, mean_q: 1.034499, mean_eps: 0.323830
 685/1000: episode: 685, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8246.058, mean reward: -8246.058 [-8246.058, -8246.058], mean action: 1.000 [1.000, 1.000],  loss: 21091094.000000, mae: 1352.133301, mean_q: 1.018904, mean_eps: 0.322840
 686/1000: episode: 686, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2457.170, mean reward: -2457.170 [-2457.170, -2457.170], mean action: 1.000 [1.000, 1.000],  loss: 14410041.000000, mae: 1119.671143, mean_q: 1.018236, mean_eps: 0.321850
 687/1000: episode: 687, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2852.485, mean reward: -2852.485 [-2852.485, -2852.485], mean action: 1.000 [1.000, 1.000],  loss: 17954828.000000, mae: 1323.683838, mean_q: 1.015972, mean_eps: 0.320860
 688/1000: episode: 688, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5112.768, mean reward: -5112.768 [-5112.768, -5112.768], mean action: 1.000 [1.000, 1.000],  loss: 16201522.000000, mae: 1193.507812, mean_q: 0.995708, mean_eps: 0.319870
 689/1000: episode: 689, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -208.967, mean reward: -208.967 [-208.967, -208.967], mean action: 1.000 [1.000, 1.000],  loss: 12290262.000000, mae: 1021.101929, mean_q: 0.998136, mean_eps: 0.318880
 690/1000: episode: 690, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1975.747, mean reward: -1975.747 [-1975.747, -1975.747], mean action: 1.000 [1.000, 1.000],  loss: 12636437.000000, mae: 1021.085876, mean_q: 0.979131, mean_eps: 0.317890
 691/1000: episode: 691, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5035.764, mean reward: -5035.764 [-5035.764, -5035.764], mean action: 3.000 [3.000, 3.000],  loss: 14611116.000000, mae: 1122.713745, mean_q: 0.959930, mean_eps: 0.316900
 692/1000: episode: 692, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5376.022, mean reward: -5376.022 [-5376.022, -5376.022], mean action: 1.000 [1.000, 1.000],  loss: 12048681.000000, mae: 1026.122314, mean_q: 0.936514, mean_eps: 0.315910
 693/1000: episode: 693, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -926.550, mean reward: -926.550 [-926.550, -926.550], mean action: 1.000 [1.000, 1.000],  loss: 7782887.500000, mae: 879.719727, mean_q: 0.925911, mean_eps: 0.314920
 694/1000: episode: 694, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1918.466, mean reward: -1918.466 [-1918.466, -1918.466], mean action: 3.000 [3.000, 3.000],  loss: 16968850.000000, mae: 1139.857178, mean_q: 0.907131, mean_eps: 0.313930
 695/1000: episode: 695, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -275.562, mean reward: -275.562 [-275.562, -275.562], mean action: 3.000 [3.000, 3.000],  loss: 18860656.000000, mae: 1299.215454, mean_q: 0.880288, mean_eps: 0.312940
 696/1000: episode: 696, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4233.329, mean reward: -4233.329 [-4233.329, -4233.329], mean action: 0.000 [0.000, 0.000],  loss: 13694051.000000, mae: 1062.563477, mean_q: 0.870243, mean_eps: 0.311950
 697/1000: episode: 697, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -7290.077, mean reward: -7290.077 [-7290.077, -7290.077], mean action: 1.000 [1.000, 1.000],  loss: 25720732.000000, mae: 1449.881836, mean_q: 0.851458, mean_eps: 0.310960
 698/1000: episode: 698, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -768.369, mean reward: -768.369 [-768.369, -768.369], mean action: 2.000 [2.000, 2.000],  loss: 19684206.000000, mae: 1208.337402, mean_q: 0.847262, mean_eps: 0.309970
 699/1000: episode: 699, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5359.757, mean reward: -5359.757 [-5359.757, -5359.757], mean action: 1.000 [1.000, 1.000],  loss: 12795386.000000, mae: 1060.027832, mean_q: 0.836017, mean_eps: 0.308980
 700/1000: episode: 700, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4002.801, mean reward: -4002.801 [-4002.801, -4002.801], mean action: 1.000 [1.000, 1.000],  loss: 19666098.000000, mae: 1236.582642, mean_q: 0.822949, mean_eps: 0.307990
 701/1000: episode: 701, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6380.207, mean reward: -6380.207 [-6380.207, -6380.207], mean action: 1.000 [1.000, 1.000],  loss: 16205554.000000, mae: 1139.068848, mean_q: 0.809590, mean_eps: 0.307000
 702/1000: episode: 702, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1185.349, mean reward: -1185.349 [-1185.349, -1185.349], mean action: 1.000 [1.000, 1.000],  loss: 12785285.000000, mae: 1005.933533, mean_q: 0.794584, mean_eps: 0.306010
 703/1000: episode: 703, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -4623.157, mean reward: -4623.157 [-4623.157, -4623.157], mean action: 1.000 [1.000, 1.000],  loss: 16537325.000000, mae: 1206.722412, mean_q: 0.780059, mean_eps: 0.305020
 704/1000: episode: 704, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5568.578, mean reward: -5568.578 [-5568.578, -5568.578], mean action: 0.000 [0.000, 0.000],  loss: 15909564.000000, mae: 1196.696655, mean_q: 0.767608, mean_eps: 0.304030
 705/1000: episode: 705, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3011.621, mean reward: -3011.621 [-3011.621, -3011.621], mean action: 3.000 [3.000, 3.000],  loss: 18151068.000000, mae: 1264.288452, mean_q: 0.745501, mean_eps: 0.303040
 706/1000: episode: 706, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3291.769, mean reward: -3291.769 [-3291.769, -3291.769], mean action: 3.000 [3.000, 3.000],  loss: 15121131.000000, mae: 1168.467896, mean_q: 0.734234, mean_eps: 0.302050
 707/1000: episode: 707, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8571.056, mean reward: -8571.056 [-8571.056, -8571.056], mean action: 1.000 [1.000, 1.000],  loss: 18518228.000000, mae: 1243.614502, mean_q: 0.707687, mean_eps: 0.301060
 708/1000: episode: 708, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1928.165, mean reward: -1928.165 [-1928.165, -1928.165], mean action: 1.000 [1.000, 1.000],  loss: 16567454.000000, mae: 1155.704346, mean_q: 0.693589, mean_eps: 0.300070
 709/1000: episode: 709, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5673.722, mean reward: -5673.722 [-5673.722, -5673.722], mean action: 1.000 [1.000, 1.000],  loss: 19770236.000000, mae: 1214.408447, mean_q: 0.671432, mean_eps: 0.299080
 710/1000: episode: 710, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -1265.805, mean reward: -1265.805 [-1265.805, -1265.805], mean action: 1.000 [1.000, 1.000],  loss: 26279800.000000, mae: 1516.548340, mean_q: 0.651873, mean_eps: 0.298090
 711/1000: episode: 711, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1360.279, mean reward: -1360.279 [-1360.279, -1360.279], mean action: 1.000 [1.000, 1.000],  loss: 17169220.000000, mae: 1242.524170, mean_q: 0.637684, mean_eps: 0.297100
 712/1000: episode: 712, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3500.355, mean reward: -3500.355 [-3500.355, -3500.355], mean action: 1.000 [1.000, 1.000],  loss: 17699020.000000, mae: 1210.987183, mean_q: 0.624439, mean_eps: 0.296110
 713/1000: episode: 713, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -980.646, mean reward: -980.646 [-980.646, -980.646], mean action: 3.000 [3.000, 3.000],  loss: 17765926.000000, mae: 1292.335938, mean_q: 0.609044, mean_eps: 0.295120
 714/1000: episode: 714, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4588.709, mean reward: -4588.709 [-4588.709, -4588.709], mean action: 1.000 [1.000, 1.000],  loss: 14798707.000000, mae: 1135.923828, mean_q: 0.595524, mean_eps: 0.294130
 715/1000: episode: 715, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -7105.745, mean reward: -7105.745 [-7105.745, -7105.745], mean action: 0.000 [0.000, 0.000],  loss: 19217102.000000, mae: 1246.674927, mean_q: 0.565447, mean_eps: 0.293140
 716/1000: episode: 716, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -6881.088, mean reward: -6881.088 [-6881.088, -6881.088], mean action: 1.000 [1.000, 1.000],  loss: 16664226.000000, mae: 1199.618164, mean_q: 0.558954, mean_eps: 0.292150
 717/1000: episode: 717, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -8323.044, mean reward: -8323.044 [-8323.044, -8323.044], mean action: 1.000 [1.000, 1.000],  loss: 16389549.000000, mae: 1226.345581, mean_q: 0.535786, mean_eps: 0.291160
 718/1000: episode: 718, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -671.185, mean reward: -671.185 [-671.185, -671.185], mean action: 3.000 [3.000, 3.000],  loss: 17415176.000000, mae: 1255.648193, mean_q: 0.514622, mean_eps: 0.290170
 719/1000: episode: 719, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4656.551, mean reward: -4656.551 [-4656.551, -4656.551], mean action: 1.000 [1.000, 1.000],  loss: 20239696.000000, mae: 1303.374756, mean_q: 0.492460, mean_eps: 0.289180
 720/1000: episode: 720, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1735.529, mean reward: -1735.529 [-1735.529, -1735.529], mean action: 1.000 [1.000, 1.000],  loss: 13730749.000000, mae: 1051.505615, mean_q: 0.467402, mean_eps: 0.288190
 721/1000: episode: 721, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4188.520, mean reward: -4188.520 [-4188.520, -4188.520], mean action: 1.000 [1.000, 1.000],  loss: 15817887.000000, mae: 1132.098755, mean_q: 0.445200, mean_eps: 0.287200
 722/1000: episode: 722, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4043.623, mean reward: -4043.623 [-4043.623, -4043.623], mean action: 1.000 [1.000, 1.000],  loss: 17378076.000000, mae: 1194.414062, mean_q: 0.411074, mean_eps: 0.286210
 723/1000: episode: 723, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5039.577, mean reward: -5039.577 [-5039.577, -5039.577], mean action: 1.000 [1.000, 1.000],  loss: 23014196.000000, mae: 1370.637085, mean_q: 0.377140, mean_eps: 0.285220
 724/1000: episode: 724, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3741.800, mean reward: -3741.800 [-3741.800, -3741.800], mean action: 1.000 [1.000, 1.000],  loss: 6193940.500000, mae: 699.725952, mean_q: 0.349515, mean_eps: 0.284230
 725/1000: episode: 725, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -12559.905, mean reward: -12559.905 [-12559.905, -12559.905], mean action: 1.000 [1.000, 1.000],  loss: 13914982.000000, mae: 1155.537842, mean_q: 0.310569, mean_eps: 0.283240
 726/1000: episode: 726, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5438.488, mean reward: -5438.488 [-5438.488, -5438.488], mean action: 1.000 [1.000, 1.000],  loss: 24638968.000000, mae: 1429.002563, mean_q: 0.278136, mean_eps: 0.282250
 727/1000: episode: 727, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -707.664, mean reward: -707.664 [-707.664, -707.664], mean action: 1.000 [1.000, 1.000],  loss: 14182868.000000, mae: 1104.490234, mean_q: 0.247050, mean_eps: 0.281260
 728/1000: episode: 728, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5463.689, mean reward: -5463.689 [-5463.689, -5463.689], mean action: 1.000 [1.000, 1.000],  loss: 18051690.000000, mae: 1243.336426, mean_q: 0.209226, mean_eps: 0.280270
 729/1000: episode: 729, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1595.415, mean reward: -1595.415 [-1595.415, -1595.415], mean action: 3.000 [3.000, 3.000],  loss: 17985186.000000, mae: 1117.499390, mean_q: 0.171624, mean_eps: 0.279280
 730/1000: episode: 730, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1147.332, mean reward: -1147.332 [-1147.332, -1147.332], mean action: 1.000 [1.000, 1.000],  loss: 20887560.000000, mae: 1276.078369, mean_q: 0.144585, mean_eps: 0.278290
 731/1000: episode: 731, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -10459.949, mean reward: -10459.949 [-10459.949, -10459.949], mean action: 1.000 [1.000, 1.000],  loss: 18269792.000000, mae: 1207.805298, mean_q: 0.112422, mean_eps: 0.277300
 732/1000: episode: 732, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3540.505, mean reward: -3540.505 [-3540.505, -3540.505], mean action: 1.000 [1.000, 1.000],  loss: 13250529.000000, mae: 1139.047241, mean_q: 0.088122, mean_eps: 0.276310
 733/1000: episode: 733, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4563.233, mean reward: -4563.233 [-4563.233, -4563.233], mean action: 1.000 [1.000, 1.000],  loss: 17707122.000000, mae: 1114.163452, mean_q: 0.057922, mean_eps: 0.275320
 734/1000: episode: 734, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -566.591, mean reward: -566.591 [-566.591, -566.591], mean action: 1.000 [1.000, 1.000],  loss: 21245658.000000, mae: 1387.850952, mean_q: 0.031673, mean_eps: 0.274330
 735/1000: episode: 735, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2366.688, mean reward: -2366.688 [-2366.688, -2366.688], mean action: 3.000 [3.000, 3.000],  loss: 17408220.000000, mae: 1312.432617, mean_q: 0.005330, mean_eps: 0.273340
 736/1000: episode: 736, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6750.842, mean reward: -6750.842 [-6750.842, -6750.842], mean action: 1.000 [1.000, 1.000],  loss: 22517422.000000, mae: 1315.633789, mean_q: -0.026552, mean_eps: 0.272350
 737/1000: episode: 737, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1757.378, mean reward: -1757.378 [-1757.378, -1757.378], mean action: 1.000 [1.000, 1.000],  loss: 13906808.000000, mae: 1107.794067, mean_q: -0.063167, mean_eps: 0.271360
 738/1000: episode: 738, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1297.514, mean reward: -1297.514 [-1297.514, -1297.514], mean action: 1.000 [1.000, 1.000],  loss: 17005824.000000, mae: 1279.186157, mean_q: -0.110879, mean_eps: 0.270370
 739/1000: episode: 739, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1996.380, mean reward: -1996.380 [-1996.380, -1996.380], mean action: 1.000 [1.000, 1.000],  loss: 19665880.000000, mae: 1316.318359, mean_q: -0.156920, mean_eps: 0.269380
 740/1000: episode: 740, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1594.941, mean reward: -1594.941 [-1594.941, -1594.941], mean action: 2.000 [2.000, 2.000],  loss: 18589404.000000, mae: 1241.773315, mean_q: -0.205034, mean_eps: 0.268390
 741/1000: episode: 741, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1124.066, mean reward: -1124.066 [-1124.066, -1124.066], mean action: 1.000 [1.000, 1.000],  loss: 16956044.000000, mae: 1186.713013, mean_q: -0.246953, mean_eps: 0.267400
 742/1000: episode: 742, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -453.380, mean reward: -453.380 [-453.380, -453.380], mean action: 1.000 [1.000, 1.000],  loss: 17823776.000000, mae: 1285.876221, mean_q: -0.292497, mean_eps: 0.266410
 743/1000: episode: 743, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3636.538, mean reward: -3636.538 [-3636.538, -3636.538], mean action: 3.000 [3.000, 3.000],  loss: 14890444.000000, mae: 1129.949219, mean_q: -0.326223, mean_eps: 0.265420
 744/1000: episode: 744, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -3190.412, mean reward: -3190.412 [-3190.412, -3190.412], mean action: 3.000 [3.000, 3.000],  loss: 15430712.000000, mae: 1171.696289, mean_q: -0.361936, mean_eps: 0.264430
 745/1000: episode: 745, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -457.834, mean reward: -457.834 [-457.834, -457.834], mean action: 1.000 [1.000, 1.000],  loss: 20238410.000000, mae: 1225.562988, mean_q: -0.401808, mean_eps: 0.263440
 746/1000: episode: 746, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3402.873, mean reward: -3402.873 [-3402.873, -3402.873], mean action: 1.000 [1.000, 1.000],  loss: 23187188.000000, mae: 1511.290161, mean_q: -0.438281, mean_eps: 0.262450
 747/1000: episode: 747, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2747.534, mean reward: -2747.534 [-2747.534, -2747.534], mean action: 1.000 [1.000, 1.000],  loss: 21392960.000000, mae: 1292.831177, mean_q: -0.477715, mean_eps: 0.261460
 748/1000: episode: 748, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7905.033, mean reward: -7905.033 [-7905.033, -7905.033], mean action: 1.000 [1.000, 1.000],  loss: 14916862.000000, mae: 1167.255005, mean_q: -0.520114, mean_eps: 0.260470
 749/1000: episode: 749, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4786.737, mean reward: -4786.737 [-4786.737, -4786.737], mean action: 1.000 [1.000, 1.000],  loss: 22425544.000000, mae: 1436.129395, mean_q: -0.561541, mean_eps: 0.259480
 750/1000: episode: 750, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2005.336, mean reward: -2005.336 [-2005.336, -2005.336], mean action: 1.000 [1.000, 1.000],  loss: 18849592.000000, mae: 1270.268066, mean_q: -0.601048, mean_eps: 0.258490
 751/1000: episode: 751, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3995.767, mean reward: -3995.767 [-3995.767, -3995.767], mean action: 1.000 [1.000, 1.000],  loss: 13113425.000000, mae: 1103.111450, mean_q: -0.638776, mean_eps: 0.257500
 752/1000: episode: 752, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -925.772, mean reward: -925.772 [-925.772, -925.772], mean action: 1.000 [1.000, 1.000],  loss: 14385117.000000, mae: 1078.161865, mean_q: -0.680559, mean_eps: 0.256510
 753/1000: episode: 753, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7719.240, mean reward: -7719.240 [-7719.240, -7719.240], mean action: 0.000 [0.000, 0.000],  loss: 14484318.000000, mae: 1176.897461, mean_q: -0.723403, mean_eps: 0.255520
 754/1000: episode: 754, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1998.056, mean reward: -1998.056 [-1998.056, -1998.056], mean action: 1.000 [1.000, 1.000],  loss: 18323192.000000, mae: 1272.161377, mean_q: -0.775111, mean_eps: 0.254530
 755/1000: episode: 755, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3424.554, mean reward: -3424.554 [-3424.554, -3424.554], mean action: 1.000 [1.000, 1.000],  loss: 14927798.000000, mae: 1108.943359, mean_q: -0.822098, mean_eps: 0.253540
 756/1000: episode: 756, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -677.212, mean reward: -677.212 [-677.212, -677.212], mean action: 1.000 [1.000, 1.000],  loss: 14663898.000000, mae: 1187.797363, mean_q: -0.864743, mean_eps: 0.252550
 757/1000: episode: 757, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2706.102, mean reward: -2706.102 [-2706.102, -2706.102], mean action: 0.000 [0.000, 0.000],  loss: 17156070.000000, mae: 1222.542969, mean_q: -0.911336, mean_eps: 0.251560
 758/1000: episode: 758, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3224.453, mean reward: -3224.453 [-3224.453, -3224.453], mean action: 2.000 [2.000, 2.000],  loss: 23604544.000000, mae: 1472.868774, mean_q: -0.959317, mean_eps: 0.250570
 759/1000: episode: 759, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2293.686, mean reward: -2293.686 [-2293.686, -2293.686], mean action: 1.000 [1.000, 1.000],  loss: 16686797.000000, mae: 1266.922119, mean_q: -1.010807, mean_eps: 0.249580
 760/1000: episode: 760, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -203.912, mean reward: -203.912 [-203.912, -203.912], mean action: 1.000 [1.000, 1.000],  loss: 13348088.000000, mae: 1106.151855, mean_q: -1.060713, mean_eps: 0.248590
 761/1000: episode: 761, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -42.153, mean reward: -42.153 [-42.153, -42.153], mean action: 2.000 [2.000, 2.000],  loss: 13503959.000000, mae: 1097.270508, mean_q: -1.109634, mean_eps: 0.247600
 762/1000: episode: 762, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2168.580, mean reward: -2168.580 [-2168.580, -2168.580], mean action: 1.000 [1.000, 1.000],  loss: 15497044.000000, mae: 1136.040527, mean_q: -1.163843, mean_eps: 0.246610
 763/1000: episode: 763, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2906.122, mean reward: -2906.122 [-2906.122, -2906.122], mean action: 1.000 [1.000, 1.000],  loss: 11349848.000000, mae: 993.278076, mean_q: -1.207843, mean_eps: 0.245620
 764/1000: episode: 764, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -854.457, mean reward: -854.457 [-854.457, -854.457], mean action: 1.000 [1.000, 1.000],  loss: 11645086.000000, mae: 970.770874, mean_q: -1.276047, mean_eps: 0.244630
 765/1000: episode: 765, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8377.198, mean reward: -8377.198 [-8377.198, -8377.198], mean action: 3.000 [3.000, 3.000],  loss: 9514172.000000, mae: 875.635254, mean_q: -1.337882, mean_eps: 0.243640
 766/1000: episode: 766, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1044.068, mean reward: -1044.068 [-1044.068, -1044.068], mean action: 1.000 [1.000, 1.000],  loss: 16539178.000000, mae: 1229.715820, mean_q: -1.391301, mean_eps: 0.242650
 767/1000: episode: 767, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1510.317, mean reward: -1510.317 [-1510.317, -1510.317], mean action: 1.000 [1.000, 1.000],  loss: 18413884.000000, mae: 1278.046875, mean_q: -1.453402, mean_eps: 0.241660
 768/1000: episode: 768, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4090.130, mean reward: -4090.130 [-4090.130, -4090.130], mean action: 1.000 [1.000, 1.000],  loss: 15834718.000000, mae: 1160.497192, mean_q: -1.519557, mean_eps: 0.240670
 769/1000: episode: 769, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -1131.901, mean reward: -1131.901 [-1131.901, -1131.901], mean action: 2.000 [2.000, 2.000],  loss: 21653072.000000, mae: 1268.594971, mean_q: -1.585855, mean_eps: 0.239680
 770/1000: episode: 770, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -364.399, mean reward: -364.399 [-364.399, -364.399], mean action: 1.000 [1.000, 1.000],  loss: 14208728.000000, mae: 1168.926514, mean_q: -1.650470, mean_eps: 0.238690
 771/1000: episode: 771, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4306.819, mean reward: -4306.819 [-4306.819, -4306.819], mean action: 1.000 [1.000, 1.000],  loss: 18734748.000000, mae: 1189.176392, mean_q: -1.719465, mean_eps: 0.237700
 772/1000: episode: 772, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -420.961, mean reward: -420.961 [-420.961, -420.961], mean action: 2.000 [2.000, 2.000],  loss: 12574603.000000, mae: 1041.078369, mean_q: -1.782450, mean_eps: 0.236710
 773/1000: episode: 773, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1668.565, mean reward: -1668.565 [-1668.565, -1668.565], mean action: 1.000 [1.000, 1.000],  loss: 18974132.000000, mae: 1305.004517, mean_q: -1.842106, mean_eps: 0.235720
 774/1000: episode: 774, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -6552.479, mean reward: -6552.479 [-6552.479, -6552.479], mean action: 1.000 [1.000, 1.000],  loss: 16730468.000000, mae: 1245.703613, mean_q: -1.909065, mean_eps: 0.234730
 775/1000: episode: 775, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6490.119, mean reward: -6490.119 [-6490.119, -6490.119], mean action: 1.000 [1.000, 1.000],  loss: 14223550.000000, mae: 1148.646973, mean_q: -1.967738, mean_eps: 0.233740
 776/1000: episode: 776, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3027.172, mean reward: -3027.172 [-3027.172, -3027.172], mean action: 1.000 [1.000, 1.000],  loss: 18067304.000000, mae: 1288.502808, mean_q: -2.025636, mean_eps: 0.232750
 777/1000: episode: 777, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5078.347, mean reward: -5078.347 [-5078.347, -5078.347], mean action: 1.000 [1.000, 1.000],  loss: 14229136.000000, mae: 1086.473022, mean_q: -2.102599, mean_eps: 0.231760
 778/1000: episode: 778, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2398.487, mean reward: -2398.487 [-2398.487, -2398.487], mean action: 1.000 [1.000, 1.000],  loss: 17169342.000000, mae: 1286.652832, mean_q: -2.170197, mean_eps: 0.230770
 779/1000: episode: 779, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -527.698, mean reward: -527.698 [-527.698, -527.698], mean action: 1.000 [1.000, 1.000],  loss: 18565826.000000, mae: 1316.715088, mean_q: -2.249066, mean_eps: 0.229780
 780/1000: episode: 780, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -805.237, mean reward: -805.237 [-805.237, -805.237], mean action: 1.000 [1.000, 1.000],  loss: 9791369.000000, mae: 939.413330, mean_q: -2.331692, mean_eps: 0.228790
 781/1000: episode: 781, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7413.298, mean reward: -7413.298 [-7413.298, -7413.298], mean action: 1.000 [1.000, 1.000],  loss: 24713100.000000, mae: 1406.813965, mean_q: -2.407843, mean_eps: 0.227800
 782/1000: episode: 782, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6102.763, mean reward: -6102.763 [-6102.763, -6102.763], mean action: 1.000 [1.000, 1.000],  loss: 17883964.000000, mae: 1246.329590, mean_q: -2.487892, mean_eps: 0.226810
 783/1000: episode: 783, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9262.486, mean reward: -9262.486 [-9262.486, -9262.486], mean action: 1.000 [1.000, 1.000],  loss: 14561030.000000, mae: 1097.429932, mean_q: -2.565341, mean_eps: 0.225820
 784/1000: episode: 784, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -912.211, mean reward: -912.211 [-912.211, -912.211], mean action: 2.000 [2.000, 2.000],  loss: 19821044.000000, mae: 1346.478760, mean_q: -2.644200, mean_eps: 0.224830
 785/1000: episode: 785, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1127.383, mean reward: -1127.383 [-1127.383, -1127.383], mean action: 1.000 [1.000, 1.000],  loss: 17467712.000000, mae: 1141.942261, mean_q: -2.746813, mean_eps: 0.223840
 786/1000: episode: 786, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -6935.661, mean reward: -6935.661 [-6935.661, -6935.661], mean action: 1.000 [1.000, 1.000],  loss: 16598431.000000, mae: 1168.926025, mean_q: -2.842408, mean_eps: 0.222850
 787/1000: episode: 787, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -736.348, mean reward: -736.348 [-736.348, -736.348], mean action: 1.000 [1.000, 1.000],  loss: 18088300.000000, mae: 1317.927490, mean_q: -2.935704, mean_eps: 0.221860
 788/1000: episode: 788, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3492.182, mean reward: -3492.182 [-3492.182, -3492.182], mean action: 3.000 [3.000, 3.000],  loss: 22950536.000000, mae: 1294.799561, mean_q: -3.010072, mean_eps: 0.220870
 789/1000: episode: 789, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2710.527, mean reward: -2710.527 [-2710.527, -2710.527], mean action: 1.000 [1.000, 1.000],  loss: 15979368.000000, mae: 1247.564697, mean_q: -3.099464, mean_eps: 0.219880
 790/1000: episode: 790, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5366.090, mean reward: -5366.090 [-5366.090, -5366.090], mean action: 1.000 [1.000, 1.000],  loss: 15147978.000000, mae: 1126.778076, mean_q: -3.189227, mean_eps: 0.218890
 791/1000: episode: 791, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4347.922, mean reward: -4347.922 [-4347.922, -4347.922], mean action: 1.000 [1.000, 1.000],  loss: 19649282.000000, mae: 1314.223145, mean_q: -3.264675, mean_eps: 0.217900
 792/1000: episode: 792, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6327.288, mean reward: -6327.288 [-6327.288, -6327.288], mean action: 1.000 [1.000, 1.000],  loss: 10085830.000000, mae: 912.099854, mean_q: -3.363188, mean_eps: 0.216910
 793/1000: episode: 793, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -7841.858, mean reward: -7841.858 [-7841.858, -7841.858], mean action: 1.000 [1.000, 1.000],  loss: 17884336.000000, mae: 1178.375244, mean_q: -3.450595, mean_eps: 0.215920
 794/1000: episode: 794, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -3932.885, mean reward: -3932.885 [-3932.885, -3932.885], mean action: 1.000 [1.000, 1.000],  loss: 19315952.000000, mae: 1350.516602, mean_q: -3.537281, mean_eps: 0.214930
 795/1000: episode: 795, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2396.507, mean reward: -2396.507 [-2396.507, -2396.507], mean action: 2.000 [2.000, 2.000],  loss: 16131425.000000, mae: 1095.909302, mean_q: -3.673449, mean_eps: 0.213940
 796/1000: episode: 796, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3980.530, mean reward: -3980.530 [-3980.530, -3980.530], mean action: 1.000 [1.000, 1.000],  loss: 9117637.000000, mae: 879.660767, mean_q: -3.782249, mean_eps: 0.212950
 797/1000: episode: 797, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3416.283, mean reward: -3416.283 [-3416.283, -3416.283], mean action: 2.000 [2.000, 2.000],  loss: 16350788.000000, mae: 1199.898315, mean_q: -3.866388, mean_eps: 0.211960
 798/1000: episode: 798, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -10882.648, mean reward: -10882.648 [-10882.648, -10882.648], mean action: 1.000 [1.000, 1.000],  loss: 14361427.000000, mae: 1143.173584, mean_q: -3.951420, mean_eps: 0.210970
 799/1000: episode: 799, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5117.678, mean reward: -5117.678 [-5117.678, -5117.678], mean action: 1.000 [1.000, 1.000],  loss: 23714386.000000, mae: 1447.665283, mean_q: -4.050094, mean_eps: 0.209980
 800/1000: episode: 800, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -9844.095, mean reward: -9844.095 [-9844.095, -9844.095], mean action: 3.000 [3.000, 3.000],  loss: 15218878.000000, mae: 1175.183594, mean_q: -4.116482, mean_eps: 0.208990
 801/1000: episode: 801, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5004.968, mean reward: -5004.968 [-5004.968, -5004.968], mean action: 1.000 [1.000, 1.000],  loss: 12882674.000000, mae: 1012.539185, mean_q: -4.254470, mean_eps: 0.208000
 802/1000: episode: 802, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1172.947, mean reward: -1172.947 [-1172.947, -1172.947], mean action: 1.000 [1.000, 1.000],  loss: 13773220.000000, mae: 1094.270874, mean_q: -4.343587, mean_eps: 0.207010
 803/1000: episode: 803, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4807.029, mean reward: -4807.029 [-4807.029, -4807.029], mean action: 1.000 [1.000, 1.000],  loss: 14292472.000000, mae: 1102.082642, mean_q: -4.461008, mean_eps: 0.206020
 804/1000: episode: 804, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8384.352, mean reward: -8384.352 [-8384.352, -8384.352], mean action: 1.000 [1.000, 1.000],  loss: 14343272.000000, mae: 1145.263672, mean_q: -4.555094, mean_eps: 0.205030
 805/1000: episode: 805, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1759.938, mean reward: -1759.938 [-1759.938, -1759.938], mean action: 1.000 [1.000, 1.000],  loss: 19672220.000000, mae: 1336.153320, mean_q: -4.683331, mean_eps: 0.204040
 806/1000: episode: 806, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3032.043, mean reward: -3032.043 [-3032.043, -3032.043], mean action: 1.000 [1.000, 1.000],  loss: 9413638.000000, mae: 876.825073, mean_q: -4.781303, mean_eps: 0.203050
 807/1000: episode: 807, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2376.136, mean reward: -2376.136 [-2376.136, -2376.136], mean action: 1.000 [1.000, 1.000],  loss: 17165948.000000, mae: 1212.183594, mean_q: -4.905618, mean_eps: 0.202060
 808/1000: episode: 808, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -626.432, mean reward: -626.432 [-626.432, -626.432], mean action: 1.000 [1.000, 1.000],  loss: 15035917.000000, mae: 1211.277832, mean_q: -5.058922, mean_eps: 0.201070
 809/1000: episode: 809, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6444.367, mean reward: -6444.367 [-6444.367, -6444.367], mean action: 1.000 [1.000, 1.000],  loss: 20350644.000000, mae: 1374.609131, mean_q: -5.124742, mean_eps: 0.200080
 810/1000: episode: 810, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6961.436, mean reward: -6961.436 [-6961.436, -6961.436], mean action: 1.000 [1.000, 1.000],  loss: 18487778.000000, mae: 1291.551270, mean_q: -5.228463, mean_eps: 0.199090
 811/1000: episode: 811, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6915.076, mean reward: -6915.076 [-6915.076, -6915.076], mean action: 3.000 [3.000, 3.000],  loss: 16285186.000000, mae: 1209.097900, mean_q: -5.347632, mean_eps: 0.198100
 812/1000: episode: 812, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -2756.805, mean reward: -2756.805 [-2756.805, -2756.805], mean action: 1.000 [1.000, 1.000],  loss: 22397732.000000, mae: 1458.806152, mean_q: -5.450524, mean_eps: 0.197110
 813/1000: episode: 813, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2474.018, mean reward: -2474.018 [-2474.018, -2474.018], mean action: 1.000 [1.000, 1.000],  loss: 15036349.000000, mae: 1076.791260, mean_q: -5.595773, mean_eps: 0.196120
 814/1000: episode: 814, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -1658.796, mean reward: -1658.796 [-1658.796, -1658.796], mean action: 1.000 [1.000, 1.000],  loss: 18564860.000000, mae: 1219.929199, mean_q: -5.699323, mean_eps: 0.195130
 815/1000: episode: 815, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7319.290, mean reward: -7319.290 [-7319.290, -7319.290], mean action: 1.000 [1.000, 1.000],  loss: 10425461.000000, mae: 1005.195862, mean_q: -5.837542, mean_eps: 0.194140
 816/1000: episode: 816, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3821.649, mean reward: -3821.649 [-3821.649, -3821.649], mean action: 1.000 [1.000, 1.000],  loss: 14147310.000000, mae: 1079.754639, mean_q: -5.959893, mean_eps: 0.193150
 817/1000: episode: 817, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -4252.919, mean reward: -4252.919 [-4252.919, -4252.919], mean action: 1.000 [1.000, 1.000],  loss: 17652284.000000, mae: 1234.774414, mean_q: -6.034449, mean_eps: 0.192160
 818/1000: episode: 818, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -6370.202, mean reward: -6370.202 [-6370.202, -6370.202], mean action: 1.000 [1.000, 1.000],  loss: 20949708.000000, mae: 1372.144531, mean_q: -6.153299, mean_eps: 0.191170
 819/1000: episode: 819, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -8580.217, mean reward: -8580.217 [-8580.217, -8580.217], mean action: 1.000 [1.000, 1.000],  loss: 14712430.000000, mae: 1052.728516, mean_q: -6.278917, mean_eps: 0.190180
 820/1000: episode: 820, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -4877.763, mean reward: -4877.763 [-4877.763, -4877.763], mean action: 1.000 [1.000, 1.000],  loss: 17872344.000000, mae: 1136.323730, mean_q: -6.432922, mean_eps: 0.189190
 821/1000: episode: 821, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -8687.509, mean reward: -8687.509 [-8687.509, -8687.509], mean action: 1.000 [1.000, 1.000],  loss: 13133058.000000, mae: 1047.987549, mean_q: -6.546474, mean_eps: 0.188200
 822/1000: episode: 822, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -8894.483, mean reward: -8894.483 [-8894.483, -8894.483], mean action: 1.000 [1.000, 1.000],  loss: 17918962.000000, mae: 1234.140137, mean_q: -6.646839, mean_eps: 0.187210
 823/1000: episode: 823, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5566.097, mean reward: -5566.097 [-5566.097, -5566.097], mean action: 1.000 [1.000, 1.000],  loss: 15061909.000000, mae: 1212.401733, mean_q: -6.786395, mean_eps: 0.186220
 824/1000: episode: 824, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1496.242, mean reward: -1496.242 [-1496.242, -1496.242], mean action: 1.000 [1.000, 1.000],  loss: 12809830.000000, mae: 1076.152466, mean_q: -6.913054, mean_eps: 0.185230
 825/1000: episode: 825, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -9640.866, mean reward: -9640.866 [-9640.866, -9640.866], mean action: 1.000 [1.000, 1.000],  loss: 17170346.000000, mae: 1234.455322, mean_q: -7.032661, mean_eps: 0.184240
 826/1000: episode: 826, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1795.010, mean reward: -1795.010 [-1795.010, -1795.010], mean action: 2.000 [2.000, 2.000],  loss: 15729888.000000, mae: 1144.475098, mean_q: -7.156808, mean_eps: 0.183250
 827/1000: episode: 827, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4761.455, mean reward: -4761.455 [-4761.455, -4761.455], mean action: 1.000 [1.000, 1.000],  loss: 11397175.000000, mae: 973.537720, mean_q: -7.294605, mean_eps: 0.182260
 828/1000: episode: 828, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6791.142, mean reward: -6791.142 [-6791.142, -6791.142], mean action: 0.000 [0.000, 0.000],  loss: 16607671.000000, mae: 1203.654053, mean_q: -7.403748, mean_eps: 0.181270
 829/1000: episode: 829, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3347.456, mean reward: -3347.456 [-3347.456, -3347.456], mean action: 1.000 [1.000, 1.000],  loss: 17919726.000000, mae: 1219.937134, mean_q: -7.516789, mean_eps: 0.180280
 830/1000: episode: 830, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -3192.984, mean reward: -3192.984 [-3192.984, -3192.984], mean action: 1.000 [1.000, 1.000],  loss: 18564080.000000, mae: 1264.610352, mean_q: -7.699663, mean_eps: 0.179290
 831/1000: episode: 831, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4522.739, mean reward: -4522.739 [-4522.739, -4522.739], mean action: 1.000 [1.000, 1.000],  loss: 18095266.000000, mae: 1317.884155, mean_q: -7.788405, mean_eps: 0.178300
 832/1000: episode: 832, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1815.186, mean reward: -1815.186 [-1815.186, -1815.186], mean action: 2.000 [2.000, 2.000],  loss: 22754510.000000, mae: 1422.598389, mean_q: -7.914726, mean_eps: 0.177310
 833/1000: episode: 833, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -897.047, mean reward: -897.047 [-897.047, -897.047], mean action: 1.000 [1.000, 1.000],  loss: 17109530.000000, mae: 1222.177856, mean_q: -8.033213, mean_eps: 0.176320
 834/1000: episode: 834, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5142.341, mean reward: -5142.341 [-5142.341, -5142.341], mean action: 1.000 [1.000, 1.000],  loss: 14760282.000000, mae: 1113.067383, mean_q: -8.222790, mean_eps: 0.175330
 835/1000: episode: 835, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2729.952, mean reward: -2729.952 [-2729.952, -2729.952], mean action: 1.000 [1.000, 1.000],  loss: 14334600.000000, mae: 1131.761719, mean_q: -8.318293, mean_eps: 0.174340
 836/1000: episode: 836, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3436.460, mean reward: -3436.460 [-3436.460, -3436.460], mean action: 1.000 [1.000, 1.000],  loss: 16722306.000000, mae: 1259.287598, mean_q: -8.495071, mean_eps: 0.173350
 837/1000: episode: 837, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2137.569, mean reward: -2137.569 [-2137.569, -2137.569], mean action: 1.000 [1.000, 1.000],  loss: 10681864.000000, mae: 941.805298, mean_q: -8.661838, mean_eps: 0.172360
 838/1000: episode: 838, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1781.732, mean reward: -1781.732 [-1781.732, -1781.732], mean action: 1.000 [1.000, 1.000],  loss: 22458956.000000, mae: 1348.310303, mean_q: -8.762162, mean_eps: 0.171370
 839/1000: episode: 839, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1205.399, mean reward: -1205.399 [-1205.399, -1205.399], mean action: 1.000 [1.000, 1.000],  loss: 16002168.000000, mae: 1129.066284, mean_q: -8.978102, mean_eps: 0.170380
 840/1000: episode: 840, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7820.756, mean reward: -7820.756 [-7820.756, -7820.756], mean action: 1.000 [1.000, 1.000],  loss: 18409692.000000, mae: 1260.003906, mean_q: -9.157591, mean_eps: 0.169390
 841/1000: episode: 841, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2543.661, mean reward: -2543.661 [-2543.661, -2543.661], mean action: 1.000 [1.000, 1.000],  loss: 12622967.000000, mae: 1073.768188, mean_q: -9.307493, mean_eps: 0.168400
 842/1000: episode: 842, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1881.136, mean reward: -1881.136 [-1881.136, -1881.136], mean action: 1.000 [1.000, 1.000],  loss: 12425082.000000, mae: 1039.451294, mean_q: -9.492788, mean_eps: 0.167410
 843/1000: episode: 843, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4182.698, mean reward: -4182.698 [-4182.698, -4182.698], mean action: 1.000 [1.000, 1.000],  loss: 21559060.000000, mae: 1346.725342, mean_q: -9.628333, mean_eps: 0.166420
 844/1000: episode: 844, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5106.870, mean reward: -5106.870 [-5106.870, -5106.870], mean action: 1.000 [1.000, 1.000],  loss: 15768066.000000, mae: 1207.859863, mean_q: -9.811561, mean_eps: 0.165430
 845/1000: episode: 845, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5617.388, mean reward: -5617.388 [-5617.388, -5617.388], mean action: 1.000 [1.000, 1.000],  loss: 11364480.000000, mae: 1049.581665, mean_q: -9.998018, mean_eps: 0.164440
 846/1000: episode: 846, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -7878.999, mean reward: -7878.999 [-7878.999, -7878.999], mean action: 1.000 [1.000, 1.000],  loss: 21119968.000000, mae: 1375.792725, mean_q: -10.141394, mean_eps: 0.163450
 847/1000: episode: 847, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8059.416, mean reward: -8059.416 [-8059.416, -8059.416], mean action: 1.000 [1.000, 1.000],  loss: 18786436.000000, mae: 1288.406128, mean_q: -10.314928, mean_eps: 0.162460
 848/1000: episode: 848, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6216.815, mean reward: -6216.815 [-6216.815, -6216.815], mean action: 1.000 [1.000, 1.000],  loss: 17567448.000000, mae: 1218.719971, mean_q: -10.491706, mean_eps: 0.161470
 849/1000: episode: 849, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2837.703, mean reward: -2837.703 [-2837.703, -2837.703], mean action: 1.000 [1.000, 1.000],  loss: 16921820.000000, mae: 1204.946167, mean_q: -10.639553, mean_eps: 0.160480
 850/1000: episode: 850, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4769.255, mean reward: -4769.255 [-4769.255, -4769.255], mean action: 1.000 [1.000, 1.000],  loss: 16392184.000000, mae: 1179.216675, mean_q: -10.949636, mean_eps: 0.159490
 851/1000: episode: 851, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -4352.269, mean reward: -4352.269 [-4352.269, -4352.269], mean action: 1.000 [1.000, 1.000],  loss: 17578044.000000, mae: 1184.952515, mean_q: -11.079894, mean_eps: 0.158500
 852/1000: episode: 852, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4889.837, mean reward: -4889.837 [-4889.837, -4889.837], mean action: 1.000 [1.000, 1.000],  loss: 13023372.000000, mae: 1054.107178, mean_q: -11.281288, mean_eps: 0.157510
 853/1000: episode: 853, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7553.600, mean reward: -7553.600 [-7553.600, -7553.600], mean action: 1.000 [1.000, 1.000],  loss: 16393250.000000, mae: 1233.981323, mean_q: -11.491007, mean_eps: 0.156520
 854/1000: episode: 854, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2050.590, mean reward: -2050.590 [-2050.590, -2050.590], mean action: 3.000 [3.000, 3.000],  loss: 18786936.000000, mae: 1167.859375, mean_q: -11.797308, mean_eps: 0.155530
 855/1000: episode: 855, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -7107.162, mean reward: -7107.162 [-7107.162, -7107.162], mean action: 0.000 [0.000, 0.000],  loss: 13539665.000000, mae: 1084.823975, mean_q: -11.966179, mean_eps: 0.154540
 856/1000: episode: 856, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3010.220, mean reward: -3010.220 [-3010.220, -3010.220], mean action: 1.000 [1.000, 1.000],  loss: 17312188.000000, mae: 1216.832886, mean_q: -12.168940, mean_eps: 0.153550
 857/1000: episode: 857, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -3353.536, mean reward: -3353.536 [-3353.536, -3353.536], mean action: 1.000 [1.000, 1.000],  loss: 12736524.000000, mae: 1037.683105, mean_q: -12.365940, mean_eps: 0.152560
 858/1000: episode: 858, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1412.103, mean reward: -1412.103 [-1412.103, -1412.103], mean action: 1.000 [1.000, 1.000],  loss: 17598324.000000, mae: 1224.846191, mean_q: -12.621827, mean_eps: 0.151570
 859/1000: episode: 859, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5316.095, mean reward: -5316.095 [-5316.095, -5316.095], mean action: 1.000 [1.000, 1.000],  loss: 15168452.000000, mae: 1134.401245, mean_q: -12.817235, mean_eps: 0.150580
 860/1000: episode: 860, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -10030.139, mean reward: -10030.139 [-10030.139, -10030.139], mean action: 1.000 [1.000, 1.000],  loss: 22473630.000000, mae: 1423.099365, mean_q: -13.022427, mean_eps: 0.149590
 861/1000: episode: 861, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5614.830, mean reward: -5614.830 [-5614.830, -5614.830], mean action: 1.000 [1.000, 1.000],  loss: 15904680.000000, mae: 1160.178955, mean_q: -13.259565, mean_eps: 0.148600
 862/1000: episode: 862, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5565.815, mean reward: -5565.815 [-5565.815, -5565.815], mean action: 1.000 [1.000, 1.000],  loss: 13518226.000000, mae: 1107.672852, mean_q: -13.536713, mean_eps: 0.147610
 863/1000: episode: 863, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5411.512, mean reward: -5411.512 [-5411.512, -5411.512], mean action: 1.000 [1.000, 1.000],  loss: 13459642.000000, mae: 1026.368164, mean_q: -13.758395, mean_eps: 0.146620
 864/1000: episode: 864, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4855.633, mean reward: -4855.633 [-4855.633, -4855.633], mean action: 1.000 [1.000, 1.000],  loss: 14640090.000000, mae: 1132.428345, mean_q: -13.990351, mean_eps: 0.145630
 865/1000: episode: 865, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5641.117, mean reward: -5641.117 [-5641.117, -5641.117], mean action: 1.000 [1.000, 1.000],  loss: 10628341.000000, mae: 946.079712, mean_q: -14.318775, mean_eps: 0.144640
 866/1000: episode: 866, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5998.560, mean reward: -5998.560 [-5998.560, -5998.560], mean action: 1.000 [1.000, 1.000],  loss: 16026639.000000, mae: 1254.263062, mean_q: -14.481425, mean_eps: 0.143650
 867/1000: episode: 867, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3906.708, mean reward: -3906.708 [-3906.708, -3906.708], mean action: 1.000 [1.000, 1.000],  loss: 24698936.000000, mae: 1533.248291, mean_q: -14.673800, mean_eps: 0.142660
 868/1000: episode: 868, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -7838.854, mean reward: -7838.854 [-7838.854, -7838.854], mean action: 1.000 [1.000, 1.000],  loss: 9273574.000000, mae: 832.800110, mean_q: -14.938919, mean_eps: 0.141670
 869/1000: episode: 869, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3428.236, mean reward: -3428.236 [-3428.236, -3428.236], mean action: 3.000 [3.000, 3.000],  loss: 20005528.000000, mae: 1278.645264, mean_q: -15.141163, mean_eps: 0.140680
 870/1000: episode: 870, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -240.558, mean reward: -240.558 [-240.558, -240.558], mean action: 3.000 [3.000, 3.000],  loss: 12940150.000000, mae: 960.453247, mean_q: -15.467962, mean_eps: 0.139690
 871/1000: episode: 871, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8731.381, mean reward: -8731.381 [-8731.381, -8731.381], mean action: 1.000 [1.000, 1.000],  loss: 10984538.000000, mae: 1027.374756, mean_q: -15.727337, mean_eps: 0.138700
 872/1000: episode: 872, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5897.107, mean reward: -5897.107 [-5897.107, -5897.107], mean action: 1.000 [1.000, 1.000],  loss: 14117829.000000, mae: 1073.629883, mean_q: -15.907408, mean_eps: 0.137710
 873/1000: episode: 873, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1842.445, mean reward: -1842.445 [-1842.445, -1842.445], mean action: 1.000 [1.000, 1.000],  loss: 12256048.000000, mae: 964.956970, mean_q: -16.186998, mean_eps: 0.136720
 874/1000: episode: 874, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -793.578, mean reward: -793.578 [-793.578, -793.578], mean action: 2.000 [2.000, 2.000],  loss: 13980312.000000, mae: 1159.047363, mean_q: -16.456503, mean_eps: 0.135730
 875/1000: episode: 875, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6649.098, mean reward: -6649.098 [-6649.098, -6649.098], mean action: 1.000 [1.000, 1.000],  loss: 15758910.000000, mae: 1210.321045, mean_q: -16.602692, mean_eps: 0.134740
 876/1000: episode: 876, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -8332.746, mean reward: -8332.746 [-8332.746, -8332.746], mean action: 2.000 [2.000, 2.000],  loss: 18083484.000000, mae: 1315.518799, mean_q: -16.942219, mean_eps: 0.133750
 877/1000: episode: 877, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4516.549, mean reward: -4516.549 [-4516.549, -4516.549], mean action: 1.000 [1.000, 1.000],  loss: 12478340.000000, mae: 1079.428833, mean_q: -17.172577, mean_eps: 0.132760
 878/1000: episode: 878, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6889.242, mean reward: -6889.242 [-6889.242, -6889.242], mean action: 1.000 [1.000, 1.000],  loss: 16424090.000000, mae: 1233.208008, mean_q: -17.501617, mean_eps: 0.131770
 879/1000: episode: 879, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -6157.834, mean reward: -6157.834 [-6157.834, -6157.834], mean action: 1.000 [1.000, 1.000],  loss: 12414424.000000, mae: 1085.118652, mean_q: -17.784195, mean_eps: 0.130780
 880/1000: episode: 880, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -759.222, mean reward: -759.222 [-759.222, -759.222], mean action: 1.000 [1.000, 1.000],  loss: 15177355.000000, mae: 1189.948364, mean_q: -17.978958, mean_eps: 0.129790
 881/1000: episode: 881, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5056.392, mean reward: -5056.392 [-5056.392, -5056.392], mean action: 1.000 [1.000, 1.000],  loss: 18528700.000000, mae: 1268.504272, mean_q: -18.251129, mean_eps: 0.128800
 882/1000: episode: 882, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7479.680, mean reward: -7479.680 [-7479.680, -7479.680], mean action: 1.000 [1.000, 1.000],  loss: 14592863.000000, mae: 1183.416992, mean_q: -18.479670, mean_eps: 0.127810
 883/1000: episode: 883, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -8420.612, mean reward: -8420.612 [-8420.612, -8420.612], mean action: 1.000 [1.000, 1.000],  loss: 14507298.000000, mae: 1140.734619, mean_q: -18.820786, mean_eps: 0.126820
 884/1000: episode: 884, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5181.146, mean reward: -5181.146 [-5181.146, -5181.146], mean action: 1.000 [1.000, 1.000],  loss: 16021884.000000, mae: 1163.972778, mean_q: -19.056900, mean_eps: 0.125830
 885/1000: episode: 885, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4033.808, mean reward: -4033.808 [-4033.808, -4033.808], mean action: 1.000 [1.000, 1.000],  loss: 17164786.000000, mae: 1200.619629, mean_q: -19.311447, mean_eps: 0.124840
 886/1000: episode: 886, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5958.613, mean reward: -5958.613 [-5958.613, -5958.613], mean action: 1.000 [1.000, 1.000],  loss: 21698714.000000, mae: 1345.239746, mean_q: -19.503092, mean_eps: 0.123850
 887/1000: episode: 887, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2982.080, mean reward: -2982.080 [-2982.080, -2982.080], mean action: 1.000 [1.000, 1.000],  loss: 13369405.000000, mae: 1118.592651, mean_q: -19.947325, mean_eps: 0.122860
 888/1000: episode: 888, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -530.686, mean reward: -530.686 [-530.686, -530.686], mean action: 1.000 [1.000, 1.000],  loss: 10235160.000000, mae: 963.209473, mean_q: -20.337143, mean_eps: 0.121870
 889/1000: episode: 889, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4773.460, mean reward: -4773.460 [-4773.460, -4773.460], mean action: 1.000 [1.000, 1.000],  loss: 19727760.000000, mae: 1326.868896, mean_q: -20.497095, mean_eps: 0.120880
 890/1000: episode: 890, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -981.233, mean reward: -981.233 [-981.233, -981.233], mean action: 1.000 [1.000, 1.000],  loss: 10579652.000000, mae: 995.274170, mean_q: -20.792801, mean_eps: 0.119890
 891/1000: episode: 891, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3667.873, mean reward: -3667.873 [-3667.873, -3667.873], mean action: 1.000 [1.000, 1.000],  loss: 13980502.000000, mae: 1120.393799, mean_q: -21.089359, mean_eps: 0.118900
 892/1000: episode: 892, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6815.546, mean reward: -6815.546 [-6815.546, -6815.546], mean action: 1.000 [1.000, 1.000],  loss: 13152919.000000, mae: 1085.090576, mean_q: -21.388210, mean_eps: 0.117910
 893/1000: episode: 893, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1419.010, mean reward: -1419.010 [-1419.010, -1419.010], mean action: 1.000 [1.000, 1.000],  loss: 11977736.000000, mae: 1018.730103, mean_q: -21.644056, mean_eps: 0.116920
 894/1000: episode: 894, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1907.251, mean reward: -1907.251 [-1907.251, -1907.251], mean action: 1.000 [1.000, 1.000],  loss: 9545012.000000, mae: 876.554077, mean_q: -22.064304, mean_eps: 0.115930
 895/1000: episode: 895, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -1413.995, mean reward: -1413.995 [-1413.995, -1413.995], mean action: 1.000 [1.000, 1.000],  loss: 11827096.000000, mae: 986.403687, mean_q: -22.298033, mean_eps: 0.114940
 896/1000: episode: 896, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4271.715, mean reward: -4271.715 [-4271.715, -4271.715], mean action: 1.000 [1.000, 1.000],  loss: 17065494.000000, mae: 1329.347168, mean_q: -22.588736, mean_eps: 0.113950
 897/1000: episode: 897, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1228.165, mean reward: -1228.165 [-1228.165, -1228.165], mean action: 1.000 [1.000, 1.000],  loss: 15868887.000000, mae: 1215.470093, mean_q: -22.962822, mean_eps: 0.112960
 898/1000: episode: 898, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3172.142, mean reward: -3172.142 [-3172.142, -3172.142], mean action: 1.000 [1.000, 1.000],  loss: 10085056.000000, mae: 935.588806, mean_q: -23.382610, mean_eps: 0.111970
 899/1000: episode: 899, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -1081.283, mean reward: -1081.283 [-1081.283, -1081.283], mean action: 1.000 [1.000, 1.000],  loss: 13871851.000000, mae: 1170.119873, mean_q: -23.669102, mean_eps: 0.110980
 900/1000: episode: 900, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -9442.181, mean reward: -9442.181 [-9442.181, -9442.181], mean action: 1.000 [1.000, 1.000],  loss: 13819898.000000, mae: 1151.057739, mean_q: -24.014952, mean_eps: 0.109990
 901/1000: episode: 901, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5454.058, mean reward: -5454.058 [-5454.058, -5454.058], mean action: 1.000 [1.000, 1.000],  loss: 16400118.000000, mae: 1191.599854, mean_q: -24.355667, mean_eps: 0.109000
 902/1000: episode: 902, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4264.010, mean reward: -4264.010 [-4264.010, -4264.010], mean action: 1.000 [1.000, 1.000],  loss: 18900536.000000, mae: 1336.791870, mean_q: -24.546780, mean_eps: 0.108010
 903/1000: episode: 903, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -2924.057, mean reward: -2924.057 [-2924.057, -2924.057], mean action: 1.000 [1.000, 1.000],  loss: 9987674.000000, mae: 975.997070, mean_q: -25.145592, mean_eps: 0.107020
 904/1000: episode: 904, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1579.644, mean reward: -1579.644 [-1579.644, -1579.644], mean action: 2.000 [2.000, 2.000],  loss: 12372424.000000, mae: 1094.132568, mean_q: -25.382418, mean_eps: 0.106030
 905/1000: episode: 905, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4318.125, mean reward: -4318.125 [-4318.125, -4318.125], mean action: 1.000 [1.000, 1.000],  loss: 15828864.000000, mae: 1168.807861, mean_q: -25.637901, mean_eps: 0.105040
 906/1000: episode: 906, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5716.428, mean reward: -5716.428 [-5716.428, -5716.428], mean action: 0.000 [0.000, 0.000],  loss: 13727926.000000, mae: 1101.469482, mean_q: -25.937595, mean_eps: 0.104050
 907/1000: episode: 907, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9002.215, mean reward: -9002.215 [-9002.215, -9002.215], mean action: 0.000 [0.000, 0.000],  loss: 15292942.000000, mae: 1161.074707, mean_q: -26.141434, mean_eps: 0.103060
 908/1000: episode: 908, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3869.247, mean reward: -3869.247 [-3869.247, -3869.247], mean action: 0.000 [0.000, 0.000],  loss: 18657940.000000, mae: 1229.356201, mean_q: -26.245754, mean_eps: 0.102070
 909/1000: episode: 909, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -1700.002, mean reward: -1700.002 [-1700.002, -1700.002], mean action: 0.000 [0.000, 0.000],  loss: 18315576.000000, mae: 1263.590332, mean_q: -26.491209, mean_eps: 0.101080
 910/1000: episode: 910, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6702.696, mean reward: -6702.696 [-6702.696, -6702.696], mean action: 0.000 [0.000, 0.000],  loss: 11749158.000000, mae: 1013.651611, mean_q: -26.793846, mean_eps: 0.100090
 911/1000: episode: 911, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3713.660, mean reward: -3713.660 [-3713.660, -3713.660], mean action: 0.000 [0.000, 0.000],  loss: 14524040.000000, mae: 1161.234375, mean_q: -27.021011, mean_eps: 0.099100
 912/1000: episode: 912, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1716.780, mean reward: -1716.780 [-1716.780, -1716.780], mean action: 3.000 [3.000, 3.000],  loss: 15793640.000000, mae: 1141.110107, mean_q: -27.257298, mean_eps: 0.098110
 913/1000: episode: 913, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6556.764, mean reward: -6556.764 [-6556.764, -6556.764], mean action: 0.000 [0.000, 0.000],  loss: 12621037.000000, mae: 1102.932617, mean_q: -27.438765, mean_eps: 0.097120
 914/1000: episode: 914, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3673.180, mean reward: -3673.180 [-3673.180, -3673.180], mean action: 0.000 [0.000, 0.000],  loss: 20482194.000000, mae: 1367.763550, mean_q: -27.569056, mean_eps: 0.096130
 915/1000: episode: 915, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8909.808, mean reward: -8909.808 [-8909.808, -8909.808], mean action: 0.000 [0.000, 0.000],  loss: 12816344.000000, mae: 1134.369141, mean_q: -27.776482, mean_eps: 0.095140
 916/1000: episode: 916, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5108.144, mean reward: -5108.144 [-5108.144, -5108.144], mean action: 0.000 [0.000, 0.000],  loss: 16796408.000000, mae: 1217.877686, mean_q: -28.084438, mean_eps: 0.094150
 917/1000: episode: 917, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2836.380, mean reward: -2836.380 [-2836.380, -2836.380], mean action: 0.000 [0.000, 0.000],  loss: 13081531.000000, mae: 1116.458618, mean_q: -28.433998, mean_eps: 0.093160
 918/1000: episode: 918, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -13052.444, mean reward: -13052.444 [-13052.444, -13052.444], mean action: 0.000 [0.000, 0.000],  loss: 20227986.000000, mae: 1318.469238, mean_q: -28.523777, mean_eps: 0.092170
 919/1000: episode: 919, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2786.525, mean reward: -2786.525 [-2786.525, -2786.525], mean action: 0.000 [0.000, 0.000],  loss: 15201650.000000, mae: 1134.707031, mean_q: -28.821959, mean_eps: 0.091180
 920/1000: episode: 920, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5215.701, mean reward: -5215.701 [-5215.701, -5215.701], mean action: 0.000 [0.000, 0.000],  loss: 13886926.000000, mae: 1223.075317, mean_q: -28.955935, mean_eps: 0.090190
 921/1000: episode: 921, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -6115.874, mean reward: -6115.874 [-6115.874, -6115.874], mean action: 0.000 [0.000, 0.000],  loss: 16204829.000000, mae: 1251.116577, mean_q: -29.249920, mean_eps: 0.089200
 922/1000: episode: 922, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -7466.592, mean reward: -7466.592 [-7466.592, -7466.592], mean action: 0.000 [0.000, 0.000],  loss: 16817098.000000, mae: 1239.338623, mean_q: -29.461960, mean_eps: 0.088210
 923/1000: episode: 923, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -6330.283, mean reward: -6330.283 [-6330.283, -6330.283], mean action: 0.000 [0.000, 0.000],  loss: 17726848.000000, mae: 1300.017334, mean_q: -29.810898, mean_eps: 0.087220
 924/1000: episode: 924, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4246.922, mean reward: -4246.922 [-4246.922, -4246.922], mean action: 0.000 [0.000, 0.000],  loss: 7245160.000000, mae: 838.174927, mean_q: -29.992573, mean_eps: 0.086230
 925/1000: episode: 925, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6210.397, mean reward: -6210.397 [-6210.397, -6210.397], mean action: 0.000 [0.000, 0.000],  loss: 12543303.000000, mae: 1059.464722, mean_q: -30.269207, mean_eps: 0.085240
 926/1000: episode: 926, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6468.807, mean reward: -6468.807 [-6468.807, -6468.807], mean action: 0.000 [0.000, 0.000],  loss: 13486690.000000, mae: 1116.901123, mean_q: -30.487606, mean_eps: 0.084250
 927/1000: episode: 927, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4645.278, mean reward: -4645.278 [-4645.278, -4645.278], mean action: 0.000 [0.000, 0.000],  loss: 14431193.000000, mae: 1137.035889, mean_q: -30.855347, mean_eps: 0.083260
 928/1000: episode: 928, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8948.846, mean reward: -8948.846 [-8948.846, -8948.846], mean action: 0.000 [0.000, 0.000],  loss: 14559022.000000, mae: 1197.996826, mean_q: -31.042410, mean_eps: 0.082270
 929/1000: episode: 929, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5190.164, mean reward: -5190.164 [-5190.164, -5190.164], mean action: 0.000 [0.000, 0.000],  loss: 10078412.000000, mae: 961.133362, mean_q: -31.215549, mean_eps: 0.081280
 930/1000: episode: 930, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -2766.015, mean reward: -2766.015 [-2766.015, -2766.015], mean action: 0.000 [0.000, 0.000],  loss: 19107908.000000, mae: 1279.073364, mean_q: -31.434931, mean_eps: 0.080290
 931/1000: episode: 931, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7156.793, mean reward: -7156.793 [-7156.793, -7156.793], mean action: 0.000 [0.000, 0.000],  loss: 14761031.000000, mae: 1138.737793, mean_q: -31.987541, mean_eps: 0.079300
 932/1000: episode: 932, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5563.384, mean reward: -5563.384 [-5563.384, -5563.384], mean action: 0.000 [0.000, 0.000],  loss: 11034801.000000, mae: 936.420044, mean_q: -32.027512, mean_eps: 0.078310
 933/1000: episode: 933, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5743.671, mean reward: -5743.671 [-5743.671, -5743.671], mean action: 0.000 [0.000, 0.000],  loss: 17507856.000000, mae: 1250.173950, mean_q: -32.004211, mean_eps: 0.077320
 934/1000: episode: 934, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6999.125, mean reward: -6999.125 [-6999.125, -6999.125], mean action: 0.000 [0.000, 0.000],  loss: 18557572.000000, mae: 1344.397949, mean_q: -32.509693, mean_eps: 0.076330
 935/1000: episode: 935, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5650.533, mean reward: -5650.533 [-5650.533, -5650.533], mean action: 0.000 [0.000, 0.000],  loss: 19395328.000000, mae: 1304.662720, mean_q: -32.806946, mean_eps: 0.075340
 936/1000: episode: 936, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5002.933, mean reward: -5002.933 [-5002.933, -5002.933], mean action: 0.000 [0.000, 0.000],  loss: 18522714.000000, mae: 1298.517578, mean_q: -32.989738, mean_eps: 0.074350
 937/1000: episode: 937, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4406.423, mean reward: -4406.423 [-4406.423, -4406.423], mean action: 0.000 [0.000, 0.000],  loss: 13804731.000000, mae: 1146.177124, mean_q: -33.520309, mean_eps: 0.073360
 938/1000: episode: 938, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6754.508, mean reward: -6754.508 [-6754.508, -6754.508], mean action: 0.000 [0.000, 0.000],  loss: 13808474.000000, mae: 1130.778076, mean_q: -33.748573, mean_eps: 0.072370
 939/1000: episode: 939, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3925.287, mean reward: -3925.287 [-3925.287, -3925.287], mean action: 0.000 [0.000, 0.000],  loss: 13991206.000000, mae: 1122.807129, mean_q: -34.035126, mean_eps: 0.071380
 940/1000: episode: 940, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -6111.204, mean reward: -6111.204 [-6111.204, -6111.204], mean action: 0.000 [0.000, 0.000],  loss: 17455648.000000, mae: 1300.899902, mean_q: -34.360279, mean_eps: 0.070390
 941/1000: episode: 941, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2931.775, mean reward: -2931.775 [-2931.775, -2931.775], mean action: 0.000 [0.000, 0.000],  loss: 15364250.000000, mae: 1210.037109, mean_q: -34.702644, mean_eps: 0.069400
 942/1000: episode: 942, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4676.056, mean reward: -4676.056 [-4676.056, -4676.056], mean action: 0.000 [0.000, 0.000],  loss: 16151462.000000, mae: 1178.038818, mean_q: -34.977840, mean_eps: 0.068410
 943/1000: episode: 943, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2270.890, mean reward: -2270.890 [-2270.890, -2270.890], mean action: 0.000 [0.000, 0.000],  loss: 21307426.000000, mae: 1414.524414, mean_q: -35.349037, mean_eps: 0.067420
 944/1000: episode: 944, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -9525.001, mean reward: -9525.001 [-9525.001, -9525.001], mean action: 0.000 [0.000, 0.000],  loss: 17375838.000000, mae: 1270.948608, mean_q: -35.529968, mean_eps: 0.066430
 945/1000: episode: 945, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -8857.178, mean reward: -8857.178 [-8857.178, -8857.178], mean action: 0.000 [0.000, 0.000],  loss: 13616308.000000, mae: 1204.490845, mean_q: -36.207306, mean_eps: 0.065440
 946/1000: episode: 946, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -13597.513, mean reward: -13597.513 [-13597.513, -13597.513], mean action: 0.000 [0.000, 0.000],  loss: 16662062.000000, mae: 1204.361084, mean_q: -36.293438, mean_eps: 0.064450
 947/1000: episode: 947, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5463.871, mean reward: -5463.871 [-5463.871, -5463.871], mean action: 0.000 [0.000, 0.000],  loss: 18923640.000000, mae: 1368.142090, mean_q: -36.522263, mean_eps: 0.063460
 948/1000: episode: 948, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -6629.386, mean reward: -6629.386 [-6629.386, -6629.386], mean action: 0.000 [0.000, 0.000],  loss: 17675272.000000, mae: 1307.590698, mean_q: -36.956108, mean_eps: 0.062470
 949/1000: episode: 949, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -12801.750, mean reward: -12801.750 [-12801.750, -12801.750], mean action: 0.000 [0.000, 0.000],  loss: 14278110.000000, mae: 1170.168579, mean_q: -37.270401, mean_eps: 0.061480
 950/1000: episode: 950, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -9516.977, mean reward: -9516.977 [-9516.977, -9516.977], mean action: 0.000 [0.000, 0.000],  loss: 18710470.000000, mae: 1303.380615, mean_q: -37.621078, mean_eps: 0.060490
 951/1000: episode: 951, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5524.880, mean reward: -5524.880 [-5524.880, -5524.880], mean action: 0.000 [0.000, 0.000],  loss: 15570902.000000, mae: 1260.539917, mean_q: -37.872856, mean_eps: 0.059500
 952/1000: episode: 952, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -17488.267, mean reward: -17488.267 [-17488.267, -17488.267], mean action: 0.000 [0.000, 0.000],  loss: 15899746.000000, mae: 1097.956543, mean_q: -38.239311, mean_eps: 0.058510
 953/1000: episode: 953, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5728.536, mean reward: -5728.536 [-5728.536, -5728.536], mean action: 0.000 [0.000, 0.000],  loss: 16651914.000000, mae: 1220.132080, mean_q: -38.548599, mean_eps: 0.057520
 954/1000: episode: 954, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4152.134, mean reward: -4152.134 [-4152.134, -4152.134], mean action: 0.000 [0.000, 0.000],  loss: 14096057.000000, mae: 1045.460571, mean_q: -38.835526, mean_eps: 0.056530
 955/1000: episode: 955, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3253.513, mean reward: -3253.513 [-3253.513, -3253.513], mean action: 0.000 [0.000, 0.000],  loss: 14331946.000000, mae: 1148.953125, mean_q: -39.123646, mean_eps: 0.055540
 956/1000: episode: 956, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7284.316, mean reward: -7284.316 [-7284.316, -7284.316], mean action: 0.000 [0.000, 0.000],  loss: 13686739.000000, mae: 1058.641602, mean_q: -39.503590, mean_eps: 0.054550
 957/1000: episode: 957, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -8657.989, mean reward: -8657.989 [-8657.989, -8657.989], mean action: 0.000 [0.000, 0.000],  loss: 24362592.000000, mae: 1519.473633, mean_q: -39.625717, mean_eps: 0.053560
 958/1000: episode: 958, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -17445.760, mean reward: -17445.760 [-17445.760, -17445.760], mean action: 0.000 [0.000, 0.000],  loss: 20750292.000000, mae: 1377.776489, mean_q: -40.163635, mean_eps: 0.052570
 959/1000: episode: 959, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -9676.238, mean reward: -9676.238 [-9676.238, -9676.238], mean action: 0.000 [0.000, 0.000],  loss: 19538244.000000, mae: 1368.728760, mean_q: -40.279228, mean_eps: 0.051580
 960/1000: episode: 960, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6222.260, mean reward: -6222.260 [-6222.260, -6222.260], mean action: 0.000 [0.000, 0.000],  loss: 16629414.000000, mae: 1222.727783, mean_q: -40.947830, mean_eps: 0.050590
 961/1000: episode: 961, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -1918.347, mean reward: -1918.347 [-1918.347, -1918.347], mean action: 0.000 [0.000, 0.000],  loss: 16034234.000000, mae: 1181.961304, mean_q: -41.279175, mean_eps: 0.049600
 962/1000: episode: 962, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -14177.429, mean reward: -14177.429 [-14177.429, -14177.429], mean action: 0.000 [0.000, 0.000],  loss: 15090633.000000, mae: 1226.519043, mean_q: -41.587559, mean_eps: 0.048610
 963/1000: episode: 963, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -11158.855, mean reward: -11158.855 [-11158.855, -11158.855], mean action: 0.000 [0.000, 0.000],  loss: 14732184.000000, mae: 1135.727295, mean_q: -42.098557, mean_eps: 0.047620
 964/1000: episode: 964, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -10401.103, mean reward: -10401.103 [-10401.103, -10401.103], mean action: 0.000 [0.000, 0.000],  loss: 15978079.000000, mae: 1222.447266, mean_q: -42.577316, mean_eps: 0.046630
 965/1000: episode: 965, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -6141.101, mean reward: -6141.101 [-6141.101, -6141.101], mean action: 0.000 [0.000, 0.000],  loss: 15913118.000000, mae: 1244.677002, mean_q: -42.623180, mean_eps: 0.045640
 966/1000: episode: 966, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -11442.720, mean reward: -11442.720 [-11442.720, -11442.720], mean action: 0.000 [0.000, 0.000],  loss: 10712613.000000, mae: 987.452820, mean_q: -43.036621, mean_eps: 0.044650
 967/1000: episode: 967, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7184.579, mean reward: -7184.579 [-7184.579, -7184.579], mean action: 0.000 [0.000, 0.000],  loss: 22737124.000000, mae: 1429.316528, mean_q: -43.256310, mean_eps: 0.043660
 968/1000: episode: 968, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5062.453, mean reward: -5062.453 [-5062.453, -5062.453], mean action: 0.000 [0.000, 0.000],  loss: 18463878.000000, mae: 1311.223877, mean_q: -43.878864, mean_eps: 0.042670
 969/1000: episode: 969, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5553.293, mean reward: -5553.293 [-5553.293, -5553.293], mean action: 0.000 [0.000, 0.000],  loss: 12892070.000000, mae: 1106.306152, mean_q: -44.137810, mean_eps: 0.041680
 970/1000: episode: 970, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -14189.488, mean reward: -14189.488 [-14189.488, -14189.488], mean action: 0.000 [0.000, 0.000],  loss: 13778741.000000, mae: 1150.501343, mean_q: -44.467590, mean_eps: 0.040690
 971/1000: episode: 971, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -17761.708, mean reward: -17761.708 [-17761.708, -17761.708], mean action: 0.000 [0.000, 0.000],  loss: 18212024.000000, mae: 1300.824219, mean_q: -44.759537, mean_eps: 0.039700
 972/1000: episode: 972, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -14439.582, mean reward: -14439.582 [-14439.582, -14439.582], mean action: 0.000 [0.000, 0.000],  loss: 20911180.000000, mae: 1291.091797, mean_q: -45.116531, mean_eps: 0.038710
 973/1000: episode: 973, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -14087.864, mean reward: -14087.864 [-14087.864, -14087.864], mean action: 0.000 [0.000, 0.000],  loss: 23392502.000000, mae: 1438.387207, mean_q: -45.574890, mean_eps: 0.037720
 974/1000: episode: 974, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9883.358, mean reward: -9883.358 [-9883.358, -9883.358], mean action: 0.000 [0.000, 0.000],  loss: 16574120.000000, mae: 1252.220215, mean_q: -46.129585, mean_eps: 0.036730
 975/1000: episode: 975, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6638.087, mean reward: -6638.087 [-6638.087, -6638.087], mean action: 0.000 [0.000, 0.000],  loss: 17653432.000000, mae: 1255.545898, mean_q: -46.532360, mean_eps: 0.035740
 976/1000: episode: 976, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -9383.065, mean reward: -9383.065 [-9383.065, -9383.065], mean action: 0.000 [0.000, 0.000],  loss: 20827692.000000, mae: 1411.001587, mean_q: -47.071320, mean_eps: 0.034750
 977/1000: episode: 977, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5475.095, mean reward: -5475.095 [-5475.095, -5475.095], mean action: 0.000 [0.000, 0.000],  loss: 18228614.000000, mae: 1313.922485, mean_q: -47.296223, mean_eps: 0.033760
 978/1000: episode: 978, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2202.272, mean reward: -2202.272 [-2202.272, -2202.272], mean action: 0.000 [0.000, 0.000],  loss: 16683057.000000, mae: 1311.734131, mean_q: -47.737820, mean_eps: 0.032770
 979/1000: episode: 979, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -14789.520, mean reward: -14789.520 [-14789.520, -14789.520], mean action: 0.000 [0.000, 0.000],  loss: 16272158.000000, mae: 1242.856934, mean_q: -48.175404, mean_eps: 0.031780
 980/1000: episode: 980, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -7312.370, mean reward: -7312.370 [-7312.370, -7312.370], mean action: 0.000 [0.000, 0.000],  loss: 12758940.000000, mae: 1134.928955, mean_q: -48.558220, mean_eps: 0.030790
 981/1000: episode: 981, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7174.358, mean reward: -7174.358 [-7174.358, -7174.358], mean action: 0.000 [0.000, 0.000],  loss: 16853972.000000, mae: 1254.919189, mean_q: -49.041435, mean_eps: 0.029800
 982/1000: episode: 982, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -3405.414, mean reward: -3405.414 [-3405.414, -3405.414], mean action: 0.000 [0.000, 0.000],  loss: 23952192.000000, mae: 1525.489502, mean_q: -49.407932, mean_eps: 0.028810
 983/1000: episode: 983, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -8991.013, mean reward: -8991.013 [-8991.013, -8991.013], mean action: 0.000 [0.000, 0.000],  loss: 19574796.000000, mae: 1396.141357, mean_q: -49.866699, mean_eps: 0.027820
 984/1000: episode: 984, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -11226.510, mean reward: -11226.510 [-11226.510, -11226.510], mean action: 0.000 [0.000, 0.000],  loss: 16414897.000000, mae: 1304.875977, mean_q: -50.215004, mean_eps: 0.026830
 985/1000: episode: 985, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2787.932, mean reward: -2787.932 [-2787.932, -2787.932], mean action: 0.000 [0.000, 0.000],  loss: 23847996.000000, mae: 1495.711060, mean_q: -50.600822, mean_eps: 0.025840
 986/1000: episode: 986, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3437.071, mean reward: -3437.071 [-3437.071, -3437.071], mean action: 0.000 [0.000, 0.000],  loss: 18873896.000000, mae: 1311.025146, mean_q: -51.377773, mean_eps: 0.024850
 987/1000: episode: 987, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5352.737, mean reward: -5352.737 [-5352.737, -5352.737], mean action: 0.000 [0.000, 0.000],  loss: 15370894.000000, mae: 1270.909546, mean_q: -51.774681, mean_eps: 0.023860
 988/1000: episode: 988, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5501.354, mean reward: -5501.354 [-5501.354, -5501.354], mean action: 0.000 [0.000, 0.000],  loss: 15094934.000000, mae: 1199.517090, mean_q: -51.986629, mean_eps: 0.022870
 989/1000: episode: 989, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -8627.687, mean reward: -8627.687 [-8627.687, -8627.687], mean action: 0.000 [0.000, 0.000],  loss: 16323530.000000, mae: 1267.840576, mean_q: -52.441742, mean_eps: 0.021880
 990/1000: episode: 990, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5901.866, mean reward: -5901.866 [-5901.866, -5901.866], mean action: 0.000 [0.000, 0.000],  loss: 10601616.000000, mae: 1037.328125, mean_q: -53.296711, mean_eps: 0.020890
 991/1000: episode: 991, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -12824.365, mean reward: -12824.365 [-12824.365, -12824.365], mean action: 0.000 [0.000, 0.000],  loss: 17605978.000000, mae: 1253.836670, mean_q: -53.289715, mean_eps: 0.019900
 992/1000: episode: 992, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7395.286, mean reward: -7395.286 [-7395.286, -7395.286], mean action: 0.000 [0.000, 0.000],  loss: 15899536.000000, mae: 1273.782715, mean_q: -53.662716, mean_eps: 0.018910
 993/1000: episode: 993, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6265.608, mean reward: -6265.608 [-6265.608, -6265.608], mean action: 0.000 [0.000, 0.000],  loss: 22635570.000000, mae: 1510.324951, mean_q: -54.037659, mean_eps: 0.017920
 994/1000: episode: 994, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2922.585, mean reward: -2922.585 [-2922.585, -2922.585], mean action: 0.000 [0.000, 0.000],  loss: 16238366.000000, mae: 1198.390625, mean_q: -54.875282, mean_eps: 0.016930
 995/1000: episode: 995, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -9333.420, mean reward: -9333.420 [-9333.420, -9333.420], mean action: 0.000 [0.000, 0.000],  loss: 19717492.000000, mae: 1366.716309, mean_q: -55.215286, mean_eps: 0.015940
 996/1000: episode: 996, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -10144.534, mean reward: -10144.534 [-10144.534, -10144.534], mean action: 0.000 [0.000, 0.000],  loss: 17902204.000000, mae: 1341.439453, mean_q: -55.509392, mean_eps: 0.014950
 997/1000: episode: 997, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -6128.437, mean reward: -6128.437 [-6128.437, -6128.437], mean action: 2.000 [2.000, 2.000],  loss: 14578596.000000, mae: 1186.144043, mean_q: -56.342682, mean_eps: 0.013960
 998/1000: episode: 998, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -8489.715, mean reward: -8489.715 [-8489.715, -8489.715], mean action: 0.000 [0.000, 0.000],  loss: 17236966.000000, mae: 1347.305298, mean_q: -56.729855, mean_eps: 0.012970
 999/1000: episode: 999, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4559.777, mean reward: -4559.777 [-4559.777, -4559.777], mean action: 0.000 [0.000, 0.000],  loss: 17525438.000000, mae: 1288.037842, mean_q: -56.778618, mean_eps: 0.011980
 1000/1000: episode: 1000, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1260.125, mean reward: -1260.125 [-1260.125, -1260.125], mean action: 0.000 [0.000, 0.000],  loss: 20360426.000000, mae: 1366.782593, mean_q: -57.418846, mean_eps: 0.010990
done, took 46.234 seconds
