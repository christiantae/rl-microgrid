Training for 1000 steps ...
   1/1000: episode: 1, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -4758.669, mean reward: -4758.669 [-4758.669, -4758.669], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
   2/1000: episode: 2, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1000.851, mean reward: -1000.851 [-1000.851, -1000.851], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
   3/1000: episode: 3, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -2754.475, mean reward: -2754.475 [-2754.475, -2754.475], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
   4/1000: episode: 4, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -6907.361, mean reward: -6907.361 [-6907.361, -6907.361], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
   5/1000: episode: 5, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -9911.907, mean reward: -9911.907 [-9911.907, -9911.907], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
   6/1000: episode: 6, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -47.306, mean reward: -47.306 [-47.306, -47.306], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
   7/1000: episode: 7, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4726.836, mean reward: -4726.836 [-4726.836, -4726.836], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
   8/1000: episode: 8, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -6047.193, mean reward: -6047.193 [-6047.193, -6047.193], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
   9/1000: episode: 9, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -965.785, mean reward: -965.785 [-965.785, -965.785], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  10/1000: episode: 10, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -164.915, mean reward: -164.915 [-164.915, -164.915], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  11/1000: episode: 11, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -2838.739, mean reward: -2838.739 [-2838.739, -2838.739], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  12/1000: episode: 12, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -2509.839, mean reward: -2509.839 [-2509.839, -2509.839], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  13/1000: episode: 13, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5660.778, mean reward: -5660.778 [-5660.778, -5660.778], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  14/1000: episode: 14, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -838.382, mean reward: -838.382 [-838.382, -838.382], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  15/1000: episode: 15, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -92.634, mean reward: -92.634 [-92.634, -92.634], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  16/1000: episode: 16, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -8339.358, mean reward: -8339.358 [-8339.358, -8339.358], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  17/1000: episode: 17, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2016.996, mean reward: -2016.996 [-2016.996, -2016.996], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  18/1000: episode: 18, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1899.894, mean reward: -1899.894 [-1899.894, -1899.894], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  19/1000: episode: 19, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -845.031, mean reward: -845.031 [-845.031, -845.031], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  20/1000: episode: 20, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -232.562, mean reward: -232.562 [-232.562, -232.562], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  21/1000: episode: 21, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -16.180, mean reward: -16.180 [-16.180, -16.180], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  22/1000: episode: 22, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1707.865, mean reward: -1707.865 [-1707.865, -1707.865], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  23/1000: episode: 23, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -6105.846, mean reward: -6105.846 [-6105.846, -6105.846], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  24/1000: episode: 24, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5735.682, mean reward: -5735.682 [-5735.682, -5735.682], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  25/1000: episode: 25, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3247.867, mean reward: -3247.867 [-3247.867, -3247.867], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  26/1000: episode: 26, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -6148.291, mean reward: -6148.291 [-6148.291, -6148.291], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  27/1000: episode: 27, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -3032.273, mean reward: -3032.273 [-3032.273, -3032.273], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  28/1000: episode: 28, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -3281.524, mean reward: -3281.524 [-3281.524, -3281.524], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  29/1000: episode: 29, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -4641.487, mean reward: -4641.487 [-4641.487, -4641.487], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  30/1000: episode: 30, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -3466.715, mean reward: -3466.715 [-3466.715, -3466.715], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  31/1000: episode: 31, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -4325.577, mean reward: -4325.577 [-4325.577, -4325.577], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  32/1000: episode: 32, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2031.271, mean reward: -2031.271 [-2031.271, -2031.271], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  33/1000: episode: 33, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1216.327, mean reward: -1216.327 [-1216.327, -1216.327], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  34/1000: episode: 34, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3157.956, mean reward: -3157.956 [-3157.956, -3157.956], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  35/1000: episode: 35, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1692.566, mean reward: -1692.566 [-1692.566, -1692.566], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  36/1000: episode: 36, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -2000.074, mean reward: -2000.074 [-2000.074, -2000.074], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  37/1000: episode: 37, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -959.797, mean reward: -959.797 [-959.797, -959.797], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  38/1000: episode: 38, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5916.095, mean reward: -5916.095 [-5916.095, -5916.095], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  39/1000: episode: 39, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -614.993, mean reward: -614.993 [-614.993, -614.993], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  40/1000: episode: 40, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5062.643, mean reward: -5062.643 [-5062.643, -5062.643], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  41/1000: episode: 41, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5943.560, mean reward: -5943.560 [-5943.560, -5943.560], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  42/1000: episode: 42, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2958.048, mean reward: -2958.048 [-2958.048, -2958.048], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  43/1000: episode: 43, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -926.191, mean reward: -926.191 [-926.191, -926.191], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  44/1000: episode: 44, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -2885.631, mean reward: -2885.631 [-2885.631, -2885.631], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  45/1000: episode: 45, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -391.770, mean reward: -391.770 [-391.770, -391.770], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  46/1000: episode: 46, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -994.668, mean reward: -994.668 [-994.668, -994.668], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  47/1000: episode: 47, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5976.778, mean reward: -5976.778 [-5976.778, -5976.778], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  48/1000: episode: 48, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -2688.383, mean reward: -2688.383 [-2688.383, -2688.383], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  49/1000: episode: 49, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1824.709, mean reward: -1824.709 [-1824.709, -1824.709], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  50/1000: episode: 50, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1182.507, mean reward: -1182.507 [-1182.507, -1182.507], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  51/1000: episode: 51, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -419.977, mean reward: -419.977 [-419.977, -419.977], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  52/1000: episode: 52, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2711.666, mean reward: -2711.666 [-2711.666, -2711.666], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  53/1000: episode: 53, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4084.870, mean reward: -4084.870 [-4084.870, -4084.870], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  54/1000: episode: 54, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -844.621, mean reward: -844.621 [-844.621, -844.621], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  55/1000: episode: 55, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5156.396, mean reward: -5156.396 [-5156.396, -5156.396], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
  56/1000: episode: 56, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2458.262, mean reward: -2458.262 [-2458.262, -2458.262], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  57/1000: episode: 57, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3681.526, mean reward: -3681.526 [-3681.526, -3681.526], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  58/1000: episode: 58, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -763.462, mean reward: -763.462 [-763.462, -763.462], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  59/1000: episode: 59, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -7910.609, mean reward: -7910.609 [-7910.609, -7910.609], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  60/1000: episode: 60, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2984.921, mean reward: -2984.921 [-2984.921, -2984.921], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  61/1000: episode: 61, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1543.228, mean reward: -1543.228 [-1543.228, -1543.228], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  62/1000: episode: 62, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -2957.403, mean reward: -2957.403 [-2957.403, -2957.403], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  63/1000: episode: 63, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4942.794, mean reward: -4942.794 [-4942.794, -4942.794], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  64/1000: episode: 64, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2088.762, mean reward: -2088.762 [-2088.762, -2088.762], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  65/1000: episode: 65, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -1634.065, mean reward: -1634.065 [-1634.065, -1634.065], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  66/1000: episode: 66, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2010.358, mean reward: -2010.358 [-2010.358, -2010.358], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  67/1000: episode: 67, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -1603.102, mean reward: -1603.102 [-1603.102, -1603.102], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  68/1000: episode: 68, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -2857.122, mean reward: -2857.122 [-2857.122, -2857.122], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  69/1000: episode: 69, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -3229.951, mean reward: -3229.951 [-3229.951, -3229.951], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  70/1000: episode: 70, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1986.052, mean reward: -1986.052 [-1986.052, -1986.052], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  71/1000: episode: 71, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2604.394, mean reward: -2604.394 [-2604.394, -2604.394], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  72/1000: episode: 72, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4360.226, mean reward: -4360.226 [-4360.226, -4360.226], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
  73/1000: episode: 73, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3040.668, mean reward: -3040.668 [-3040.668, -3040.668], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  74/1000: episode: 74, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3693.676, mean reward: -3693.676 [-3693.676, -3693.676], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  75/1000: episode: 75, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1639.651, mean reward: -1639.651 [-1639.651, -1639.651], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  76/1000: episode: 76, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -1146.465, mean reward: -1146.465 [-1146.465, -1146.465], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  77/1000: episode: 77, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -1495.466, mean reward: -1495.466 [-1495.466, -1495.466], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  78/1000: episode: 78, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -2353.587, mean reward: -2353.587 [-2353.587, -2353.587], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
  79/1000: episode: 79, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -63.437, mean reward: -63.437 [-63.437, -63.437], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  80/1000: episode: 80, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -643.893, mean reward: -643.893 [-643.893, -643.893], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  81/1000: episode: 81, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4513.957, mean reward: -4513.957 [-4513.957, -4513.957], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  82/1000: episode: 82, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4290.887, mean reward: -4290.887 [-4290.887, -4290.887], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  83/1000: episode: 83, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4242.577, mean reward: -4242.577 [-4242.577, -4242.577], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  84/1000: episode: 84, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -3895.933, mean reward: -3895.933 [-3895.933, -3895.933], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  85/1000: episode: 85, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4253.508, mean reward: -4253.508 [-4253.508, -4253.508], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  86/1000: episode: 86, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5433.519, mean reward: -5433.519 [-5433.519, -5433.519], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  87/1000: episode: 87, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -867.872, mean reward: -867.872 [-867.872, -867.872], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  88/1000: episode: 88, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -994.369, mean reward: -994.369 [-994.369, -994.369], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  89/1000: episode: 89, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -1228.149, mean reward: -1228.149 [-1228.149, -1228.149], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  90/1000: episode: 90, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3574.956, mean reward: -3574.956 [-3574.956, -3574.956], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  91/1000: episode: 91, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5763.768, mean reward: -5763.768 [-5763.768, -5763.768], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  92/1000: episode: 92, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2266.661, mean reward: -2266.661 [-2266.661, -2266.661], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  93/1000: episode: 93, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -4160.851, mean reward: -4160.851 [-4160.851, -4160.851], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  94/1000: episode: 94, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -1795.343, mean reward: -1795.343 [-1795.343, -1795.343], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  95/1000: episode: 95, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -2166.212, mean reward: -2166.212 [-2166.212, -2166.212], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  96/1000: episode: 96, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -2524.241, mean reward: -2524.241 [-2524.241, -2524.241], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  97/1000: episode: 97, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -311.234, mean reward: -311.234 [-311.234, -311.234], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  98/1000: episode: 98, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -2821.087, mean reward: -2821.087 [-2821.087, -2821.087], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
  99/1000: episode: 99, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -1189.550, mean reward: -1189.550 [-1189.550, -1189.550], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 100/1000: episode: 100, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -8100.775, mean reward: -8100.775 [-8100.775, -8100.775], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 101/1000: episode: 101, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -398.588, mean reward: -398.588 [-398.588, -398.588], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 102/1000: episode: 102, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5667.738, mean reward: -5667.738 [-5667.738, -5667.738], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 103/1000: episode: 103, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -3037.802, mean reward: -3037.802 [-3037.802, -3037.802], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 104/1000: episode: 104, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -556.600, mean reward: -556.600 [-556.600, -556.600], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 105/1000: episode: 105, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -2041.389, mean reward: -2041.389 [-2041.389, -2041.389], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 106/1000: episode: 106, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4715.752, mean reward: -4715.752 [-4715.752, -4715.752], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 107/1000: episode: 107, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -2961.806, mean reward: -2961.806 [-2961.806, -2961.806], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 108/1000: episode: 108, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -2480.750, mean reward: -2480.750 [-2480.750, -2480.750], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 109/1000: episode: 109, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -4386.926, mean reward: -4386.926 [-4386.926, -4386.926], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 110/1000: episode: 110, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -1754.280, mean reward: -1754.280 [-1754.280, -1754.280], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 111/1000: episode: 111, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -937.884, mean reward: -937.884 [-937.884, -937.884], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 112/1000: episode: 112, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2017.122, mean reward: -2017.122 [-2017.122, -2017.122], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 113/1000: episode: 113, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -3120.226, mean reward: -3120.226 [-3120.226, -3120.226], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 114/1000: episode: 114, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -155.655, mean reward: -155.655 [-155.655, -155.655], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 115/1000: episode: 115, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4011.117, mean reward: -4011.117 [-4011.117, -4011.117], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 116/1000: episode: 116, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5822.101, mean reward: -5822.101 [-5822.101, -5822.101], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 117/1000: episode: 117, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -3314.632, mean reward: -3314.632 [-3314.632, -3314.632], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 118/1000: episode: 118, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2357.390, mean reward: -2357.390 [-2357.390, -2357.390], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 119/1000: episode: 119, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3290.970, mean reward: -3290.970 [-3290.970, -3290.970], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 120/1000: episode: 120, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1377.769, mean reward: -1377.769 [-1377.769, -1377.769], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 121/1000: episode: 121, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4546.955, mean reward: -4546.955 [-4546.955, -4546.955], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 122/1000: episode: 122, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1110.656, mean reward: -1110.656 [-1110.656, -1110.656], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 123/1000: episode: 123, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1517.150, mean reward: -1517.150 [-1517.150, -1517.150], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 124/1000: episode: 124, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1457.774, mean reward: -1457.774 [-1457.774, -1457.774], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 125/1000: episode: 125, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -294.205, mean reward: -294.205 [-294.205, -294.205], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 126/1000: episode: 126, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5493.174, mean reward: -5493.174 [-5493.174, -5493.174], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 127/1000: episode: 127, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -63.319, mean reward: -63.319 [-63.319, -63.319], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 128/1000: episode: 128, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -973.432, mean reward: -973.432 [-973.432, -973.432], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 129/1000: episode: 129, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3894.199, mean reward: -3894.199 [-3894.199, -3894.199], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 130/1000: episode: 130, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -837.575, mean reward: -837.575 [-837.575, -837.575], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 131/1000: episode: 131, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1658.964, mean reward: -1658.964 [-1658.964, -1658.964], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 132/1000: episode: 132, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -1298.113, mean reward: -1298.113 [-1298.113, -1298.113], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 133/1000: episode: 133, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -8594.344, mean reward: -8594.344 [-8594.344, -8594.344], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 134/1000: episode: 134, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -1043.062, mean reward: -1043.062 [-1043.062, -1043.062], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 135/1000: episode: 135, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -1851.432, mean reward: -1851.432 [-1851.432, -1851.432], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 136/1000: episode: 136, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -3161.156, mean reward: -3161.156 [-3161.156, -3161.156], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 137/1000: episode: 137, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2110.031, mean reward: -2110.031 [-2110.031, -2110.031], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 138/1000: episode: 138, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -673.985, mean reward: -673.985 [-673.985, -673.985], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 139/1000: episode: 139, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1913.200, mean reward: -1913.200 [-1913.200, -1913.200], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 140/1000: episode: 140, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -2656.000, mean reward: -2656.000 [-2656.000, -2656.000], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 141/1000: episode: 141, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -923.027, mean reward: -923.027 [-923.027, -923.027], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 142/1000: episode: 142, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2497.973, mean reward: -2497.973 [-2497.973, -2497.973], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 143/1000: episode: 143, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -2166.486, mean reward: -2166.486 [-2166.486, -2166.486], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 144/1000: episode: 144, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -2848.144, mean reward: -2848.144 [-2848.144, -2848.144], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 145/1000: episode: 145, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -4571.085, mean reward: -4571.085 [-4571.085, -4571.085], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 146/1000: episode: 146, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2233.120, mean reward: -2233.120 [-2233.120, -2233.120], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 147/1000: episode: 147, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5151.241, mean reward: -5151.241 [-5151.241, -5151.241], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 148/1000: episode: 148, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -8686.689, mean reward: -8686.689 [-8686.689, -8686.689], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 149/1000: episode: 149, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -6220.334, mean reward: -6220.334 [-6220.334, -6220.334], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 150/1000: episode: 150, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2648.059, mean reward: -2648.059 [-2648.059, -2648.059], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 151/1000: episode: 151, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4005.407, mean reward: -4005.407 [-4005.407, -4005.407], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 152/1000: episode: 152, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -1085.352, mean reward: -1085.352 [-1085.352, -1085.352], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 153/1000: episode: 153, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2333.941, mean reward: -2333.941 [-2333.941, -2333.941], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 154/1000: episode: 154, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -4265.349, mean reward: -4265.349 [-4265.349, -4265.349], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 155/1000: episode: 155, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4386.103, mean reward: -4386.103 [-4386.103, -4386.103], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 156/1000: episode: 156, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -815.839, mean reward: -815.839 [-815.839, -815.839], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 157/1000: episode: 157, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -2168.327, mean reward: -2168.327 [-2168.327, -2168.327], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
 158/1000: episode: 158, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -415.122, mean reward: -415.122 [-415.122, -415.122], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 159/1000: episode: 159, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -3084.784, mean reward: -3084.784 [-3084.784, -3084.784], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 160/1000: episode: 160, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -1715.917, mean reward: -1715.917 [-1715.917, -1715.917], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 161/1000: episode: 161, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -12134.095, mean reward: -12134.095 [-12134.095, -12134.095], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 162/1000: episode: 162, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -658.851, mean reward: -658.851 [-658.851, -658.851], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 163/1000: episode: 163, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -1315.497, mean reward: -1315.497 [-1315.497, -1315.497], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 164/1000: episode: 164, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -1808.531, mean reward: -1808.531 [-1808.531, -1808.531], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 165/1000: episode: 165, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1138.937, mean reward: -1138.937 [-1138.937, -1138.937], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 166/1000: episode: 166, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1265.317, mean reward: -1265.317 [-1265.317, -1265.317], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 167/1000: episode: 167, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -7049.318, mean reward: -7049.318 [-7049.318, -7049.318], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 168/1000: episode: 168, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -876.429, mean reward: -876.429 [-876.429, -876.429], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 169/1000: episode: 169, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1957.722, mean reward: -1957.722 [-1957.722, -1957.722], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 170/1000: episode: 170, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1118.023, mean reward: -1118.023 [-1118.023, -1118.023], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 171/1000: episode: 171, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4224.229, mean reward: -4224.229 [-4224.229, -4224.229], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 172/1000: episode: 172, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -2933.804, mean reward: -2933.804 [-2933.804, -2933.804], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 173/1000: episode: 173, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1803.255, mean reward: -1803.255 [-1803.255, -1803.255], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 174/1000: episode: 174, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -2512.403, mean reward: -2512.403 [-2512.403, -2512.403], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 175/1000: episode: 175, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -52.765, mean reward: -52.765 [-52.765, -52.765], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 176/1000: episode: 176, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -569.279, mean reward: -569.279 [-569.279, -569.279], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 177/1000: episode: 177, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -342.394, mean reward: -342.394 [-342.394, -342.394], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 178/1000: episode: 178, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -214.372, mean reward: -214.372 [-214.372, -214.372], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 179/1000: episode: 179, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2081.332, mean reward: -2081.332 [-2081.332, -2081.332], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
 180/1000: episode: 180, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -4534.234, mean reward: -4534.234 [-4534.234, -4534.234], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 181/1000: episode: 181, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -212.355, mean reward: -212.355 [-212.355, -212.355], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 182/1000: episode: 182, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -921.733, mean reward: -921.733 [-921.733, -921.733], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 183/1000: episode: 183, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -6994.821, mean reward: -6994.821 [-6994.821, -6994.821], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 184/1000: episode: 184, duration: 0.118s, episode steps:   1, steps per second:   9, episode reward: -7694.850, mean reward: -7694.850 [-7694.850, -7694.850], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 185/1000: episode: 185, duration: 0.248s, episode steps:   1, steps per second:   4, episode reward: -3759.370, mean reward: -3759.370 [-3759.370, -3759.370], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 186/1000: episode: 186, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -3973.284, mean reward: -3973.284 [-3973.284, -3973.284], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 187/1000: episode: 187, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -122.955, mean reward: -122.955 [-122.955, -122.955], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 188/1000: episode: 188, duration: 0.172s, episode steps:   1, steps per second:   6, episode reward: -1649.037, mean reward: -1649.037 [-1649.037, -1649.037], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 189/1000: episode: 189, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -1385.561, mean reward: -1385.561 [-1385.561, -1385.561], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 190/1000: episode: 190, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -2948.457, mean reward: -2948.457 [-2948.457, -2948.457], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 191/1000: episode: 191, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -4056.801, mean reward: -4056.801 [-4056.801, -4056.801], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 192/1000: episode: 192, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -697.568, mean reward: -697.568 [-697.568, -697.568], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 193/1000: episode: 193, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2188.492, mean reward: -2188.492 [-2188.492, -2188.492], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 194/1000: episode: 194, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -8636.677, mean reward: -8636.677 [-8636.677, -8636.677], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 195/1000: episode: 195, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -4884.675, mean reward: -4884.675 [-4884.675, -4884.675], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 196/1000: episode: 196, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -715.893, mean reward: -715.893 [-715.893, -715.893], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 197/1000: episode: 197, duration: 0.151s, episode steps:   1, steps per second:   7, episode reward: -3446.143, mean reward: -3446.143 [-3446.143, -3446.143], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 198/1000: episode: 198, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -2663.384, mean reward: -2663.384 [-2663.384, -2663.384], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 199/1000: episode: 199, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -2780.536, mean reward: -2780.536 [-2780.536, -2780.536], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 200/1000: episode: 200, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -152.247, mean reward: -152.247 [-152.247, -152.247], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 201/1000: episode: 201, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -1381.182, mean reward: -1381.182 [-1381.182, -1381.182], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 202/1000: episode: 202, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -3770.839, mean reward: -3770.839 [-3770.839, -3770.839], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 203/1000: episode: 203, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -6122.344, mean reward: -6122.344 [-6122.344, -6122.344], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 204/1000: episode: 204, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5151.187, mean reward: -5151.187 [-5151.187, -5151.187], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 205/1000: episode: 205, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -6866.643, mean reward: -6866.643 [-6866.643, -6866.643], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 206/1000: episode: 206, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4206.794, mean reward: -4206.794 [-4206.794, -4206.794], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 207/1000: episode: 207, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -594.489, mean reward: -594.489 [-594.489, -594.489], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 208/1000: episode: 208, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -382.689, mean reward: -382.689 [-382.689, -382.689], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 209/1000: episode: 209, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -3276.961, mean reward: -3276.961 [-3276.961, -3276.961], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 210/1000: episode: 210, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -778.024, mean reward: -778.024 [-778.024, -778.024], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 211/1000: episode: 211, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -6102.070, mean reward: -6102.070 [-6102.070, -6102.070], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 212/1000: episode: 212, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5887.876, mean reward: -5887.876 [-5887.876, -5887.876], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 213/1000: episode: 213, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -4322.279, mean reward: -4322.279 [-4322.279, -4322.279], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 214/1000: episode: 214, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3171.731, mean reward: -3171.731 [-3171.731, -3171.731], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 215/1000: episode: 215, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -412.415, mean reward: -412.415 [-412.415, -412.415], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 216/1000: episode: 216, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1709.302, mean reward: -1709.302 [-1709.302, -1709.302], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 217/1000: episode: 217, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -677.825, mean reward: -677.825 [-677.825, -677.825], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 218/1000: episode: 218, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -3405.459, mean reward: -3405.459 [-3405.459, -3405.459], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 219/1000: episode: 219, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1865.583, mean reward: -1865.583 [-1865.583, -1865.583], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 220/1000: episode: 220, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -15292.153, mean reward: -15292.153 [-15292.153, -15292.153], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 221/1000: episode: 221, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4459.484, mean reward: -4459.484 [-4459.484, -4459.484], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 222/1000: episode: 222, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5330.304, mean reward: -5330.304 [-5330.304, -5330.304], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 223/1000: episode: 223, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1292.285, mean reward: -1292.285 [-1292.285, -1292.285], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 224/1000: episode: 224, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -11166.214, mean reward: -11166.214 [-11166.214, -11166.214], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 225/1000: episode: 225, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -246.975, mean reward: -246.975 [-246.975, -246.975], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 226/1000: episode: 226, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -144.581, mean reward: -144.581 [-144.581, -144.581], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 227/1000: episode: 227, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3920.389, mean reward: -3920.389 [-3920.389, -3920.389], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 228/1000: episode: 228, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -9142.164, mean reward: -9142.164 [-9142.164, -9142.164], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 229/1000: episode: 229, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3726.099, mean reward: -3726.099 [-3726.099, -3726.099], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 230/1000: episode: 230, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5144.790, mean reward: -5144.790 [-5144.790, -5144.790], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
 231/1000: episode: 231, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -379.572, mean reward: -379.572 [-379.572, -379.572], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 232/1000: episode: 232, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4189.952, mean reward: -4189.952 [-4189.952, -4189.952], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 233/1000: episode: 233, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -4723.713, mean reward: -4723.713 [-4723.713, -4723.713], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 234/1000: episode: 234, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -7534.537, mean reward: -7534.537 [-7534.537, -7534.537], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 235/1000: episode: 235, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -1478.928, mean reward: -1478.928 [-1478.928, -1478.928], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 236/1000: episode: 236, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -886.559, mean reward: -886.559 [-886.559, -886.559], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 237/1000: episode: 237, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -2211.565, mean reward: -2211.565 [-2211.565, -2211.565], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 238/1000: episode: 238, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1915.950, mean reward: -1915.950 [-1915.950, -1915.950], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 239/1000: episode: 239, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2120.605, mean reward: -2120.605 [-2120.605, -2120.605], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 240/1000: episode: 240, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3892.403, mean reward: -3892.403 [-3892.403, -3892.403], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 241/1000: episode: 241, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3970.504, mean reward: -3970.504 [-3970.504, -3970.504], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 242/1000: episode: 242, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2157.944, mean reward: -2157.944 [-2157.944, -2157.944], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 243/1000: episode: 243, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -1215.284, mean reward: -1215.284 [-1215.284, -1215.284], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 244/1000: episode: 244, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -186.445, mean reward: -186.445 [-186.445, -186.445], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 245/1000: episode: 245, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -151.215, mean reward: -151.215 [-151.215, -151.215], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 246/1000: episode: 246, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -3905.696, mean reward: -3905.696 [-3905.696, -3905.696], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 247/1000: episode: 247, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1942.204, mean reward: -1942.204 [-1942.204, -1942.204], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 248/1000: episode: 248, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1183.133, mean reward: -1183.133 [-1183.133, -1183.133], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 249/1000: episode: 249, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -1188.197, mean reward: -1188.197 [-1188.197, -1188.197], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 250/1000: episode: 250, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -2309.319, mean reward: -2309.319 [-2309.319, -2309.319], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 251/1000: episode: 251, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1028.488, mean reward: -1028.488 [-1028.488, -1028.488], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 252/1000: episode: 252, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -2654.478, mean reward: -2654.478 [-2654.478, -2654.478], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 253/1000: episode: 253, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2528.609, mean reward: -2528.609 [-2528.609, -2528.609], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 254/1000: episode: 254, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5494.160, mean reward: -5494.160 [-5494.160, -5494.160], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 255/1000: episode: 255, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -619.387, mean reward: -619.387 [-619.387, -619.387], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 256/1000: episode: 256, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -8908.275, mean reward: -8908.275 [-8908.275, -8908.275], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 257/1000: episode: 257, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -16353.060, mean reward: -16353.060 [-16353.060, -16353.060], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 258/1000: episode: 258, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -2082.358, mean reward: -2082.358 [-2082.358, -2082.358], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 259/1000: episode: 259, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -7641.269, mean reward: -7641.269 [-7641.269, -7641.269], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 260/1000: episode: 260, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -735.824, mean reward: -735.824 [-735.824, -735.824], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 261/1000: episode: 261, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -686.914, mean reward: -686.914 [-686.914, -686.914], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 262/1000: episode: 262, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -4420.165, mean reward: -4420.165 [-4420.165, -4420.165], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 263/1000: episode: 263, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2778.412, mean reward: -2778.412 [-2778.412, -2778.412], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 264/1000: episode: 264, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5498.197, mean reward: -5498.197 [-5498.197, -5498.197], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 265/1000: episode: 265, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2208.321, mean reward: -2208.321 [-2208.321, -2208.321], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 266/1000: episode: 266, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -3815.633, mean reward: -3815.633 [-3815.633, -3815.633], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 267/1000: episode: 267, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -2496.750, mean reward: -2496.750 [-2496.750, -2496.750], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 268/1000: episode: 268, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1444.830, mean reward: -1444.830 [-1444.830, -1444.830], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 269/1000: episode: 269, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2461.338, mean reward: -2461.338 [-2461.338, -2461.338], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 270/1000: episode: 270, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -2368.368, mean reward: -2368.368 [-2368.368, -2368.368], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 271/1000: episode: 271, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1143.283, mean reward: -1143.283 [-1143.283, -1143.283], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 272/1000: episode: 272, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -725.901, mean reward: -725.901 [-725.901, -725.901], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 273/1000: episode: 273, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1106.003, mean reward: -1106.003 [-1106.003, -1106.003], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 274/1000: episode: 274, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2189.005, mean reward: -2189.005 [-2189.005, -2189.005], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 275/1000: episode: 275, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -1808.481, mean reward: -1808.481 [-1808.481, -1808.481], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 276/1000: episode: 276, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -6002.961, mean reward: -6002.961 [-6002.961, -6002.961], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 277/1000: episode: 277, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -154.469, mean reward: -154.469 [-154.469, -154.469], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 278/1000: episode: 278, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1168.813, mean reward: -1168.813 [-1168.813, -1168.813], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 279/1000: episode: 279, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5112.547, mean reward: -5112.547 [-5112.547, -5112.547], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 280/1000: episode: 280, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -948.148, mean reward: -948.148 [-948.148, -948.148], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 281/1000: episode: 281, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2813.655, mean reward: -2813.655 [-2813.655, -2813.655], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
 282/1000: episode: 282, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1766.557, mean reward: -1766.557 [-1766.557, -1766.557], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 283/1000: episode: 283, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -795.236, mean reward: -795.236 [-795.236, -795.236], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 284/1000: episode: 284, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1461.310, mean reward: -1461.310 [-1461.310, -1461.310], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 285/1000: episode: 285, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -993.810, mean reward: -993.810 [-993.810, -993.810], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 286/1000: episode: 286, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5902.078, mean reward: -5902.078 [-5902.078, -5902.078], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 287/1000: episode: 287, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -3818.412, mean reward: -3818.412 [-3818.412, -3818.412], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
 288/1000: episode: 288, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -6368.687, mean reward: -6368.687 [-6368.687, -6368.687], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 289/1000: episode: 289, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -441.747, mean reward: -441.747 [-441.747, -441.747], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 290/1000: episode: 290, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -2914.819, mean reward: -2914.819 [-2914.819, -2914.819], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 291/1000: episode: 291, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -2091.870, mean reward: -2091.870 [-2091.870, -2091.870], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 292/1000: episode: 292, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -887.508, mean reward: -887.508 [-887.508, -887.508], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 293/1000: episode: 293, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -3101.204, mean reward: -3101.204 [-3101.204, -3101.204], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 294/1000: episode: 294, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -2113.646, mean reward: -2113.646 [-2113.646, -2113.646], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 295/1000: episode: 295, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -11196.756, mean reward: -11196.756 [-11196.756, -11196.756], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 296/1000: episode: 296, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -3594.630, mean reward: -3594.630 [-3594.630, -3594.630], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 297/1000: episode: 297, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1867.494, mean reward: -1867.494 [-1867.494, -1867.494], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 298/1000: episode: 298, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -179.522, mean reward: -179.522 [-179.522, -179.522], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 299/1000: episode: 299, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -4604.255, mean reward: -4604.255 [-4604.255, -4604.255], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 300/1000: episode: 300, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -713.915, mean reward: -713.915 [-713.915, -713.915], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 301/1000: episode: 301, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1522.470, mean reward: -1522.470 [-1522.470, -1522.470], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 302/1000: episode: 302, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -9426.295, mean reward: -9426.295 [-9426.295, -9426.295], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 303/1000: episode: 303, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2851.141, mean reward: -2851.141 [-2851.141, -2851.141], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 304/1000: episode: 304, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1141.749, mean reward: -1141.749 [-1141.749, -1141.749], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 305/1000: episode: 305, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3318.550, mean reward: -3318.550 [-3318.550, -3318.550], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 306/1000: episode: 306, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2658.274, mean reward: -2658.274 [-2658.274, -2658.274], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 307/1000: episode: 307, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -9054.921, mean reward: -9054.921 [-9054.921, -9054.921], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 308/1000: episode: 308, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4380.245, mean reward: -4380.245 [-4380.245, -4380.245], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 309/1000: episode: 309, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7043.832, mean reward: -7043.832 [-7043.832, -7043.832], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 310/1000: episode: 310, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -948.968, mean reward: -948.968 [-948.968, -948.968], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 311/1000: episode: 311, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2400.547, mean reward: -2400.547 [-2400.547, -2400.547], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 312/1000: episode: 312, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2815.190, mean reward: -2815.190 [-2815.190, -2815.190], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 313/1000: episode: 313, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -3771.879, mean reward: -3771.879 [-3771.879, -3771.879], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 314/1000: episode: 314, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3838.353, mean reward: -3838.353 [-3838.353, -3838.353], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 315/1000: episode: 315, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2343.196, mean reward: -2343.196 [-2343.196, -2343.196], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 316/1000: episode: 316, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1061.839, mean reward: -1061.839 [-1061.839, -1061.839], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 317/1000: episode: 317, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1427.225, mean reward: -1427.225 [-1427.225, -1427.225], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 318/1000: episode: 318, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -891.930, mean reward: -891.930 [-891.930, -891.930], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 319/1000: episode: 319, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1277.918, mean reward: -1277.918 [-1277.918, -1277.918], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 320/1000: episode: 320, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -373.189, mean reward: -373.189 [-373.189, -373.189], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 321/1000: episode: 321, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1975.391, mean reward: -1975.391 [-1975.391, -1975.391], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 322/1000: episode: 322, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2753.582, mean reward: -2753.582 [-2753.582, -2753.582], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 323/1000: episode: 323, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -2848.280, mean reward: -2848.280 [-2848.280, -2848.280], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 324/1000: episode: 324, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -929.594, mean reward: -929.594 [-929.594, -929.594], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 325/1000: episode: 325, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -857.291, mean reward: -857.291 [-857.291, -857.291], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 326/1000: episode: 326, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -1277.490, mean reward: -1277.490 [-1277.490, -1277.490], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 327/1000: episode: 327, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -2315.689, mean reward: -2315.689 [-2315.689, -2315.689], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 328/1000: episode: 328, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -118.168, mean reward: -118.168 [-118.168, -118.168], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 329/1000: episode: 329, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2515.132, mean reward: -2515.132 [-2515.132, -2515.132], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 330/1000: episode: 330, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1131.985, mean reward: -1131.985 [-1131.985, -1131.985], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 331/1000: episode: 331, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -194.714, mean reward: -194.714 [-194.714, -194.714], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 332/1000: episode: 332, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -11623.892, mean reward: -11623.892 [-11623.892, -11623.892], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 333/1000: episode: 333, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -4774.880, mean reward: -4774.880 [-4774.880, -4774.880], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 334/1000: episode: 334, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -496.880, mean reward: -496.880 [-496.880, -496.880], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 335/1000: episode: 335, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -1505.501, mean reward: -1505.501 [-1505.501, -1505.501], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 336/1000: episode: 336, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -719.771, mean reward: -719.771 [-719.771, -719.771], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 337/1000: episode: 337, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -844.234, mean reward: -844.234 [-844.234, -844.234], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 338/1000: episode: 338, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -348.093, mean reward: -348.093 [-348.093, -348.093], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 339/1000: episode: 339, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -1778.194, mean reward: -1778.194 [-1778.194, -1778.194], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 340/1000: episode: 340, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -2307.050, mean reward: -2307.050 [-2307.050, -2307.050], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 341/1000: episode: 341, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1504.748, mean reward: -1504.748 [-1504.748, -1504.748], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 342/1000: episode: 342, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -1590.814, mean reward: -1590.814 [-1590.814, -1590.814], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 343/1000: episode: 343, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4289.687, mean reward: -4289.687 [-4289.687, -4289.687], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 344/1000: episode: 344, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -1851.994, mean reward: -1851.994 [-1851.994, -1851.994], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 345/1000: episode: 345, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -7872.008, mean reward: -7872.008 [-7872.008, -7872.008], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 346/1000: episode: 346, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5191.084, mean reward: -5191.084 [-5191.084, -5191.084], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 347/1000: episode: 347, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -8102.328, mean reward: -8102.328 [-8102.328, -8102.328], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 348/1000: episode: 348, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3910.568, mean reward: -3910.568 [-3910.568, -3910.568], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 349/1000: episode: 349, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3785.597, mean reward: -3785.597 [-3785.597, -3785.597], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 350/1000: episode: 350, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2951.342, mean reward: -2951.342 [-2951.342, -2951.342], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 351/1000: episode: 351, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -3384.851, mean reward: -3384.851 [-3384.851, -3384.851], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 352/1000: episode: 352, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2784.609, mean reward: -2784.609 [-2784.609, -2784.609], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 353/1000: episode: 353, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -8708.669, mean reward: -8708.669 [-8708.669, -8708.669], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 354/1000: episode: 354, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -1033.254, mean reward: -1033.254 [-1033.254, -1033.254], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 355/1000: episode: 355, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -276.441, mean reward: -276.441 [-276.441, -276.441], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 356/1000: episode: 356, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -1472.264, mean reward: -1472.264 [-1472.264, -1472.264], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 357/1000: episode: 357, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -7556.672, mean reward: -7556.672 [-7556.672, -7556.672], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
 358/1000: episode: 358, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1360.641, mean reward: -1360.641 [-1360.641, -1360.641], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 359/1000: episode: 359, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3035.044, mean reward: -3035.044 [-3035.044, -3035.044], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 360/1000: episode: 360, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3142.870, mean reward: -3142.870 [-3142.870, -3142.870], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 361/1000: episode: 361, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -4696.222, mean reward: -4696.222 [-4696.222, -4696.222], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 362/1000: episode: 362, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1713.124, mean reward: -1713.124 [-1713.124, -1713.124], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 363/1000: episode: 363, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3397.176, mean reward: -3397.176 [-3397.176, -3397.176], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 364/1000: episode: 364, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -2140.130, mean reward: -2140.130 [-2140.130, -2140.130], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 365/1000: episode: 365, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2589.446, mean reward: -2589.446 [-2589.446, -2589.446], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 366/1000: episode: 366, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5706.399, mean reward: -5706.399 [-5706.399, -5706.399], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 367/1000: episode: 367, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2504.784, mean reward: -2504.784 [-2504.784, -2504.784], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 368/1000: episode: 368, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1033.350, mean reward: -1033.350 [-1033.350, -1033.350], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 369/1000: episode: 369, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -3619.278, mean reward: -3619.278 [-3619.278, -3619.278], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 370/1000: episode: 370, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -1111.423, mean reward: -1111.423 [-1111.423, -1111.423], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 371/1000: episode: 371, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -3851.089, mean reward: -3851.089 [-3851.089, -3851.089], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 372/1000: episode: 372, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4885.253, mean reward: -4885.253 [-4885.253, -4885.253], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 373/1000: episode: 373, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -732.532, mean reward: -732.532 [-732.532, -732.532], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 374/1000: episode: 374, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4084.853, mean reward: -4084.853 [-4084.853, -4084.853], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 375/1000: episode: 375, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -1212.468, mean reward: -1212.468 [-1212.468, -1212.468], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 376/1000: episode: 376, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -1143.626, mean reward: -1143.626 [-1143.626, -1143.626], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 377/1000: episode: 377, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -925.721, mean reward: -925.721 [-925.721, -925.721], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 378/1000: episode: 378, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -8480.383, mean reward: -8480.383 [-8480.383, -8480.383], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 379/1000: episode: 379, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2586.088, mean reward: -2586.088 [-2586.088, -2586.088], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 380/1000: episode: 380, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3654.472, mean reward: -3654.472 [-3654.472, -3654.472], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 381/1000: episode: 381, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -3836.516, mean reward: -3836.516 [-3836.516, -3836.516], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 382/1000: episode: 382, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -3278.566, mean reward: -3278.566 [-3278.566, -3278.566], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 383/1000: episode: 383, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -343.950, mean reward: -343.950 [-343.950, -343.950], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 384/1000: episode: 384, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1104.516, mean reward: -1104.516 [-1104.516, -1104.516], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 385/1000: episode: 385, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5855.667, mean reward: -5855.667 [-5855.667, -5855.667], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 386/1000: episode: 386, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -2071.228, mean reward: -2071.228 [-2071.228, -2071.228], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 387/1000: episode: 387, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1880.630, mean reward: -1880.630 [-1880.630, -1880.630], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 388/1000: episode: 388, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -1799.224, mean reward: -1799.224 [-1799.224, -1799.224], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 389/1000: episode: 389, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -2242.368, mean reward: -2242.368 [-2242.368, -2242.368], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 390/1000: episode: 390, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -447.556, mean reward: -447.556 [-447.556, -447.556], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 391/1000: episode: 391, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -4691.767, mean reward: -4691.767 [-4691.767, -4691.767], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 392/1000: episode: 392, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5890.728, mean reward: -5890.728 [-5890.728, -5890.728], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 393/1000: episode: 393, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -411.211, mean reward: -411.211 [-411.211, -411.211], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 394/1000: episode: 394, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -2120.920, mean reward: -2120.920 [-2120.920, -2120.920], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 395/1000: episode: 395, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -978.774, mean reward: -978.774 [-978.774, -978.774], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 396/1000: episode: 396, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -604.124, mean reward: -604.124 [-604.124, -604.124], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 397/1000: episode: 397, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -4306.151, mean reward: -4306.151 [-4306.151, -4306.151], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 398/1000: episode: 398, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1538.439, mean reward: -1538.439 [-1538.439, -1538.439], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 399/1000: episode: 399, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1994.974, mean reward: -1994.974 [-1994.974, -1994.974], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 400/1000: episode: 400, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -3133.982, mean reward: -3133.982 [-3133.982, -3133.982], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 401/1000: episode: 401, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -3788.372, mean reward: -3788.372 [-3788.372, -3788.372], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 402/1000: episode: 402, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -2653.087, mean reward: -2653.087 [-2653.087, -2653.087], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 403/1000: episode: 403, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3401.823, mean reward: -3401.823 [-3401.823, -3401.823], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 404/1000: episode: 404, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -2486.541, mean reward: -2486.541 [-2486.541, -2486.541], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 405/1000: episode: 405, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -269.017, mean reward: -269.017 [-269.017, -269.017], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 406/1000: episode: 406, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3639.786, mean reward: -3639.786 [-3639.786, -3639.786], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 407/1000: episode: 407, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -3459.260, mean reward: -3459.260 [-3459.260, -3459.260], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 408/1000: episode: 408, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2385.106, mean reward: -2385.106 [-2385.106, -2385.106], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 409/1000: episode: 409, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -477.449, mean reward: -477.449 [-477.449, -477.449], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 410/1000: episode: 410, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1119.969, mean reward: -1119.969 [-1119.969, -1119.969], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 411/1000: episode: 411, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -3247.924, mean reward: -3247.924 [-3247.924, -3247.924], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 412/1000: episode: 412, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -15.110, mean reward: -15.110 [-15.110, -15.110], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 413/1000: episode: 413, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -2077.800, mean reward: -2077.800 [-2077.800, -2077.800], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 414/1000: episode: 414, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2765.476, mean reward: -2765.476 [-2765.476, -2765.476], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 415/1000: episode: 415, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -3908.515, mean reward: -3908.515 [-3908.515, -3908.515], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 416/1000: episode: 416, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2603.928, mean reward: -2603.928 [-2603.928, -2603.928], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 417/1000: episode: 417, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -405.350, mean reward: -405.350 [-405.350, -405.350], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 418/1000: episode: 418, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -58.241, mean reward: -58.241 [-58.241, -58.241], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 419/1000: episode: 419, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6404.902, mean reward: -6404.902 [-6404.902, -6404.902], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 420/1000: episode: 420, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -6693.753, mean reward: -6693.753 [-6693.753, -6693.753], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 421/1000: episode: 421, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3553.455, mean reward: -3553.455 [-3553.455, -3553.455], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 422/1000: episode: 422, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5994.498, mean reward: -5994.498 [-5994.498, -5994.498], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
 423/1000: episode: 423, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -23.801, mean reward: -23.801 [-23.801, -23.801], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 424/1000: episode: 424, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5425.641, mean reward: -5425.641 [-5425.641, -5425.641], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 425/1000: episode: 425, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5619.450, mean reward: -5619.450 [-5619.450, -5619.450], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 426/1000: episode: 426, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -280.275, mean reward: -280.275 [-280.275, -280.275], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 427/1000: episode: 427, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1795.262, mean reward: -1795.262 [-1795.262, -1795.262], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 428/1000: episode: 428, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1865.994, mean reward: -1865.994 [-1865.994, -1865.994], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 429/1000: episode: 429, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2478.942, mean reward: -2478.942 [-2478.942, -2478.942], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 430/1000: episode: 430, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2461.616, mean reward: -2461.616 [-2461.616, -2461.616], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 431/1000: episode: 431, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2749.591, mean reward: -2749.591 [-2749.591, -2749.591], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 432/1000: episode: 432, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -2296.208, mean reward: -2296.208 [-2296.208, -2296.208], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 433/1000: episode: 433, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -2077.595, mean reward: -2077.595 [-2077.595, -2077.595], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 434/1000: episode: 434, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1218.999, mean reward: -1218.999 [-1218.999, -1218.999], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 435/1000: episode: 435, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -1348.300, mean reward: -1348.300 [-1348.300, -1348.300], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 436/1000: episode: 436, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -7122.367, mean reward: -7122.367 [-7122.367, -7122.367], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 437/1000: episode: 437, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1138.732, mean reward: -1138.732 [-1138.732, -1138.732], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 438/1000: episode: 438, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -8178.035, mean reward: -8178.035 [-8178.035, -8178.035], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 439/1000: episode: 439, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -7490.165, mean reward: -7490.165 [-7490.165, -7490.165], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
 440/1000: episode: 440, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -505.587, mean reward: -505.587 [-505.587, -505.587], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 441/1000: episode: 441, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5844.703, mean reward: -5844.703 [-5844.703, -5844.703], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --
 442/1000: episode: 442, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -1748.418, mean reward: -1748.418 [-1748.418, -1748.418], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 443/1000: episode: 443, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -4408.715, mean reward: -4408.715 [-4408.715, -4408.715], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 444/1000: episode: 444, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5955.378, mean reward: -5955.378 [-5955.378, -5955.378], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 445/1000: episode: 445, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5512.067, mean reward: -5512.067 [-5512.067, -5512.067], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 446/1000: episode: 446, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -384.922, mean reward: -384.922 [-384.922, -384.922], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 447/1000: episode: 447, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -4407.466, mean reward: -4407.466 [-4407.466, -4407.466], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 448/1000: episode: 448, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3765.545, mean reward: -3765.545 [-3765.545, -3765.545], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 449/1000: episode: 449, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -3713.512, mean reward: -3713.512 [-3713.512, -3713.512], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 450/1000: episode: 450, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -984.024, mean reward: -984.024 [-984.024, -984.024], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 451/1000: episode: 451, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -7221.575, mean reward: -7221.575 [-7221.575, -7221.575], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 452/1000: episode: 452, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -519.447, mean reward: -519.447 [-519.447, -519.447], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 453/1000: episode: 453, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -999.636, mean reward: -999.636 [-999.636, -999.636], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 454/1000: episode: 454, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5249.630, mean reward: -5249.630 [-5249.630, -5249.630], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 455/1000: episode: 455, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -1601.002, mean reward: -1601.002 [-1601.002, -1601.002], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 456/1000: episode: 456, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -6262.273, mean reward: -6262.273 [-6262.273, -6262.273], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 457/1000: episode: 457, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -781.732, mean reward: -781.732 [-781.732, -781.732], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 458/1000: episode: 458, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -996.224, mean reward: -996.224 [-996.224, -996.224], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 459/1000: episode: 459, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3163.101, mean reward: -3163.101 [-3163.101, -3163.101], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 460/1000: episode: 460, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1081.342, mean reward: -1081.342 [-1081.342, -1081.342], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 461/1000: episode: 461, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -3234.945, mean reward: -3234.945 [-3234.945, -3234.945], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 462/1000: episode: 462, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2512.966, mean reward: -2512.966 [-2512.966, -2512.966], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 463/1000: episode: 463, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -1957.797, mean reward: -1957.797 [-1957.797, -1957.797], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 464/1000: episode: 464, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -371.428, mean reward: -371.428 [-371.428, -371.428], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 465/1000: episode: 465, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3049.680, mean reward: -3049.680 [-3049.680, -3049.680], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 466/1000: episode: 466, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -7871.659, mean reward: -7871.659 [-7871.659, -7871.659], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --
 467/1000: episode: 467, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -19.920, mean reward: -19.920 [-19.920, -19.920], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 468/1000: episode: 468, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -1601.184, mean reward: -1601.184 [-1601.184, -1601.184], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 469/1000: episode: 469, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -530.180, mean reward: -530.180 [-530.180, -530.180], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 470/1000: episode: 470, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -3546.601, mean reward: -3546.601 [-3546.601, -3546.601], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 471/1000: episode: 471, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -2546.303, mean reward: -2546.303 [-2546.303, -2546.303], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 472/1000: episode: 472, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5568.720, mean reward: -5568.720 [-5568.720, -5568.720], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 473/1000: episode: 473, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -1198.029, mean reward: -1198.029 [-1198.029, -1198.029], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 474/1000: episode: 474, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -925.638, mean reward: -925.638 [-925.638, -925.638], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 475/1000: episode: 475, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -4068.530, mean reward: -4068.530 [-4068.530, -4068.530], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 476/1000: episode: 476, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5631.472, mean reward: -5631.472 [-5631.472, -5631.472], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 477/1000: episode: 477, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -2819.350, mean reward: -2819.350 [-2819.350, -2819.350], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 478/1000: episode: 478, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -299.258, mean reward: -299.258 [-299.258, -299.258], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 479/1000: episode: 479, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2003.039, mean reward: -2003.039 [-2003.039, -2003.039], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 480/1000: episode: 480, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -578.921, mean reward: -578.921 [-578.921, -578.921], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 481/1000: episode: 481, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4044.177, mean reward: -4044.177 [-4044.177, -4044.177], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 482/1000: episode: 482, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1723.997, mean reward: -1723.997 [-1723.997, -1723.997], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 483/1000: episode: 483, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -285.857, mean reward: -285.857 [-285.857, -285.857], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 484/1000: episode: 484, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -2938.169, mean reward: -2938.169 [-2938.169, -2938.169], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 485/1000: episode: 485, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -4476.503, mean reward: -4476.503 [-4476.503, -4476.503], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 486/1000: episode: 486, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -500.927, mean reward: -500.927 [-500.927, -500.927], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 487/1000: episode: 487, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -6682.944, mean reward: -6682.944 [-6682.944, -6682.944], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 488/1000: episode: 488, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5321.575, mean reward: -5321.575 [-5321.575, -5321.575], mean action: 0.000 [0.000, 0.000],  loss: --, mae: --, mean_q: --
 489/1000: episode: 489, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -400.549, mean reward: -400.549 [-400.549, -400.549], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 490/1000: episode: 490, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2001.328, mean reward: -2001.328 [-2001.328, -2001.328], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 491/1000: episode: 491, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -820.154, mean reward: -820.154 [-820.154, -820.154], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 492/1000: episode: 492, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -2713.401, mean reward: -2713.401 [-2713.401, -2713.401], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 493/1000: episode: 493, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -2080.783, mean reward: -2080.783 [-2080.783, -2080.783], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 494/1000: episode: 494, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -8486.915, mean reward: -8486.915 [-8486.915, -8486.915], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 495/1000: episode: 495, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -594.119, mean reward: -594.119 [-594.119, -594.119], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 496/1000: episode: 496, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6706.706, mean reward: -6706.706 [-6706.706, -6706.706], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 497/1000: episode: 497, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -10.621, mean reward: -10.621 [-10.621, -10.621], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 498/1000: episode: 498, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -2002.043, mean reward: -2002.043 [-2002.043, -2002.043], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 499/1000: episode: 499, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -4027.195, mean reward: -4027.195 [-4027.195, -4027.195], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 500/1000: episode: 500, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -2779.964, mean reward: -2779.964 [-2779.964, -2779.964], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 501/1000: episode: 501, duration: 0.483s, episode steps:   1, steps per second:   2, episode reward: -233.907, mean reward: -233.907 [-233.907, -233.907], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --
 502/1000: episode: 502, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1652.112, mean reward: -1652.112 [-1652.112, -1652.112], mean action: 2.000 [2.000, 2.000],  loss: 4622503.500000, mae: 608.531860, mean_q: 0.827916
 503/1000: episode: 503, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -121.633, mean reward: -121.633 [-121.633, -121.633], mean action: 2.000 [2.000, 2.000],  loss: 6649643.500000, mae: 772.770142, mean_q: 0.785223
 504/1000: episode: 504, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -734.109, mean reward: -734.109 [-734.109, -734.109], mean action: 2.000 [2.000, 2.000],  loss: 5492474.000000, mae: 645.944214, mean_q: 0.751782
 505/1000: episode: 505, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2726.245, mean reward: -2726.245 [-2726.245, -2726.245], mean action: 2.000 [2.000, 2.000],  loss: 3776062.750000, mae: 601.080750, mean_q: 0.726278
 506/1000: episode: 506, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1556.637, mean reward: -1556.637 [-1556.637, -1556.637], mean action: 2.000 [2.000, 2.000],  loss: 3936980.500000, mae: 586.157776, mean_q: 0.694668
 507/1000: episode: 507, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -940.440, mean reward: -940.440 [-940.440, -940.440], mean action: 2.000 [2.000, 2.000],  loss: 4175406.250000, mae: 532.994202, mean_q: 0.656940
 508/1000: episode: 508, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4611.596, mean reward: -4611.596 [-4611.596, -4611.596], mean action: 2.000 [2.000, 2.000],  loss: 8701046.000000, mae: 872.504028, mean_q: 0.618959
 509/1000: episode: 509, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3517.464, mean reward: -3517.464 [-3517.464, -3517.464], mean action: 2.000 [2.000, 2.000],  loss: 6377824.000000, mae: 645.042358, mean_q: 0.591925
 510/1000: episode: 510, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -7282.271, mean reward: -7282.271 [-7282.271, -7282.271], mean action: 2.000 [2.000, 2.000],  loss: 11229068.000000, mae: 863.908936, mean_q: 0.551501
 511/1000: episode: 511, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4879.983, mean reward: -4879.983 [-4879.983, -4879.983], mean action: 2.000 [2.000, 2.000],  loss: 5616156.000000, mae: 648.601013, mean_q: 0.520632
 512/1000: episode: 512, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1475.084, mean reward: -1475.084 [-1475.084, -1475.084], mean action: 2.000 [2.000, 2.000],  loss: 7361927.000000, mae: 802.697144, mean_q: 0.492380
 513/1000: episode: 513, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -337.111, mean reward: -337.111 [-337.111, -337.111], mean action: 2.000 [2.000, 2.000],  loss: 5843644.000000, mae: 733.898804, mean_q: 0.468179
 514/1000: episode: 514, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2460.340, mean reward: -2460.340 [-2460.340, -2460.340], mean action: 2.000 [2.000, 2.000],  loss: 3845498.000000, mae: 578.059387, mean_q: 0.424116
 515/1000: episode: 515, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -295.038, mean reward: -295.038 [-295.038, -295.038], mean action: 2.000 [2.000, 2.000],  loss: 5484831.500000, mae: 680.529663, mean_q: 0.399335
 516/1000: episode: 516, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -1273.997, mean reward: -1273.997 [-1273.997, -1273.997], mean action: 2.000 [2.000, 2.000],  loss: 4250644.000000, mae: 590.956848, mean_q: 0.361126
 517/1000: episode: 517, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4056.001, mean reward: -4056.001 [-4056.001, -4056.001], mean action: 2.000 [2.000, 2.000],  loss: 7952142.500000, mae: 766.478516, mean_q: 0.339884
 518/1000: episode: 518, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4678.390, mean reward: -4678.390 [-4678.390, -4678.390], mean action: 2.000 [2.000, 2.000],  loss: 6670779.000000, mae: 754.737183, mean_q: 0.300035
 519/1000: episode: 519, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -1021.736, mean reward: -1021.736 [-1021.736, -1021.736], mean action: 2.000 [2.000, 2.000],  loss: 5188553.000000, mae: 640.719543, mean_q: 0.275465
 520/1000: episode: 520, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2190.674, mean reward: -2190.674 [-2190.674, -2190.674], mean action: 2.000 [2.000, 2.000],  loss: 4501246.000000, mae: 546.402100, mean_q: 0.254210
 521/1000: episode: 521, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4298.270, mean reward: -4298.270 [-4298.270, -4298.270], mean action: 2.000 [2.000, 2.000],  loss: 7864821.500000, mae: 833.740540, mean_q: 0.241586
 522/1000: episode: 522, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -283.196, mean reward: -283.196 [-283.196, -283.196], mean action: 2.000 [2.000, 2.000],  loss: 4668071.500000, mae: 600.030151, mean_q: 0.218271
 523/1000: episode: 523, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4012.790, mean reward: -4012.790 [-4012.790, -4012.790], mean action: 3.000 [3.000, 3.000],  loss: 6146283.500000, mae: 716.569336, mean_q: 0.207204
 524/1000: episode: 524, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -954.345, mean reward: -954.345 [-954.345, -954.345], mean action: 3.000 [3.000, 3.000],  loss: 5189987.500000, mae: 713.283447, mean_q: 0.207963
 525/1000: episode: 525, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -10454.824, mean reward: -10454.824 [-10454.824, -10454.824], mean action: 3.000 [3.000, 3.000],  loss: 6455338.000000, mae: 721.941223, mean_q: 0.194569
 526/1000: episode: 526, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -1178.728, mean reward: -1178.728 [-1178.728, -1178.728], mean action: 3.000 [3.000, 3.000],  loss: 5277662.500000, mae: 646.265259, mean_q: 0.176133
 527/1000: episode: 527, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5817.625, mean reward: -5817.625 [-5817.625, -5817.625], mean action: 3.000 [3.000, 3.000],  loss: 3795723.250000, mae: 522.690857, mean_q: 0.152876
 528/1000: episode: 528, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -254.675, mean reward: -254.675 [-254.675, -254.675], mean action: 3.000 [3.000, 3.000],  loss: 8664586.000000, mae: 725.109802, mean_q: 0.156663
 529/1000: episode: 529, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -616.320, mean reward: -616.320 [-616.320, -616.320], mean action: 3.000 [3.000, 3.000],  loss: 4226375.000000, mae: 525.734680, mean_q: 0.133968
 530/1000: episode: 530, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -724.633, mean reward: -724.633 [-724.633, -724.633], mean action: 3.000 [3.000, 3.000],  loss: 6654975.000000, mae: 701.034058, mean_q: 0.136736
 531/1000: episode: 531, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -6671.257, mean reward: -6671.257 [-6671.257, -6671.257], mean action: 3.000 [3.000, 3.000],  loss: 5674405.000000, mae: 733.041870, mean_q: 0.118248
 532/1000: episode: 532, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2507.232, mean reward: -2507.232 [-2507.232, -2507.232], mean action: 3.000 [3.000, 3.000],  loss: 8354010.000000, mae: 751.216553, mean_q: 0.104755
 533/1000: episode: 533, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4467.312, mean reward: -4467.312 [-4467.312, -4467.312], mean action: 3.000 [3.000, 3.000],  loss: 9647950.000000, mae: 925.669434, mean_q: 0.100428
 534/1000: episode: 534, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -10038.719, mean reward: -10038.719 [-10038.719, -10038.719], mean action: 3.000 [3.000, 3.000],  loss: 7771263.000000, mae: 817.368408, mean_q: 0.094328
 535/1000: episode: 535, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -21.927, mean reward: -21.927 [-21.927, -21.927], mean action: 3.000 [3.000, 3.000],  loss: 5207138.000000, mae: 639.230713, mean_q: 0.075674
 536/1000: episode: 536, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5630.302, mean reward: -5630.302 [-5630.302, -5630.302], mean action: 3.000 [3.000, 3.000],  loss: 9956814.000000, mae: 862.022705, mean_q: 0.061474
 537/1000: episode: 537, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -8848.191, mean reward: -8848.191 [-8848.191, -8848.191], mean action: 3.000 [3.000, 3.000],  loss: 5616713.000000, mae: 654.083130, mean_q: 0.044040
 538/1000: episode: 538, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -4341.721, mean reward: -4341.721 [-4341.721, -4341.721], mean action: 3.000 [3.000, 3.000],  loss: 5915994.500000, mae: 703.475708, mean_q: 0.036053
 539/1000: episode: 539, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -4857.160, mean reward: -4857.160 [-4857.160, -4857.160], mean action: 3.000 [3.000, 3.000],  loss: 9920564.000000, mae: 924.363281, mean_q: 0.033983
 540/1000: episode: 540, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -6213.452, mean reward: -6213.452 [-6213.452, -6213.452], mean action: 3.000 [3.000, 3.000],  loss: 8881996.000000, mae: 792.054688, mean_q: 0.016356
 541/1000: episode: 541, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -2073.110, mean reward: -2073.110 [-2073.110, -2073.110], mean action: 3.000 [3.000, 3.000],  loss: 8297280.500000, mae: 805.769409, mean_q: 0.000287
 542/1000: episode: 542, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -4831.721, mean reward: -4831.721 [-4831.721, -4831.721], mean action: 3.000 [3.000, 3.000],  loss: 11647805.000000, mae: 890.588501, mean_q: -0.017751
 543/1000: episode: 543, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4096.982, mean reward: -4096.982 [-4096.982, -4096.982], mean action: 0.000 [0.000, 0.000],  loss: 5784063.000000, mae: 694.791992, mean_q: -0.032435
 544/1000: episode: 544, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4999.527, mean reward: -4999.527 [-4999.527, -4999.527], mean action: 3.000 [3.000, 3.000],  loss: 5860405.000000, mae: 697.545105, mean_q: -0.047014
 545/1000: episode: 545, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3841.899, mean reward: -3841.899 [-3841.899, -3841.899], mean action: 3.000 [3.000, 3.000],  loss: 6788651.000000, mae: 762.095581, mean_q: -0.053311
 546/1000: episode: 546, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -7811.653, mean reward: -7811.653 [-7811.653, -7811.653], mean action: 1.000 [1.000, 1.000],  loss: 6859878.000000, mae: 782.891602, mean_q: -0.065030
 547/1000: episode: 547, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -562.575, mean reward: -562.575 [-562.575, -562.575], mean action: 3.000 [3.000, 3.000],  loss: 9156476.000000, mae: 817.553955, mean_q: -0.083731
 548/1000: episode: 548, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -8239.620, mean reward: -8239.620 [-8239.620, -8239.620], mean action: 3.000 [3.000, 3.000],  loss: 5704301.000000, mae: 631.077026, mean_q: -0.112302
 549/1000: episode: 549, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5096.329, mean reward: -5096.329 [-5096.329, -5096.329], mean action: 1.000 [1.000, 1.000],  loss: 5020490.000000, mae: 636.101868, mean_q: -0.120212
 550/1000: episode: 550, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8701.742, mean reward: -8701.742 [-8701.742, -8701.742], mean action: 3.000 [3.000, 3.000],  loss: 8688384.000000, mae: 840.054199, mean_q: -0.121635
 551/1000: episode: 551, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -9172.956, mean reward: -9172.956 [-9172.956, -9172.956], mean action: 3.000 [3.000, 3.000],  loss: 7264603.000000, mae: 749.395386, mean_q: -0.142418
 552/1000: episode: 552, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -3041.061, mean reward: -3041.061 [-3041.061, -3041.061], mean action: 1.000 [1.000, 1.000],  loss: 7911228.000000, mae: 683.267456, mean_q: -0.140780
 553/1000: episode: 553, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -8288.652, mean reward: -8288.652 [-8288.652, -8288.652], mean action: 1.000 [1.000, 1.000],  loss: 8897181.000000, mae: 787.878784, mean_q: -0.150495
 554/1000: episode: 554, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -901.915, mean reward: -901.915 [-901.915, -901.915], mean action: 1.000 [1.000, 1.000],  loss: 7789757.000000, mae: 734.038269, mean_q: -0.152897
 555/1000: episode: 555, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4902.568, mean reward: -4902.568 [-4902.568, -4902.568], mean action: 2.000 [2.000, 2.000],  loss: 7075499.500000, mae: 677.295227, mean_q: -0.157231
 556/1000: episode: 556, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3562.491, mean reward: -3562.491 [-3562.491, -3562.491], mean action: 1.000 [1.000, 1.000],  loss: 6914727.000000, mae: 748.302612, mean_q: -0.157110
 557/1000: episode: 557, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3533.530, mean reward: -3533.530 [-3533.530, -3533.530], mean action: 1.000 [1.000, 1.000],  loss: 9942319.000000, mae: 909.390503, mean_q: -0.162576
 558/1000: episode: 558, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -2734.707, mean reward: -2734.707 [-2734.707, -2734.707], mean action: 0.000 [0.000, 0.000],  loss: 4518951.000000, mae: 602.082275, mean_q: -0.173644
 559/1000: episode: 559, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -2952.189, mean reward: -2952.189 [-2952.189, -2952.189], mean action: 1.000 [1.000, 1.000],  loss: 7361330.000000, mae: 700.397705, mean_q: -0.164278
 560/1000: episode: 560, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -407.530, mean reward: -407.530 [-407.530, -407.530], mean action: 1.000 [1.000, 1.000],  loss: 6914958.000000, mae: 781.886902, mean_q: -0.184074
 561/1000: episode: 561, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -10180.275, mean reward: -10180.275 [-10180.275, -10180.275], mean action: 3.000 [3.000, 3.000],  loss: 8567432.000000, mae: 872.876282, mean_q: -0.188445
 562/1000: episode: 562, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -3806.211, mean reward: -3806.211 [-3806.211, -3806.211], mean action: 1.000 [1.000, 1.000],  loss: 10563148.000000, mae: 818.081055, mean_q: -0.181419
 563/1000: episode: 563, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1810.600, mean reward: -1810.600 [-1810.600, -1810.600], mean action: 1.000 [1.000, 1.000],  loss: 7343506.500000, mae: 756.053833, mean_q: -0.180254
 564/1000: episode: 564, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -11973.579, mean reward: -11973.579 [-11973.579, -11973.579], mean action: 0.000 [0.000, 0.000],  loss: 5845016.000000, mae: 643.755005, mean_q: -0.189136
 565/1000: episode: 565, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7073.428, mean reward: -7073.428 [-7073.428, -7073.428], mean action: 0.000 [0.000, 0.000],  loss: 6874499.500000, mae: 788.616211, mean_q: -0.195950
 566/1000: episode: 566, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -6080.899, mean reward: -6080.899 [-6080.899, -6080.899], mean action: 0.000 [0.000, 0.000],  loss: 7080502.000000, mae: 755.707520, mean_q: -0.186529
 567/1000: episode: 567, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -6964.893, mean reward: -6964.893 [-6964.893, -6964.893], mean action: 0.000 [0.000, 0.000],  loss: 8315012.000000, mae: 778.621460, mean_q: -0.189134
 568/1000: episode: 568, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5721.685, mean reward: -5721.685 [-5721.685, -5721.685], mean action: 0.000 [0.000, 0.000],  loss: 5513288.500000, mae: 668.273315, mean_q: -0.186067
 569/1000: episode: 569, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -1970.457, mean reward: -1970.457 [-1970.457, -1970.457], mean action: 0.000 [0.000, 0.000],  loss: 7510879.500000, mae: 802.171265, mean_q: -0.181509
 570/1000: episode: 570, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -4559.561, mean reward: -4559.561 [-4559.561, -4559.561], mean action: 0.000 [0.000, 0.000],  loss: 11073326.000000, mae: 856.443298, mean_q: -0.182298
 571/1000: episode: 571, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -7673.513, mean reward: -7673.513 [-7673.513, -7673.513], mean action: 3.000 [3.000, 3.000],  loss: 7789411.500000, mae: 812.803101, mean_q: -0.189424
 572/1000: episode: 572, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -2424.819, mean reward: -2424.819 [-2424.819, -2424.819], mean action: 1.000 [1.000, 1.000],  loss: 10141145.000000, mae: 948.641235, mean_q: -0.173862
 573/1000: episode: 573, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1437.289, mean reward: -1437.289 [-1437.289, -1437.289], mean action: 2.000 [2.000, 2.000],  loss: 6051501.000000, mae: 662.792786, mean_q: -0.184336
 574/1000: episode: 574, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -9196.111, mean reward: -9196.111 [-9196.111, -9196.111], mean action: 0.000 [0.000, 0.000],  loss: 12619092.000000, mae: 909.848694, mean_q: -0.184849
 575/1000: episode: 575, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3574.072, mean reward: -3574.072 [-3574.072, -3574.072], mean action: 1.000 [1.000, 1.000],  loss: 9340984.000000, mae: 817.512146, mean_q: -0.181449
 576/1000: episode: 576, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -12390.593, mean reward: -12390.593 [-12390.593, -12390.593], mean action: 0.000 [0.000, 0.000],  loss: 6250848.000000, mae: 720.686401, mean_q: -0.183993
 577/1000: episode: 577, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -12761.688, mean reward: -12761.688 [-12761.688, -12761.688], mean action: 0.000 [0.000, 0.000],  loss: 9280982.000000, mae: 753.372437, mean_q: -0.180541
 578/1000: episode: 578, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2849.726, mean reward: -2849.726 [-2849.726, -2849.726], mean action: 1.000 [1.000, 1.000],  loss: 9609372.000000, mae: 889.264954, mean_q: -0.172707
 579/1000: episode: 579, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -6764.606, mean reward: -6764.606 [-6764.606, -6764.606], mean action: 0.000 [0.000, 0.000],  loss: 6743796.000000, mae: 668.303162, mean_q: -0.176475
 580/1000: episode: 580, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -9134.281, mean reward: -9134.281 [-9134.281, -9134.281], mean action: 0.000 [0.000, 0.000],  loss: 12107892.000000, mae: 1007.648682, mean_q: -0.174107
 581/1000: episode: 581, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -8628.556, mean reward: -8628.556 [-8628.556, -8628.556], mean action: 0.000 [0.000, 0.000],  loss: 9005292.000000, mae: 785.415894, mean_q: -0.171738
 582/1000: episode: 582, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -4976.434, mean reward: -4976.434 [-4976.434, -4976.434], mean action: 0.000 [0.000, 0.000],  loss: 6543713.500000, mae: 700.065918, mean_q: -0.171023
 583/1000: episode: 583, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -6358.141, mean reward: -6358.141 [-6358.141, -6358.141], mean action: 0.000 [0.000, 0.000],  loss: 8556848.000000, mae: 816.481079, mean_q: -0.164410
 584/1000: episode: 584, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2033.109, mean reward: -2033.109 [-2033.109, -2033.109], mean action: 0.000 [0.000, 0.000],  loss: 5844965.000000, mae: 622.434204, mean_q: -0.174518
 585/1000: episode: 585, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4569.999, mean reward: -4569.999 [-4569.999, -4569.999], mean action: 0.000 [0.000, 0.000],  loss: 5230235.000000, mae: 639.039307, mean_q: -0.170304
 586/1000: episode: 586, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -6962.060, mean reward: -6962.060 [-6962.060, -6962.060], mean action: 0.000 [0.000, 0.000],  loss: 12526810.000000, mae: 935.119751, mean_q: -0.166661
 587/1000: episode: 587, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -2911.118, mean reward: -2911.118 [-2911.118, -2911.118], mean action: 0.000 [0.000, 0.000],  loss: 10444883.000000, mae: 966.016663, mean_q: -0.164409
 588/1000: episode: 588, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -9893.822, mean reward: -9893.822 [-9893.822, -9893.822], mean action: 0.000 [0.000, 0.000],  loss: 7664886.500000, mae: 637.936707, mean_q: -0.166625
 589/1000: episode: 589, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4940.158, mean reward: -4940.158 [-4940.158, -4940.158], mean action: 0.000 [0.000, 0.000],  loss: 6057084.500000, mae: 711.664978, mean_q: -0.161271
 590/1000: episode: 590, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -13311.357, mean reward: -13311.357 [-13311.357, -13311.357], mean action: 0.000 [0.000, 0.000],  loss: 9527670.000000, mae: 798.902649, mean_q: -0.176270
 591/1000: episode: 591, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7208.194, mean reward: -7208.194 [-7208.194, -7208.194], mean action: 0.000 [0.000, 0.000],  loss: 11548162.000000, mae: 895.281616, mean_q: -0.174196
 592/1000: episode: 592, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -7770.110, mean reward: -7770.110 [-7770.110, -7770.110], mean action: 0.000 [0.000, 0.000],  loss: 11220022.000000, mae: 824.737427, mean_q: -0.169554
 593/1000: episode: 593, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -6780.666, mean reward: -6780.666 [-6780.666, -6780.666], mean action: 0.000 [0.000, 0.000],  loss: 10215158.000000, mae: 816.769165, mean_q: -0.183171
 594/1000: episode: 594, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -7373.929, mean reward: -7373.929 [-7373.929, -7373.929], mean action: 0.000 [0.000, 0.000],  loss: 9728660.000000, mae: 923.769165, mean_q: -0.182817
 595/1000: episode: 595, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -6221.038, mean reward: -6221.038 [-6221.038, -6221.038], mean action: 0.000 [0.000, 0.000],  loss: 8717670.000000, mae: 815.913818, mean_q: -0.192254
 596/1000: episode: 596, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5095.324, mean reward: -5095.324 [-5095.324, -5095.324], mean action: 0.000 [0.000, 0.000],  loss: 9582812.000000, mae: 847.783325, mean_q: -0.201514
 597/1000: episode: 597, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -8415.473, mean reward: -8415.473 [-8415.473, -8415.473], mean action: 3.000 [3.000, 3.000],  loss: 7073273.000000, mae: 808.300903, mean_q: -0.202845
 598/1000: episode: 598, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4917.917, mean reward: -4917.917 [-4917.917, -4917.917], mean action: 0.000 [0.000, 0.000],  loss: 11425098.000000, mae: 968.932007, mean_q: -0.203567
 599/1000: episode: 599, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5465.193, mean reward: -5465.193 [-5465.193, -5465.193], mean action: 0.000 [0.000, 0.000],  loss: 6638705.500000, mae: 696.241699, mean_q: -0.220105
 600/1000: episode: 600, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -3846.503, mean reward: -3846.503 [-3846.503, -3846.503], mean action: 0.000 [0.000, 0.000],  loss: 8328256.500000, mae: 804.241821, mean_q: -0.214031
 601/1000: episode: 601, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -641.242, mean reward: -641.242 [-641.242, -641.242], mean action: 3.000 [3.000, 3.000],  loss: 5515224.500000, mae: 634.762451, mean_q: -0.220215
 602/1000: episode: 602, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -13981.220, mean reward: -13981.220 [-13981.220, -13981.220], mean action: 0.000 [0.000, 0.000],  loss: 7804741.500000, mae: 763.384277, mean_q: -0.215386
 603/1000: episode: 603, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -7089.929, mean reward: -7089.929 [-7089.929, -7089.929], mean action: 0.000 [0.000, 0.000],  loss: 8295558.500000, mae: 799.031494, mean_q: -0.224595
 604/1000: episode: 604, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4704.186, mean reward: -4704.186 [-4704.186, -4704.186], mean action: 0.000 [0.000, 0.000],  loss: 5938691.000000, mae: 694.596558, mean_q: -0.237610
 605/1000: episode: 605, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -3083.061, mean reward: -3083.061 [-3083.061, -3083.061], mean action: 0.000 [0.000, 0.000],  loss: 7232497.000000, mae: 756.514038, mean_q: -0.232279
 606/1000: episode: 606, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -8166.687, mean reward: -8166.687 [-8166.687, -8166.687], mean action: 0.000 [0.000, 0.000],  loss: 10580616.000000, mae: 854.200317, mean_q: -0.249216
 607/1000: episode: 607, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -13840.120, mean reward: -13840.120 [-13840.120, -13840.120], mean action: 0.000 [0.000, 0.000],  loss: 7533306.500000, mae: 757.615356, mean_q: -0.250678
 608/1000: episode: 608, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2471.158, mean reward: -2471.158 [-2471.158, -2471.158], mean action: 3.000 [3.000, 3.000],  loss: 6261754.000000, mae: 743.421265, mean_q: -0.255948
 609/1000: episode: 609, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -12131.760, mean reward: -12131.760 [-12131.760, -12131.760], mean action: 0.000 [0.000, 0.000],  loss: 14729709.000000, mae: 1037.038452, mean_q: -0.263191
 610/1000: episode: 610, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -8047.946, mean reward: -8047.946 [-8047.946, -8047.946], mean action: 0.000 [0.000, 0.000],  loss: 7955696.000000, mae: 809.860229, mean_q: -0.277655
 611/1000: episode: 611, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -12043.994, mean reward: -12043.994 [-12043.994, -12043.994], mean action: 0.000 [0.000, 0.000],  loss: 10759114.000000, mae: 816.498901, mean_q: -0.288656
 612/1000: episode: 612, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.874, mean reward: -5000.874 [-5000.874, -5000.874], mean action: 0.000 [0.000, 0.000],  loss: 10774698.000000, mae: 881.342285, mean_q: -0.288736
 613/1000: episode: 613, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -907.914, mean reward: -907.914 [-907.914, -907.914], mean action: 1.000 [1.000, 1.000],  loss: 11630164.000000, mae: 942.020142, mean_q: -0.291614
 614/1000: episode: 614, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4444.446, mean reward: -4444.446 [-4444.446, -4444.446], mean action: 1.000 [1.000, 1.000],  loss: 10510664.000000, mae: 846.947571, mean_q: -0.298054
 615/1000: episode: 615, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -8870.048, mean reward: -8870.048 [-8870.048, -8870.048], mean action: 0.000 [0.000, 0.000],  loss: 9119736.000000, mae: 866.358765, mean_q: -0.306350
 616/1000: episode: 616, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5613.874, mean reward: -5613.874 [-5613.874, -5613.874], mean action: 0.000 [0.000, 0.000],  loss: 5384409.000000, mae: 595.014099, mean_q: -0.311199
 617/1000: episode: 617, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4393.859, mean reward: -4393.859 [-4393.859, -4393.859], mean action: 0.000 [0.000, 0.000],  loss: 12296466.000000, mae: 858.139404, mean_q: -0.324336
 618/1000: episode: 618, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -12011.601, mean reward: -12011.601 [-12011.601, -12011.601], mean action: 0.000 [0.000, 0.000],  loss: 11514672.000000, mae: 977.147217, mean_q: -0.313652
 619/1000: episode: 619, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4321.569, mean reward: -4321.569 [-4321.569, -4321.569], mean action: 0.000 [0.000, 0.000],  loss: 10896208.000000, mae: 833.510315, mean_q: -0.326920
 620/1000: episode: 620, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3349.629, mean reward: -3349.629 [-3349.629, -3349.629], mean action: 1.000 [1.000, 1.000],  loss: 11479500.000000, mae: 968.035767, mean_q: -0.334985
 621/1000: episode: 621, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -10879.494, mean reward: -10879.494 [-10879.494, -10879.494], mean action: 0.000 [0.000, 0.000],  loss: 5028061.000000, mae: 662.583740, mean_q: -0.341811
 622/1000: episode: 622, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -3115.402, mean reward: -3115.402 [-3115.402, -3115.402], mean action: 1.000 [1.000, 1.000],  loss: 6457449.500000, mae: 728.685425, mean_q: -0.347349
 623/1000: episode: 623, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2437.081, mean reward: -2437.081 [-2437.081, -2437.081], mean action: 1.000 [1.000, 1.000],  loss: 11903504.000000, mae: 1039.485596, mean_q: -0.350704
 624/1000: episode: 624, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -3679.946, mean reward: -3679.946 [-3679.946, -3679.946], mean action: 1.000 [1.000, 1.000],  loss: 8184544.000000, mae: 840.887634, mean_q: -0.362620
 625/1000: episode: 625, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -3152.644, mean reward: -3152.644 [-3152.644, -3152.644], mean action: 0.000 [0.000, 0.000],  loss: 10918550.000000, mae: 888.049438, mean_q: -0.372496
 626/1000: episode: 626, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -3287.121, mean reward: -3287.121 [-3287.121, -3287.121], mean action: 1.000 [1.000, 1.000],  loss: 10656294.000000, mae: 836.006470, mean_q: -0.376225
 627/1000: episode: 627, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6048.793, mean reward: -6048.793 [-6048.793, -6048.793], mean action: 0.000 [0.000, 0.000],  loss: 8776320.000000, mae: 849.061401, mean_q: -0.386273
 628/1000: episode: 628, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -9986.085, mean reward: -9986.085 [-9986.085, -9986.085], mean action: 0.000 [0.000, 0.000],  loss: 17294188.000000, mae: 1119.553467, mean_q: -0.388089
 629/1000: episode: 629, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -16339.671, mean reward: -16339.671 [-16339.671, -16339.671], mean action: 0.000 [0.000, 0.000],  loss: 10198490.000000, mae: 932.576050, mean_q: -0.389945
 630/1000: episode: 630, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -7877.486, mean reward: -7877.486 [-7877.486, -7877.486], mean action: 1.000 [1.000, 1.000],  loss: 10003140.000000, mae: 853.038940, mean_q: -0.400757
 631/1000: episode: 631, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -6886.280, mean reward: -6886.280 [-6886.280, -6886.280], mean action: 0.000 [0.000, 0.000],  loss: 11737023.000000, mae: 969.001282, mean_q: -0.413748
 632/1000: episode: 632, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7281.790, mean reward: -7281.790 [-7281.790, -7281.790], mean action: 1.000 [1.000, 1.000],  loss: 10495472.000000, mae: 927.409180, mean_q: -0.418707
 633/1000: episode: 633, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2375.499, mean reward: -2375.499 [-2375.499, -2375.499], mean action: 1.000 [1.000, 1.000],  loss: 9452121.000000, mae: 773.270752, mean_q: -0.410569
 634/1000: episode: 634, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -7358.451, mean reward: -7358.451 [-7358.451, -7358.451], mean action: 1.000 [1.000, 1.000],  loss: 6856573.000000, mae: 715.274536, mean_q: -0.419074
 635/1000: episode: 635, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7217.590, mean reward: -7217.590 [-7217.590, -7217.590], mean action: 1.000 [1.000, 1.000],  loss: 7338035.500000, mae: 765.579102, mean_q: -0.422193
 636/1000: episode: 636, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1843.346, mean reward: -1843.346 [-1843.346, -1843.346], mean action: 1.000 [1.000, 1.000],  loss: 8055843.500000, mae: 825.318604, mean_q: -0.438366
 637/1000: episode: 637, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3730.112, mean reward: -3730.112 [-3730.112, -3730.112], mean action: 1.000 [1.000, 1.000],  loss: 9194258.000000, mae: 889.517090, mean_q: -0.441318
 638/1000: episode: 638, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -2145.363, mean reward: -2145.363 [-2145.363, -2145.363], mean action: 1.000 [1.000, 1.000],  loss: 5093788.500000, mae: 671.947876, mean_q: -0.431618
 639/1000: episode: 639, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -194.974, mean reward: -194.974 [-194.974, -194.974], mean action: 1.000 [1.000, 1.000],  loss: 7877364.000000, mae: 733.147217, mean_q: -0.438561
 640/1000: episode: 640, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -664.895, mean reward: -664.895 [-664.895, -664.895], mean action: 1.000 [1.000, 1.000],  loss: 8296052.500000, mae: 866.729431, mean_q: -0.455844
 641/1000: episode: 641, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -951.170, mean reward: -951.170 [-951.170, -951.170], mean action: 1.000 [1.000, 1.000],  loss: 10272701.000000, mae: 935.984192, mean_q: -0.457735
 642/1000: episode: 642, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4952.973, mean reward: -4952.973 [-4952.973, -4952.973], mean action: 1.000 [1.000, 1.000],  loss: 9139674.000000, mae: 784.056519, mean_q: -0.457701
 643/1000: episode: 643, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -3621.626, mean reward: -3621.626 [-3621.626, -3621.626], mean action: 1.000 [1.000, 1.000],  loss: 14599279.000000, mae: 1042.535522, mean_q: -0.463327
 644/1000: episode: 644, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -8526.178, mean reward: -8526.178 [-8526.178, -8526.178], mean action: 1.000 [1.000, 1.000],  loss: 7327214.000000, mae: 758.062683, mean_q: -0.470229
 645/1000: episode: 645, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -4366.407, mean reward: -4366.407 [-4366.407, -4366.407], mean action: 1.000 [1.000, 1.000],  loss: 12840000.000000, mae: 969.571045, mean_q: -0.475755
 646/1000: episode: 646, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -6221.203, mean reward: -6221.203 [-6221.203, -6221.203], mean action: 1.000 [1.000, 1.000],  loss: 10513790.000000, mae: 961.334473, mean_q: -0.489293
 647/1000: episode: 647, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -3394.711, mean reward: -3394.711 [-3394.711, -3394.711], mean action: 1.000 [1.000, 1.000],  loss: 6342304.500000, mae: 721.832275, mean_q: -0.485859
 648/1000: episode: 648, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4460.738, mean reward: -4460.738 [-4460.738, -4460.738], mean action: 1.000 [1.000, 1.000],  loss: 8250395.000000, mae: 828.141113, mean_q: -0.499441
 649/1000: episode: 649, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -3964.040, mean reward: -3964.040 [-3964.040, -3964.040], mean action: 2.000 [2.000, 2.000],  loss: 9064974.000000, mae: 839.775635, mean_q: -0.505637
 650/1000: episode: 650, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5785.099, mean reward: -5785.099 [-5785.099, -5785.099], mean action: 0.000 [0.000, 0.000],  loss: 7906926.000000, mae: 799.371460, mean_q: -0.518596
 651/1000: episode: 651, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -796.125, mean reward: -796.125 [-796.125, -796.125], mean action: 1.000 [1.000, 1.000],  loss: 10088515.000000, mae: 851.852783, mean_q: -0.522003
 652/1000: episode: 652, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -4059.136, mean reward: -4059.136 [-4059.136, -4059.136], mean action: 1.000 [1.000, 1.000],  loss: 7739953.000000, mae: 810.012512, mean_q: -0.537727
 653/1000: episode: 653, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3090.210, mean reward: -3090.210 [-3090.210, -3090.210], mean action: 1.000 [1.000, 1.000],  loss: 8688185.000000, mae: 873.916382, mean_q: -0.540877
 654/1000: episode: 654, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -7923.018, mean reward: -7923.018 [-7923.018, -7923.018], mean action: 3.000 [3.000, 3.000],  loss: 8691172.000000, mae: 811.565918, mean_q: -0.552260
 655/1000: episode: 655, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -4423.202, mean reward: -4423.202 [-4423.202, -4423.202], mean action: 1.000 [1.000, 1.000],  loss: 9696268.000000, mae: 890.802979, mean_q: -0.565103
 656/1000: episode: 656, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -7728.624, mean reward: -7728.624 [-7728.624, -7728.624], mean action: 1.000 [1.000, 1.000],  loss: 9764904.000000, mae: 895.409790, mean_q: -0.580910
 657/1000: episode: 657, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5082.206, mean reward: -5082.206 [-5082.206, -5082.206], mean action: 1.000 [1.000, 1.000],  loss: 10452882.000000, mae: 809.695984, mean_q: -0.584356
 658/1000: episode: 658, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5541.861, mean reward: -5541.861 [-5541.861, -5541.861], mean action: 1.000 [1.000, 1.000],  loss: 9602934.000000, mae: 804.331543, mean_q: -0.603743
 659/1000: episode: 659, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -2852.964, mean reward: -2852.964 [-2852.964, -2852.964], mean action: 1.000 [1.000, 1.000],  loss: 10612876.000000, mae: 908.326538, mean_q: -0.606687
 660/1000: episode: 660, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -7650.841, mean reward: -7650.841 [-7650.841, -7650.841], mean action: 1.000 [1.000, 1.000],  loss: 14486646.000000, mae: 1024.970947, mean_q: -0.615920
 661/1000: episode: 661, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5387.869, mean reward: -5387.869 [-5387.869, -5387.869], mean action: 1.000 [1.000, 1.000],  loss: 9184659.000000, mae: 868.619202, mean_q: -0.632792
 662/1000: episode: 662, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -2281.622, mean reward: -2281.622 [-2281.622, -2281.622], mean action: 1.000 [1.000, 1.000],  loss: 9026120.000000, mae: 786.579346, mean_q: -0.644459
 663/1000: episode: 663, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -2500.961, mean reward: -2500.961 [-2500.961, -2500.961], mean action: 1.000 [1.000, 1.000],  loss: 11323554.000000, mae: 1021.172546, mean_q: -0.655554
 664/1000: episode: 664, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4910.740, mean reward: -4910.740 [-4910.740, -4910.740], mean action: 1.000 [1.000, 1.000],  loss: 10002786.000000, mae: 851.515991, mean_q: -0.659002
 665/1000: episode: 665, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -3737.601, mean reward: -3737.601 [-3737.601, -3737.601], mean action: 1.000 [1.000, 1.000],  loss: 15408908.000000, mae: 1063.315674, mean_q: -0.675961
 666/1000: episode: 666, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4169.208, mean reward: -4169.208 [-4169.208, -4169.208], mean action: 1.000 [1.000, 1.000],  loss: 7863322.000000, mae: 822.787598, mean_q: -0.689491
 667/1000: episode: 667, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -1149.996, mean reward: -1149.996 [-1149.996, -1149.996], mean action: 1.000 [1.000, 1.000],  loss: 9547992.000000, mae: 884.698120, mean_q: -0.697679
 668/1000: episode: 668, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5273.917, mean reward: -5273.917 [-5273.917, -5273.917], mean action: 1.000 [1.000, 1.000],  loss: 10263929.000000, mae: 846.788330, mean_q: -0.710970
 669/1000: episode: 669, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -8392.203, mean reward: -8392.203 [-8392.203, -8392.203], mean action: 1.000 [1.000, 1.000],  loss: 14833740.000000, mae: 1101.558960, mean_q: -0.719975
 670/1000: episode: 670, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -4878.838, mean reward: -4878.838 [-4878.838, -4878.838], mean action: 1.000 [1.000, 1.000],  loss: 10173763.000000, mae: 885.597412, mean_q: -0.729658
 671/1000: episode: 671, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -3579.981, mean reward: -3579.981 [-3579.981, -3579.981], mean action: 1.000 [1.000, 1.000],  loss: 12839977.000000, mae: 1033.152100, mean_q: -0.743256
 672/1000: episode: 672, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3843.135, mean reward: -3843.135 [-3843.135, -3843.135], mean action: 1.000 [1.000, 1.000],  loss: 4893055.500000, mae: 609.356262, mean_q: -0.751320
 673/1000: episode: 673, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -8317.901, mean reward: -8317.901 [-8317.901, -8317.901], mean action: 1.000 [1.000, 1.000],  loss: 6492772.000000, mae: 689.034058, mean_q: -0.761694
 674/1000: episode: 674, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -4067.574, mean reward: -4067.574 [-4067.574, -4067.574], mean action: 0.000 [0.000, 0.000],  loss: 6225448.000000, mae: 686.073364, mean_q: -0.776847
 675/1000: episode: 675, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -7765.069, mean reward: -7765.069 [-7765.069, -7765.069], mean action: 1.000 [1.000, 1.000],  loss: 9789376.000000, mae: 910.348755, mean_q: -0.787959
 676/1000: episode: 676, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1608.778, mean reward: -1608.778 [-1608.778, -1608.778], mean action: 1.000 [1.000, 1.000],  loss: 9929129.000000, mae: 943.060486, mean_q: -0.804703
 677/1000: episode: 677, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -8443.324, mean reward: -8443.324 [-8443.324, -8443.324], mean action: 1.000 [1.000, 1.000],  loss: 15437962.000000, mae: 1058.503784, mean_q: -0.808824
 678/1000: episode: 678, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -4943.790, mean reward: -4943.790 [-4943.790, -4943.790], mean action: 1.000 [1.000, 1.000],  loss: 8526824.000000, mae: 825.400757, mean_q: -0.821998
 679/1000: episode: 679, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2078.119, mean reward: -2078.119 [-2078.119, -2078.119], mean action: 1.000 [1.000, 1.000],  loss: 19235490.000000, mae: 1090.210327, mean_q: -0.836735
 680/1000: episode: 680, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3262.424, mean reward: -3262.424 [-3262.424, -3262.424], mean action: 1.000 [1.000, 1.000],  loss: 8901864.000000, mae: 827.229736, mean_q: -0.854579
 681/1000: episode: 681, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6162.242, mean reward: -6162.242 [-6162.242, -6162.242], mean action: 1.000 [1.000, 1.000],  loss: 12676816.000000, mae: 1004.215820, mean_q: -0.860864
 682/1000: episode: 682, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3363.641, mean reward: -3363.641 [-3363.641, -3363.641], mean action: 1.000 [1.000, 1.000],  loss: 8618311.000000, mae: 817.669250, mean_q: -0.877916
 683/1000: episode: 683, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3258.571, mean reward: -3258.571 [-3258.571, -3258.571], mean action: 1.000 [1.000, 1.000],  loss: 11742760.000000, mae: 889.687622, mean_q: -0.899891
 684/1000: episode: 684, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5636.164, mean reward: -5636.164 [-5636.164, -5636.164], mean action: 0.000 [0.000, 0.000],  loss: 11842861.000000, mae: 980.672607, mean_q: -0.906002
 685/1000: episode: 685, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3742.004, mean reward: -3742.004 [-3742.004, -3742.004], mean action: 1.000 [1.000, 1.000],  loss: 8592818.000000, mae: 796.558105, mean_q: -0.918562
 686/1000: episode: 686, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -6860.644, mean reward: -6860.644 [-6860.644, -6860.644], mean action: 0.000 [0.000, 0.000],  loss: 9430083.000000, mae: 872.975647, mean_q: -0.938395
 687/1000: episode: 687, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -8258.034, mean reward: -8258.034 [-8258.034, -8258.034], mean action: 1.000 [1.000, 1.000],  loss: 8800628.000000, mae: 824.137817, mean_q: -0.942064
 688/1000: episode: 688, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -10647.624, mean reward: -10647.624 [-10647.624, -10647.624], mean action: 0.000 [0.000, 0.000],  loss: 11245503.000000, mae: 1005.190979, mean_q: -0.959748
 689/1000: episode: 689, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -10932.092, mean reward: -10932.092 [-10932.092, -10932.092], mean action: 0.000 [0.000, 0.000],  loss: 11771676.000000, mae: 991.930969, mean_q: -0.977860
 690/1000: episode: 690, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -7401.009, mean reward: -7401.009 [-7401.009, -7401.009], mean action: 0.000 [0.000, 0.000],  loss: 13591230.000000, mae: 1008.482300, mean_q: -0.986033
 691/1000: episode: 691, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4205.501, mean reward: -4205.501 [-4205.501, -4205.501], mean action: 0.000 [0.000, 0.000],  loss: 7880111.500000, mae: 706.679077, mean_q: -1.003178
 692/1000: episode: 692, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -8230.345, mean reward: -8230.345 [-8230.345, -8230.345], mean action: 1.000 [1.000, 1.000],  loss: 15896724.000000, mae: 1050.034912, mean_q: -1.014935
 693/1000: episode: 693, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -6197.810, mean reward: -6197.810 [-6197.810, -6197.810], mean action: 1.000 [1.000, 1.000],  loss: 11398532.000000, mae: 993.311157, mean_q: -1.016386
 694/1000: episode: 694, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -12217.908, mean reward: -12217.908 [-12217.908, -12217.908], mean action: 0.000 [0.000, 0.000],  loss: 7092293.500000, mae: 716.345703, mean_q: -1.039200
 695/1000: episode: 695, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -6550.152, mean reward: -6550.152 [-6550.152, -6550.152], mean action: 0.000 [0.000, 0.000],  loss: 10914988.000000, mae: 944.519592, mean_q: -1.034733
 696/1000: episode: 696, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5724.421, mean reward: -5724.421 [-5724.421, -5724.421], mean action: 0.000 [0.000, 0.000],  loss: 12483737.000000, mae: 1055.612305, mean_q: -1.044738
 697/1000: episode: 697, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3989.635, mean reward: -3989.635 [-3989.635, -3989.635], mean action: 0.000 [0.000, 0.000],  loss: 12889432.000000, mae: 1025.535767, mean_q: -1.047853
 698/1000: episode: 698, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3038.801, mean reward: -3038.801 [-3038.801, -3038.801], mean action: 0.000 [0.000, 0.000],  loss: 11104381.000000, mae: 921.555908, mean_q: -1.068419
 699/1000: episode: 699, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -10596.937, mean reward: -10596.937 [-10596.937, -10596.937], mean action: 0.000 [0.000, 0.000],  loss: 8746498.000000, mae: 810.491699, mean_q: -1.070905
 700/1000: episode: 700, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1848.815, mean reward: -1848.815 [-1848.815, -1848.815], mean action: 0.000 [0.000, 0.000],  loss: 13875958.000000, mae: 1025.687012, mean_q: -1.080620
 701/1000: episode: 701, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5699.173, mean reward: -5699.173 [-5699.173, -5699.173], mean action: 0.000 [0.000, 0.000],  loss: 15070502.000000, mae: 1100.440674, mean_q: -1.088012
 702/1000: episode: 702, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5434.882, mean reward: -5434.882 [-5434.882, -5434.882], mean action: 0.000 [0.000, 0.000],  loss: 13079278.000000, mae: 943.015747, mean_q: -1.111856
 703/1000: episode: 703, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -10814.753, mean reward: -10814.753 [-10814.753, -10814.753], mean action: 0.000 [0.000, 0.000],  loss: 10719986.000000, mae: 828.258179, mean_q: -1.118985
 704/1000: episode: 704, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4805.290, mean reward: -4805.290 [-4805.290, -4805.290], mean action: 0.000 [0.000, 0.000],  loss: 11246135.000000, mae: 974.945923, mean_q: -1.123262
 705/1000: episode: 705, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2006.387, mean reward: -2006.387 [-2006.387, -2006.387], mean action: 0.000 [0.000, 0.000],  loss: 14135552.000000, mae: 1005.892883, mean_q: -1.139024
 706/1000: episode: 706, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -3607.801, mean reward: -3607.801 [-3607.801, -3607.801], mean action: 0.000 [0.000, 0.000],  loss: 9464195.000000, mae: 784.781128, mean_q: -1.164507
 707/1000: episode: 707, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -10415.852, mean reward: -10415.852 [-10415.852, -10415.852], mean action: 0.000 [0.000, 0.000],  loss: 9742158.000000, mae: 845.874390, mean_q: -1.172125
 708/1000: episode: 708, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -3807.148, mean reward: -3807.148 [-3807.148, -3807.148], mean action: 0.000 [0.000, 0.000],  loss: 9398007.000000, mae: 895.206970, mean_q: -1.204694
 709/1000: episode: 709, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5984.217, mean reward: -5984.217 [-5984.217, -5984.217], mean action: 0.000 [0.000, 0.000],  loss: 7898056.000000, mae: 861.281738, mean_q: -1.202140
 710/1000: episode: 710, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4388.799, mean reward: -4388.799 [-4388.799, -4388.799], mean action: 0.000 [0.000, 0.000],  loss: 9747590.000000, mae: 858.426697, mean_q: -1.214697
 711/1000: episode: 711, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -4023.652, mean reward: -4023.652 [-4023.652, -4023.652], mean action: 1.000 [1.000, 1.000],  loss: 10696739.000000, mae: 940.700684, mean_q: -1.212844
 712/1000: episode: 712, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -7707.894, mean reward: -7707.894 [-7707.894, -7707.894], mean action: 0.000 [0.000, 0.000],  loss: 9926118.000000, mae: 855.329224, mean_q: -1.228590
 713/1000: episode: 713, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -1741.811, mean reward: -1741.811 [-1741.811, -1741.811], mean action: 0.000 [0.000, 0.000],  loss: 13071537.000000, mae: 963.784424, mean_q: -1.238847
 714/1000: episode: 714, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -6705.825, mean reward: -6705.825 [-6705.825, -6705.825], mean action: 0.000 [0.000, 0.000],  loss: 8217971.000000, mae: 776.270386, mean_q: -1.255296
 715/1000: episode: 715, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -1943.390, mean reward: -1943.390 [-1943.390, -1943.390], mean action: 2.000 [2.000, 2.000],  loss: 13348062.000000, mae: 986.074341, mean_q: -1.256939
 716/1000: episode: 716, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1889.015, mean reward: -1889.015 [-1889.015, -1889.015], mean action: 0.000 [0.000, 0.000],  loss: 10534260.000000, mae: 939.800659, mean_q: -1.263460
 717/1000: episode: 717, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6144.444, mean reward: -6144.444 [-6144.444, -6144.444], mean action: 0.000 [0.000, 0.000],  loss: 11617645.000000, mae: 975.719543, mean_q: -1.271904
 718/1000: episode: 718, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3984.532, mean reward: -3984.532 [-3984.532, -3984.532], mean action: 1.000 [1.000, 1.000],  loss: 4993808.000000, mae: 612.678711, mean_q: -1.300788
 719/1000: episode: 719, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -7263.605, mean reward: -7263.605 [-7263.605, -7263.605], mean action: 0.000 [0.000, 0.000],  loss: 14258227.000000, mae: 1058.941162, mean_q: -1.298886
 720/1000: episode: 720, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -1786.276, mean reward: -1786.276 [-1786.276, -1786.276], mean action: 0.000 [0.000, 0.000],  loss: 17077260.000000, mae: 1096.581665, mean_q: -1.313478
 721/1000: episode: 721, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -10847.221, mean reward: -10847.221 [-10847.221, -10847.221], mean action: 0.000 [0.000, 0.000],  loss: 12448667.000000, mae: 1069.289429, mean_q: -1.327482
 722/1000: episode: 722, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -4956.907, mean reward: -4956.907 [-4956.907, -4956.907], mean action: 0.000 [0.000, 0.000],  loss: 10944078.000000, mae: 974.483643, mean_q: -1.341123
 723/1000: episode: 723, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -10068.136, mean reward: -10068.136 [-10068.136, -10068.136], mean action: 0.000 [0.000, 0.000],  loss: 12420259.000000, mae: 997.701538, mean_q: -1.370409
 724/1000: episode: 724, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -12714.231, mean reward: -12714.231 [-12714.231, -12714.231], mean action: 0.000 [0.000, 0.000],  loss: 7491886.500000, mae: 767.633057, mean_q: -1.370998
 725/1000: episode: 725, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -15364.163, mean reward: -15364.163 [-15364.163, -15364.163], mean action: 0.000 [0.000, 0.000],  loss: 10242668.000000, mae: 922.469971, mean_q: -1.386311
 726/1000: episode: 726, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -7893.512, mean reward: -7893.512 [-7893.512, -7893.512], mean action: 0.000 [0.000, 0.000],  loss: 10533158.000000, mae: 887.692383, mean_q: -1.417385
 727/1000: episode: 727, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -7837.024, mean reward: -7837.024 [-7837.024, -7837.024], mean action: 0.000 [0.000, 0.000],  loss: 10878549.000000, mae: 871.483521, mean_q: -1.423715
 728/1000: episode: 728, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2035.587, mean reward: -2035.587 [-2035.587, -2035.587], mean action: 0.000 [0.000, 0.000],  loss: 12782982.000000, mae: 983.688416, mean_q: -1.427732
 729/1000: episode: 729, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -10484.980, mean reward: -10484.980 [-10484.980, -10484.980], mean action: 0.000 [0.000, 0.000],  loss: 7845628.000000, mae: 808.915833, mean_q: -1.454440
 730/1000: episode: 730, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1401.257, mean reward: -1401.257 [-1401.257, -1401.257], mean action: 0.000 [0.000, 0.000],  loss: 7205333.000000, mae: 752.043335, mean_q: -1.457439
 731/1000: episode: 731, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3626.450, mean reward: -3626.450 [-3626.450, -3626.450], mean action: 0.000 [0.000, 0.000],  loss: 13197398.000000, mae: 1018.105469, mean_q: -1.470460
 732/1000: episode: 732, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -10074.684, mean reward: -10074.684 [-10074.684, -10074.684], mean action: 0.000 [0.000, 0.000],  loss: 15034982.000000, mae: 1115.786743, mean_q: -1.485711
 733/1000: episode: 733, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -7219.062, mean reward: -7219.062 [-7219.062, -7219.062], mean action: 0.000 [0.000, 0.000],  loss: 8802748.000000, mae: 847.750610, mean_q: -1.511082
 734/1000: episode: 734, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -1556.873, mean reward: -1556.873 [-1556.873, -1556.873], mean action: 3.000 [3.000, 3.000],  loss: 13677824.000000, mae: 972.532654, mean_q: -1.521542
 735/1000: episode: 735, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1686.749, mean reward: -1686.749 [-1686.749, -1686.749], mean action: 3.000 [3.000, 3.000],  loss: 15456078.000000, mae: 1168.279175, mean_q: -1.531163
 736/1000: episode: 736, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2641.259, mean reward: -2641.259 [-2641.259, -2641.259], mean action: 3.000 [3.000, 3.000],  loss: 9319204.000000, mae: 866.337280, mean_q: -1.542138
 737/1000: episode: 737, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6642.453, mean reward: -6642.453 [-6642.453, -6642.453], mean action: 3.000 [3.000, 3.000],  loss: 9576184.000000, mae: 892.064941, mean_q: -1.567904
 738/1000: episode: 738, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2455.496, mean reward: -2455.496 [-2455.496, -2455.496], mean action: 3.000 [3.000, 3.000],  loss: 9804140.000000, mae: 826.836182, mean_q: -1.554943
 739/1000: episode: 739, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -86.048, mean reward: -86.048 [-86.048, -86.048], mean action: 3.000 [3.000, 3.000],  loss: 10682638.000000, mae: 950.421326, mean_q: -1.566384
 740/1000: episode: 740, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -22.217, mean reward: -22.217 [-22.217, -22.217], mean action: 3.000 [3.000, 3.000],  loss: 10252560.000000, mae: 921.098450, mean_q: -1.584890
 741/1000: episode: 741, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -2789.656, mean reward: -2789.656 [-2789.656, -2789.656], mean action: 3.000 [3.000, 3.000],  loss: 5664422.500000, mae: 668.633606, mean_q: -1.588031
 742/1000: episode: 742, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -608.831, mean reward: -608.831 [-608.831, -608.831], mean action: 3.000 [3.000, 3.000],  loss: 12018494.000000, mae: 1026.526855, mean_q: -1.581724
 743/1000: episode: 743, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -1726.733, mean reward: -1726.733 [-1726.733, -1726.733], mean action: 3.000 [3.000, 3.000],  loss: 10334803.000000, mae: 912.808167, mean_q: -1.592422
 744/1000: episode: 744, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3122.248, mean reward: -3122.248 [-3122.248, -3122.248], mean action: 3.000 [3.000, 3.000],  loss: 11194546.000000, mae: 942.866699, mean_q: -1.609280
 745/1000: episode: 745, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -1968.590, mean reward: -1968.590 [-1968.590, -1968.590], mean action: 3.000 [3.000, 3.000],  loss: 10450866.000000, mae: 849.930237, mean_q: -1.620723
 746/1000: episode: 746, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3943.957, mean reward: -3943.957 [-3943.957, -3943.957], mean action: 3.000 [3.000, 3.000],  loss: 10729942.000000, mae: 896.221680, mean_q: -1.624801
 747/1000: episode: 747, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -355.990, mean reward: -355.990 [-355.990, -355.990], mean action: 3.000 [3.000, 3.000],  loss: 12926030.000000, mae: 974.618408, mean_q: -1.629245
 748/1000: episode: 748, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -334.579, mean reward: -334.579 [-334.579, -334.579], mean action: 3.000 [3.000, 3.000],  loss: 15752546.000000, mae: 1097.771118, mean_q: -1.638013
 749/1000: episode: 749, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -2462.954, mean reward: -2462.954 [-2462.954, -2462.954], mean action: 3.000 [3.000, 3.000],  loss: 10683355.000000, mae: 961.815063, mean_q: -1.653504
 750/1000: episode: 750, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4305.499, mean reward: -4305.499 [-4305.499, -4305.499], mean action: 3.000 [3.000, 3.000],  loss: 7763139.000000, mae: 838.334045, mean_q: -1.666155
 751/1000: episode: 751, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -9591.684, mean reward: -9591.684 [-9591.684, -9591.684], mean action: 1.000 [1.000, 1.000],  loss: 9379014.000000, mae: 886.307007, mean_q: -1.663721
 752/1000: episode: 752, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6506.664, mean reward: -6506.664 [-6506.664, -6506.664], mean action: 3.000 [3.000, 3.000],  loss: 9094900.000000, mae: 865.756592, mean_q: -1.683056
 753/1000: episode: 753, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -120.638, mean reward: -120.638 [-120.638, -120.638], mean action: 3.000 [3.000, 3.000],  loss: 13003017.000000, mae: 1009.544556, mean_q: -1.699867
 754/1000: episode: 754, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -298.129, mean reward: -298.129 [-298.129, -298.129], mean action: 3.000 [3.000, 3.000],  loss: 7744036.500000, mae: 789.463013, mean_q: -1.710734
 755/1000: episode: 755, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3001.241, mean reward: -3001.241 [-3001.241, -3001.241], mean action: 3.000 [3.000, 3.000],  loss: 10330540.000000, mae: 941.457947, mean_q: -1.712533
 756/1000: episode: 756, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -6565.535, mean reward: -6565.535 [-6565.535, -6565.535], mean action: 0.000 [0.000, 0.000],  loss: 7509015.000000, mae: 770.333679, mean_q: -1.733673
 757/1000: episode: 757, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -1574.680, mean reward: -1574.680 [-1574.680, -1574.680], mean action: 1.000 [1.000, 1.000],  loss: 12569652.000000, mae: 1068.280396, mean_q: -1.726203
 758/1000: episode: 758, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -2810.583, mean reward: -2810.583 [-2810.583, -2810.583], mean action: 3.000 [3.000, 3.000],  loss: 14582769.000000, mae: 1060.006104, mean_q: -1.731849
 759/1000: episode: 759, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3859.333, mean reward: -3859.333 [-3859.333, -3859.333], mean action: 3.000 [3.000, 3.000],  loss: 13294411.000000, mae: 942.159668, mean_q: -1.748544
 760/1000: episode: 760, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2152.985, mean reward: -2152.985 [-2152.985, -2152.985], mean action: 2.000 [2.000, 2.000],  loss: 11810206.000000, mae: 974.792114, mean_q: -1.756455
 761/1000: episode: 761, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2819.201, mean reward: -2819.201 [-2819.201, -2819.201], mean action: 3.000 [3.000, 3.000],  loss: 8695766.000000, mae: 896.318359, mean_q: -1.774976
 762/1000: episode: 762, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1145.674, mean reward: -1145.674 [-1145.674, -1145.674], mean action: 3.000 [3.000, 3.000],  loss: 14229832.000000, mae: 1131.231934, mean_q: -1.763022
 763/1000: episode: 763, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -4996.794, mean reward: -4996.794 [-4996.794, -4996.794], mean action: 3.000 [3.000, 3.000],  loss: 15498500.000000, mae: 1109.804443, mean_q: -1.777973
 764/1000: episode: 764, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -6654.912, mean reward: -6654.912 [-6654.912, -6654.912], mean action: 3.000 [3.000, 3.000],  loss: 14215472.000000, mae: 989.356873, mean_q: -1.790094
 765/1000: episode: 765, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -1208.409, mean reward: -1208.409 [-1208.409, -1208.409], mean action: 3.000 [3.000, 3.000],  loss: 7118150.000000, mae: 771.314819, mean_q: -1.800375
 766/1000: episode: 766, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -7448.817, mean reward: -7448.817 [-7448.817, -7448.817], mean action: 3.000 [3.000, 3.000],  loss: 6176470.000000, mae: 678.002258, mean_q: -1.811361
 767/1000: episode: 767, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -720.895, mean reward: -720.895 [-720.895, -720.895], mean action: 3.000 [3.000, 3.000],  loss: 15505344.000000, mae: 1111.681152, mean_q: -1.817596
 768/1000: episode: 768, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -1919.362, mean reward: -1919.362 [-1919.362, -1919.362], mean action: 3.000 [3.000, 3.000],  loss: 18240010.000000, mae: 1178.519775, mean_q: -1.834722
 769/1000: episode: 769, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -7305.019, mean reward: -7305.019 [-7305.019, -7305.019], mean action: 3.000 [3.000, 3.000],  loss: 9424334.000000, mae: 836.425171, mean_q: -1.847469
 770/1000: episode: 770, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -1074.180, mean reward: -1074.180 [-1074.180, -1074.180], mean action: 3.000 [3.000, 3.000],  loss: 11156477.000000, mae: 882.689758, mean_q: -1.857147
 771/1000: episode: 771, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -7972.891, mean reward: -7972.891 [-7972.891, -7972.891], mean action: 3.000 [3.000, 3.000],  loss: 11775398.000000, mae: 1018.089172, mean_q: -1.862512
 772/1000: episode: 772, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4757.820, mean reward: -4757.820 [-4757.820, -4757.820], mean action: 3.000 [3.000, 3.000],  loss: 13085542.000000, mae: 980.519348, mean_q: -1.883829
 773/1000: episode: 773, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -6164.888, mean reward: -6164.888 [-6164.888, -6164.888], mean action: 3.000 [3.000, 3.000],  loss: 5813568.000000, mae: 666.285950, mean_q: -1.900239
 774/1000: episode: 774, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -3379.651, mean reward: -3379.651 [-3379.651, -3379.651], mean action: 3.000 [3.000, 3.000],  loss: 14495096.000000, mae: 1051.285645, mean_q: -1.920089
 775/1000: episode: 775, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -4385.758, mean reward: -4385.758 [-4385.758, -4385.758], mean action: 3.000 [3.000, 3.000],  loss: 8303598.000000, mae: 803.730103, mean_q: -1.925707
 776/1000: episode: 776, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -7596.662, mean reward: -7596.662 [-7596.662, -7596.662], mean action: 3.000 [3.000, 3.000],  loss: 9814342.000000, mae: 848.967834, mean_q: -1.932779
 777/1000: episode: 777, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3322.401, mean reward: -3322.401 [-3322.401, -3322.401], mean action: 3.000 [3.000, 3.000],  loss: 15248605.000000, mae: 1048.582397, mean_q: -1.935607
 778/1000: episode: 778, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4834.899, mean reward: -4834.899 [-4834.899, -4834.899], mean action: 3.000 [3.000, 3.000],  loss: 4045546.250000, mae: 597.958862, mean_q: -1.975289
 779/1000: episode: 779, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -6883.851, mean reward: -6883.851 [-6883.851, -6883.851], mean action: 3.000 [3.000, 3.000],  loss: 10259804.000000, mae: 886.585938, mean_q: -1.983479
 780/1000: episode: 780, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3119.921, mean reward: -3119.921 [-3119.921, -3119.921], mean action: 3.000 [3.000, 3.000],  loss: 12000532.000000, mae: 947.098877, mean_q: -1.987653
 781/1000: episode: 781, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2654.391, mean reward: -2654.391 [-2654.391, -2654.391], mean action: 3.000 [3.000, 3.000],  loss: 9720672.000000, mae: 796.477966, mean_q: -2.006208
 782/1000: episode: 782, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5104.756, mean reward: -5104.756 [-5104.756, -5104.756], mean action: 3.000 [3.000, 3.000],  loss: 13364690.000000, mae: 1003.450989, mean_q: -2.011709
 783/1000: episode: 783, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -2927.452, mean reward: -2927.452 [-2927.452, -2927.452], mean action: 3.000 [3.000, 3.000],  loss: 6901854.000000, mae: 709.850952, mean_q: -2.019012
 784/1000: episode: 784, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3288.987, mean reward: -3288.987 [-3288.987, -3288.987], mean action: 3.000 [3.000, 3.000],  loss: 12542876.000000, mae: 1045.734131, mean_q: -2.033828
 785/1000: episode: 785, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1416.290, mean reward: -1416.290 [-1416.290, -1416.290], mean action: 3.000 [3.000, 3.000],  loss: 15328649.000000, mae: 1058.106445, mean_q: -2.054269
 786/1000: episode: 786, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5391.463, mean reward: -5391.463 [-5391.463, -5391.463], mean action: 3.000 [3.000, 3.000],  loss: 8450721.000000, mae: 886.512634, mean_q: -2.071144
 787/1000: episode: 787, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -3265.311, mean reward: -3265.311 [-3265.311, -3265.311], mean action: 3.000 [3.000, 3.000],  loss: 13792746.000000, mae: 984.224182, mean_q: -2.090128
 788/1000: episode: 788, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -572.856, mean reward: -572.856 [-572.856, -572.856], mean action: 3.000 [3.000, 3.000],  loss: 8685295.000000, mae: 762.905396, mean_q: -2.110112
 789/1000: episode: 789, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -361.258, mean reward: -361.258 [-361.258, -361.258], mean action: 3.000 [3.000, 3.000],  loss: 10112158.000000, mae: 882.696045, mean_q: -2.123096
 790/1000: episode: 790, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -3959.226, mean reward: -3959.226 [-3959.226, -3959.226], mean action: 3.000 [3.000, 3.000],  loss: 14922325.000000, mae: 1050.667114, mean_q: -2.127802
 791/1000: episode: 791, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1567.857, mean reward: -1567.857 [-1567.857, -1567.857], mean action: 3.000 [3.000, 3.000],  loss: 7984172.500000, mae: 765.055298, mean_q: -2.162181
 792/1000: episode: 792, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -8769.379, mean reward: -8769.379 [-8769.379, -8769.379], mean action: 3.000 [3.000, 3.000],  loss: 11109777.000000, mae: 903.686401, mean_q: -2.170780
 793/1000: episode: 793, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -114.362, mean reward: -114.362 [-114.362, -114.362], mean action: 3.000 [3.000, 3.000],  loss: 9511112.000000, mae: 905.612915, mean_q: -2.193029
 794/1000: episode: 794, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5696.521, mean reward: -5696.521 [-5696.521, -5696.521], mean action: 3.000 [3.000, 3.000],  loss: 9409933.000000, mae: 885.093079, mean_q: -2.197178
 795/1000: episode: 795, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1094.791, mean reward: -1094.791 [-1094.791, -1094.791], mean action: 3.000 [3.000, 3.000],  loss: 11673523.000000, mae: 984.467407, mean_q: -2.215803
 796/1000: episode: 796, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -8340.628, mean reward: -8340.628 [-8340.628, -8340.628], mean action: 3.000 [3.000, 3.000],  loss: 11998161.000000, mae: 959.097595, mean_q: -2.233510
 797/1000: episode: 797, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -4566.837, mean reward: -4566.837 [-4566.837, -4566.837], mean action: 3.000 [3.000, 3.000],  loss: 12136368.000000, mae: 859.773560, mean_q: -2.257624
 798/1000: episode: 798, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -1104.836, mean reward: -1104.836 [-1104.836, -1104.836], mean action: 3.000 [3.000, 3.000],  loss: 7077037.000000, mae: 806.365967, mean_q: -2.261215
 799/1000: episode: 799, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -865.544, mean reward: -865.544 [-865.544, -865.544], mean action: 3.000 [3.000, 3.000],  loss: 6599657.000000, mae: 742.678345, mean_q: -2.294980
 800/1000: episode: 800, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -880.041, mean reward: -880.041 [-880.041, -880.041], mean action: 3.000 [3.000, 3.000],  loss: 11465664.000000, mae: 1030.956787, mean_q: -2.302106
 801/1000: episode: 801, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -669.650, mean reward: -669.650 [-669.650, -669.650], mean action: 3.000 [3.000, 3.000],  loss: 10260750.000000, mae: 945.100952, mean_q: -2.312269
 802/1000: episode: 802, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3330.950, mean reward: -3330.950 [-3330.950, -3330.950], mean action: 3.000 [3.000, 3.000],  loss: 13547924.000000, mae: 1065.475342, mean_q: -2.333828
 803/1000: episode: 803, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -4573.309, mean reward: -4573.309 [-4573.309, -4573.309], mean action: 3.000 [3.000, 3.000],  loss: 16503608.000000, mae: 1056.189331, mean_q: -2.350732
 804/1000: episode: 804, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -1586.029, mean reward: -1586.029 [-1586.029, -1586.029], mean action: 3.000 [3.000, 3.000],  loss: 6509081.500000, mae: 754.916504, mean_q: -2.386280
 805/1000: episode: 805, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -1031.967, mean reward: -1031.967 [-1031.967, -1031.967], mean action: 3.000 [3.000, 3.000],  loss: 12383770.000000, mae: 937.510864, mean_q: -2.382279
 806/1000: episode: 806, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5210.289, mean reward: -5210.289 [-5210.289, -5210.289], mean action: 3.000 [3.000, 3.000],  loss: 11739604.000000, mae: 1002.526733, mean_q: -2.408493
 807/1000: episode: 807, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -2575.197, mean reward: -2575.197 [-2575.197, -2575.197], mean action: 3.000 [3.000, 3.000],  loss: 10005204.000000, mae: 821.888367, mean_q: -2.421095
 808/1000: episode: 808, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2399.214, mean reward: -2399.214 [-2399.214, -2399.214], mean action: 3.000 [3.000, 3.000],  loss: 9691904.000000, mae: 830.579712, mean_q: -2.434386
 809/1000: episode: 809, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4402.788, mean reward: -4402.788 [-4402.788, -4402.788], mean action: 3.000 [3.000, 3.000],  loss: 9937800.000000, mae: 926.638916, mean_q: -2.457339
 810/1000: episode: 810, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -1489.485, mean reward: -1489.485 [-1489.485, -1489.485], mean action: 3.000 [3.000, 3.000],  loss: 17121376.000000, mae: 1200.838867, mean_q: -2.468423
 811/1000: episode: 811, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -1796.520, mean reward: -1796.520 [-1796.520, -1796.520], mean action: 3.000 [3.000, 3.000],  loss: 15179276.000000, mae: 1059.787842, mean_q: -2.490217
 812/1000: episode: 812, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5698.797, mean reward: -5698.797 [-5698.797, -5698.797], mean action: 1.000 [1.000, 1.000],  loss: 5714863.000000, mae: 720.023804, mean_q: -2.505457
 813/1000: episode: 813, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -10195.935, mean reward: -10195.935 [-10195.935, -10195.935], mean action: 3.000 [3.000, 3.000],  loss: 6264721.000000, mae: 664.166382, mean_q: -2.507457
 814/1000: episode: 814, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -10262.765, mean reward: -10262.765 [-10262.765, -10262.765], mean action: 3.000 [3.000, 3.000],  loss: 14004270.000000, mae: 1015.770142, mean_q: -2.524291
 815/1000: episode: 815, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5538.157, mean reward: -5538.157 [-5538.157, -5538.157], mean action: 3.000 [3.000, 3.000],  loss: 18940436.000000, mae: 1145.319580, mean_q: -2.537974
 816/1000: episode: 816, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -7556.365, mean reward: -7556.365 [-7556.365, -7556.365], mean action: 3.000 [3.000, 3.000],  loss: 14244966.000000, mae: 992.418091, mean_q: -2.557923
 817/1000: episode: 817, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4880.087, mean reward: -4880.087 [-4880.087, -4880.087], mean action: 3.000 [3.000, 3.000],  loss: 9723850.000000, mae: 935.416016, mean_q: -2.562215
 818/1000: episode: 818, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5841.212, mean reward: -5841.212 [-5841.212, -5841.212], mean action: 2.000 [2.000, 2.000],  loss: 14441546.000000, mae: 1206.854004, mean_q: -2.590261
 819/1000: episode: 819, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -72.765, mean reward: -72.765 [-72.765, -72.765], mean action: 3.000 [3.000, 3.000],  loss: 7396013.000000, mae: 817.184204, mean_q: -2.604562
 820/1000: episode: 820, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2901.158, mean reward: -2901.158 [-2901.158, -2901.158], mean action: 3.000 [3.000, 3.000],  loss: 9964813.000000, mae: 874.920532, mean_q: -2.629774
 821/1000: episode: 821, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1996.545, mean reward: -1996.545 [-1996.545, -1996.545], mean action: 1.000 [1.000, 1.000],  loss: 13975563.000000, mae: 1045.465820, mean_q: -2.637955
 822/1000: episode: 822, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5686.603, mean reward: -5686.603 [-5686.603, -5686.603], mean action: 3.000 [3.000, 3.000],  loss: 9031827.000000, mae: 804.371826, mean_q: -2.666892
 823/1000: episode: 823, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -7957.124, mean reward: -7957.124 [-7957.124, -7957.124], mean action: 1.000 [1.000, 1.000],  loss: 13004255.000000, mae: 1017.593384, mean_q: -2.684263
 824/1000: episode: 824, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -3040.484, mean reward: -3040.484 [-3040.484, -3040.484], mean action: 1.000 [1.000, 1.000],  loss: 8079409.000000, mae: 776.823792, mean_q: -2.696254
 825/1000: episode: 825, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -12486.468, mean reward: -12486.468 [-12486.468, -12486.468], mean action: 3.000 [3.000, 3.000],  loss: 8388219.000000, mae: 786.090454, mean_q: -2.713747
 826/1000: episode: 826, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -146.611, mean reward: -146.611 [-146.611, -146.611], mean action: 1.000 [1.000, 1.000],  loss: 11896948.000000, mae: 1017.418518, mean_q: -2.727249
 827/1000: episode: 827, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1958.573, mean reward: -1958.573 [-1958.573, -1958.573], mean action: 1.000 [1.000, 1.000],  loss: 11389388.000000, mae: 975.514587, mean_q: -2.734888
 828/1000: episode: 828, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1329.788, mean reward: -1329.788 [-1329.788, -1329.788], mean action: 1.000 [1.000, 1.000],  loss: 11149832.000000, mae: 925.718628, mean_q: -2.759262
 829/1000: episode: 829, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -10950.055, mean reward: -10950.055 [-10950.055, -10950.055], mean action: 1.000 [1.000, 1.000],  loss: 5914910.000000, mae: 713.896240, mean_q: -2.780018
 830/1000: episode: 830, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -4580.040, mean reward: -4580.040 [-4580.040, -4580.040], mean action: 1.000 [1.000, 1.000],  loss: 13787161.000000, mae: 1097.848633, mean_q: -2.804214
 831/1000: episode: 831, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -3026.369, mean reward: -3026.369 [-3026.369, -3026.369], mean action: 3.000 [3.000, 3.000],  loss: 8309106.000000, mae: 798.865967, mean_q: -2.812802
 832/1000: episode: 832, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -6911.031, mean reward: -6911.031 [-6911.031, -6911.031], mean action: 1.000 [1.000, 1.000],  loss: 11274384.000000, mae: 944.642517, mean_q: -2.836256
 833/1000: episode: 833, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6066.217, mean reward: -6066.217 [-6066.217, -6066.217], mean action: 1.000 [1.000, 1.000],  loss: 11697148.000000, mae: 951.508667, mean_q: -2.851936
 834/1000: episode: 834, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -7062.618, mean reward: -7062.618 [-7062.618, -7062.618], mean action: 1.000 [1.000, 1.000],  loss: 13995480.000000, mae: 1025.276855, mean_q: -2.851847
 835/1000: episode: 835, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -2291.943, mean reward: -2291.943 [-2291.943, -2291.943], mean action: 0.000 [0.000, 0.000],  loss: 9677240.000000, mae: 863.701599, mean_q: -2.864144
 836/1000: episode: 836, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5420.502, mean reward: -5420.502 [-5420.502, -5420.502], mean action: 1.000 [1.000, 1.000],  loss: 5814111.000000, mae: 669.407227, mean_q: -2.879600
 837/1000: episode: 837, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4050.861, mean reward: -4050.861 [-4050.861, -4050.861], mean action: 1.000 [1.000, 1.000],  loss: 6451347.500000, mae: 679.559448, mean_q: -2.895873
 838/1000: episode: 838, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -6213.851, mean reward: -6213.851 [-6213.851, -6213.851], mean action: 1.000 [1.000, 1.000],  loss: 7891590.000000, mae: 824.385254, mean_q: -2.918907
 839/1000: episode: 839, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -199.143, mean reward: -199.143 [-199.143, -199.143], mean action: 1.000 [1.000, 1.000],  loss: 12875039.000000, mae: 914.291199, mean_q: -2.917333
 840/1000: episode: 840, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -1076.843, mean reward: -1076.843 [-1076.843, -1076.843], mean action: 1.000 [1.000, 1.000],  loss: 9258176.000000, mae: 813.841675, mean_q: -2.959309
 841/1000: episode: 841, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -2885.045, mean reward: -2885.045 [-2885.045, -2885.045], mean action: 1.000 [1.000, 1.000],  loss: 8473918.000000, mae: 755.149902, mean_q: -2.957808
 842/1000: episode: 842, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5179.908, mean reward: -5179.908 [-5179.908, -5179.908], mean action: 1.000 [1.000, 1.000],  loss: 12577188.000000, mae: 954.079834, mean_q: -2.950462
 843/1000: episode: 843, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -2374.405, mean reward: -2374.405 [-2374.405, -2374.405], mean action: 0.000 [0.000, 0.000],  loss: 10508954.000000, mae: 981.643005, mean_q: -2.973723
 844/1000: episode: 844, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2931.631, mean reward: -2931.631 [-2931.631, -2931.631], mean action: 1.000 [1.000, 1.000],  loss: 15527468.000000, mae: 1140.648315, mean_q: -2.986376
 845/1000: episode: 845, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -893.953, mean reward: -893.953 [-893.953, -893.953], mean action: 1.000 [1.000, 1.000],  loss: 7233458.000000, mae: 750.866821, mean_q: -3.013447
 846/1000: episode: 846, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -2609.904, mean reward: -2609.904 [-2609.904, -2609.904], mean action: 1.000 [1.000, 1.000],  loss: 6515690.000000, mae: 741.175537, mean_q: -3.036314
 847/1000: episode: 847, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -4425.790, mean reward: -4425.790 [-4425.790, -4425.790], mean action: 1.000 [1.000, 1.000],  loss: 8544362.000000, mae: 845.953491, mean_q: -3.057329
 848/1000: episode: 848, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -6870.094, mean reward: -6870.094 [-6870.094, -6870.094], mean action: 1.000 [1.000, 1.000],  loss: 14494697.000000, mae: 1058.232666, mean_q: -3.077971
 849/1000: episode: 849, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -218.326, mean reward: -218.326 [-218.326, -218.326], mean action: 1.000 [1.000, 1.000],  loss: 10827047.000000, mae: 941.640381, mean_q: -3.082870
 850/1000: episode: 850, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -38.467, mean reward: -38.467 [-38.467, -38.467], mean action: 1.000 [1.000, 1.000],  loss: 9791572.000000, mae: 793.560181, mean_q: -3.091472
 851/1000: episode: 851, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -2643.614, mean reward: -2643.614 [-2643.614, -2643.614], mean action: 1.000 [1.000, 1.000],  loss: 12358428.000000, mae: 932.228394, mean_q: -3.113351
 852/1000: episode: 852, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -4069.586, mean reward: -4069.586 [-4069.586, -4069.586], mean action: 1.000 [1.000, 1.000],  loss: 8624408.000000, mae: 808.531860, mean_q: -3.137048
 853/1000: episode: 853, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -744.954, mean reward: -744.954 [-744.954, -744.954], mean action: 1.000 [1.000, 1.000],  loss: 7518707.000000, mae: 747.340454, mean_q: -3.176515
 854/1000: episode: 854, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3531.824, mean reward: -3531.824 [-3531.824, -3531.824], mean action: 1.000 [1.000, 1.000],  loss: 7804049.500000, mae: 809.899170, mean_q: -3.188143
 855/1000: episode: 855, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -8494.983, mean reward: -8494.983 [-8494.983, -8494.983], mean action: 1.000 [1.000, 1.000],  loss: 7767804.000000, mae: 783.861694, mean_q: -3.209471
 856/1000: episode: 856, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -8369.754, mean reward: -8369.754 [-8369.754, -8369.754], mean action: 1.000 [1.000, 1.000],  loss: 11936845.000000, mae: 1003.529053, mean_q: -3.211766
 857/1000: episode: 857, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4160.702, mean reward: -4160.702 [-4160.702, -4160.702], mean action: 1.000 [1.000, 1.000],  loss: 7933007.000000, mae: 762.562988, mean_q: -3.258360
 858/1000: episode: 858, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -313.547, mean reward: -313.547 [-313.547, -313.547], mean action: 1.000 [1.000, 1.000],  loss: 10425384.000000, mae: 858.150391, mean_q: -3.271013
 859/1000: episode: 859, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4842.295, mean reward: -4842.295 [-4842.295, -4842.295], mean action: 1.000 [1.000, 1.000],  loss: 11546794.000000, mae: 887.434692, mean_q: -3.278361
 860/1000: episode: 860, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -3879.903, mean reward: -3879.903 [-3879.903, -3879.903], mean action: 1.000 [1.000, 1.000],  loss: 7167619.000000, mae: 751.865845, mean_q: -3.307263
 861/1000: episode: 861, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -4551.608, mean reward: -4551.608 [-4551.608, -4551.608], mean action: 1.000 [1.000, 1.000],  loss: 8810520.000000, mae: 800.373657, mean_q: -3.326037
 862/1000: episode: 862, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -10256.031, mean reward: -10256.031 [-10256.031, -10256.031], mean action: 0.000 [0.000, 0.000],  loss: 10872832.000000, mae: 914.924805, mean_q: -3.341697
 863/1000: episode: 863, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -4019.589, mean reward: -4019.589 [-4019.589, -4019.589], mean action: 1.000 [1.000, 1.000],  loss: 14798562.000000, mae: 1116.344727, mean_q: -3.370741
 864/1000: episode: 864, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -4194.128, mean reward: -4194.128 [-4194.128, -4194.128], mean action: 1.000 [1.000, 1.000],  loss: 10456162.000000, mae: 920.874634, mean_q: -3.405962
 865/1000: episode: 865, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -3182.379, mean reward: -3182.379 [-3182.379, -3182.379], mean action: 1.000 [1.000, 1.000],  loss: 8331289.500000, mae: 784.956421, mean_q: -3.407411
 866/1000: episode: 866, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -7125.921, mean reward: -7125.921 [-7125.921, -7125.921], mean action: 1.000 [1.000, 1.000],  loss: 8040683.000000, mae: 799.263550, mean_q: -3.434551
 867/1000: episode: 867, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3038.724, mean reward: -3038.724 [-3038.724, -3038.724], mean action: 1.000 [1.000, 1.000],  loss: 6655364.000000, mae: 661.542114, mean_q: -3.464434
 868/1000: episode: 868, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4147.429, mean reward: -4147.429 [-4147.429, -4147.429], mean action: 1.000 [1.000, 1.000],  loss: 6366874.000000, mae: 715.311157, mean_q: -3.469175
 869/1000: episode: 869, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -7584.436, mean reward: -7584.436 [-7584.436, -7584.436], mean action: 1.000 [1.000, 1.000],  loss: 11903768.000000, mae: 908.216309, mean_q: -3.464771
 870/1000: episode: 870, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -2848.864, mean reward: -2848.864 [-2848.864, -2848.864], mean action: 1.000 [1.000, 1.000],  loss: 8572333.000000, mae: 804.855347, mean_q: -3.491109
 871/1000: episode: 871, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -448.154, mean reward: -448.154 [-448.154, -448.154], mean action: 1.000 [1.000, 1.000],  loss: 16901070.000000, mae: 1201.025635, mean_q: -3.508464
 872/1000: episode: 872, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -2212.231, mean reward: -2212.231 [-2212.231, -2212.231], mean action: 1.000 [1.000, 1.000],  loss: 10804726.000000, mae: 870.899048, mean_q: -3.549437
 873/1000: episode: 873, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1706.121, mean reward: -1706.121 [-1706.121, -1706.121], mean action: 2.000 [2.000, 2.000],  loss: 16354001.000000, mae: 1149.127686, mean_q: -3.564603
 874/1000: episode: 874, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5284.292, mean reward: -5284.292 [-5284.292, -5284.292], mean action: 1.000 [1.000, 1.000],  loss: 10249849.000000, mae: 874.392212, mean_q: -3.581725
 875/1000: episode: 875, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -656.360, mean reward: -656.360 [-656.360, -656.360], mean action: 1.000 [1.000, 1.000],  loss: 6303895.000000, mae: 749.594238, mean_q: -3.593804
 876/1000: episode: 876, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -185.616, mean reward: -185.616 [-185.616, -185.616], mean action: 2.000 [2.000, 2.000],  loss: 12787186.000000, mae: 1052.951782, mean_q: -3.615960
 877/1000: episode: 877, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4664.559, mean reward: -4664.559 [-4664.559, -4664.559], mean action: 1.000 [1.000, 1.000],  loss: 14052890.000000, mae: 1025.421387, mean_q: -3.649931
 878/1000: episode: 878, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -7098.701, mean reward: -7098.701 [-7098.701, -7098.701], mean action: 1.000 [1.000, 1.000],  loss: 11401292.000000, mae: 960.668335, mean_q: -3.662218
 879/1000: episode: 879, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -9699.715, mean reward: -9699.715 [-9699.715, -9699.715], mean action: 1.000 [1.000, 1.000],  loss: 6675950.000000, mae: 699.498962, mean_q: -3.683567
 880/1000: episode: 880, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5001.962, mean reward: -5001.962 [-5001.962, -5001.962], mean action: 1.000 [1.000, 1.000],  loss: 15177813.000000, mae: 1134.357056, mean_q: -3.717067
 881/1000: episode: 881, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -8309.982, mean reward: -8309.982 [-8309.982, -8309.982], mean action: 1.000 [1.000, 1.000],  loss: 11231875.000000, mae: 1063.794800, mean_q: -3.737128
 882/1000: episode: 882, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -13081.585, mean reward: -13081.585 [-13081.585, -13081.585], mean action: 1.000 [1.000, 1.000],  loss: 9083542.000000, mae: 834.551575, mean_q: -3.762049
 883/1000: episode: 883, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -627.740, mean reward: -627.740 [-627.740, -627.740], mean action: 1.000 [1.000, 1.000],  loss: 9118339.000000, mae: 925.160889, mean_q: -3.753840
 884/1000: episode: 884, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -2618.454, mean reward: -2618.454 [-2618.454, -2618.454], mean action: 1.000 [1.000, 1.000],  loss: 15244993.000000, mae: 1219.346802, mean_q: -3.786591
 885/1000: episode: 885, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -11459.479, mean reward: -11459.479 [-11459.479, -11459.479], mean action: 0.000 [0.000, 0.000],  loss: 21878326.000000, mae: 1402.532959, mean_q: -3.795801
 886/1000: episode: 886, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5028.496, mean reward: -5028.496 [-5028.496, -5028.496], mean action: 1.000 [1.000, 1.000],  loss: 10474803.000000, mae: 888.404236, mean_q: -3.836374
 887/1000: episode: 887, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -4346.869, mean reward: -4346.869 [-4346.869, -4346.869], mean action: 1.000 [1.000, 1.000],  loss: 10432940.000000, mae: 876.730713, mean_q: -3.854069
 888/1000: episode: 888, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4088.146, mean reward: -4088.146 [-4088.146, -4088.146], mean action: 1.000 [1.000, 1.000],  loss: 7704578.500000, mae: 746.128052, mean_q: -3.882709
 889/1000: episode: 889, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5137.367, mean reward: -5137.367 [-5137.367, -5137.367], mean action: 1.000 [1.000, 1.000],  loss: 10157026.000000, mae: 924.512146, mean_q: -3.886761
 890/1000: episode: 890, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -3363.099, mean reward: -3363.099 [-3363.099, -3363.099], mean action: 1.000 [1.000, 1.000],  loss: 17954724.000000, mae: 1231.496338, mean_q: -3.906691
 891/1000: episode: 891, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -4894.777, mean reward: -4894.777 [-4894.777, -4894.777], mean action: 1.000 [1.000, 1.000],  loss: 7331022.000000, mae: 771.912170, mean_q: -3.941365
 892/1000: episode: 892, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -6391.763, mean reward: -6391.763 [-6391.763, -6391.763], mean action: 1.000 [1.000, 1.000],  loss: 12986144.000000, mae: 906.562134, mean_q: -3.963960
 893/1000: episode: 893, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -12230.339, mean reward: -12230.339 [-12230.339, -12230.339], mean action: 1.000 [1.000, 1.000],  loss: 8724069.000000, mae: 857.091187, mean_q: -3.975688
 894/1000: episode: 894, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -1061.408, mean reward: -1061.408 [-1061.408, -1061.408], mean action: 1.000 [1.000, 1.000],  loss: 13537870.000000, mae: 1087.518433, mean_q: -3.989326
 895/1000: episode: 895, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1023.039, mean reward: -1023.039 [-1023.039, -1023.039], mean action: 1.000 [1.000, 1.000],  loss: 13349031.000000, mae: 967.251038, mean_q: -4.034002
 896/1000: episode: 896, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -7131.455, mean reward: -7131.455 [-7131.455, -7131.455], mean action: 1.000 [1.000, 1.000],  loss: 9720904.000000, mae: 898.997131, mean_q: -4.031302
 897/1000: episode: 897, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -10554.873, mean reward: -10554.873 [-10554.873, -10554.873], mean action: 0.000 [0.000, 0.000],  loss: 8716657.000000, mae: 825.892273, mean_q: -4.074550
 898/1000: episode: 898, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -823.858, mean reward: -823.858 [-823.858, -823.858], mean action: 1.000 [1.000, 1.000],  loss: 9975437.000000, mae: 845.596436, mean_q: -4.087716
 899/1000: episode: 899, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -7069.176, mean reward: -7069.176 [-7069.176, -7069.176], mean action: 1.000 [1.000, 1.000],  loss: 11714889.000000, mae: 929.939697, mean_q: -4.104177
 900/1000: episode: 900, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -2695.566, mean reward: -2695.566 [-2695.566, -2695.566], mean action: 1.000 [1.000, 1.000],  loss: 9796222.000000, mae: 912.196289, mean_q: -4.156275
 901/1000: episode: 901, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5420.982, mean reward: -5420.982 [-5420.982, -5420.982], mean action: 1.000 [1.000, 1.000],  loss: 11162291.000000, mae: 1008.414124, mean_q: -4.193275
 902/1000: episode: 902, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5143.121, mean reward: -5143.121 [-5143.121, -5143.121], mean action: 1.000 [1.000, 1.000],  loss: 12322156.000000, mae: 1048.364258, mean_q: -4.203323
 903/1000: episode: 903, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -11370.305, mean reward: -11370.305 [-11370.305, -11370.305], mean action: 1.000 [1.000, 1.000],  loss: 8926270.000000, mae: 839.658447, mean_q: -4.227872
 904/1000: episode: 904, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -773.278, mean reward: -773.278 [-773.278, -773.278], mean action: 1.000 [1.000, 1.000],  loss: 13411933.000000, mae: 946.113525, mean_q: -4.247587
 905/1000: episode: 905, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -310.142, mean reward: -310.142 [-310.142, -310.142], mean action: 3.000 [3.000, 3.000],  loss: 10114978.000000, mae: 846.296997, mean_q: -4.279296
 906/1000: episode: 906, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -9048.894, mean reward: -9048.894 [-9048.894, -9048.894], mean action: 3.000 [3.000, 3.000],  loss: 7485678.000000, mae: 739.502686, mean_q: -4.290660
 907/1000: episode: 907, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -4220.325, mean reward: -4220.325 [-4220.325, -4220.325], mean action: 3.000 [3.000, 3.000],  loss: 11352379.000000, mae: 908.841187, mean_q: -4.307948
 908/1000: episode: 908, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -4936.018, mean reward: -4936.018 [-4936.018, -4936.018], mean action: 3.000 [3.000, 3.000],  loss: 14431985.000000, mae: 1061.951660, mean_q: -4.331956
 909/1000: episode: 909, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -6087.741, mean reward: -6087.741 [-6087.741, -6087.741], mean action: 1.000 [1.000, 1.000],  loss: 7579834.000000, mae: 809.420837, mean_q: -4.370361
 910/1000: episode: 910, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -8044.007, mean reward: -8044.007 [-8044.007, -8044.007], mean action: 3.000 [3.000, 3.000],  loss: 12682765.000000, mae: 1000.141235, mean_q: -4.381305
 911/1000: episode: 911, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -6622.304, mean reward: -6622.304 [-6622.304, -6622.304], mean action: 3.000 [3.000, 3.000],  loss: 11915753.000000, mae: 972.903259, mean_q: -4.398740
 912/1000: episode: 912, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -4552.282, mean reward: -4552.282 [-4552.282, -4552.282], mean action: 3.000 [3.000, 3.000],  loss: 7302265.000000, mae: 780.912292, mean_q: -4.410591
 913/1000: episode: 913, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5946.334, mean reward: -5946.334 [-5946.334, -5946.334], mean action: 3.000 [3.000, 3.000],  loss: 10142156.000000, mae: 870.236816, mean_q: -4.440417
 914/1000: episode: 914, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -1773.110, mean reward: -1773.110 [-1773.110, -1773.110], mean action: 3.000 [3.000, 3.000],  loss: 12302662.000000, mae: 947.602539, mean_q: -4.467275
 915/1000: episode: 915, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -1791.849, mean reward: -1791.849 [-1791.849, -1791.849], mean action: 3.000 [3.000, 3.000],  loss: 11179688.000000, mae: 969.232605, mean_q: -4.493093
 916/1000: episode: 916, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -1883.535, mean reward: -1883.535 [-1883.535, -1883.535], mean action: 3.000 [3.000, 3.000],  loss: 12081072.000000, mae: 919.566284, mean_q: -4.491141
 917/1000: episode: 917, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5107.720, mean reward: -5107.720 [-5107.720, -5107.720], mean action: 3.000 [3.000, 3.000],  loss: 9065951.000000, mae: 809.914673, mean_q: -4.540290
 918/1000: episode: 918, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -1645.792, mean reward: -1645.792 [-1645.792, -1645.792], mean action: 1.000 [1.000, 1.000],  loss: 13346334.000000, mae: 1031.507568, mean_q: -4.563247
 919/1000: episode: 919, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -4040.961, mean reward: -4040.961 [-4040.961, -4040.961], mean action: 0.000 [0.000, 0.000],  loss: 10052226.000000, mae: 935.846680, mean_q: -4.575808
 920/1000: episode: 920, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -9730.335, mean reward: -9730.335 [-9730.335, -9730.335], mean action: 0.000 [0.000, 0.000],  loss: 5945302.000000, mae: 707.574768, mean_q: -4.647137
 921/1000: episode: 921, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -6961.351, mean reward: -6961.351 [-6961.351, -6961.351], mean action: 0.000 [0.000, 0.000],  loss: 15587018.000000, mae: 1069.827637, mean_q: -4.633660
 922/1000: episode: 922, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -6800.376, mean reward: -6800.376 [-6800.376, -6800.376], mean action: 0.000 [0.000, 0.000],  loss: 19277538.000000, mae: 1302.903198, mean_q: -4.647793
 923/1000: episode: 923, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -2405.755, mean reward: -2405.755 [-2405.755, -2405.755], mean action: 0.000 [0.000, 0.000],  loss: 11047595.000000, mae: 896.573608, mean_q: -4.675196
 924/1000: episode: 924, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -7705.691, mean reward: -7705.691 [-7705.691, -7705.691], mean action: 1.000 [1.000, 1.000],  loss: 9344665.000000, mae: 912.719849, mean_q: -4.699283
 925/1000: episode: 925, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -10919.719, mean reward: -10919.719 [-10919.719, -10919.719], mean action: 0.000 [0.000, 0.000],  loss: 10657102.000000, mae: 878.655273, mean_q: -4.713738
 926/1000: episode: 926, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -10017.046, mean reward: -10017.046 [-10017.046, -10017.046], mean action: 0.000 [0.000, 0.000],  loss: 14368987.000000, mae: 1080.442139, mean_q: -4.740809
 927/1000: episode: 927, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -13931.000, mean reward: -13931.000 [-13931.000, -13931.000], mean action: 0.000 [0.000, 0.000],  loss: 11611090.000000, mae: 984.716553, mean_q: -4.762447
 928/1000: episode: 928, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -8102.959, mean reward: -8102.959 [-8102.959, -8102.959], mean action: 0.000 [0.000, 0.000],  loss: 9629750.000000, mae: 846.419617, mean_q: -4.802072
 929/1000: episode: 929, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -3804.250, mean reward: -3804.250 [-3804.250, -3804.250], mean action: 1.000 [1.000, 1.000],  loss: 12125892.000000, mae: 1044.187500, mean_q: -4.821083
 930/1000: episode: 930, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5089.177, mean reward: -5089.177 [-5089.177, -5089.177], mean action: 0.000 [0.000, 0.000],  loss: 12973674.000000, mae: 1147.701294, mean_q: -4.822614
 931/1000: episode: 931, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -4803.553, mean reward: -4803.553 [-4803.553, -4803.553], mean action: 0.000 [0.000, 0.000],  loss: 9328928.000000, mae: 871.794678, mean_q: -4.862464
 932/1000: episode: 932, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -8568.983, mean reward: -8568.983 [-8568.983, -8568.983], mean action: 0.000 [0.000, 0.000],  loss: 14393778.000000, mae: 1162.285522, mean_q: -4.892139
 933/1000: episode: 933, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -12119.225, mean reward: -12119.225 [-12119.225, -12119.225], mean action: 0.000 [0.000, 0.000],  loss: 10984677.000000, mae: 989.027893, mean_q: -4.887218
 934/1000: episode: 934, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -6871.799, mean reward: -6871.799 [-6871.799, -6871.799], mean action: 0.000 [0.000, 0.000],  loss: 10894678.000000, mae: 977.562195, mean_q: -4.953848
 935/1000: episode: 935, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5049.235, mean reward: -5049.235 [-5049.235, -5049.235], mean action: 0.000 [0.000, 0.000],  loss: 17112552.000000, mae: 1187.828369, mean_q: -4.968044
 936/1000: episode: 936, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5400.872, mean reward: -5400.872 [-5400.872, -5400.872], mean action: 0.000 [0.000, 0.000],  loss: 7070196.000000, mae: 791.610229, mean_q: -4.986889
 937/1000: episode: 937, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -9265.125, mean reward: -9265.125 [-9265.125, -9265.125], mean action: 0.000 [0.000, 0.000],  loss: 9440698.000000, mae: 861.111145, mean_q: -4.974274
 938/1000: episode: 938, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -6175.043, mean reward: -6175.043 [-6175.043, -6175.043], mean action: 0.000 [0.000, 0.000],  loss: 6203450.000000, mae: 683.072510, mean_q: -5.020960
 939/1000: episode: 939, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -8801.327, mean reward: -8801.327 [-8801.327, -8801.327], mean action: 0.000 [0.000, 0.000],  loss: 8193551.000000, mae: 789.839783, mean_q: -5.028515
 940/1000: episode: 940, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -3857.253, mean reward: -3857.253 [-3857.253, -3857.253], mean action: 0.000 [0.000, 0.000],  loss: 7548086.000000, mae: 798.795715, mean_q: -5.064340
 941/1000: episode: 941, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -10106.892, mean reward: -10106.892 [-10106.892, -10106.892], mean action: 0.000 [0.000, 0.000],  loss: 13786893.000000, mae: 1063.241943, mean_q: -5.073899
 942/1000: episode: 942, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -11853.649, mean reward: -11853.649 [-11853.649, -11853.649], mean action: 0.000 [0.000, 0.000],  loss: 15741145.000000, mae: 1140.558838, mean_q: -5.098070
 943/1000: episode: 943, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -10967.756, mean reward: -10967.756 [-10967.756, -10967.756], mean action: 0.000 [0.000, 0.000],  loss: 8783764.000000, mae: 870.798523, mean_q: -5.114862
 944/1000: episode: 944, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -3958.592, mean reward: -3958.592 [-3958.592, -3958.592], mean action: 0.000 [0.000, 0.000],  loss: 9574544.000000, mae: 831.989319, mean_q: -5.138181
 945/1000: episode: 945, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -13763.090, mean reward: -13763.090 [-13763.090, -13763.090], mean action: 0.000 [0.000, 0.000],  loss: 6741717.000000, mae: 791.869995, mean_q: -5.157098
 946/1000: episode: 946, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -4053.690, mean reward: -4053.690 [-4053.690, -4053.690], mean action: 0.000 [0.000, 0.000],  loss: 8507577.000000, mae: 886.415344, mean_q: -5.151696
 947/1000: episode: 947, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -4888.314, mean reward: -4888.314 [-4888.314, -4888.314], mean action: 0.000 [0.000, 0.000],  loss: 8143440.000000, mae: 827.079041, mean_q: -5.195723
 948/1000: episode: 948, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5963.445, mean reward: -5963.445 [-5963.445, -5963.445], mean action: 1.000 [1.000, 1.000],  loss: 15734440.000000, mae: 1107.537598, mean_q: -5.178410
 949/1000: episode: 949, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -11657.488, mean reward: -11657.488 [-11657.488, -11657.488], mean action: 0.000 [0.000, 0.000],  loss: 14025124.000000, mae: 1076.586426, mean_q: -5.220002
 950/1000: episode: 950, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -1975.726, mean reward: -1975.726 [-1975.726, -1975.726], mean action: 0.000 [0.000, 0.000],  loss: 9791902.000000, mae: 940.305542, mean_q: -5.258090
 951/1000: episode: 951, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -15986.020, mean reward: -15986.020 [-15986.020, -15986.020], mean action: 0.000 [0.000, 0.000],  loss: 7946669.000000, mae: 798.883362, mean_q: -5.290312
 952/1000: episode: 952, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -8514.566, mean reward: -8514.566 [-8514.566, -8514.566], mean action: 0.000 [0.000, 0.000],  loss: 11851092.000000, mae: 891.765625, mean_q: -5.306747
 953/1000: episode: 953, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -8248.433, mean reward: -8248.433 [-8248.433, -8248.433], mean action: 0.000 [0.000, 0.000],  loss: 12322534.000000, mae: 1042.182373, mean_q: -5.330987
 954/1000: episode: 954, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5615.093, mean reward: -5615.093 [-5615.093, -5615.093], mean action: 0.000 [0.000, 0.000],  loss: 6794347.000000, mae: 699.102173, mean_q: -5.366874
 955/1000: episode: 955, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -7861.662, mean reward: -7861.662 [-7861.662, -7861.662], mean action: 1.000 [1.000, 1.000],  loss: 13291840.000000, mae: 1085.273193, mean_q: -5.386161
 956/1000: episode: 956, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -7891.448, mean reward: -7891.448 [-7891.448, -7891.448], mean action: 0.000 [0.000, 0.000],  loss: 13208602.000000, mae: 1065.516235, mean_q: -5.376288
 957/1000: episode: 957, duration: 0.177s, episode steps:   1, steps per second:   6, episode reward: -6998.564, mean reward: -6998.564 [-6998.564, -6998.564], mean action: 0.000 [0.000, 0.000],  loss: 15329298.000000, mae: 1080.591553, mean_q: -5.388171
 958/1000: episode: 958, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -11940.104, mean reward: -11940.104 [-11940.104, -11940.104], mean action: 0.000 [0.000, 0.000],  loss: 14508883.000000, mae: 953.388916, mean_q: -5.437800
 959/1000: episode: 959, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -7445.777, mean reward: -7445.777 [-7445.777, -7445.777], mean action: 0.000 [0.000, 0.000],  loss: 16893250.000000, mae: 1211.425659, mean_q: -5.450361
 960/1000: episode: 960, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -3976.267, mean reward: -3976.267 [-3976.267, -3976.267], mean action: 0.000 [0.000, 0.000],  loss: 12303279.000000, mae: 1003.330322, mean_q: -5.481650
 961/1000: episode: 961, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5379.931, mean reward: -5379.931 [-5379.931, -5379.931], mean action: 0.000 [0.000, 0.000],  loss: 13180728.000000, mae: 962.196411, mean_q: -5.521157
 962/1000: episode: 962, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -9124.640, mean reward: -9124.640 [-9124.640, -9124.640], mean action: 0.000 [0.000, 0.000],  loss: 13432393.000000, mae: 1120.140137, mean_q: -5.555154
 963/1000: episode: 963, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -4499.110, mean reward: -4499.110 [-4499.110, -4499.110], mean action: 0.000 [0.000, 0.000],  loss: 19594272.000000, mae: 1305.595337, mean_q: -5.562297
 964/1000: episode: 964, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -7782.167, mean reward: -7782.167 [-7782.167, -7782.167], mean action: 0.000 [0.000, 0.000],  loss: 13628554.000000, mae: 1043.785278, mean_q: -5.584843
 965/1000: episode: 965, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5895.222, mean reward: -5895.222 [-5895.222, -5895.222], mean action: 0.000 [0.000, 0.000],  loss: 10413988.000000, mae: 948.457886, mean_q: -5.626288
 966/1000: episode: 966, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -7201.214, mean reward: -7201.214 [-7201.214, -7201.214], mean action: 0.000 [0.000, 0.000],  loss: 12290604.000000, mae: 1059.967651, mean_q: -5.646808
 967/1000: episode: 967, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -7048.219, mean reward: -7048.219 [-7048.219, -7048.219], mean action: 0.000 [0.000, 0.000],  loss: 6042790.000000, mae: 706.203979, mean_q: -5.694892
 968/1000: episode: 968, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -11385.289, mean reward: -11385.289 [-11385.289, -11385.289], mean action: 0.000 [0.000, 0.000],  loss: 11875664.000000, mae: 937.490051, mean_q: -5.708556
 969/1000: episode: 969, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -3308.399, mean reward: -3308.399 [-3308.399, -3308.399], mean action: 0.000 [0.000, 0.000],  loss: 9115448.000000, mae: 904.492126, mean_q: -5.735409
 970/1000: episode: 970, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -9335.592, mean reward: -9335.592 [-9335.592, -9335.592], mean action: 0.000 [0.000, 0.000],  loss: 13193225.000000, mae: 1015.195496, mean_q: -5.760794
 971/1000: episode: 971, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -3974.662, mean reward: -3974.662 [-3974.662, -3974.662], mean action: 0.000 [0.000, 0.000],  loss: 13630945.000000, mae: 1071.657471, mean_q: -5.771381
 972/1000: episode: 972, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -6286.602, mean reward: -6286.602 [-6286.602, -6286.602], mean action: 0.000 [0.000, 0.000],  loss: 11730152.000000, mae: 965.749146, mean_q: -5.817664
 973/1000: episode: 973, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4534.190, mean reward: -4534.190 [-4534.190, -4534.190], mean action: 0.000 [0.000, 0.000],  loss: 19271604.000000, mae: 1273.343506, mean_q: -5.849646
 974/1000: episode: 974, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -7547.566, mean reward: -7547.566 [-7547.566, -7547.566], mean action: 0.000 [0.000, 0.000],  loss: 12127305.000000, mae: 863.991394, mean_q: -5.881574
 975/1000: episode: 975, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -578.015, mean reward: -578.015 [-578.015, -578.015], mean action: 2.000 [2.000, 2.000],  loss: 12429320.000000, mae: 986.969360, mean_q: -5.945090
 976/1000: episode: 976, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -7610.242, mean reward: -7610.242 [-7610.242, -7610.242], mean action: 1.000 [1.000, 1.000],  loss: 11608253.000000, mae: 981.905212, mean_q: -5.969679
 977/1000: episode: 977, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -2302.319, mean reward: -2302.319 [-2302.319, -2302.319], mean action: 0.000 [0.000, 0.000],  loss: 10799533.000000, mae: 1005.514465, mean_q: -5.971316
 978/1000: episode: 978, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -10160.652, mean reward: -10160.652 [-10160.652, -10160.652], mean action: 0.000 [0.000, 0.000],  loss: 7055145.500000, mae: 749.928467, mean_q: -6.028556
 979/1000: episode: 979, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -1881.015, mean reward: -1881.015 [-1881.015, -1881.015], mean action: 0.000 [0.000, 0.000],  loss: 21031564.000000, mae: 1267.908813, mean_q: -6.006110
 980/1000: episode: 980, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -6224.698, mean reward: -6224.698 [-6224.698, -6224.698], mean action: 0.000 [0.000, 0.000],  loss: 7974510.500000, mae: 784.207642, mean_q: -6.047078
 981/1000: episode: 981, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -9014.381, mean reward: -9014.381 [-9014.381, -9014.381], mean action: 0.000 [0.000, 0.000],  loss: 11516028.000000, mae: 970.507812, mean_q: -6.092371
 982/1000: episode: 982, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -7050.985, mean reward: -7050.985 [-7050.985, -7050.985], mean action: 1.000 [1.000, 1.000],  loss: 9033299.000000, mae: 864.961914, mean_q: -6.102764
 983/1000: episode: 983, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -6320.176, mean reward: -6320.176 [-6320.176, -6320.176], mean action: 0.000 [0.000, 0.000],  loss: 8905450.000000, mae: 817.675537, mean_q: -6.115406
 984/1000: episode: 984, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -11294.546, mean reward: -11294.546 [-11294.546, -11294.546], mean action: 0.000 [0.000, 0.000],  loss: 11422544.000000, mae: 860.778381, mean_q: -6.131150
 985/1000: episode: 985, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -4443.488, mean reward: -4443.488 [-4443.488, -4443.488], mean action: 1.000 [1.000, 1.000],  loss: 11449317.000000, mae: 969.872803, mean_q: -6.180382
 986/1000: episode: 986, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -11118.194, mean reward: -11118.194 [-11118.194, -11118.194], mean action: 0.000 [0.000, 0.000],  loss: 14105914.000000, mae: 1083.744141, mean_q: -6.198715
 987/1000: episode: 987, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -3037.172, mean reward: -3037.172 [-3037.172, -3037.172], mean action: 1.000 [1.000, 1.000],  loss: 11993502.000000, mae: 887.392944, mean_q: -6.240399
 988/1000: episode: 988, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -13969.454, mean reward: -13969.454 [-13969.454, -13969.454], mean action: 0.000 [0.000, 0.000],  loss: 12754541.000000, mae: 1046.250000, mean_q: -6.268584
 989/1000: episode: 989, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -2990.440, mean reward: -2990.440 [-2990.440, -2990.440], mean action: 1.000 [1.000, 1.000],  loss: 21364532.000000, mae: 1295.738159, mean_q: -6.266103
 990/1000: episode: 990, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -1973.011, mean reward: -1973.011 [-1973.011, -1973.011], mean action: 1.000 [1.000, 1.000],  loss: 16656076.000000, mae: 1171.498413, mean_q: -6.267784
 991/1000: episode: 991, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5338.227, mean reward: -5338.227 [-5338.227, -5338.227], mean action: 1.000 [1.000, 1.000],  loss: 12577428.000000, mae: 1049.421631, mean_q: -6.276306
 992/1000: episode: 992, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -3146.190, mean reward: -3146.190 [-3146.190, -3146.190], mean action: 1.000 [1.000, 1.000],  loss: 13187528.000000, mae: 1020.036133, mean_q: -6.325590
 993/1000: episode: 993, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5459.285, mean reward: -5459.285 [-5459.285, -5459.285], mean action: 1.000 [1.000, 1.000],  loss: 12554462.000000, mae: 1003.592896, mean_q: -6.342090
 994/1000: episode: 994, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -10494.366, mean reward: -10494.366 [-10494.366, -10494.366], mean action: 1.000 [1.000, 1.000],  loss: 10469088.000000, mae: 894.988464, mean_q: -6.378394
 995/1000: episode: 995, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -6051.096, mean reward: -6051.096 [-6051.096, -6051.096], mean action: 1.000 [1.000, 1.000],  loss: 11750120.000000, mae: 940.620605, mean_q: -6.366230
 996/1000: episode: 996, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -4193.209, mean reward: -4193.209 [-4193.209, -4193.209], mean action: 1.000 [1.000, 1.000],  loss: 7949541.000000, mae: 813.299683, mean_q: -6.461045
 997/1000: episode: 997, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5238.955, mean reward: -5238.955 [-5238.955, -5238.955], mean action: 1.000 [1.000, 1.000],  loss: 8527918.000000, mae: 844.181519, mean_q: -6.481398
 998/1000: episode: 998, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -984.672, mean reward: -984.672 [-984.672, -984.672], mean action: 1.000 [1.000, 1.000],  loss: 10492265.000000, mae: 864.810669, mean_q: -6.513732
 999/1000: episode: 999, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -2166.431, mean reward: -2166.431 [-2166.431, -2166.431], mean action: 1.000 [1.000, 1.000],  loss: 12336788.000000, mae: 975.269531, mean_q: -6.520379
 1000/1000: episode: 1000, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -790.893, mean reward: -790.893 [-790.893, -790.893], mean action: 1.000 [1.000, 1.000],  loss: 10828535.000000, mae: 855.639587, mean_q: -6.569703
done, took 60.358 seconds
